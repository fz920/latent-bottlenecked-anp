{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b87ad1-0844-419e-a8bb-9f8707b1c4b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train_mod(modelo, name,val_seed=0, val_l=None,val_kernel='rbf'):\n",
    "\n",
    "    #track time and memory\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    initial_memory = torch.cuda.max_memory_allocated()\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)        \n",
    "    starter.record()\n",
    "    \n",
    "    if modelo=='lbanp' or modelo=='isanp' or modelo=='isanp2':\n",
    "        %run gp.py --mode train --expid {name} --model {modelo} --eval_kernel {val_kernel} --train_seed {val_seed} --eval_seed {val_seed} --num_latents {val_l}\n",
    "    else:\n",
    "        %run gp.py --mode train --expid {name} --model {modelo} --eval_kernel {val_kernel} --train_seed {val_seed} --eval_seed {val_seed} \n",
    "    \n",
    "    ender.record()\n",
    "    torch.cuda.synchronize()\n",
    "    curr_time = starter.elapsed_time(ender)\n",
    "    #print('Batches: ', len(eval_batches))\n",
    "    print(\"Execution time:\", curr_time, \"miliseconds\")\n",
    "    print(\"Execution time:\", curr_time/1000, \"seconds\")\n",
    "\n",
    "    #track time and memory\n",
    "    final_memory = torch.cuda.max_memory_allocated()\n",
    "    print(\"Initial Memory Usage:\", initial_memory / (1024 ** 2), \"MB\")\n",
    "    print(\"Final Memory Usage:\", final_memory / (1024 ** 2), \"MB\")\n",
    "    print(\"Memory Usage Change:\", (final_memory - initial_memory) / (1024 ** 2), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943254e1-01fd-4623-b5a0-e75e192611fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "generating evaluation sets...\n",
      "Generating Evaluation Sets with rbf kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:04<00:00, 690.87it/s]\n",
      "Experiment: lbanp-lbanp-num_latents-8\n",
      "Total number of parameters: 784834\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 200 lr 5.000e-04 [train_loss] tar_ll -0.7252 loss 0.7252 (7.637 secs)\n",
      "lbanp:lbanp-num_latents-8 step 400 lr 5.000e-04 [train_loss] tar_ll -0.6560 loss 0.6560 (8.977 secs)\n",
      "lbanp:lbanp-num_latents-8 step 600 lr 5.000e-04 [train_loss] tar_ll -0.5769 loss 0.5769 (8.376 secs)\n",
      "lbanp:lbanp-num_latents-8 step 800 lr 4.999e-04 [train_loss] tar_ll -0.5675 loss 0.5675 (8.035 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1000 lr 4.999e-04 [train_loss] tar_ll -0.4814 loss 0.4814 (7.399 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.4298 loss 0.4298 (8.416 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1400 lr 4.998e-04 [train_loss] tar_ll -0.2346 loss 0.2346 (7.836 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1600 lr 4.997e-04 [train_loss] tar_ll -0.2048 loss 0.2048 (8.345 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1800 lr 4.996e-04 [train_loss] tar_ll -0.1349 loss 0.1349 (8.732 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2000 lr 4.995e-04 [train_loss] tar_ll -0.0932 loss 0.0932 (9.193 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.0198 loss -0.0198 (8.618 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2400 lr 4.993e-04 [train_loss] tar_ll -0.0042 loss 0.0042 (8.383 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.0589 loss -0.0589 (9.273 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.1152 loss -0.1152 (8.444 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.1568 loss -0.1568 (8.508 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.1037 loss -0.1037 (8.072 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.1636 loss -0.1636 (8.109 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.1920 loss -0.1920 (7.281 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.2076 loss -0.2076 (9.168 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.2207 loss -0.2207 (7.403 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.2024 loss -0.2024 (7.661 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.2437 loss -0.2437 (8.055 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.2970 loss -0.2970 (8.762 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.2901 loss -0.2901 (8.526 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.2784 loss -0.2784 (8.939 secs)\n",
      "100%|##########| 3000/3000 [00:44<00:00, 67.81it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.2555 loss -0.2555 (44.246 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.2872 loss -0.2872 (8.837 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.2088 loss -0.2088 (9.238 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.2999 loss -0.2999 (8.824 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.2932 loss -0.2932 (8.632 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.3531 loss -0.3531 (8.537 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.2698 loss -0.2698 (8.527 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.2475 loss -0.2475 (8.575 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.3576 loss -0.3576 (9.121 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.3978 loss -0.3978 (9.042 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.4033 loss -0.4033 (8.004 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.3947 loss -0.3947 (8.142 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.4281 loss -0.4281 (8.667 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.4373 loss -0.4373 (8.701 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.4932 loss -0.4932 (7.887 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.2671 loss -0.2671 (8.255 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.2152 loss -0.2152 (8.448 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.3440 loss -0.3440 (8.145 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.4159 loss -0.4159 (7.469 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.4389 loss -0.4389 (7.970 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.4783 loss -0.4783 (7.539 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.3471 loss -0.3471 (8.961 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.4662 loss -0.4662 (8.499 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.4082 loss -0.4082 (8.116 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.3935 loss -0.3935 (7.639 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.4464 loss -0.4464 (7.983 secs)\n",
      "100%|##########| 3000/3000 [00:38<00:00, 77.75it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.4157 loss -0.4157 (38.588 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.4923 loss -0.4923 (8.078 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.4651 loss -0.4651 (7.574 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.5448 loss -0.5448 (7.692 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.5371 loss -0.5371 (7.950 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.4348 loss -0.4348 (8.034 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.4944 loss -0.4944 (8.213 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.5094 loss -0.5094 (8.584 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.4863 loss -0.4863 (7.703 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.5391 loss -0.5391 (7.729 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.5667 loss -0.5667 (7.567 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.4920 loss -0.4920 (8.426 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.5259 loss -0.5259 (8.979 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.4966 loss -0.4966 (8.649 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.5333 loss -0.5333 (9.061 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.5688 loss -0.5688 (8.318 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.5518 loss -0.5518 (8.590 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.5227 loss -0.5227 (9.891 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.4009 loss -0.4009 (9.064 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.4102 loss -0.4102 (8.168 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.4144 loss -0.4144 (8.314 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.4612 loss -0.4612 (8.613 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.6044 loss -0.6044 (7.909 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.5196 loss -0.5196 (7.938 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.5422 loss -0.5422 (8.687 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.5222 loss -0.5222 (8.533 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 72.36it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.6106 loss -0.6106 (41.459 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.5563 loss -0.5563 (8.312 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.5468 loss -0.5468 (8.359 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.5362 loss -0.5362 (8.036 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.5599 loss -0.5599 (8.139 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.5062 loss -0.5062 (8.320 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.5879 loss -0.5879 (8.602 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.5780 loss -0.5780 (7.919 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.6100 loss -0.6100 (8.452 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.6339 loss -0.6339 (8.540 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.5978 loss -0.5978 (8.521 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.5964 loss -0.5964 (8.451 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.5411 loss -0.5411 (8.070 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.6025 loss -0.6025 (8.594 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.6048 loss -0.6048 (8.325 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.6156 loss -0.6156 (7.764 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.5884 loss -0.5884 (7.538 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.7323 loss -0.7323 (7.945 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.6580 loss -0.6580 (8.159 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.5871 loss -0.5871 (8.104 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.6308 loss -0.6308 (8.018 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.6478 loss -0.6478 (8.125 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.6153 loss -0.6153 (8.048 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.5552 loss -0.5552 (9.044 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.6492 loss -0.6492 (7.936 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.5749 loss -0.5749 (8.768 secs)\n",
      "100%|##########| 3000/3000 [00:39<00:00, 75.31it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.6604 loss -0.6604 (39.839 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.5707 loss -0.5707 (7.892 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.6553 loss -0.6553 (8.366 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.6343 loss -0.6343 (8.025 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.6000 loss -0.6000 (8.551 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.6826 loss -0.6826 (8.197 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.5962 loss -0.5962 (8.452 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.6645 loss -0.6645 (8.630 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.6901 loss -0.6901 (8.090 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.7096 loss -0.7096 (8.401 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.6192 loss -0.6192 (7.780 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.6364 loss -0.6364 (9.084 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.6432 loss -0.6432 (8.955 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.5882 loss -0.5882 (8.110 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.6258 loss -0.6258 (8.740 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.6124 loss -0.6124 (8.178 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.7086 loss -0.7086 (8.752 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.6871 loss -0.6871 (8.237 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.6976 loss -0.6976 (8.173 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.6669 loss -0.6669 (7.947 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.6443 loss -0.6443 (8.487 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.5765 loss -0.5765 (8.474 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.6974 loss -0.6974 (8.802 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.6514 loss -0.6514 (7.989 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.6199 loss -0.6199 (8.245 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.6792 loss -0.6792 (7.340 secs)\n",
      "100%|##########| 3000/3000 [00:40<00:00, 73.93it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.6813 loss -0.6813 (40.583 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.5912 loss -0.5912 (11.850 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.7101 loss -0.7101 (10.114 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.6506 loss -0.6506 (14.594 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.6176 loss -0.6176 (16.427 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.6670 loss -0.6670 (12.894 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.6898 loss -0.6898 (9.856 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.6653 loss -0.6653 (9.616 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.6760 loss -0.6760 (9.466 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.5887 loss -0.5887 (9.188 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.7442 loss -0.7442 (10.573 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.7482 loss -0.7482 (11.906 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.7645 loss -0.7645 (9.501 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.7536 loss -0.7536 (10.069 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.6383 loss -0.6383 (9.920 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.6257 loss -0.6257 (9.688 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.6810 loss -0.6810 (9.964 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.6322 loss -0.6322 (9.682 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.7221 loss -0.7221 (9.827 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.6967 loss -0.6967 (9.632 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.7664 loss -0.7664 (10.646 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.7337 loss -0.7337 (10.283 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.7079 loss -0.7079 (9.691 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.7218 loss -0.7218 (11.105 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.7441 loss -0.7441 (10.537 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.7321 loss -0.7321 (10.369 secs)\n",
      "100%|##########| 3000/3000 [00:49<00:00, 60.51it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.7531 loss -0.7531 (49.580 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.7987 loss -0.7987 (10.243 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.7215 loss -0.7215 (10.379 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.8428 loss -0.8428 (9.814 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.6841 loss -0.6841 (9.903 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.7538 loss -0.7538 (10.905 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.8168 loss -0.8168 (10.243 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.7735 loss -0.7735 (10.155 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.6400 loss -0.6400 (10.194 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.6798 loss -0.6798 (9.975 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.8176 loss -0.8176 (10.017 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.7650 loss -0.7650 (10.130 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.8318 loss -0.8318 (10.106 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.7837 loss -0.7837 (10.056 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.7825 loss -0.7825 (10.014 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.7956 loss -0.7956 (9.931 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.8142 loss -0.8142 (10.263 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.8397 loss -0.8397 (9.973 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.8081 loss -0.8081 (9.914 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.7656 loss -0.7656 (10.117 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.7547 loss -0.7547 (9.862 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.8574 loss -0.8574 (9.983 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.8555 loss -0.8555 (9.736 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.8158 loss -0.8158 (10.434 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.8501 loss -0.8501 (10.345 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.8732 loss -0.8732 (9.916 secs)\n",
      "100%|##########| 3000/3000 [00:50<00:00, 59.33it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.7761 loss -0.7761 (50.570 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.8288 loss -0.8288 (10.061 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.7675 loss -0.7675 (10.182 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.7997 loss -0.7997 (9.970 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.7802 loss -0.7802 (10.587 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.8335 loss -0.8335 (10.260 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.8242 loss -0.8242 (10.072 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.8629 loss -0.8629 (9.889 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.7661 loss -0.7661 (10.068 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.8460 loss -0.8460 (9.868 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.8435 loss -0.8435 (9.920 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.8811 loss -0.8811 (10.014 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.7157 loss -0.7157 (9.898 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.7873 loss -0.7873 (10.030 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.7298 loss -0.7298 (9.990 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.8870 loss -0.8870 (9.907 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.8841 loss -0.8841 (9.958 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.8342 loss -0.8342 (9.911 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.8359 loss -0.8359 (10.066 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.7861 loss -0.7861 (9.932 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.8488 loss -0.8488 (9.913 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.8440 loss -0.8440 (10.097 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.8004 loss -0.8004 (9.925 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.7112 loss -0.7112 (9.884 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.7482 loss -0.7482 (9.731 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.6970 loss -0.6970 (10.544 secs)\n",
      "100%|##########| 3000/3000 [00:51<00:00, 58.19it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.8817 loss -0.8817 (51.561 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.8391 loss -0.8391 (10.011 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.8009 loss -0.8009 (10.371 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.7933 loss -0.7933 (10.380 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.6805 loss -0.6805 (9.897 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.8526 loss -0.8526 (11.073 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.7896 loss -0.7896 (10.439 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.8092 loss -0.8092 (9.939 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.9255 loss -0.9255 (9.928 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.8300 loss -0.8300 (9.904 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.9070 loss -0.9070 (9.993 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.8728 loss -0.8728 (10.147 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.9048 loss -0.9048 (9.854 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42600 lr 3.076e-04 [train_loss] tar_ll 0.8575 loss -0.8575 (10.586 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.8487 loss -0.8487 (10.243 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.8372 loss -0.8372 (9.623 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.9035 loss -0.9035 (11.092 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.8380 loss -0.8380 (10.009 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.8554 loss -0.8554 (9.858 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.9016 loss -0.9016 (11.150 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.8938 loss -0.8938 (10.009 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.9260 loss -0.9260 (10.222 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.9702 loss -0.9702 (10.237 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.8651 loss -0.8651 (9.995 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.9237 loss -0.9237 (10.402 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.9935 loss -0.9935 (10.370 secs)\n",
      "100%|##########| 3000/3000 [00:51<00:00, 58.55it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.9885 loss -0.9885 (51.238 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.9089 loss -0.9089 (10.606 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.9857 loss -0.9857 (9.957 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.8993 loss -0.8993 (10.278 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.9776 loss -0.9776 (10.170 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.9781 loss -0.9781 (10.162 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.9572 loss -0.9572 (10.519 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.9328 loss -0.9328 (9.928 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.8934 loss -0.8934 (10.187 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.8601 loss -0.8601 (10.331 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.9623 loss -0.9623 (10.023 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.9193 loss -0.9193 (11.556 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.9686 loss -0.9686 (10.217 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47600 lr 2.688e-04 [train_loss] tar_ll 0.8345 loss -0.8345 (9.961 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.9614 loss -0.9614 (10.066 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.9861 loss -0.9861 (10.024 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.9410 loss -0.9410 (9.817 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48400 lr 2.626e-04 [train_loss] tar_ll 0.9642 loss -0.9642 (10.060 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.8570 loss -0.8570 (9.823 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48800 lr 2.594e-04 [train_loss] tar_ll 1.0215 loss -1.0215 (9.947 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.9594 loss -0.9594 (10.248 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49200 lr 2.563e-04 [train_loss] tar_ll 0.9408 loss -0.9408 (9.931 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.9276 loss -0.9276 (10.018 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49600 lr 2.531e-04 [train_loss] tar_ll 0.9220 loss -0.9220 (10.003 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.9733 loss -0.9733 (9.879 secs)\n",
      "lbanp:lbanp-num_latents-8 step 50000 lr 2.500e-04 [train_loss] tar_ll 0.9169 loss -0.9169 (10.110 secs)\n",
      "100%|##########| 3000/3000 [00:50<00:00, 58.92it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.9863 loss -0.9863 (50.920 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 50200 lr 2.484e-04 [train_loss] tar_ll 0.9566 loss -0.9566 (11.157 secs)\n",
      "lbanp:lbanp-num_latents-8 step 50400 lr 2.469e-04 [train_loss] tar_ll 0.9080 loss -0.9080 (9.881 secs)\n",
      "lbanp:lbanp-num_latents-8 step 50600 lr 2.453e-04 [train_loss] tar_ll 0.9618 loss -0.9618 (10.733 secs)\n",
      "lbanp:lbanp-num_latents-8 step 50800 lr 2.437e-04 [train_loss] tar_ll 0.9405 loss -0.9405 (10.029 secs)\n",
      "lbanp:lbanp-num_latents-8 step 51000 lr 2.421e-04 [train_loss] tar_ll 0.9781 loss -0.9781 (9.952 secs)\n",
      "lbanp:lbanp-num_latents-8 step 51200 lr 2.406e-04 [train_loss] tar_ll 0.9565 loss -0.9565 (10.333 secs)\n",
      "lbanp:lbanp-num_latents-8 step 51400 lr 2.390e-04 [train_loss] tar_ll 0.9778 loss -0.9778 (10.084 secs)\n",
      "lbanp:lbanp-num_latents-8 step 51600 lr 2.374e-04 [train_loss] tar_ll 0.9596 loss -0.9596 (9.971 secs)\n",
      "lbanp:lbanp-num_latents-8 step 51800 lr 2.359e-04 [train_loss] tar_ll 0.8509 loss -0.8509 (9.877 secs)\n",
      "lbanp:lbanp-num_latents-8 step 52000 lr 2.343e-04 [train_loss] tar_ll 0.9811 loss -0.9811 (9.855 secs)\n",
      "lbanp:lbanp-num_latents-8 step 52200 lr 2.327e-04 [train_loss] tar_ll 0.9421 loss -0.9421 (9.976 secs)\n",
      "lbanp:lbanp-num_latents-8 step 52400 lr 2.312e-04 [train_loss] tar_ll 0.8881 loss -0.8881 (9.982 secs)\n",
      "lbanp:lbanp-num_latents-8 step 52600 lr 2.296e-04 [train_loss] tar_ll 0.9424 loss -0.9424 (9.834 secs)\n",
      "lbanp:lbanp-num_latents-8 step 52800 lr 2.280e-04 [train_loss] tar_ll 0.9181 loss -0.9181 (10.038 secs)\n",
      "lbanp:lbanp-num_latents-8 step 53000 lr 2.265e-04 [train_loss] tar_ll 0.9142 loss -0.9142 (10.050 secs)\n",
      "lbanp:lbanp-num_latents-8 step 53200 lr 2.249e-04 [train_loss] tar_ll 1.0167 loss -1.0167 (9.926 secs)\n",
      "lbanp:lbanp-num_latents-8 step 53400 lr 2.233e-04 [train_loss] tar_ll 0.9719 loss -0.9719 (10.006 secs)\n",
      "lbanp:lbanp-num_latents-8 step 53600 lr 2.218e-04 [train_loss] tar_ll 0.9738 loss -0.9738 (10.132 secs)\n",
      "lbanp:lbanp-num_latents-8 step 53800 lr 2.202e-04 [train_loss] tar_ll 0.9207 loss -0.9207 (10.041 secs)\n",
      "lbanp:lbanp-num_latents-8 step 54000 lr 2.187e-04 [train_loss] tar_ll 1.0353 loss -1.0353 (10.102 secs)\n",
      "lbanp:lbanp-num_latents-8 step 54200 lr 2.171e-04 [train_loss] tar_ll 0.9501 loss -0.9501 (9.948 secs)\n",
      "lbanp:lbanp-num_latents-8 step 54400 lr 2.156e-04 [train_loss] tar_ll 0.9855 loss -0.9855 (10.078 secs)\n",
      "lbanp:lbanp-num_latents-8 step 54600 lr 2.140e-04 [train_loss] tar_ll 0.9949 loss -0.9949 (9.902 secs)\n",
      "lbanp:lbanp-num_latents-8 step 54800 lr 2.124e-04 [train_loss] tar_ll 0.9942 loss -0.9942 (9.715 secs)\n",
      "lbanp:lbanp-num_latents-8 step 55000 lr 2.109e-04 [train_loss] tar_ll 0.9929 loss -0.9929 (9.883 secs)\n",
      "100%|##########| 3000/3000 [00:49<00:00, 60.14it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.0169 loss -1.0169 (49.887 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 55200 lr 2.093e-04 [train_loss] tar_ll 0.9199 loss -0.9199 (10.222 secs)\n",
      "lbanp:lbanp-num_latents-8 step 55400 lr 2.078e-04 [train_loss] tar_ll 1.0067 loss -1.0067 (10.068 secs)\n",
      "lbanp:lbanp-num_latents-8 step 55600 lr 2.062e-04 [train_loss] tar_ll 0.9893 loss -0.9893 (10.268 secs)\n",
      "lbanp:lbanp-num_latents-8 step 55800 lr 2.047e-04 [train_loss] tar_ll 1.0017 loss -1.0017 (10.782 secs)\n",
      "lbanp:lbanp-num_latents-8 step 56000 lr 2.032e-04 [train_loss] tar_ll 0.9280 loss -0.9280 (11.199 secs)\n",
      "lbanp:lbanp-num_latents-8 step 56200 lr 2.016e-04 [train_loss] tar_ll 0.9312 loss -0.9312 (10.450 secs)\n",
      "lbanp:lbanp-num_latents-8 step 56400 lr 2.001e-04 [train_loss] tar_ll 0.9667 loss -0.9667 (11.169 secs)\n",
      "lbanp:lbanp-num_latents-8 step 56600 lr 1.985e-04 [train_loss] tar_ll 0.9338 loss -0.9338 (9.917 secs)\n",
      "lbanp:lbanp-num_latents-8 step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0809 loss -1.0809 (9.992 secs)\n",
      "lbanp:lbanp-num_latents-8 step 57000 lr 1.955e-04 [train_loss] tar_ll 1.0568 loss -1.0568 (10.044 secs)\n",
      "lbanp:lbanp-num_latents-8 step 57200 lr 1.939e-04 [train_loss] tar_ll 0.9912 loss -0.9912 (10.041 secs)\n",
      "lbanp:lbanp-num_latents-8 step 57400 lr 1.924e-04 [train_loss] tar_ll 1.0320 loss -1.0320 (10.178 secs)\n",
      "lbanp:lbanp-num_latents-8 step 57600 lr 1.909e-04 [train_loss] tar_ll 0.9647 loss -0.9647 (10.002 secs)\n",
      "lbanp:lbanp-num_latents-8 step 57800 lr 1.894e-04 [train_loss] tar_ll 1.0457 loss -1.0457 (10.166 secs)\n",
      "lbanp:lbanp-num_latents-8 step 58000 lr 1.878e-04 [train_loss] tar_ll 0.9541 loss -0.9541 (9.937 secs)\n",
      "lbanp:lbanp-num_latents-8 step 58200 lr 1.863e-04 [train_loss] tar_ll 0.9988 loss -0.9988 (9.965 secs)\n",
      "lbanp:lbanp-num_latents-8 step 58400 lr 1.848e-04 [train_loss] tar_ll 1.0412 loss -1.0412 (9.956 secs)\n",
      "lbanp:lbanp-num_latents-8 step 58600 lr 1.833e-04 [train_loss] tar_ll 1.0075 loss -1.0075 (10.101 secs)\n",
      "lbanp:lbanp-num_latents-8 step 58800 lr 1.818e-04 [train_loss] tar_ll 1.0658 loss -1.0658 (9.894 secs)\n",
      "lbanp:lbanp-num_latents-8 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.0095 loss -1.0095 (9.914 secs)\n",
      "lbanp:lbanp-num_latents-8 step 59200 lr 1.787e-04 [train_loss] tar_ll 1.0704 loss -1.0704 (9.981 secs)\n",
      "lbanp:lbanp-num_latents-8 step 59400 lr 1.772e-04 [train_loss] tar_ll 1.0369 loss -1.0369 (9.858 secs)\n",
      "lbanp:lbanp-num_latents-8 step 59600 lr 1.757e-04 [train_loss] tar_ll 1.0203 loss -1.0203 (9.965 secs)\n",
      "lbanp:lbanp-num_latents-8 step 59800 lr 1.742e-04 [train_loss] tar_ll 1.0842 loss -1.0842 (9.756 secs)\n",
      "lbanp:lbanp-num_latents-8 step 60000 lr 1.727e-04 [train_loss] tar_ll 1.0616 loss -1.0616 (10.390 secs)\n",
      "100%|##########| 3000/3000 [00:49<00:00, 60.07it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.0519 loss -1.0519 (49.941 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 60200 lr 1.713e-04 [train_loss] tar_ll 1.0724 loss -1.0724 (8.041 secs)\n",
      "lbanp:lbanp-num_latents-8 step 60400 lr 1.698e-04 [train_loss] tar_ll 1.0899 loss -1.0899 (7.822 secs)\n",
      "lbanp:lbanp-num_latents-8 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.0894 loss -1.0894 (8.195 secs)\n",
      "lbanp:lbanp-num_latents-8 step 60800 lr 1.668e-04 [train_loss] tar_ll 0.9710 loss -0.9710 (8.514 secs)\n",
      "lbanp:lbanp-num_latents-8 step 61000 lr 1.653e-04 [train_loss] tar_ll 0.9997 loss -0.9997 (7.952 secs)\n",
      "lbanp:lbanp-num_latents-8 step 61200 lr 1.638e-04 [train_loss] tar_ll 1.0048 loss -1.0048 (7.455 secs)\n",
      "lbanp:lbanp-num_latents-8 step 61400 lr 1.624e-04 [train_loss] tar_ll 0.9933 loss -0.9933 (7.900 secs)\n",
      "lbanp:lbanp-num_latents-8 step 61600 lr 1.609e-04 [train_loss] tar_ll 1.0207 loss -1.0207 (9.059 secs)\n",
      "lbanp:lbanp-num_latents-8 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.0247 loss -1.0247 (8.007 secs)\n",
      "lbanp:lbanp-num_latents-8 step 62000 lr 1.580e-04 [train_loss] tar_ll 1.0660 loss -1.0660 (8.843 secs)\n",
      "lbanp:lbanp-num_latents-8 step 62200 lr 1.565e-04 [train_loss] tar_ll 0.9996 loss -0.9996 (8.394 secs)\n",
      "lbanp:lbanp-num_latents-8 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.0927 loss -1.0927 (8.453 secs)\n",
      "lbanp:lbanp-num_latents-8 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.0472 loss -1.0472 (7.954 secs)\n",
      "lbanp:lbanp-num_latents-8 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.0548 loss -1.0548 (7.559 secs)\n",
      "lbanp:lbanp-num_latents-8 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.0196 loss -1.0196 (7.826 secs)\n",
      "lbanp:lbanp-num_latents-8 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.0672 loss -1.0672 (7.630 secs)\n",
      "lbanp:lbanp-num_latents-8 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.0666 loss -1.0666 (8.019 secs)\n",
      "lbanp:lbanp-num_latents-8 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.0931 loss -1.0931 (7.519 secs)\n",
      "lbanp:lbanp-num_latents-8 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.0279 loss -1.0279 (7.800 secs)\n",
      "lbanp:lbanp-num_latents-8 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.1038 loss -1.1038 (8.018 secs)\n",
      "lbanp:lbanp-num_latents-8 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.0905 loss -1.0905 (7.989 secs)\n",
      "lbanp:lbanp-num_latents-8 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.1080 loss -1.1080 (7.864 secs)\n",
      "lbanp:lbanp-num_latents-8 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.0678 loss -1.0678 (7.942 secs)\n",
      "lbanp:lbanp-num_latents-8 step 64800 lr 1.379e-04 [train_loss] tar_ll 0.9806 loss -0.9806 (8.006 secs)\n",
      "lbanp:lbanp-num_latents-8 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.0613 loss -1.0613 (7.983 secs)\n",
      "100%|##########| 3000/3000 [00:38<00:00, 77.61it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.0577 loss -1.0577 (38.655 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.0752 loss -1.0752 (8.074 secs)\n",
      "lbanp:lbanp-num_latents-8 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.0843 loss -1.0843 (7.491 secs)\n",
      "lbanp:lbanp-num_latents-8 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.0516 loss -1.0516 (7.993 secs)\n",
      "lbanp:lbanp-num_latents-8 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.1446 loss -1.1446 (7.748 secs)\n",
      "lbanp:lbanp-num_latents-8 step 66000 lr 1.296e-04 [train_loss] tar_ll 0.9965 loss -0.9965 (7.916 secs)\n",
      "lbanp:lbanp-num_latents-8 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.0699 loss -1.0699 (7.623 secs)\n",
      "lbanp:lbanp-num_latents-8 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.0691 loss -1.0691 (7.807 secs)\n",
      "lbanp:lbanp-num_latents-8 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.0947 loss -1.0947 (7.593 secs)\n",
      "lbanp:lbanp-num_latents-8 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.0253 loss -1.0253 (7.671 secs)\n",
      "lbanp:lbanp-num_latents-8 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.0727 loss -1.0727 (7.721 secs)\n",
      "lbanp:lbanp-num_latents-8 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.0834 loss -1.0834 (7.633 secs)\n",
      "lbanp:lbanp-num_latents-8 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.1294 loss -1.1294 (7.756 secs)\n",
      "lbanp:lbanp-num_latents-8 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.0763 loss -1.0763 (7.520 secs)\n",
      "lbanp:lbanp-num_latents-8 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.0754 loss -1.0754 (7.778 secs)\n",
      "lbanp:lbanp-num_latents-8 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.1329 loss -1.1329 (7.384 secs)\n",
      "lbanp:lbanp-num_latents-8 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.0898 loss -1.0898 (7.675 secs)\n",
      "lbanp:lbanp-num_latents-8 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.1029 loss -1.1029 (7.357 secs)\n",
      "lbanp:lbanp-num_latents-8 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.2157 loss -1.2157 (7.794 secs)\n",
      "lbanp:lbanp-num_latents-8 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.1119 loss -1.1119 (7.535 secs)\n",
      "lbanp:lbanp-num_latents-8 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.0706 loss -1.0706 (7.819 secs)\n",
      "lbanp:lbanp-num_latents-8 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.1717 loss -1.1717 (8.072 secs)\n",
      "lbanp:lbanp-num_latents-8 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.0252 loss -1.0252 (7.855 secs)\n",
      "lbanp:lbanp-num_latents-8 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.0618 loss -1.0618 (6.865 secs)\n",
      "lbanp:lbanp-num_latents-8 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.1570 loss -1.1570 (7.574 secs)\n",
      "lbanp:lbanp-num_latents-8 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.0694 loss -1.0694 (7.180 secs)\n",
      "100%|##########| 3000/3000 [00:37<00:00, 80.55it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.1335 loss -1.1335 (37.248 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.0743 loss -1.0743 (7.836 secs)\n",
      "lbanp:lbanp-num_latents-8 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.0630 loss -1.0630 (7.191 secs)\n",
      "lbanp:lbanp-num_latents-8 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.1629 loss -1.1629 (7.427 secs)\n",
      "lbanp:lbanp-num_latents-8 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.1871 loss -1.1871 (7.039 secs)\n",
      "lbanp:lbanp-num_latents-8 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.1004 loss -1.1004 (7.142 secs)\n",
      "lbanp:lbanp-num_latents-8 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.1638 loss -1.1638 (7.230 secs)\n",
      "lbanp:lbanp-num_latents-8 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.1092 loss -1.1092 (7.022 secs)\n",
      "lbanp:lbanp-num_latents-8 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.1467 loss -1.1467 (6.631 secs)\n",
      "lbanp:lbanp-num_latents-8 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.0557 loss -1.0557 (6.905 secs)\n",
      "lbanp:lbanp-num_latents-8 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.0934 loss -1.0934 (6.827 secs)\n",
      "lbanp:lbanp-num_latents-8 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.1592 loss -1.1592 (6.634 secs)\n",
      "lbanp:lbanp-num_latents-8 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.0853 loss -1.0853 (8.257 secs)\n",
      "lbanp:lbanp-num_latents-8 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.1199 loss -1.1199 (8.904 secs)\n",
      "lbanp:lbanp-num_latents-8 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.2082 loss -1.2082 (9.312 secs)\n",
      "lbanp:lbanp-num_latents-8 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.0639 loss -1.0639 (8.665 secs)\n",
      "lbanp:lbanp-num_latents-8 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.1186 loss -1.1186 (7.314 secs)\n",
      "lbanp:lbanp-num_latents-8 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.1798 loss -1.1798 (7.152 secs)\n",
      "lbanp:lbanp-num_latents-8 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.2067 loss -1.2067 (7.459 secs)\n",
      "lbanp:lbanp-num_latents-8 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.1174 loss -1.1174 (7.408 secs)\n",
      "lbanp:lbanp-num_latents-8 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.0866 loss -1.0866 (7.104 secs)\n",
      "lbanp:lbanp-num_latents-8 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.0780 loss -1.0780 (7.453 secs)\n",
      "lbanp:lbanp-num_latents-8 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.2121 loss -1.2121 (7.241 secs)\n",
      "lbanp:lbanp-num_latents-8 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.1788 loss -1.1788 (7.503 secs)\n",
      "lbanp:lbanp-num_latents-8 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.0778 loss -1.0778 (7.175 secs)\n",
      "lbanp:lbanp-num_latents-8 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1340 loss -1.1340 (7.575 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.61it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.1394 loss -1.1394 (34.640 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1747 loss -1.1747 (7.199 secs)\n",
      "lbanp:lbanp-num_latents-8 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.0991 loss -1.0991 (6.655 secs)\n",
      "lbanp:lbanp-num_latents-8 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.0317 loss -1.0317 (6.878 secs)\n",
      "lbanp:lbanp-num_latents-8 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.1531 loss -1.1531 (6.591 secs)\n",
      "lbanp:lbanp-num_latents-8 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.1338 loss -1.1338 (6.574 secs)\n",
      "lbanp:lbanp-num_latents-8 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.1535 loss -1.1535 (9.249 secs)\n",
      "lbanp:lbanp-num_latents-8 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.2183 loss -1.2183 (13.238 secs)\n",
      "lbanp:lbanp-num_latents-8 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.1849 loss -1.1849 (10.111 secs)\n",
      "lbanp:lbanp-num_latents-8 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.1096 loss -1.1096 (9.847 secs)\n",
      "lbanp:lbanp-num_latents-8 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.1507 loss -1.1507 (10.771 secs)\n",
      "lbanp:lbanp-num_latents-8 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.1559 loss -1.1559 (11.258 secs)\n",
      "lbanp:lbanp-num_latents-8 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.1600 loss -1.1600 (9.850 secs)\n",
      "lbanp:lbanp-num_latents-8 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.1065 loss -1.1065 (9.627 secs)\n",
      "lbanp:lbanp-num_latents-8 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2033 loss -1.2033 (9.524 secs)\n",
      "lbanp:lbanp-num_latents-8 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.2219 loss -1.2219 (9.539 secs)\n",
      "lbanp:lbanp-num_latents-8 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.1206 loss -1.1206 (8.862 secs)\n",
      "lbanp:lbanp-num_latents-8 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.0973 loss -1.0973 (8.039 secs)\n",
      "lbanp:lbanp-num_latents-8 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.1758 loss -1.1758 (9.374 secs)\n",
      "lbanp:lbanp-num_latents-8 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.1223 loss -1.1223 (8.195 secs)\n",
      "lbanp:lbanp-num_latents-8 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.1953 loss -1.1953 (8.477 secs)\n",
      "lbanp:lbanp-num_latents-8 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.1937 loss -1.1937 (9.056 secs)\n",
      "lbanp:lbanp-num_latents-8 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.2304 loss -1.2304 (7.845 secs)\n",
      "lbanp:lbanp-num_latents-8 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.0784 loss -1.0784 (7.788 secs)\n",
      "lbanp:lbanp-num_latents-8 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.2689 loss -1.2689 (8.624 secs)\n",
      "lbanp:lbanp-num_latents-8 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.1588 loss -1.1588 (8.950 secs)\n",
      "100%|##########| 3000/3000 [00:38<00:00, 77.19it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.1775 loss -1.1775 (38.866 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.1642 loss -1.1642 (9.394 secs)\n",
      "lbanp:lbanp-num_latents-8 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.2435 loss -1.2435 (9.000 secs)\n",
      "lbanp:lbanp-num_latents-8 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.0829 loss -1.0829 (7.758 secs)\n",
      "lbanp:lbanp-num_latents-8 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2099 loss -1.2099 (8.050 secs)\n",
      "lbanp:lbanp-num_latents-8 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.1015 loss -1.1015 (8.124 secs)\n",
      "lbanp:lbanp-num_latents-8 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.1952 loss -1.1952 (8.619 secs)\n",
      "lbanp:lbanp-num_latents-8 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.0884 loss -1.0884 (8.252 secs)\n",
      "lbanp:lbanp-num_latents-8 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.1952 loss -1.1952 (8.491 secs)\n",
      "lbanp:lbanp-num_latents-8 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.1199 loss -1.1199 (8.375 secs)\n",
      "lbanp:lbanp-num_latents-8 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.1791 loss -1.1791 (9.924 secs)\n",
      "lbanp:lbanp-num_latents-8 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.2792 loss -1.2792 (10.137 secs)\n",
      "lbanp:lbanp-num_latents-8 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.1401 loss -1.1401 (9.100 secs)\n",
      "lbanp:lbanp-num_latents-8 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.1050 loss -1.1050 (8.815 secs)\n",
      "lbanp:lbanp-num_latents-8 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.2230 loss -1.2230 (8.442 secs)\n",
      "lbanp:lbanp-num_latents-8 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.1870 loss -1.1870 (8.507 secs)\n",
      "lbanp:lbanp-num_latents-8 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.2214 loss -1.2214 (8.287 secs)\n",
      "lbanp:lbanp-num_latents-8 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.1488 loss -1.1488 (8.592 secs)\n",
      "lbanp:lbanp-num_latents-8 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.1267 loss -1.1267 (8.363 secs)\n",
      "lbanp:lbanp-num_latents-8 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.2507 loss -1.2507 (8.715 secs)\n",
      "lbanp:lbanp-num_latents-8 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.2336 loss -1.2336 (8.506 secs)\n",
      "lbanp:lbanp-num_latents-8 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.0746 loss -1.0746 (8.113 secs)\n",
      "lbanp:lbanp-num_latents-8 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.2476 loss -1.2476 (7.983 secs)\n",
      "lbanp:lbanp-num_latents-8 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.1874 loss -1.1874 (7.767 secs)\n",
      "lbanp:lbanp-num_latents-8 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.1984 loss -1.1984 (9.224 secs)\n",
      "lbanp:lbanp-num_latents-8 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.1945 loss -1.1945 (8.658 secs)\n",
      "100%|##########| 3000/3000 [00:38<00:00, 77.58it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.1972 loss -1.1972 (38.673 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.2491 loss -1.2491 (8.507 secs)\n",
      "lbanp:lbanp-num_latents-8 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.1986 loss -1.1986 (9.828 secs)\n",
      "lbanp:lbanp-num_latents-8 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.2180 loss -1.2180 (9.322 secs)\n",
      "lbanp:lbanp-num_latents-8 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.2170 loss -1.2170 (9.512 secs)\n",
      "lbanp:lbanp-num_latents-8 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.2742 loss -1.2742 (9.316 secs)\n",
      "lbanp:lbanp-num_latents-8 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.1872 loss -1.1872 (9.519 secs)\n",
      "lbanp:lbanp-num_latents-8 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.1854 loss -1.1854 (9.485 secs)\n",
      "lbanp:lbanp-num_latents-8 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.2641 loss -1.2641 (9.171 secs)\n",
      "lbanp:lbanp-num_latents-8 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.1924 loss -1.1924 (9.510 secs)\n",
      "lbanp:lbanp-num_latents-8 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.1835 loss -1.1835 (9.251 secs)\n",
      "lbanp:lbanp-num_latents-8 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.1809 loss -1.1809 (9.289 secs)\n",
      "lbanp:lbanp-num_latents-8 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.2042 loss -1.2042 (9.237 secs)\n",
      "lbanp:lbanp-num_latents-8 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.2466 loss -1.2466 (8.988 secs)\n",
      "lbanp:lbanp-num_latents-8 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.1602 loss -1.1602 (9.380 secs)\n",
      "lbanp:lbanp-num_latents-8 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.2272 loss -1.2272 (9.202 secs)\n",
      "lbanp:lbanp-num_latents-8 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.1986 loss -1.1986 (9.657 secs)\n",
      "lbanp:lbanp-num_latents-8 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.1784 loss -1.1784 (9.218 secs)\n",
      "lbanp:lbanp-num_latents-8 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.1908 loss -1.1908 (9.387 secs)\n",
      "lbanp:lbanp-num_latents-8 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.1249 loss -1.1249 (9.176 secs)\n",
      "lbanp:lbanp-num_latents-8 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.1901 loss -1.1901 (9.214 secs)\n",
      "lbanp:lbanp-num_latents-8 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.0995 loss -1.0995 (9.486 secs)\n",
      "lbanp:lbanp-num_latents-8 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.2103 loss -1.2103 (9.152 secs)\n",
      "lbanp:lbanp-num_latents-8 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.1617 loss -1.1617 (9.319 secs)\n",
      "lbanp:lbanp-num_latents-8 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.1718 loss -1.1718 (9.072 secs)\n",
      "lbanp:lbanp-num_latents-8 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.2168 loss -1.2168 (8.836 secs)\n",
      "100%|##########| 3000/3000 [00:44<00:00, 67.85it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.2138 loss -1.2138 (44.218 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.1940 loss -1.1940 (10.655 secs)\n",
      "lbanp:lbanp-num_latents-8 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.1099 loss -1.1099 (9.451 secs)\n",
      "lbanp:lbanp-num_latents-8 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.1776 loss -1.1776 (9.441 secs)\n",
      "lbanp:lbanp-num_latents-8 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.2911 loss -1.2911 (9.223 secs)\n",
      "lbanp:lbanp-num_latents-8 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.1710 loss -1.1710 (9.801 secs)\n",
      "lbanp:lbanp-num_latents-8 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2093 loss -1.2093 (9.647 secs)\n",
      "lbanp:lbanp-num_latents-8 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2750 loss -1.2750 (9.114 secs)\n",
      "lbanp:lbanp-num_latents-8 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.1256 loss -1.1256 (9.431 secs)\n",
      "lbanp:lbanp-num_latents-8 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2405 loss -1.2405 (9.019 secs)\n",
      "lbanp:lbanp-num_latents-8 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2550 loss -1.2550 (9.480 secs)\n",
      "lbanp:lbanp-num_latents-8 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.2758 loss -1.2758 (9.402 secs)\n",
      "lbanp:lbanp-num_latents-8 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.1853 loss -1.1853 (9.588 secs)\n",
      "lbanp:lbanp-num_latents-8 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.2402 loss -1.2402 (9.643 secs)\n",
      "lbanp:lbanp-num_latents-8 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.1715 loss -1.1715 (9.571 secs)\n",
      "lbanp:lbanp-num_latents-8 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.2201 loss -1.2201 (9.416 secs)\n",
      "lbanp:lbanp-num_latents-8 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.2019 loss -1.2019 (9.396 secs)\n",
      "lbanp:lbanp-num_latents-8 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.2650 loss -1.2650 (9.455 secs)\n",
      "lbanp:lbanp-num_latents-8 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.1373 loss -1.1373 (9.323 secs)\n",
      "lbanp:lbanp-num_latents-8 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.2159 loss -1.2159 (9.122 secs)\n",
      "lbanp:lbanp-num_latents-8 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.0899 loss -1.0899 (9.359 secs)\n",
      "lbanp:lbanp-num_latents-8 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.2840 loss -1.2840 (9.193 secs)\n",
      "lbanp:lbanp-num_latents-8 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.1123 loss -1.1123 (9.362 secs)\n",
      "lbanp:lbanp-num_latents-8 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.2654 loss -1.2654 (9.413 secs)\n",
      "lbanp:lbanp-num_latents-8 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.2828 loss -1.2828 (9.179 secs)\n",
      "lbanp:lbanp-num_latents-8 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.1432 loss -1.1432 (9.369 secs)\n",
      "100%|##########| 3000/3000 [00:44<00:00, 67.30it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.2231 loss -1.2231 (44.578 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.1699 loss -1.1699 (9.630 secs)\n",
      "lbanp:lbanp-num_latents-8 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.2182 loss -1.2182 (9.063 secs)\n",
      "lbanp:lbanp-num_latents-8 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.2333 loss -1.2333 (8.913 secs)\n",
      "lbanp:lbanp-num_latents-8 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.2386 loss -1.2386 (9.107 secs)\n",
      "lbanp:lbanp-num_latents-8 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.2899 loss -1.2899 (8.887 secs)\n",
      "lbanp:lbanp-num_latents-8 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.1900 loss -1.1900 (9.407 secs)\n",
      "lbanp:lbanp-num_latents-8 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.2147 loss -1.2147 (8.944 secs)\n",
      "lbanp:lbanp-num_latents-8 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.2153 loss -1.2153 (9.083 secs)\n",
      "lbanp:lbanp-num_latents-8 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.2252 loss -1.2252 (9.117 secs)\n",
      "lbanp:lbanp-num_latents-8 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.2269 loss -1.2269 (9.123 secs)\n",
      "lbanp:lbanp-num_latents-8 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.1849 loss -1.1849 (9.539 secs)\n",
      "lbanp:lbanp-num_latents-8 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.1750 loss -1.1750 (9.286 secs)\n",
      "lbanp:lbanp-num_latents-8 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.2346 loss -1.2346 (9.240 secs)\n",
      "lbanp:lbanp-num_latents-8 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.2706 loss -1.2706 (9.325 secs)\n",
      "lbanp:lbanp-num_latents-8 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.1189 loss -1.1189 (9.227 secs)\n",
      "lbanp:lbanp-num_latents-8 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.1802 loss -1.1802 (10.469 secs)\n",
      "lbanp:lbanp-num_latents-8 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.2546 loss -1.2546 (9.154 secs)\n",
      "lbanp:lbanp-num_latents-8 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.2752 loss -1.2752 (9.449 secs)\n",
      "lbanp:lbanp-num_latents-8 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2612 loss -1.2612 (9.307 secs)\n",
      "lbanp:lbanp-num_latents-8 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.1327 loss -1.1327 (9.467 secs)\n",
      "lbanp:lbanp-num_latents-8 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.1760 loss -1.1760 (9.422 secs)\n",
      "lbanp:lbanp-num_latents-8 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2346 loss -1.2346 (9.145 secs)\n",
      "lbanp:lbanp-num_latents-8 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.2289 loss -1.2289 (9.491 secs)\n",
      "lbanp:lbanp-num_latents-8 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.1956 loss -1.1956 (9.334 secs)\n",
      "lbanp:lbanp-num_latents-8 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.1395 loss -1.1395 (9.310 secs)\n",
      "100%|##########| 3000/3000 [00:44<00:00, 67.89it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.2258 loss -1.2258 (44.189 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:45<00:00, 66.52it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.2258 loss -1.2258 (45.098 secs)\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.2258 loss -1.2258 (45.098 secs)\n"
     ]
    }
   ],
   "source": [
    "#%run gp.py --mode train --expid lbanp-num_latents-8 --model lbanp --num_latents 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601f0493-4044-42af-aff2-002eb83cf5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b4fd51-8bef-40c7-8a23-696860b6e516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d71a0b5-8863-4b43-9746-674cfd6eecf2",
   "metadata": {},
   "source": [
    "## Warm-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56af1e28-37c9-4ae7-aed9-c28565839da2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: np-warmup\n",
      "Total number of parameters: 232194\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "rbf-seed0.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "np:warmup step 200 lr 5.000e-04 [train_loss] loss 0.6588 (3.013 secs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\OneDrive - University of Cambridge\\Courses Material\\MLMI 4 Advanced Machine Learning\\Project\\code\\latent-bottlenecked-anp-main\\latent-bottlenecked-anp-main\\regression\\gp.py:322\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m line\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 322\u001b[0m     main()\n",
      "File \u001b[1;32m~\\OneDrive - University of Cambridge\\Courses Material\\MLMI 4 Advanced Machine Learning\\Project\\code\\latent-bottlenecked-anp-main\\latent-bottlenecked-anp-main\\regression\\gp.py:112\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 112\u001b[0m     train(args, model)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28meval\u001b[39m(args, model)\n",
      "File \u001b[1;32m~\\OneDrive - University of Cambridge\\Courses Material\\MLMI 4 Advanced Machine Learning\\Project\\code\\latent-bottlenecked-anp-main\\latent-bottlenecked-anp-main\\regression\\gp.py:176\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, model)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     outs \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[1;32m--> 176\u001b[0m outs\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    177\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    178\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 6501.6025390625 miliseconds\n",
      "Execution time: 6.5016025390625 seconds\n",
      "Initial Memory Usage: 0.0 MB\n",
      "Final Memory Usage: 32.8037109375 MB\n",
      "Memory Usage Change: 32.8037109375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod('np', 'warmup',val_seed=0, val_l=None,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5a5b4-8605-438e-97b6-d4dc6dd38ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63190376-b76e-4c8f-bc30-1160597f780d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RBF Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac195366-83c1-4ccd-a63b-2523fe46ea2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: isanp-isanp-num_latents-8_rbf_0\n",
      "Total number of parameters: 785602\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf_0 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6915 loss 0.6915 (7.297 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 400 lr 5.000e-04 [train_loss] tar_ll -0.6330 loss 0.6330 (9.376 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 600 lr 5.000e-04 [train_loss] tar_ll -0.5263 loss 0.5263 (9.791 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 800 lr 4.999e-04 [train_loss] tar_ll -0.4680 loss 0.4680 (8.179 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 1000 lr 4.999e-04 [train_loss] tar_ll -0.3958 loss 0.3958 (7.821 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.3140 loss 0.3140 (7.915 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 1400 lr 4.998e-04 [train_loss] tar_ll -0.1633 loss 0.1633 (8.307 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 1600 lr 4.997e-04 [train_loss] tar_ll -0.1452 loss 0.1452 (9.681 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 1800 lr 4.996e-04 [train_loss] tar_ll -0.0902 loss 0.0902 (7.749 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 2000 lr 4.995e-04 [train_loss] tar_ll 0.0231 loss -0.0231 (7.566 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.1067 loss -0.1067 (7.452 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.1370 loss -0.1370 (7.390 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1346 loss -0.1346 (7.467 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.1930 loss -0.1930 (7.643 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.2597 loss -0.2597 (7.028 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.1426 loss -0.1426 (7.441 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.2355 loss -0.2355 (6.962 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2389 loss -0.2389 (7.413 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.2350 loss -0.2350 (7.095 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.3059 loss -0.3059 (7.007 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3111 loss -0.3111 (7.449 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.3224 loss -0.3224 (6.943 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.3827 loss -0.3827 (7.526 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.3307 loss -0.3307 (6.865 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.3496 loss -0.3496 (7.490 secs)\n",
      "100%|##########| 3000/3000 [00:38<00:00, 78.75it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 0.3272 loss -0.3272 (38.096 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.4234 loss -0.4234 (7.133 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.2453 loss -0.2453 (6.550 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.3382 loss -0.3382 (6.630 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.3649 loss -0.3649 (7.129 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4543 loss -0.4543 (8.029 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.3917 loss -0.3917 (7.729 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.3883 loss -0.3883 (8.487 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.4082 loss -0.4082 (8.063 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.4312 loss -0.4312 (7.889 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.4517 loss -0.4517 (8.249 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.5321 loss -0.5321 (7.798 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5040 loss -0.5040 (8.070 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.4922 loss -0.4922 (7.656 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.6007 loss -0.6007 (7.768 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.5469 loss -0.5469 (8.310 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.4651 loss -0.4651 (8.053 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.4737 loss -0.4737 (8.475 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.5369 loss -0.5369 (7.885 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.4822 loss -0.4822 (8.394 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.5544 loss -0.5544 (7.276 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.4979 loss -0.4979 (7.205 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.4652 loss -0.4652 (7.779 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.5140 loss -0.5140 (8.039 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.5922 loss -0.5922 (7.452 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.5458 loss -0.5458 (7.139 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.09it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 0.6490 loss -0.6490 (36.996 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.5556 loss -0.5556 (7.125 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.5509 loss -0.5509 (7.390 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.5773 loss -0.5773 (7.473 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.5948 loss -0.5948 (7.013 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.5040 loss -0.5040 (6.989 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.6422 loss -0.6422 (6.857 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.6146 loss -0.6146 (7.180 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.6633 loss -0.6633 (6.967 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.5250 loss -0.5250 (7.152 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.6248 loss -0.6248 (6.729 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.6602 loss -0.6602 (6.747 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.5517 loss -0.5517 (7.377 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.5923 loss -0.5923 (6.887 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.6389 loss -0.6389 (6.876 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.6092 loss -0.6092 (6.575 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.6284 loss -0.6284 (7.110 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.5942 loss -0.5942 (6.748 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.6042 loss -0.6042 (7.269 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.6189 loss -0.6189 (7.226 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.6211 loss -0.6211 (6.520 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.6972 loss -0.6972 (7.094 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.5940 loss -0.5940 (7.135 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.6890 loss -0.6890 (8.372 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.7004 loss -0.7004 (8.940 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.5669 loss -0.5669 (8.595 secs)\n",
      "100%|##########| 3000/3000 [00:39<00:00, 75.51it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 0.6850 loss -0.6850 (39.729 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.6647 loss -0.6647 (7.556 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.5987 loss -0.5987 (7.143 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.6540 loss -0.6540 (7.408 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.6425 loss -0.6425 (8.147 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.5019 loss -0.5019 (7.744 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.6313 loss -0.6313 (8.138 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.6008 loss -0.6008 (7.513 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.6332 loss -0.6332 (6.958 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.6612 loss -0.6612 (7.985 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.6988 loss -0.6988 (8.317 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.6563 loss -0.6563 (8.636 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.7102 loss -0.7102 (8.714 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.6239 loss -0.6239 (9.178 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.6579 loss -0.6579 (8.076 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.6625 loss -0.6625 (8.059 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.6440 loss -0.6440 (9.103 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.7047 loss -0.7047 (8.748 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.6845 loss -0.6845 (10.278 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.5713 loss -0.5713 (8.614 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.6433 loss -0.6433 (7.558 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.7415 loss -0.7415 (8.701 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.7161 loss -0.7161 (8.320 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.6776 loss -0.6776 (8.438 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.6959 loss -0.6959 (8.396 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.7537 loss -0.7537 (7.126 secs)\n",
      "100%|##########| 3000/3000 [00:37<00:00, 79.81it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 0.8416 loss -0.8416 (37.605 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.6901 loss -0.6901 (9.507 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.5899 loss -0.5899 (9.060 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.7035 loss -0.7035 (8.316 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.7219 loss -0.7219 (8.183 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.7518 loss -0.7518 (7.273 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.8472 loss -0.8472 (9.008 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.6427 loss -0.6427 (8.338 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.7665 loss -0.7665 (8.492 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.7720 loss -0.7720 (8.436 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.7140 loss -0.7140 (7.915 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.6781 loss -0.6781 (7.667 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.7506 loss -0.7506 (7.595 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.7685 loss -0.7685 (7.556 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.6609 loss -0.6609 (7.739 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.7736 loss -0.7736 (6.885 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.7587 loss -0.7587 (8.432 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.7825 loss -0.7825 (7.760 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.6434 loss -0.6434 (7.756 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.6393 loss -0.6393 (7.227 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.7564 loss -0.7564 (7.671 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.7672 loss -0.7672 (7.288 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.7139 loss -0.7139 (7.863 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.7860 loss -0.7860 (7.019 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.7590 loss -0.7590 (7.121 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.7440 loss -0.7440 (7.144 secs)\n",
      "100%|##########| 3000/3000 [00:37<00:00, 79.62it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 0.7797 loss -0.7797 (37.681 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.7696 loss -0.7696 (7.540 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.8028 loss -0.8028 (7.281 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.7629 loss -0.7629 (8.061 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.7750 loss -0.7750 (7.525 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.7605 loss -0.7605 (7.857 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.8428 loss -0.8428 (7.050 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.8115 loss -0.8115 (7.991 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.7715 loss -0.7715 (7.102 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.8662 loss -0.8662 (7.600 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.8693 loss -0.8693 (7.949 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.7984 loss -0.7984 (8.408 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.7846 loss -0.7846 (7.423 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.8616 loss -0.8616 (7.884 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.7293 loss -0.7293 (7.592 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.8271 loss -0.8271 (7.159 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.8167 loss -0.8167 (7.984 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.8537 loss -0.8537 (7.366 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.8637 loss -0.8637 (8.045 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.8384 loss -0.8384 (7.640 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.6318 loss -0.6318 (8.752 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.7433 loss -0.7433 (8.165 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.8162 loss -0.8162 (7.477 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.8674 loss -0.8674 (7.698 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.7933 loss -0.7933 (8.026 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.8966 loss -0.8966 (8.155 secs)\n",
      "100%|##########| 3000/3000 [00:40<00:00, 73.84it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 0.8270 loss -0.8270 (40.627 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.7400 loss -0.7400 (7.818 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.7500 loss -0.7500 (8.544 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.7549 loss -0.7549 (8.522 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.8148 loss -0.8148 (8.316 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.7977 loss -0.7977 (8.115 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.8902 loss -0.8902 (7.413 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.8014 loss -0.8014 (7.876 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.9040 loss -0.9040 (7.119 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.8884 loss -0.8884 (6.800 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.8660 loss -0.8660 (7.428 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.9066 loss -0.9066 (7.067 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.8468 loss -0.8468 (7.836 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.7733 loss -0.7733 (8.372 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.8542 loss -0.8542 (8.853 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.8828 loss -0.8828 (7.620 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.8203 loss -0.8203 (9.698 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.9571 loss -0.9571 (7.745 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.9249 loss -0.9249 (7.342 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.8167 loss -0.8167 (7.019 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.8393 loss -0.8393 (8.159 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.8773 loss -0.8773 (7.595 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.8824 loss -0.8824 (8.229 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.8954 loss -0.8954 (10.300 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.7865 loss -0.7865 (15.051 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.8045 loss -0.8045 (14.326 secs)\n",
      "100%|##########| 3000/3000 [01:23<00:00, 35.84it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 0.7441 loss -0.7441 (83.706 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.7677 loss -0.7677 (14.057 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.8465 loss -0.8465 (13.773 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.8955 loss -0.8955 (14.235 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.9401 loss -0.9401 (14.101 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.7659 loss -0.7659 (14.527 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.8837 loss -0.8837 (14.543 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.8403 loss -0.8403 (14.164 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.8469 loss -0.8469 (14.570 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.8512 loss -0.8512 (14.250 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.8333 loss -0.8333 (14.567 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.8554 loss -0.8554 (14.242 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.9315 loss -0.9315 (14.336 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.9618 loss -0.9618 (13.928 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.8700 loss -0.8700 (14.825 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.8732 loss -0.8732 (15.535 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.8712 loss -0.8712 (15.511 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.9379 loss -0.9379 (13.958 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.9359 loss -0.9359 (14.648 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.9693 loss -0.9693 (14.986 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.9511 loss -0.9511 (14.667 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.9422 loss -0.9422 (14.934 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.9969 loss -0.9969 (15.192 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.8894 loss -0.8894 (15.152 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.9369 loss -0.9369 (16.156 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.8292 loss -0.8292 (14.346 secs)\n",
      "100%|##########| 3000/3000 [01:19<00:00, 37.77it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 0.7817 loss -0.7817 (79.443 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.8594 loss -0.8594 (16.774 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.8839 loss -0.8839 (15.595 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.9028 loss -0.9028 (15.131 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.8446 loss -0.8446 (15.158 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.8968 loss -0.8968 (15.004 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.8222 loss -0.8222 (14.624 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.8727 loss -0.8727 (13.894 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.9335 loss -0.9335 (14.851 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.8632 loss -0.8632 (16.004 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.9567 loss -0.9567 (15.268 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.9392 loss -0.9392 (15.827 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.8850 loss -0.8850 (15.664 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 42600 lr 3.076e-04 [train_loss] tar_ll 1.0437 loss -1.0437 (18.166 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.9044 loss -0.9044 (15.527 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.8800 loss -0.8800 (15.237 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.9675 loss -0.9675 (17.828 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.8731 loss -0.8731 (21.023 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.9628 loss -0.9628 (18.582 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.9325 loss -0.9325 (18.406 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.9563 loss -0.9563 (17.247 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.8467 loss -0.8467 (17.226 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.9763 loss -0.9763 (16.395 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.9519 loss -0.9519 (17.499 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.9140 loss -0.9140 (17.126 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.8689 loss -0.8689 (15.830 secs)\n",
      "100%|##########| 3000/3000 [01:27<00:00, 34.47it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 0.9808 loss -0.9808 (87.037 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 45200 lr 2.876e-04 [train_loss] tar_ll 1.0222 loss -1.0222 (15.274 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 45400 lr 2.860e-04 [train_loss] tar_ll 1.0288 loss -1.0288 (16.105 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 45600 lr 2.844e-04 [train_loss] tar_ll 1.0513 loss -1.0513 (15.384 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.9687 loss -0.9687 (18.190 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.9924 loss -0.9924 (20.118 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 46200 lr 2.798e-04 [train_loss] tar_ll 1.0411 loss -1.0411 (16.016 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.9646 loss -0.9646 (15.880 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.9355 loss -0.9355 (15.568 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.9920 loss -0.9920 (16.471 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 47000 lr 2.735e-04 [train_loss] tar_ll 1.0299 loss -1.0299 (16.360 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 47200 lr 2.720e-04 [train_loss] tar_ll 1.0264 loss -1.0264 (17.347 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.9663 loss -0.9663 (17.066 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 47600 lr 2.688e-04 [train_loss] tar_ll 1.0701 loss -1.0701 (16.083 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 47800 lr 2.673e-04 [train_loss] tar_ll 1.0240 loss -1.0240 (16.022 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.9730 loss -0.9730 (15.998 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.9610 loss -0.9610 (16.420 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 48400 lr 2.626e-04 [train_loss] tar_ll 1.0606 loss -1.0606 (16.372 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.9227 loss -0.9227 (17.074 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 48800 lr 2.594e-04 [train_loss] tar_ll 0.9554 loss -0.9554 (17.244 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 49000 lr 2.579e-04 [train_loss] tar_ll 1.0149 loss -1.0149 (16.569 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 49200 lr 2.563e-04 [train_loss] tar_ll 0.9152 loss -0.9152 (17.219 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 49400 lr 2.547e-04 [train_loss] tar_ll 1.1132 loss -1.1132 (17.537 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 49600 lr 2.531e-04 [train_loss] tar_ll 0.9843 loss -0.9843 (16.134 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.9646 loss -0.9646 (16.178 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 50000 lr 2.500e-04 [train_loss] tar_ll 0.9783 loss -0.9783 (15.891 secs)\n",
      "100%|##########| 3000/3000 [01:26<00:00, 34.76it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 0.9892 loss -0.9892 (86.319 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 50200 lr 2.484e-04 [train_loss] tar_ll 1.0023 loss -1.0023 (15.996 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 50400 lr 2.469e-04 [train_loss] tar_ll 1.1067 loss -1.1067 (15.280 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 50600 lr 2.453e-04 [train_loss] tar_ll 1.0742 loss -1.0742 (15.627 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 50800 lr 2.437e-04 [train_loss] tar_ll 1.1119 loss -1.1119 (15.698 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 51000 lr 2.421e-04 [train_loss] tar_ll 0.9513 loss -0.9513 (16.081 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 51200 lr 2.406e-04 [train_loss] tar_ll 1.0166 loss -1.0166 (16.475 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 51400 lr 2.390e-04 [train_loss] tar_ll 1.0382 loss -1.0382 (15.888 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 51600 lr 2.374e-04 [train_loss] tar_ll 1.0667 loss -1.0667 (17.081 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 51800 lr 2.359e-04 [train_loss] tar_ll 0.9849 loss -0.9849 (16.504 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 52000 lr 2.343e-04 [train_loss] tar_ll 1.0519 loss -1.0519 (17.008 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 52200 lr 2.327e-04 [train_loss] tar_ll 1.0687 loss -1.0687 (17.681 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 52400 lr 2.312e-04 [train_loss] tar_ll 1.1010 loss -1.1010 (16.565 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 52600 lr 2.296e-04 [train_loss] tar_ll 1.0532 loss -1.0532 (16.489 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 52800 lr 2.280e-04 [train_loss] tar_ll 1.0196 loss -1.0196 (16.156 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 53000 lr 2.265e-04 [train_loss] tar_ll 1.0557 loss -1.0557 (18.166 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 53200 lr 2.249e-04 [train_loss] tar_ll 1.0942 loss -1.0942 (18.302 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 53400 lr 2.233e-04 [train_loss] tar_ll 1.1172 loss -1.1172 (18.588 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 53600 lr 2.218e-04 [train_loss] tar_ll 0.9458 loss -0.9458 (19.261 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 53800 lr 2.202e-04 [train_loss] tar_ll 1.0523 loss -1.0523 (18.623 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 54000 lr 2.187e-04 [train_loss] tar_ll 1.0035 loss -1.0035 (17.480 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 54200 lr 2.171e-04 [train_loss] tar_ll 1.0619 loss -1.0619 (17.002 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 54400 lr 2.156e-04 [train_loss] tar_ll 1.0329 loss -1.0329 (16.594 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 54600 lr 2.140e-04 [train_loss] tar_ll 0.9394 loss -0.9394 (15.740 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 54800 lr 2.124e-04 [train_loss] tar_ll 1.0931 loss -1.0931 (16.107 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 55000 lr 2.109e-04 [train_loss] tar_ll 1.0760 loss -1.0760 (16.732 secs)\n",
      "100%|##########| 3000/3000 [01:33<00:00, 31.92it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 1.0569 loss -1.0569 (93.991 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 55200 lr 2.093e-04 [train_loss] tar_ll 1.0489 loss -1.0489 (17.568 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 55400 lr 2.078e-04 [train_loss] tar_ll 1.0705 loss -1.0705 (16.487 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 55600 lr 2.062e-04 [train_loss] tar_ll 1.0925 loss -1.0925 (16.494 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 55800 lr 2.047e-04 [train_loss] tar_ll 1.0705 loss -1.0705 (17.615 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 56000 lr 2.032e-04 [train_loss] tar_ll 1.0804 loss -1.0804 (17.350 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 56200 lr 2.016e-04 [train_loss] tar_ll 0.9675 loss -0.9675 (17.394 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 56400 lr 2.001e-04 [train_loss] tar_ll 1.0742 loss -1.0742 (17.178 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 56600 lr 1.985e-04 [train_loss] tar_ll 1.0814 loss -1.0814 (18.263 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0537 loss -1.0537 (18.698 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 57000 lr 1.955e-04 [train_loss] tar_ll 1.1083 loss -1.1083 (20.886 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 57200 lr 1.939e-04 [train_loss] tar_ll 1.0538 loss -1.0538 (18.071 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 57400 lr 1.924e-04 [train_loss] tar_ll 1.1220 loss -1.1220 (18.801 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 57600 lr 1.909e-04 [train_loss] tar_ll 1.0314 loss -1.0314 (19.166 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 57800 lr 1.894e-04 [train_loss] tar_ll 0.9928 loss -0.9928 (20.754 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 58000 lr 1.878e-04 [train_loss] tar_ll 1.1136 loss -1.1136 (21.100 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 58200 lr 1.863e-04 [train_loss] tar_ll 1.0861 loss -1.0861 (21.288 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 58400 lr 1.848e-04 [train_loss] tar_ll 1.0487 loss -1.0487 (20.945 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 58600 lr 1.833e-04 [train_loss] tar_ll 1.1148 loss -1.1148 (19.193 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 58800 lr 1.818e-04 [train_loss] tar_ll 1.0522 loss -1.0522 (18.267 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.0631 loss -1.0631 (18.328 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 59200 lr 1.787e-04 [train_loss] tar_ll 1.0850 loss -1.0850 (18.387 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 59400 lr 1.772e-04 [train_loss] tar_ll 1.1168 loss -1.1168 (18.676 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 59600 lr 1.757e-04 [train_loss] tar_ll 1.1126 loss -1.1126 (17.697 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 59800 lr 1.742e-04 [train_loss] tar_ll 1.0547 loss -1.0547 (17.301 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 60000 lr 1.727e-04 [train_loss] tar_ll 1.0633 loss -1.0633 (17.727 secs)\n",
      "100%|##########| 3000/3000 [01:46<00:00, 28.28it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 1.1423 loss -1.1423 (106.074 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 60200 lr 1.713e-04 [train_loss] tar_ll 1.0590 loss -1.0590 (20.574 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 60400 lr 1.698e-04 [train_loss] tar_ll 1.1018 loss -1.1018 (19.952 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.0921 loss -1.0921 (18.760 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.1446 loss -1.1446 (18.296 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 61000 lr 1.653e-04 [train_loss] tar_ll 1.0250 loss -1.0250 (18.120 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 61200 lr 1.638e-04 [train_loss] tar_ll 1.0528 loss -1.0528 (18.360 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 61400 lr 1.624e-04 [train_loss] tar_ll 1.0570 loss -1.0570 (19.832 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 61600 lr 1.609e-04 [train_loss] tar_ll 1.0908 loss -1.0908 (20.105 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.1376 loss -1.1376 (19.561 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 62000 lr 1.580e-04 [train_loss] tar_ll 1.0921 loss -1.0921 (19.789 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 62200 lr 1.565e-04 [train_loss] tar_ll 0.9899 loss -0.9899 (17.105 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.1366 loss -1.1366 (9.023 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.2138 loss -1.2138 (8.640 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.1353 loss -1.1353 (9.170 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.0898 loss -1.0898 (8.735 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.0522 loss -1.0522 (9.287 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.0722 loss -1.0722 (9.459 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.1004 loss -1.1004 (8.668 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.0771 loss -1.0771 (9.132 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.1215 loss -1.1215 (8.305 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.1152 loss -1.1152 (9.596 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.1046 loss -1.1046 (8.820 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.0257 loss -1.0257 (10.094 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.1100 loss -1.1100 (8.969 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.1661 loss -1.1661 (9.258 secs)\n",
      "100%|##########| 3000/3000 [00:45<00:00, 65.47it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 1.1633 loss -1.1633 (45.822 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.1265 loss -1.1265 (9.412 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.0438 loss -1.0438 (9.364 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.1462 loss -1.1462 (9.796 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.1328 loss -1.1328 (9.773 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.1400 loss -1.1400 (10.088 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.1225 loss -1.1225 (9.740 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.1306 loss -1.1306 (9.060 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.1813 loss -1.1813 (9.827 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.2015 loss -1.2015 (9.108 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.1305 loss -1.1305 (9.724 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.0708 loss -1.0708 (9.844 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.1301 loss -1.1301 (8.897 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.1271 loss -1.1271 (9.557 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.1750 loss -1.1750 (9.000 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.1631 loss -1.1631 (9.339 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.0740 loss -1.0740 (9.543 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.1709 loss -1.1709 (9.716 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.0959 loss -1.0959 (9.834 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.2003 loss -1.2003 (9.102 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.0998 loss -1.0998 (9.618 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.1506 loss -1.1506 (10.556 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.1569 loss -1.1569 (8.654 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.2517 loss -1.2517 (10.081 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.2030 loss -1.2030 (9.644 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.0252 loss -1.0252 (9.984 secs)\n",
      "100%|##########| 3000/3000 [00:46<00:00, 64.49it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 1.1604 loss -1.1604 (46.520 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.1157 loss -1.1157 (8.144 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.2434 loss -1.2434 (8.226 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.1703 loss -1.1703 (8.997 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.1833 loss -1.1833 (8.255 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.1805 loss -1.1805 (8.210 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.1859 loss -1.1859 (8.279 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.2335 loss -1.2335 (8.639 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.1786 loss -1.1786 (8.579 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.2424 loss -1.2424 (8.706 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.1597 loss -1.1597 (8.260 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.2568 loss -1.2568 (8.514 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.1856 loss -1.1856 (12.119 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.2414 loss -1.2414 (9.142 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.1471 loss -1.1471 (8.758 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.2677 loss -1.2677 (8.063 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.1664 loss -1.1664 (9.077 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.2417 loss -1.2417 (8.142 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.2309 loss -1.2309 (8.358 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.2099 loss -1.2099 (8.098 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.2149 loss -1.2149 (8.224 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.2282 loss -1.2282 (8.118 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.1719 loss -1.1719 (8.232 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.2278 loss -1.2278 (8.039 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.2161 loss -1.2161 (8.162 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1780 loss -1.1780 (7.970 secs)\n",
      "100%|##########| 3000/3000 [00:43<00:00, 68.90it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 1.1990 loss -1.1990 (43.547 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.2410 loss -1.2410 (8.988 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.2419 loss -1.2419 (8.376 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.2303 loss -1.2303 (8.721 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.1903 loss -1.1903 (8.293 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.2302 loss -1.2302 (8.331 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.1577 loss -1.1577 (8.131 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.1910 loss -1.1910 (9.467 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.1512 loss -1.1512 (8.351 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.2295 loss -1.2295 (8.267 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.2941 loss -1.2941 (7.817 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.2017 loss -1.2017 (8.394 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.2765 loss -1.2765 (8.234 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.1776 loss -1.1776 (8.433 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2118 loss -1.2118 (8.403 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.1688 loss -1.1688 (8.550 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.1383 loss -1.1383 (8.766 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.2071 loss -1.2071 (8.410 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.2468 loss -1.2468 (8.606 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.2658 loss -1.2658 (8.260 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.1818 loss -1.1818 (8.403 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.2492 loss -1.2492 (8.512 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.1261 loss -1.1261 (8.487 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.2795 loss -1.2795 (7.956 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.2700 loss -1.2700 (8.211 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.1872 loss -1.1872 (8.013 secs)\n",
      "100%|##########| 3000/3000 [00:44<00:00, 67.70it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 1.2288 loss -1.2288 (44.313 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.1806 loss -1.1806 (8.278 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.2144 loss -1.2144 (8.099 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1855 loss -1.1855 (8.631 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2754 loss -1.2754 (8.275 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.2926 loss -1.2926 (8.440 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.2697 loss -1.2697 (7.929 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.2483 loss -1.2483 (8.822 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.3177 loss -1.3177 (7.887 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2633 loss -1.2633 (8.082 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.2658 loss -1.2658 (8.633 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.2242 loss -1.2242 (8.653 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.2464 loss -1.2464 (8.036 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.2586 loss -1.2586 (8.910 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.2468 loss -1.2468 (8.332 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.2421 loss -1.2421 (8.657 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.2492 loss -1.2492 (8.851 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.1540 loss -1.1540 (8.417 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.2150 loss -1.2150 (8.661 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.3108 loss -1.3108 (8.334 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.2795 loss -1.2795 (8.469 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2178 loss -1.2178 (7.836 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.3060 loss -1.3060 (8.103 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.2814 loss -1.2814 (9.188 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.2506 loss -1.2506 (9.324 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.1794 loss -1.1794 (8.748 secs)\n",
      "100%|##########| 3000/3000 [00:48<00:00, 61.98it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 1.2405 loss -1.2405 (48.403 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.2471 loss -1.2471 (9.305 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.1608 loss -1.1608 (9.134 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.3074 loss -1.3074 (9.012 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.2241 loss -1.2241 (8.377 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.2743 loss -1.2743 (8.420 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.2713 loss -1.2713 (8.451 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.2476 loss -1.2476 (7.900 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.2737 loss -1.2737 (8.692 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.2549 loss -1.2549 (8.613 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.3471 loss -1.3471 (8.968 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.3108 loss -1.3108 (8.568 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.2983 loss -1.2983 (8.554 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.2147 loss -1.2147 (8.500 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.3019 loss -1.3019 (8.503 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.3506 loss -1.3506 (7.920 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.2565 loss -1.2565 (8.816 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.2087 loss -1.2087 (8.582 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.1714 loss -1.1714 (8.046 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.2394 loss -1.2394 (7.870 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.2427 loss -1.2427 (8.162 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.2839 loss -1.2839 (8.527 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.2814 loss -1.2814 (7.911 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.2278 loss -1.2278 (7.922 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2991 loss -1.2991 (8.167 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.2626 loss -1.2626 (8.567 secs)\n",
      "100%|##########| 3000/3000 [00:40<00:00, 74.34it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 1.2661 loss -1.2661 (40.358 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.2537 loss -1.2537 (8.213 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.2983 loss -1.2983 (8.004 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.1933 loss -1.1933 (8.007 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.2367 loss -1.2367 (7.672 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.2823 loss -1.2823 (8.265 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.1791 loss -1.1791 (8.087 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2320 loss -1.2320 (8.247 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.1501 loss -1.1501 (8.099 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2893 loss -1.2893 (8.417 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2850 loss -1.2850 (8.349 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.2421 loss -1.2421 (8.747 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.2409 loss -1.2409 (8.021 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.2702 loss -1.2702 (8.069 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.2947 loss -1.2947 (7.781 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.2568 loss -1.2568 (8.330 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.2887 loss -1.2887 (8.459 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.3211 loss -1.3211 (8.136 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.2870 loss -1.2870 (7.917 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.2944 loss -1.2944 (8.398 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.2820 loss -1.2820 (8.500 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.2304 loss -1.2304 (8.631 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.2273 loss -1.2273 (8.505 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.2940 loss -1.2940 (8.487 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.2575 loss -1.2575 (8.817 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.2937 loss -1.2937 (8.036 secs)\n",
      "100%|##########| 3000/3000 [00:43<00:00, 69.40it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 1.2743 loss -1.2743 (43.231 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_rbf_0 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.2508 loss -1.2508 (7.892 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.2446 loss -1.2446 (7.931 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.2671 loss -1.2671 (8.191 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.2339 loss -1.2339 (8.283 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.2841 loss -1.2841 (7.922 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2746 loss -1.2746 (8.543 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.2371 loss -1.2371 (7.682 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.3002 loss -1.3002 (7.915 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.3079 loss -1.3079 (8.442 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.2761 loss -1.2761 (8.480 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.2955 loss -1.2955 (8.136 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.2823 loss -1.2823 (8.660 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.2325 loss -1.2325 (9.053 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.3272 loss -1.3272 (7.792 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.2813 loss -1.2813 (7.735 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.2192 loss -1.2192 (8.302 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.2642 loss -1.2642 (7.563 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.2826 loss -1.2826 (8.013 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2427 loss -1.2427 (7.946 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.2287 loss -1.2287 (8.202 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.3153 loss -1.3153 (8.194 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2967 loss -1.2967 (8.412 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.2537 loss -1.2537 (7.964 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.2805 loss -1.2805 (8.103 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.2120 loss -1.2120 (7.775 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 72.04it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 1.2764 loss -1.2764 (41.645 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:41<00:00, 71.54it/s]\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 1.2764 loss -1.2764 (41.939 secs)\n",
      "isanp:isanp-num_latents-8_rbf_0 rbf tar_ll 1.2764 loss -1.2764 (41.939 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 6467538.5 miliseconds\n",
      "Execution time: 6467.5385 seconds\n",
      "Initial Memory Usage: 0.0 MB\n",
      "Final Memory Usage: 55.13623046875 MB\n",
      "Memory Usage Change: 55.13623046875 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='isanp-num_latents-8_rbf_0',val_seed=0, val_l=8,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15626146-772a-4a5e-94ca-1d3cb7b780ee",
   "metadata": {},
   "source": [
    "## ISANP (16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d6eeea2-8d1f-4780-bdd1-362480af6c39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: isanp-isanp-num_latents-16_rbf_0\n",
      "Total number of parameters: 786114\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 16\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "rbf-seed0.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 200 lr 5.000e-04 [train_loss] tar_ll -0.7092 loss 0.7092 (15.546 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 400 lr 5.000e-04 [train_loss] tar_ll -0.6594 loss 0.6594 (14.265 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 600 lr 5.000e-04 [train_loss] tar_ll -0.5512 loss 0.5512 (14.902 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 800 lr 4.999e-04 [train_loss] tar_ll -0.4883 loss 0.4883 (13.838 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 1000 lr 4.999e-04 [train_loss] tar_ll -0.4234 loss 0.4234 (13.342 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.3483 loss 0.3483 (14.153 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 1400 lr 4.998e-04 [train_loss] tar_ll -0.1743 loss 0.1743 (15.307 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 1600 lr 4.997e-04 [train_loss] tar_ll -0.1579 loss 0.1579 (14.406 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 1800 lr 4.996e-04 [train_loss] tar_ll -0.0658 loss 0.0658 (14.624 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 2000 lr 4.995e-04 [train_loss] tar_ll 0.0144 loss -0.0144 (14.815 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.1433 loss -0.1433 (14.389 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.1357 loss -0.1357 (14.341 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1622 loss -0.1622 (14.021 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.1897 loss -0.1897 (13.918 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.2761 loss -0.2761 (13.871 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.1853 loss -0.1853 (14.143 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.1413 loss -0.1413 (14.246 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2770 loss -0.2770 (13.951 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.2968 loss -0.2968 (14.568 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.2853 loss -0.2853 (13.911 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3211 loss -0.3211 (14.776 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.3260 loss -0.3260 (15.700 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4204 loss -0.4204 (14.959 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.3748 loss -0.3748 (14.609 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.4283 loss -0.4283 (14.839 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 29.66it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 0.8365 loss -0.8365 (0.045 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.4844 loss -0.4844 (15.661 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.3200 loss -0.3200 (14.800 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4406 loss -0.4406 (13.997 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.4929 loss -0.4929 (14.112 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4934 loss -0.4934 (13.920 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.5822 loss -0.5822 (14.053 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.5345 loss -0.5345 (13.876 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5255 loss -0.5255 (13.951 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.5486 loss -0.5486 (15.830 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.5418 loss -0.5418 (14.137 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.5773 loss -0.5773 (13.673 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5233 loss -0.5233 (13.802 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.6054 loss -0.6054 (13.551 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.6175 loss -0.6175 (13.493 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.5166 loss -0.5166 (13.549 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.6476 loss -0.6476 (13.603 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.5669 loss -0.5669 (13.592 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.5794 loss -0.5794 (13.499 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.5594 loss -0.5594 (13.399 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.6337 loss -0.6337 (13.658 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.6019 loss -0.6019 (14.024 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.5713 loss -0.5713 (13.590 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.5960 loss -0.5960 (13.735 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.7001 loss -0.7001 (13.942 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.6343 loss -0.6343 (13.693 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 37.12it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 1.1919 loss -1.1919 (0.032 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.8300 loss -0.8300 (14.009 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.7087 loss -0.7087 (13.964 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.6584 loss -0.6584 (14.043 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.6212 loss -0.6212 (14.066 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.7042 loss -0.7042 (14.224 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.7022 loss -0.7022 (14.728 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.6945 loss -0.6945 (14.094 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.6858 loss -0.6858 (13.953 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.7226 loss -0.7226 (13.725 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.7523 loss -0.7523 (13.762 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.7542 loss -0.7542 (13.853 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.7883 loss -0.7883 (13.774 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.7464 loss -0.7464 (13.899 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.7797 loss -0.7797 (13.989 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.6393 loss -0.6393 (14.140 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.5608 loss -0.5608 (13.995 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.6704 loss -0.6704 (13.755 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.5075 loss -0.5075 (13.845 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.6285 loss -0.6285 (14.021 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.6471 loss -0.6471 (14.131 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.6823 loss -0.6823 (14.290 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.7419 loss -0.7419 (13.852 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.8087 loss -0.8087 (14.022 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.8321 loss -0.8321 (13.920 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.7318 loss -0.7318 (13.771 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 38.74it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 1.0139 loss -1.0139 (0.029 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.7751 loss -0.7751 (14.218 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.8029 loss -0.8029 (14.013 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.7571 loss -0.7571 (13.844 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.8116 loss -0.8116 (13.868 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.7873 loss -0.7873 (13.937 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.8384 loss -0.8384 (14.139 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.6243 loss -0.6243 (13.844 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.7746 loss -0.7746 (13.809 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.8939 loss -0.8939 (13.751 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.8276 loss -0.8276 (14.013 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.7677 loss -0.7677 (14.268 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.7527 loss -0.7527 (13.987 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.7981 loss -0.7981 (13.900 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.7758 loss -0.7758 (14.087 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.6990 loss -0.6990 (13.968 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.8251 loss -0.8251 (13.997 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.7972 loss -0.7972 (14.075 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.8654 loss -0.8654 (14.085 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.8838 loss -0.8838 (14.089 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.9073 loss -0.9073 (13.798 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.8233 loss -0.8233 (14.285 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.8667 loss -0.8667 (13.832 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.8457 loss -0.8457 (14.083 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.8088 loss -0.8088 (13.683 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.8205 loss -0.8205 (13.890 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 38.14it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 1.1932 loss -1.1932 (0.030 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.7866 loss -0.7866 (14.032 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.8444 loss -0.8444 (13.954 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.8802 loss -0.8802 (14.227 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.8161 loss -0.8161 (14.079 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.8057 loss -0.8057 (14.261 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.8618 loss -0.8618 (14.408 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.7930 loss -0.7930 (14.152 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.8833 loss -0.8833 (13.990 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.9079 loss -0.9079 (13.923 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.9028 loss -0.9028 (13.905 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.9438 loss -0.9438 (14.102 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.9170 loss -0.9170 (13.855 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.8385 loss -0.8385 (13.955 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.8612 loss -0.8612 (13.736 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.8596 loss -0.8596 (13.649 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.8293 loss -0.8293 (13.908 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.8175 loss -0.8175 (13.742 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.9308 loss -0.9308 (13.693 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.8361 loss -0.8361 (13.832 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.8635 loss -0.8635 (14.136 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.9337 loss -0.9337 (14.418 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.8002 loss -0.8002 (14.441 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.8437 loss -0.8437 (13.881 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.9016 loss -0.9016 (14.134 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.9047 loss -0.9047 (14.087 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 37.86it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 1.5426 loss -1.5426 (0.030 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.9487 loss -0.9487 (14.188 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.9517 loss -0.9517 (13.884 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.9309 loss -0.9309 (13.914 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.9057 loss -0.9057 (13.949 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.8704 loss -0.8704 (14.161 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.8801 loss -0.8801 (14.101 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.9356 loss -0.9356 (13.715 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.9560 loss -0.9560 (13.946 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.8690 loss -0.8690 (13.928 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 27000 lr 4.153e-04 [train_loss] tar_ll 1.0063 loss -1.0063 (13.710 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.9504 loss -0.9504 (13.984 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.9161 loss -0.9161 (14.126 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.9722 loss -0.9722 (14.065 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.9399 loss -0.9399 (13.982 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.9941 loss -0.9941 (13.870 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.9317 loss -0.9317 (14.193 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.8444 loss -0.8444 (14.036 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.9595 loss -0.9595 (14.148 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.9151 loss -0.9151 (14.067 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.9193 loss -0.9193 (14.048 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.9537 loss -0.9537 (13.967 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.9259 loss -0.9259 (13.865 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 29600 lr 3.995e-04 [train_loss] tar_ll 1.0149 loss -1.0149 (14.114 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.9463 loss -0.9463 (14.072 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.9856 loss -0.9856 (13.811 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 32.21it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 1.7242 loss -1.7242 (0.035 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 30200 lr 3.957e-04 [train_loss] tar_ll 1.0086 loss -1.0086 (13.997 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 30400 lr 3.944e-04 [train_loss] tar_ll 1.0428 loss -1.0428 (13.855 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.9803 loss -0.9803 (13.967 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.9641 loss -0.9641 (14.100 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.9068 loss -0.9068 (13.818 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.9880 loss -0.9880 (14.395 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.9626 loss -0.9626 (14.030 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.8926 loss -0.8926 (14.060 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.9391 loss -0.9391 (13.890 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.9439 loss -0.9439 (13.708 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 32200 lr 3.826e-04 [train_loss] tar_ll 1.0200 loss -1.0200 (14.319 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.9979 loss -0.9979 (14.017 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.8390 loss -0.8390 (13.835 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.9255 loss -0.9255 (14.018 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.8933 loss -0.8933 (14.000 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 33200 lr 3.759e-04 [train_loss] tar_ll 1.0191 loss -1.0191 (13.639 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 33400 lr 3.745e-04 [train_loss] tar_ll 1.0370 loss -1.0370 (13.990 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.9909 loss -0.9909 (13.839 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 33800 lr 3.718e-04 [train_loss] tar_ll 1.0275 loss -1.0275 (13.795 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.9217 loss -0.9217 (13.853 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.9194 loss -0.9194 (14.154 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.9633 loss -0.9633 (14.109 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.9748 loss -0.9748 (14.016 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.9585 loss -0.9585 (14.314 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 35000 lr 3.635e-04 [train_loss] tar_ll 1.0118 loss -1.0118 (13.990 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 39.70it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 1.4844 loss -1.4844 (0.030 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.9924 loss -0.9924 (14.248 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.9750 loss -0.9750 (14.214 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 35600 lr 3.593e-04 [train_loss] tar_ll 1.0189 loss -1.0189 (13.975 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 35800 lr 3.579e-04 [train_loss] tar_ll 1.0899 loss -1.0899 (13.720 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 36000 lr 3.564e-04 [train_loss] tar_ll 1.0264 loss -1.0264 (13.960 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 36200 lr 3.550e-04 [train_loss] tar_ll 1.0735 loss -1.0735 (14.101 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.8333 loss -0.8333 (14.173 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.8321 loss -0.8321 (14.015 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 36800 lr 3.507e-04 [train_loss] tar_ll 1.0109 loss -1.0109 (14.093 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.9507 loss -0.9507 (13.966 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.9967 loss -0.9967 (14.068 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 37400 lr 3.464e-04 [train_loss] tar_ll 1.0246 loss -1.0246 (13.862 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 37600 lr 3.449e-04 [train_loss] tar_ll 1.0368 loss -1.0368 (14.179 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 37800 lr 3.435e-04 [train_loss] tar_ll 1.0674 loss -1.0674 (14.316 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.9894 loss -0.9894 (14.238 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 38200 lr 3.406e-04 [train_loss] tar_ll 1.0378 loss -1.0378 (14.401 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 38400 lr 3.391e-04 [train_loss] tar_ll 1.0276 loss -1.0276 (14.120 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 38600 lr 3.376e-04 [train_loss] tar_ll 1.0350 loss -1.0350 (14.047 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 38800 lr 3.362e-04 [train_loss] tar_ll 1.0556 loss -1.0556 (14.013 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 39000 lr 3.347e-04 [train_loss] tar_ll 1.0492 loss -1.0492 (13.969 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.9743 loss -0.9743 (14.288 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 39400 lr 3.317e-04 [train_loss] tar_ll 1.0399 loss -1.0399 (14.146 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.9297 loss -0.9297 (13.788 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 39800 lr 3.287e-04 [train_loss] tar_ll 1.0060 loss -1.0060 (13.814 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 40000 lr 3.273e-04 [train_loss] tar_ll 1.0974 loss -1.0974 (13.852 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 37.21it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 1.8384 loss -1.8384 (0.031 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 40200 lr 3.258e-04 [train_loss] tar_ll 1.0644 loss -1.0644 (14.008 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.9993 loss -0.9993 (13.860 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 40600 lr 3.228e-04 [train_loss] tar_ll 1.0479 loss -1.0479 (13.643 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.9563 loss -0.9563 (13.935 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 41000 lr 3.197e-04 [train_loss] tar_ll 1.0814 loss -1.0814 (13.990 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 41200 lr 3.182e-04 [train_loss] tar_ll 1.0537 loss -1.0537 (14.174 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 41400 lr 3.167e-04 [train_loss] tar_ll 1.0120 loss -1.0120 (14.162 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.9960 loss -0.9960 (14.283 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.9561 loss -0.9561 (14.073 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 42000 lr 3.122e-04 [train_loss] tar_ll 1.0727 loss -1.0727 (14.178 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 42200 lr 3.106e-04 [train_loss] tar_ll 1.1453 loss -1.1453 (13.994 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 42400 lr 3.091e-04 [train_loss] tar_ll 1.0371 loss -1.0371 (13.893 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 42600 lr 3.076e-04 [train_loss] tar_ll 1.0381 loss -1.0381 (14.068 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.9486 loss -0.9486 (14.032 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 43000 lr 3.045e-04 [train_loss] tar_ll 1.0272 loss -1.0272 (13.902 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 43200 lr 3.030e-04 [train_loss] tar_ll 1.0851 loss -1.0851 (14.103 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 43400 lr 3.015e-04 [train_loss] tar_ll 1.0686 loss -1.0686 (13.849 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 43600 lr 2.999e-04 [train_loss] tar_ll 1.0723 loss -1.0723 (13.822 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 43800 lr 2.984e-04 [train_loss] tar_ll 1.0302 loss -1.0302 (13.599 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 44000 lr 2.968e-04 [train_loss] tar_ll 1.0681 loss -1.0681 (13.780 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 44200 lr 2.953e-04 [train_loss] tar_ll 1.1009 loss -1.1009 (14.229 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 44400 lr 2.938e-04 [train_loss] tar_ll 1.0121 loss -1.0121 (14.058 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 44600 lr 2.922e-04 [train_loss] tar_ll 1.1663 loss -1.1663 (13.976 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 44800 lr 2.907e-04 [train_loss] tar_ll 1.0707 loss -1.0707 (14.070 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 45000 lr 2.891e-04 [train_loss] tar_ll 1.1152 loss -1.1152 (14.435 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 41.27it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 1.6653 loss -1.6653 (0.029 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 45200 lr 2.876e-04 [train_loss] tar_ll 1.1331 loss -1.1331 (14.365 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 45400 lr 2.860e-04 [train_loss] tar_ll 1.0728 loss -1.0728 (13.945 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 45600 lr 2.844e-04 [train_loss] tar_ll 1.0976 loss -1.0976 (14.228 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 45800 lr 2.829e-04 [train_loss] tar_ll 1.0922 loss -1.0922 (14.162 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 46000 lr 2.813e-04 [train_loss] tar_ll 1.0220 loss -1.0220 (14.253 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 46200 lr 2.798e-04 [train_loss] tar_ll 1.0887 loss -1.0887 (14.096 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 46400 lr 2.782e-04 [train_loss] tar_ll 1.1367 loss -1.1367 (13.815 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 46600 lr 2.767e-04 [train_loss] tar_ll 1.1502 loss -1.1502 (13.957 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 46800 lr 2.751e-04 [train_loss] tar_ll 1.1849 loss -1.1849 (14.083 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.9847 loss -0.9847 (14.008 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 47200 lr 2.720e-04 [train_loss] tar_ll 1.1237 loss -1.1237 (13.828 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 47400 lr 2.704e-04 [train_loss] tar_ll 1.0883 loss -1.0883 (13.544 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 47600 lr 2.688e-04 [train_loss] tar_ll 1.1081 loss -1.1081 (13.700 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 47800 lr 2.673e-04 [train_loss] tar_ll 1.0979 loss -1.0979 (13.892 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 48000 lr 2.657e-04 [train_loss] tar_ll 1.1710 loss -1.1710 (13.944 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 48200 lr 2.641e-04 [train_loss] tar_ll 1.0476 loss -1.0476 (14.578 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 48400 lr 2.626e-04 [train_loss] tar_ll 1.0526 loss -1.0526 (13.943 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 48600 lr 2.610e-04 [train_loss] tar_ll 1.0931 loss -1.0931 (13.938 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 48800 lr 2.594e-04 [train_loss] tar_ll 0.9981 loss -0.9981 (13.861 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 49000 lr 2.579e-04 [train_loss] tar_ll 1.0812 loss -1.0812 (13.748 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 49200 lr 2.563e-04 [train_loss] tar_ll 1.0753 loss -1.0753 (14.142 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 49400 lr 2.547e-04 [train_loss] tar_ll 1.1267 loss -1.1267 (13.952 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 49600 lr 2.531e-04 [train_loss] tar_ll 1.1135 loss -1.1135 (14.294 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 49800 lr 2.516e-04 [train_loss] tar_ll 1.1350 loss -1.1350 (14.190 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 50000 lr 2.500e-04 [train_loss] tar_ll 1.1436 loss -1.1436 (13.827 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 36.68it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 1.9020 loss -1.9020 (0.032 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 50200 lr 2.484e-04 [train_loss] tar_ll 1.0722 loss -1.0722 (13.922 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 50400 lr 2.469e-04 [train_loss] tar_ll 1.0614 loss -1.0614 (13.859 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 50600 lr 2.453e-04 [train_loss] tar_ll 1.1301 loss -1.1301 (14.055 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 50800 lr 2.437e-04 [train_loss] tar_ll 1.1254 loss -1.1254 (13.690 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 51000 lr 2.421e-04 [train_loss] tar_ll 1.1286 loss -1.1286 (13.748 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 51200 lr 2.406e-04 [train_loss] tar_ll 1.0797 loss -1.0797 (14.222 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 51400 lr 2.390e-04 [train_loss] tar_ll 1.1077 loss -1.1077 (14.060 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 51600 lr 2.374e-04 [train_loss] tar_ll 1.1398 loss -1.1398 (14.130 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 51800 lr 2.359e-04 [train_loss] tar_ll 1.1054 loss -1.1054 (14.230 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 52000 lr 2.343e-04 [train_loss] tar_ll 1.0996 loss -1.0996 (13.949 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 52200 lr 2.327e-04 [train_loss] tar_ll 1.0659 loss -1.0659 (14.291 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 52400 lr 2.312e-04 [train_loss] tar_ll 1.1301 loss -1.1301 (14.069 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 52600 lr 2.296e-04 [train_loss] tar_ll 1.1618 loss -1.1618 (14.028 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 52800 lr 2.280e-04 [train_loss] tar_ll 1.2019 loss -1.2019 (13.757 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 53000 lr 2.265e-04 [train_loss] tar_ll 1.1288 loss -1.1288 (13.915 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 53200 lr 2.249e-04 [train_loss] tar_ll 1.1016 loss -1.1016 (14.127 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 53400 lr 2.233e-04 [train_loss] tar_ll 1.1484 loss -1.1484 (13.945 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 53600 lr 2.218e-04 [train_loss] tar_ll 1.0474 loss -1.0474 (13.936 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 53800 lr 2.202e-04 [train_loss] tar_ll 1.0960 loss -1.0960 (13.833 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 54000 lr 2.187e-04 [train_loss] tar_ll 1.0689 loss -1.0689 (13.841 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 54200 lr 2.171e-04 [train_loss] tar_ll 1.1594 loss -1.1594 (13.761 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 54400 lr 2.156e-04 [train_loss] tar_ll 1.1291 loss -1.1291 (13.693 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 54600 lr 2.140e-04 [train_loss] tar_ll 1.1956 loss -1.1956 (13.853 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 54800 lr 2.124e-04 [train_loss] tar_ll 1.1828 loss -1.1828 (14.158 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 55000 lr 2.109e-04 [train_loss] tar_ll 1.2032 loss -1.2032 (14.083 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 53.06it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 1.8778 loss -1.8778 (0.026 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 55200 lr 2.093e-04 [train_loss] tar_ll 1.1681 loss -1.1681 (14.051 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 55400 lr 2.078e-04 [train_loss] tar_ll 1.1717 loss -1.1717 (14.016 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 55600 lr 2.062e-04 [train_loss] tar_ll 1.1426 loss -1.1426 (13.994 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 55800 lr 2.047e-04 [train_loss] tar_ll 1.1123 loss -1.1123 (13.877 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 56000 lr 2.032e-04 [train_loss] tar_ll 1.1248 loss -1.1248 (14.015 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 56200 lr 2.016e-04 [train_loss] tar_ll 1.1522 loss -1.1522 (14.369 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 56400 lr 2.001e-04 [train_loss] tar_ll 1.2703 loss -1.2703 (13.966 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 56600 lr 1.985e-04 [train_loss] tar_ll 1.2548 loss -1.2548 (13.916 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 56800 lr 1.970e-04 [train_loss] tar_ll 1.1635 loss -1.1635 (13.717 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 57000 lr 1.955e-04 [train_loss] tar_ll 1.1902 loss -1.1902 (13.864 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 57200 lr 1.939e-04 [train_loss] tar_ll 1.1188 loss -1.1188 (14.045 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 57400 lr 1.924e-04 [train_loss] tar_ll 1.2233 loss -1.2233 (13.919 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 57600 lr 1.909e-04 [train_loss] tar_ll 1.1717 loss -1.1717 (13.702 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 57800 lr 1.894e-04 [train_loss] tar_ll 1.1832 loss -1.1832 (13.936 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 58000 lr 1.878e-04 [train_loss] tar_ll 1.1016 loss -1.1016 (13.711 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 58200 lr 1.863e-04 [train_loss] tar_ll 1.1854 loss -1.1854 (14.249 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 58400 lr 1.848e-04 [train_loss] tar_ll 1.1627 loss -1.1627 (14.293 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 58600 lr 1.833e-04 [train_loss] tar_ll 1.2029 loss -1.2029 (14.127 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 58800 lr 1.818e-04 [train_loss] tar_ll 1.1510 loss -1.1510 (14.089 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.1668 loss -1.1668 (13.921 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 59200 lr 1.787e-04 [train_loss] tar_ll 1.1827 loss -1.1827 (14.204 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 59400 lr 1.772e-04 [train_loss] tar_ll 1.0545 loss -1.0545 (13.974 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 59600 lr 1.757e-04 [train_loss] tar_ll 1.2242 loss -1.2242 (14.018 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 59800 lr 1.742e-04 [train_loss] tar_ll 1.1895 loss -1.1895 (13.677 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 60000 lr 1.727e-04 [train_loss] tar_ll 1.0828 loss -1.0828 (13.766 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 37.43it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 2.0124 loss -2.0124 (0.031 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 60200 lr 1.713e-04 [train_loss] tar_ll 1.2319 loss -1.2319 (13.956 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 60400 lr 1.698e-04 [train_loss] tar_ll 1.1038 loss -1.1038 (14.154 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.1863 loss -1.1863 (14.007 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.2461 loss -1.2461 (13.913 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 61000 lr 1.653e-04 [train_loss] tar_ll 1.2441 loss -1.2441 (13.963 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 61200 lr 1.638e-04 [train_loss] tar_ll 1.1087 loss -1.1087 (14.160 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 61400 lr 1.624e-04 [train_loss] tar_ll 1.2628 loss -1.2628 (13.990 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 61600 lr 1.609e-04 [train_loss] tar_ll 1.2520 loss -1.2520 (14.055 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.0859 loss -1.0859 (13.902 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 62000 lr 1.580e-04 [train_loss] tar_ll 1.1895 loss -1.1895 (14.061 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 62200 lr 1.565e-04 [train_loss] tar_ll 1.1662 loss -1.1662 (14.386 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.2354 loss -1.2354 (14.135 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.2526 loss -1.2526 (14.199 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.1531 loss -1.1531 (13.993 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.2501 loss -1.2501 (14.040 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.2386 loss -1.2386 (14.107 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.2941 loss -1.2941 (13.831 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.1830 loss -1.1830 (13.848 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.3048 loss -1.3048 (13.801 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.0984 loss -1.0984 (13.923 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.2470 loss -1.2470 (14.157 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.2384 loss -1.2384 (13.916 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.2455 loss -1.2455 (14.000 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.3065 loss -1.3065 (13.814 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.2389 loss -1.2389 (13.902 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 38.62it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 2.0149 loss -2.0149 (0.029 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.1532 loss -1.1532 (14.089 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.2465 loss -1.2465 (14.268 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.2211 loss -1.2211 (14.236 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.2531 loss -1.2531 (14.105 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.1600 loss -1.1600 (14.082 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.2066 loss -1.2066 (14.185 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.2315 loss -1.2315 (13.927 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.1574 loss -1.1574 (14.052 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.2058 loss -1.2058 (13.750 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.1904 loss -1.1904 (13.854 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.1943 loss -1.1943 (14.172 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.2215 loss -1.2215 (13.865 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.2591 loss -1.2591 (13.815 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.2062 loss -1.2062 (14.005 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.2595 loss -1.2595 (13.811 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.2547 loss -1.2547 (14.007 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.2147 loss -1.2147 (13.979 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.2491 loss -1.2491 (14.100 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.2267 loss -1.2267 (14.087 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.1997 loss -1.1997 (14.041 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.2698 loss -1.2698 (14.111 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.2274 loss -1.2274 (14.045 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.2932 loss -1.2932 (13.789 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.2911 loss -1.2911 (13.968 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.2566 loss -1.2566 (14.110 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 47.29it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 2.1739 loss -2.1739 (0.027 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.1087 loss -1.1087 (14.042 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.3129 loss -1.3129 (13.969 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.2913 loss -1.2913 (13.810 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.1913 loss -1.1913 (14.144 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.3313 loss -1.3313 (13.827 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.2288 loss -1.2288 (13.936 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.3174 loss -1.3174 (13.810 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.2390 loss -1.2390 (13.774 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.2704 loss -1.2704 (13.750 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.2692 loss -1.2692 (14.062 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.2531 loss -1.2531 (14.647 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.3173 loss -1.3173 (14.357 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.2968 loss -1.2968 (14.052 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.1672 loss -1.1672 (13.888 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.3009 loss -1.3009 (13.995 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.2962 loss -1.2962 (14.187 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.2459 loss -1.2459 (14.010 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.2395 loss -1.2395 (13.723 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.2869 loss -1.2869 (13.956 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.2827 loss -1.2827 (13.865 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.3065 loss -1.3065 (14.068 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.3430 loss -1.3430 (13.528 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.2792 loss -1.2792 (13.589 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.2488 loss -1.2488 (13.757 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.2590 loss -1.2590 (13.810 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 44.22it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 2.2081 loss -2.2081 (0.026 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.2572 loss -1.2572 (13.973 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.2552 loss -1.2552 (13.901 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.3331 loss -1.3331 (14.190 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.3093 loss -1.3093 (14.154 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.2964 loss -1.2964 (14.037 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.3017 loss -1.3017 (14.087 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.2882 loss -1.2882 (14.015 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.3150 loss -1.3150 (14.207 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.2490 loss -1.2490 (13.862 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.3227 loss -1.3227 (13.846 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.2865 loss -1.2865 (14.064 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.2248 loss -1.2248 (13.747 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.2831 loss -1.2831 (14.059 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2905 loss -1.2905 (13.917 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.3021 loss -1.3021 (13.947 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.2652 loss -1.2652 (14.101 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.2577 loss -1.2577 (13.721 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.3699 loss -1.3699 (13.724 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.2919 loss -1.2919 (13.750 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.3076 loss -1.3076 (13.869 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.2210 loss -1.2210 (14.346 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.1983 loss -1.1983 (14.209 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.3433 loss -1.3433 (14.028 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.2348 loss -1.2348 (13.904 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.2606 loss -1.2606 (13.910 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 47.08it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 2.2817 loss -2.2817 (0.024 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.3872 loss -1.3872 (14.338 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.2610 loss -1.2610 (13.829 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.2831 loss -1.2831 (13.886 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2255 loss -1.2255 (13.849 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.3455 loss -1.3455 (14.294 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.2986 loss -1.2986 (14.257 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.2375 loss -1.2375 (13.921 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.3145 loss -1.3145 (13.933 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.4121 loss -1.4121 (13.727 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.3407 loss -1.3407 (13.811 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.3925 loss -1.3925 (14.036 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.2828 loss -1.2828 (13.992 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.2398 loss -1.2398 (14.070 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.3037 loss -1.3037 (14.035 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.2850 loss -1.2850 (13.995 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.4094 loss -1.4094 (14.181 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.2595 loss -1.2595 (14.072 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.2618 loss -1.2618 (14.127 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.3193 loss -1.3193 (13.998 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.3990 loss -1.3990 (13.996 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2956 loss -1.2956 (14.054 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.4386 loss -1.4386 (13.711 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.3390 loss -1.3390 (13.771 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.3461 loss -1.3461 (14.002 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.2734 loss -1.2734 (13.617 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 40.90it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 2.3201 loss -2.3201 (0.030 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.3502 loss -1.3502 (14.137 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.2841 loss -1.2841 (13.878 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.3580 loss -1.3580 (14.075 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.4232 loss -1.4232 (13.941 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.3088 loss -1.3088 (14.037 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.3351 loss -1.3351 (14.304 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.4090 loss -1.4090 (14.063 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.3003 loss -1.3003 (14.076 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.1931 loss -1.1931 (13.770 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.3000 loss -1.3000 (13.883 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.3519 loss -1.3519 (14.268 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.2640 loss -1.2640 (13.783 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.4002 loss -1.4002 (13.916 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.3106 loss -1.3106 (13.851 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.2962 loss -1.2962 (14.236 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.2287 loss -1.2287 (14.164 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.3390 loss -1.3390 (13.883 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.3220 loss -1.3220 (13.656 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.3825 loss -1.3825 (13.989 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.3602 loss -1.3602 (13.985 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.3490 loss -1.3490 (14.500 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.2379 loss -1.2379 (14.187 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.2512 loss -1.2512 (13.953 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2021 loss -1.2021 (13.933 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.3720 loss -1.3720 (13.893 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 47.97it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 2.3312 loss -2.3312 (0.025 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.3895 loss -1.3895 (14.195 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.3079 loss -1.3079 (13.867 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.3671 loss -1.3671 (14.004 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.3858 loss -1.3858 (13.907 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.2832 loss -1.2832 (13.834 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2819 loss -1.2819 (14.268 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2821 loss -1.2821 (14.136 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.3607 loss -1.3607 (13.763 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.3259 loss -1.3259 (13.824 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.3480 loss -1.3480 (13.841 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.3300 loss -1.3300 (13.769 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.3840 loss -1.3840 (14.093 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.3604 loss -1.3604 (14.100 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.3305 loss -1.3305 (14.203 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.3948 loss -1.3948 (13.948 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.3647 loss -1.3647 (14.317 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.3598 loss -1.3598 (14.030 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.3478 loss -1.3478 (14.121 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.3459 loss -1.3459 (13.925 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.3750 loss -1.3750 (13.870 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.3509 loss -1.3509 (14.278 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.3146 loss -1.3146 (13.879 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.3375 loss -1.3375 (13.821 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.3197 loss -1.3197 (14.301 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.3914 loss -1.3914 (13.665 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 44.70it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 2.3399 loss -2.3399 (0.025 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-16_rbf_0 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.3801 loss -1.3801 (14.118 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.2879 loss -1.2879 (13.648 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.3989 loss -1.3989 (13.685 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.2239 loss -1.2239 (13.807 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.2531 loss -1.2531 (14.220 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2496 loss -1.2496 (14.305 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.3102 loss -1.3102 (14.105 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.2879 loss -1.2879 (14.208 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.3575 loss -1.3575 (14.022 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.2967 loss -1.2967 (13.973 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.3290 loss -1.3290 (13.997 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.3898 loss -1.3898 (13.900 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.4306 loss -1.4306 (13.981 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.3557 loss -1.3557 (13.977 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.3552 loss -1.3552 (13.803 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.3711 loss -1.3711 (14.323 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.3737 loss -1.3737 (14.021 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.3809 loss -1.3809 (13.799 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.3336 loss -1.3336 (13.873 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.2891 loss -1.2891 (13.823 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.3388 loss -1.3388 (14.052 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.3363 loss -1.3363 (13.854 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.2877 loss -1.2877 (13.840 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.3608 loss -1.3608 (14.000 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.3454 loss -1.3454 (14.052 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 48.64it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 2.3558 loss -2.3558 (0.025 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 45.82it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 2.3558 loss -2.3558 (0.024 secs)\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 2.3558 loss -2.3558 (0.024 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "Execution time: 7011935.0 miliseconds\n",
      "Execution time: 7011.935 seconds\n",
      "Initial Memory Usage: 0.0 MB\n",
      "Final Memory Usage: 52.951171875 MB\n",
      "Memory Usage Change: 52.951171875 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='isanp-num_latents-16_rbf_0',val_seed=0, val_l=16,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90534f0b-b1d6-4ca7-8081-42dfae96e622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: isanp-isanp-num_latents-16_rbf_100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 16\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 786114\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 200 lr 5.000e-04 [train_loss] tar_ll -0.7202 loss 0.7202 (25.645 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 400 lr 5.000e-04 [train_loss] tar_ll -0.7061 loss 0.7061 (23.875 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 600 lr 5.000e-04 [train_loss] tar_ll -0.5053 loss 0.5053 (24.954 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 800 lr 4.999e-04 [train_loss] tar_ll -0.3858 loss 0.3858 (24.777 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 1000 lr 4.999e-04 [train_loss] tar_ll -0.2989 loss 0.2989 (24.074 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.2692 loss 0.2692 (25.346 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 1400 lr 4.998e-04 [train_loss] tar_ll -0.2014 loss 0.2014 (24.591 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 1600 lr 4.997e-04 [train_loss] tar_ll -0.0848 loss 0.0848 (24.163 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 1800 lr 4.996e-04 [train_loss] tar_ll -0.0128 loss 0.0128 (24.632 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 2000 lr 4.995e-04 [train_loss] tar_ll -0.0558 loss 0.0558 (25.017 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.1154 loss -0.1154 (24.703 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.1508 loss -0.1508 (24.931 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1797 loss -0.1797 (23.394 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.1616 loss -0.1616 (15.876 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.3102 loss -0.3102 (6.166 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.2888 loss -0.2888 (6.728 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.3370 loss -0.3370 (6.737 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.3649 loss -0.3649 (7.195 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.3691 loss -0.3691 (7.620 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.3896 loss -0.3896 (6.786 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.4237 loss -0.4237 (6.257 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.3920 loss -0.3920 (6.108 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4681 loss -0.4681 (6.246 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.5126 loss -0.5126 (6.384 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.5448 loss -0.5448 (6.207 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 93.51it/s]\n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 0.6685 loss -0.6685 (32.086 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.4834 loss -0.4834 (6.360 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.5254 loss -0.5254 (6.450 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4775 loss -0.4775 (6.285 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.6034 loss -0.6034 (6.297 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.6260 loss -0.6260 (6.459 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.5178 loss -0.5178 (6.372 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.5569 loss -0.5569 (6.428 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5462 loss -0.5462 (6.391 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.6658 loss -0.6658 (6.323 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.6730 loss -0.6730 (6.529 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.6163 loss -0.6163 (6.289 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5767 loss -0.5767 (6.125 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.6822 loss -0.6822 (6.162 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.6207 loss -0.6207 (6.058 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.6388 loss -0.6388 (6.314 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.6024 loss -0.6024 (6.120 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.6902 loss -0.6902 (6.077 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.6992 loss -0.6992 (6.295 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.6744 loss -0.6744 (6.040 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.6850 loss -0.6850 (6.189 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.6628 loss -0.6628 (6.090 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.7336 loss -0.7336 (6.039 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.7888 loss -0.7888 (6.172 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.6764 loss -0.6764 (6.130 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.6679 loss -0.6679 (6.275 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 94.37it/s] \n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 0.6786 loss -0.6786 (31.792 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.7383 loss -0.7383 (6.224 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.7642 loss -0.7642 (6.369 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.7836 loss -0.7836 (6.237 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.7115 loss -0.7115 (6.220 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.7271 loss -0.7271 (6.681 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.6951 loss -0.6951 (6.440 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.7704 loss -0.7704 (6.306 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.6318 loss -0.6318 (6.359 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.7424 loss -0.7424 (6.570 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.7396 loss -0.7396 (6.498 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.7067 loss -0.7067 (6.640 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.6830 loss -0.6830 (6.137 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.8290 loss -0.8290 (6.409 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.7171 loss -0.7171 (6.107 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.7588 loss -0.7588 (6.271 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.7663 loss -0.7663 (6.793 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.6647 loss -0.6647 (6.184 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.7808 loss -0.7808 (6.351 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.8040 loss -0.8040 (6.038 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.7266 loss -0.7266 (6.114 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.7001 loss -0.7001 (6.244 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.8448 loss -0.8448 (6.145 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.7036 loss -0.7036 (6.294 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.8315 loss -0.8315 (6.135 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.8679 loss -0.8679 (6.099 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 96.48it/s] \n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 0.8777 loss -0.8777 (31.099 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.8027 loss -0.8027 (6.397 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.8424 loss -0.8424 (6.081 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.8705 loss -0.8705 (6.259 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.8410 loss -0.8410 (6.291 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.8721 loss -0.8721 (6.253 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.7403 loss -0.7403 (6.389 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.7703 loss -0.7703 (6.193 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.8156 loss -0.8156 (6.257 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.7849 loss -0.7849 (6.031 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.8100 loss -0.8100 (5.879 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.9070 loss -0.9070 (6.373 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.8438 loss -0.8438 (6.081 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.8035 loss -0.8035 (6.013 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.8787 loss -0.8787 (6.173 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.8098 loss -0.8098 (5.963 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.8837 loss -0.8837 (6.299 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.8577 loss -0.8577 (6.123 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.8714 loss -0.8714 (6.164 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.8862 loss -0.8862 (6.184 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.7151 loss -0.7151 (6.119 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.7680 loss -0.7680 (6.652 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.8575 loss -0.8575 (6.338 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.9046 loss -0.9046 (6.715 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.8394 loss -0.8394 (6.770 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.8073 loss -0.8073 (7.018 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 94.22it/s] \n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 0.9220 loss -0.9220 (31.843 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.7319 loss -0.7319 (6.205 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.7909 loss -0.7909 (6.124 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.8623 loss -0.8623 (6.125 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.8450 loss -0.8450 (6.264 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.9008 loss -0.9008 (6.204 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.9302 loss -0.9302 (7.195 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.8499 loss -0.8499 (7.045 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.9151 loss -0.9151 (6.271 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.9115 loss -0.9115 (6.275 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.9139 loss -0.9139 (6.260 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.9643 loss -0.9643 (6.319 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.8580 loss -0.8580 (6.155 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.8664 loss -0.8664 (6.224 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.8753 loss -0.8753 (6.309 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.9477 loss -0.9477 (6.521 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.9469 loss -0.9469 (6.676 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.8663 loss -0.8663 (6.288 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.9223 loss -0.9223 (6.474 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.9996 loss -0.9996 (6.287 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.8744 loss -0.8744 (6.159 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.8601 loss -0.8601 (6.537 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.8968 loss -0.8968 (6.328 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.9045 loss -0.9045 (6.319 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.9235 loss -0.9235 (6.445 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.8774 loss -0.8774 (6.253 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 93.22it/s]\n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 0.9261 loss -0.9261 (32.181 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.9301 loss -0.9301 (6.627 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.9874 loss -0.9874 (6.464 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.9461 loss -0.9461 (6.449 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.8617 loss -0.8617 (6.516 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.9647 loss -0.9647 (6.177 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.8434 loss -0.8434 (6.434 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.8708 loss -0.8708 (6.181 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.9957 loss -0.9957 (6.150 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.9027 loss -0.9027 (6.353 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.9993 loss -0.9993 (6.182 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.9359 loss -0.9359 (6.299 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.9322 loss -0.9322 (6.153 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.9241 loss -0.9241 (6.323 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.9641 loss -0.9641 (6.238 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.9126 loss -0.9126 (6.185 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 28200 lr 4.081e-04 [train_loss] tar_ll 1.0193 loss -1.0193 (6.303 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.9582 loss -0.9582 (6.167 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.9452 loss -0.9452 (6.265 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 28800 lr 4.045e-04 [train_loss] tar_ll 1.0074 loss -1.0074 (6.288 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 29000 lr 4.032e-04 [train_loss] tar_ll 1.0136 loss -1.0136 (6.114 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.9543 loss -0.9543 (6.345 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.9172 loss -0.9172 (6.269 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.9700 loss -0.9700 (6.176 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.9932 loss -0.9932 (6.222 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.9383 loss -0.9383 (6.219 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 96.06it/s] \n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.0297 loss -1.0297 (31.234 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.9839 loss -0.9839 (6.348 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 30400 lr 3.944e-04 [train_loss] tar_ll 1.0368 loss -1.0368 (6.501 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 30600 lr 3.931e-04 [train_loss] tar_ll 1.0865 loss -1.0865 (6.278 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.9054 loss -0.9054 (6.296 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.9786 loss -0.9786 (6.245 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.9241 loss -0.9241 (6.296 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.9734 loss -0.9734 (6.442 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 31600 lr 3.866e-04 [train_loss] tar_ll 1.0037 loss -1.0037 (6.390 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.9235 loss -0.9235 (6.517 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.9405 loss -0.9405 (6.346 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.9689 loss -0.9689 (6.425 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.9411 loss -0.9411 (6.496 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 32600 lr 3.800e-04 [train_loss] tar_ll 1.0492 loss -1.0492 (6.186 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.9068 loss -0.9068 (6.442 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 33000 lr 3.773e-04 [train_loss] tar_ll 1.0565 loss -1.0565 (6.312 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 33200 lr 3.759e-04 [train_loss] tar_ll 1.1127 loss -1.1127 (6.255 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.9211 loss -0.9211 (6.539 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.9232 loss -0.9232 (6.462 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 33800 lr 3.718e-04 [train_loss] tar_ll 1.0625 loss -1.0625 (6.383 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.9261 loss -0.9261 (6.242 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.9012 loss -0.9012 (6.265 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 34400 lr 3.677e-04 [train_loss] tar_ll 1.0687 loss -1.0687 (6.356 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.8671 loss -0.8671 (6.309 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.9869 loss -0.9869 (6.353 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 35000 lr 3.635e-04 [train_loss] tar_ll 1.0082 loss -1.0082 (6.515 secs)\n",
      "100%|##########| 3000/3000 [00:30<00:00, 97.55it/s] \n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.0845 loss -1.0845 (30.757 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 35200 lr 3.621e-04 [train_loss] tar_ll 1.0227 loss -1.0227 (6.371 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 35400 lr 3.607e-04 [train_loss] tar_ll 1.0060 loss -1.0060 (6.388 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 35600 lr 3.593e-04 [train_loss] tar_ll 1.0094 loss -1.0094 (6.466 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.9950 loss -0.9950 (6.354 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 36000 lr 3.564e-04 [train_loss] tar_ll 1.0352 loss -1.0352 (6.251 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 36200 lr 3.550e-04 [train_loss] tar_ll 1.0381 loss -1.0381 (6.295 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 36400 lr 3.536e-04 [train_loss] tar_ll 1.0811 loss -1.0811 (6.250 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.9322 loss -0.9322 (6.060 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 36800 lr 3.507e-04 [train_loss] tar_ll 1.0379 loss -1.0379 (6.220 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.9985 loss -0.9985 (6.236 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.9682 loss -0.9682 (6.190 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 37400 lr 3.464e-04 [train_loss] tar_ll 1.0858 loss -1.0858 (6.230 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 37600 lr 3.449e-04 [train_loss] tar_ll 1.0236 loss -1.0236 (6.146 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 37800 lr 3.435e-04 [train_loss] tar_ll 1.0702 loss -1.0702 (6.270 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 38000 lr 3.420e-04 [train_loss] tar_ll 1.0000 loss -1.0000 (6.485 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 38200 lr 3.406e-04 [train_loss] tar_ll 1.0370 loss -1.0370 (6.245 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 38400 lr 3.391e-04 [train_loss] tar_ll 1.0053 loss -1.0053 (6.300 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 38600 lr 3.376e-04 [train_loss] tar_ll 1.0575 loss -1.0575 (6.228 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 38800 lr 3.362e-04 [train_loss] tar_ll 1.0504 loss -1.0504 (6.138 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 39000 lr 3.347e-04 [train_loss] tar_ll 1.1438 loss -1.1438 (6.272 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 39200 lr 3.332e-04 [train_loss] tar_ll 1.0267 loss -1.0267 (6.320 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 39400 lr 3.317e-04 [train_loss] tar_ll 1.1399 loss -1.1399 (6.214 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 39600 lr 3.302e-04 [train_loss] tar_ll 1.0687 loss -1.0687 (6.287 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 39800 lr 3.287e-04 [train_loss] tar_ll 1.1040 loss -1.1040 (6.267 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 40000 lr 3.273e-04 [train_loss] tar_ll 1.1434 loss -1.1434 (6.344 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.34it/s] \n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.0480 loss -1.0480 (34.748 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 40200 lr 3.258e-04 [train_loss] tar_ll 1.0823 loss -1.0823 (6.925 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.9918 loss -0.9918 (6.658 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 40600 lr 3.228e-04 [train_loss] tar_ll 1.0939 loss -1.0939 (6.924 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 40800 lr 3.213e-04 [train_loss] tar_ll 1.0077 loss -1.0077 (7.577 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 41000 lr 3.197e-04 [train_loss] tar_ll 1.0763 loss -1.0763 (6.515 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 41200 lr 3.182e-04 [train_loss] tar_ll 1.1320 loss -1.1320 (7.107 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 41400 lr 3.167e-04 [train_loss] tar_ll 1.0516 loss -1.0516 (6.711 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.9683 loss -0.9683 (6.949 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 41800 lr 3.137e-04 [train_loss] tar_ll 1.0553 loss -1.0553 (6.828 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 42000 lr 3.122e-04 [train_loss] tar_ll 1.0017 loss -1.0017 (6.824 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 42200 lr 3.106e-04 [train_loss] tar_ll 1.0563 loss -1.0563 (7.072 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 42400 lr 3.091e-04 [train_loss] tar_ll 1.1057 loss -1.1057 (6.773 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 42600 lr 3.076e-04 [train_loss] tar_ll 1.0904 loss -1.0904 (6.975 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 42800 lr 3.061e-04 [train_loss] tar_ll 1.0960 loss -1.0960 (7.295 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 43000 lr 3.045e-04 [train_loss] tar_ll 1.0797 loss -1.0797 (7.258 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 43200 lr 3.030e-04 [train_loss] tar_ll 1.0472 loss -1.0472 (6.838 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.9848 loss -0.9848 (6.956 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 43600 lr 2.999e-04 [train_loss] tar_ll 1.0717 loss -1.0717 (7.238 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 43800 lr 2.984e-04 [train_loss] tar_ll 1.0274 loss -1.0274 (6.623 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 44000 lr 2.968e-04 [train_loss] tar_ll 1.0002 loss -1.0002 (7.167 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 44200 lr 2.953e-04 [train_loss] tar_ll 1.0980 loss -1.0980 (7.152 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 44400 lr 2.938e-04 [train_loss] tar_ll 1.0311 loss -1.0311 (6.964 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 44600 lr 2.922e-04 [train_loss] tar_ll 1.1180 loss -1.1180 (6.907 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.9407 loss -0.9407 (7.216 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 45000 lr 2.891e-04 [train_loss] tar_ll 1.1611 loss -1.1611 (7.013 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.44it/s] \n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.1643 loss -1.1643 (33.170 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 45200 lr 2.876e-04 [train_loss] tar_ll 1.0852 loss -1.0852 (6.600 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 45400 lr 2.860e-04 [train_loss] tar_ll 1.1269 loss -1.1269 (6.462 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 45600 lr 2.844e-04 [train_loss] tar_ll 1.1070 loss -1.1070 (6.776 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 45800 lr 2.829e-04 [train_loss] tar_ll 1.1210 loss -1.1210 (6.617 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 46000 lr 2.813e-04 [train_loss] tar_ll 1.0371 loss -1.0371 (6.479 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 46200 lr 2.798e-04 [train_loss] tar_ll 1.1662 loss -1.1662 (6.978 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 46400 lr 2.782e-04 [train_loss] tar_ll 1.1013 loss -1.1013 (6.570 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 46600 lr 2.767e-04 [train_loss] tar_ll 1.0741 loss -1.0741 (6.481 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 46800 lr 2.751e-04 [train_loss] tar_ll 1.1652 loss -1.1652 (7.003 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 47000 lr 2.735e-04 [train_loss] tar_ll 1.0952 loss -1.0952 (6.444 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 47200 lr 2.720e-04 [train_loss] tar_ll 1.1381 loss -1.1381 (6.919 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 47400 lr 2.704e-04 [train_loss] tar_ll 1.0753 loss -1.0753 (7.575 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 47600 lr 2.688e-04 [train_loss] tar_ll 1.0875 loss -1.0875 (7.342 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 47800 lr 2.673e-04 [train_loss] tar_ll 1.1052 loss -1.1052 (6.930 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 48000 lr 2.657e-04 [train_loss] tar_ll 1.1812 loss -1.1812 (7.010 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 48200 lr 2.641e-04 [train_loss] tar_ll 1.1141 loss -1.1141 (7.133 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 48400 lr 2.626e-04 [train_loss] tar_ll 1.0733 loss -1.0733 (6.868 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 48600 lr 2.610e-04 [train_loss] tar_ll 1.0718 loss -1.0718 (7.054 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 48800 lr 2.594e-04 [train_loss] tar_ll 1.0647 loss -1.0647 (7.325 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 49000 lr 2.579e-04 [train_loss] tar_ll 1.0442 loss -1.0442 (6.706 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 49200 lr 2.563e-04 [train_loss] tar_ll 1.0979 loss -1.0979 (6.569 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 49400 lr 2.547e-04 [train_loss] tar_ll 1.0345 loss -1.0345 (6.961 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 49600 lr 2.531e-04 [train_loss] tar_ll 1.1633 loss -1.1633 (6.541 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 49800 lr 2.516e-04 [train_loss] tar_ll 1.1644 loss -1.1644 (6.807 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 50000 lr 2.500e-04 [train_loss] tar_ll 1.1187 loss -1.1187 (6.894 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.49it/s] \n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.2083 loss -1.2083 (32.792 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 50200 lr 2.484e-04 [train_loss] tar_ll 1.0467 loss -1.0467 (6.884 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 50400 lr 2.469e-04 [train_loss] tar_ll 1.0897 loss -1.0897 (6.803 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 50600 lr 2.453e-04 [train_loss] tar_ll 1.1243 loss -1.1243 (6.856 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 50800 lr 2.437e-04 [train_loss] tar_ll 1.1223 loss -1.1223 (6.935 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 51000 lr 2.421e-04 [train_loss] tar_ll 1.1208 loss -1.1208 (6.641 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 51200 lr 2.406e-04 [train_loss] tar_ll 1.1462 loss -1.1462 (6.929 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 51400 lr 2.390e-04 [train_loss] tar_ll 1.1078 loss -1.1078 (7.163 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 51600 lr 2.374e-04 [train_loss] tar_ll 1.0954 loss -1.0954 (6.637 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 51800 lr 2.359e-04 [train_loss] tar_ll 1.0365 loss -1.0365 (6.937 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 52000 lr 2.343e-04 [train_loss] tar_ll 1.1508 loss -1.1508 (7.143 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 52200 lr 2.327e-04 [train_loss] tar_ll 1.1263 loss -1.1263 (6.856 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 52400 lr 2.312e-04 [train_loss] tar_ll 1.1135 loss -1.1135 (6.900 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 52600 lr 2.296e-04 [train_loss] tar_ll 1.1784 loss -1.1784 (7.131 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 52800 lr 2.280e-04 [train_loss] tar_ll 1.1093 loss -1.1093 (6.919 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 53000 lr 2.265e-04 [train_loss] tar_ll 1.1050 loss -1.1050 (6.820 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 53200 lr 2.249e-04 [train_loss] tar_ll 1.1436 loss -1.1436 (7.280 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 53400 lr 2.233e-04 [train_loss] tar_ll 1.0931 loss -1.0931 (7.121 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 53600 lr 2.218e-04 [train_loss] tar_ll 1.1496 loss -1.1496 (6.692 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 53800 lr 2.202e-04 [train_loss] tar_ll 1.1311 loss -1.1311 (6.764 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 54000 lr 2.187e-04 [train_loss] tar_ll 1.1611 loss -1.1611 (6.611 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 54200 lr 2.171e-04 [train_loss] tar_ll 1.1790 loss -1.1790 (6.668 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 54400 lr 2.156e-04 [train_loss] tar_ll 1.1072 loss -1.1072 (6.758 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 54600 lr 2.140e-04 [train_loss] tar_ll 1.1657 loss -1.1657 (6.680 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 54800 lr 2.124e-04 [train_loss] tar_ll 1.1177 loss -1.1177 (6.378 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 55000 lr 2.109e-04 [train_loss] tar_ll 1.0827 loss -1.0827 (6.768 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 92.36it/s] \n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.0408 loss -1.0408 (32.486 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 55200 lr 2.093e-04 [train_loss] tar_ll 1.1755 loss -1.1755 (6.935 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 55400 lr 2.078e-04 [train_loss] tar_ll 1.1926 loss -1.1926 (6.442 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 55600 lr 2.062e-04 [train_loss] tar_ll 1.1940 loss -1.1940 (6.530 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 55800 lr 2.047e-04 [train_loss] tar_ll 1.2348 loss -1.2348 (6.964 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 56000 lr 2.032e-04 [train_loss] tar_ll 1.2019 loss -1.2019 (6.607 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 56200 lr 2.016e-04 [train_loss] tar_ll 1.1743 loss -1.1743 (6.671 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 56400 lr 2.001e-04 [train_loss] tar_ll 1.1806 loss -1.1806 (6.954 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 56600 lr 1.985e-04 [train_loss] tar_ll 1.1092 loss -1.1092 (6.563 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 56800 lr 1.970e-04 [train_loss] tar_ll 1.1450 loss -1.1450 (6.829 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 57000 lr 1.955e-04 [train_loss] tar_ll 1.1108 loss -1.1108 (6.994 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 57200 lr 1.939e-04 [train_loss] tar_ll 1.1838 loss -1.1838 (6.812 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 57400 lr 1.924e-04 [train_loss] tar_ll 1.2636 loss -1.2636 (6.797 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 57600 lr 1.909e-04 [train_loss] tar_ll 1.2421 loss -1.2421 (6.997 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 57800 lr 1.894e-04 [train_loss] tar_ll 1.1557 loss -1.1557 (6.705 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 58000 lr 1.878e-04 [train_loss] tar_ll 1.1609 loss -1.1609 (6.766 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 58200 lr 1.863e-04 [train_loss] tar_ll 1.1938 loss -1.1938 (6.957 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 58400 lr 1.848e-04 [train_loss] tar_ll 1.1611 loss -1.1611 (6.793 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 58600 lr 1.833e-04 [train_loss] tar_ll 1.1850 loss -1.1850 (6.924 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 58800 lr 1.818e-04 [train_loss] tar_ll 1.1716 loss -1.1716 (6.958 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.1483 loss -1.1483 (6.654 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 59200 lr 1.787e-04 [train_loss] tar_ll 1.2060 loss -1.2060 (6.552 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 59400 lr 1.772e-04 [train_loss] tar_ll 1.2028 loss -1.2028 (7.206 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 59600 lr 1.757e-04 [train_loss] tar_ll 1.2855 loss -1.2855 (7.230 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 59800 lr 1.742e-04 [train_loss] tar_ll 1.1945 loss -1.1945 (6.641 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 60000 lr 1.727e-04 [train_loss] tar_ll 1.1744 loss -1.1744 (6.881 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.92it/s]\n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.1991 loss -1.1991 (33.366 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 60200 lr 1.713e-04 [train_loss] tar_ll 1.1641 loss -1.1641 (7.266 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 60400 lr 1.698e-04 [train_loss] tar_ll 1.2342 loss -1.2342 (7.038 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.1760 loss -1.1760 (6.887 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.1642 loss -1.1642 (6.988 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 61000 lr 1.653e-04 [train_loss] tar_ll 1.2981 loss -1.2981 (7.224 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 61200 lr 1.638e-04 [train_loss] tar_ll 1.2734 loss -1.2734 (6.868 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 61400 lr 1.624e-04 [train_loss] tar_ll 1.1803 loss -1.1803 (6.873 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 61600 lr 1.609e-04 [train_loss] tar_ll 1.2623 loss -1.2623 (7.257 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.2054 loss -1.2054 (6.898 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 62000 lr 1.580e-04 [train_loss] tar_ll 1.2559 loss -1.2559 (7.023 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 62200 lr 1.565e-04 [train_loss] tar_ll 1.2503 loss -1.2503 (7.087 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.1912 loss -1.1912 (6.697 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.2085 loss -1.2085 (6.622 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.1855 loss -1.1855 (6.832 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.2653 loss -1.2653 (6.692 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.1533 loss -1.1533 (6.635 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.2183 loss -1.2183 (6.823 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.2203 loss -1.2203 (6.487 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.1886 loss -1.1886 (6.605 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.1341 loss -1.1341 (6.852 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.1043 loss -1.1043 (6.628 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.1547 loss -1.1547 (6.735 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.3287 loss -1.3287 (6.870 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.2491 loss -1.2491 (6.651 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.1610 loss -1.1610 (6.604 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.22it/s]\n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.2745 loss -1.2745 (34.398 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.2512 loss -1.2512 (6.539 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.2347 loss -1.2347 (6.975 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.2512 loss -1.2512 (6.937 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.2978 loss -1.2978 (6.636 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.1630 loss -1.1630 (7.330 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.2860 loss -1.2860 (7.065 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.2650 loss -1.2650 (6.422 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.1580 loss -1.1580 (7.061 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.1275 loss -1.1275 (7.043 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.2582 loss -1.2582 (6.878 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.2637 loss -1.2637 (6.875 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.2700 loss -1.2700 (6.732 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.2117 loss -1.2117 (6.765 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.3156 loss -1.3156 (6.820 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.3013 loss -1.3013 (6.924 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.2488 loss -1.2488 (6.460 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.1565 loss -1.1565 (6.724 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.2386 loss -1.2386 (6.830 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.1640 loss -1.1640 (6.483 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.2654 loss -1.2654 (7.102 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.2957 loss -1.2957 (7.025 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.2291 loss -1.2291 (6.677 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.2440 loss -1.2440 (6.889 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.2770 loss -1.2770 (7.152 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.2240 loss -1.2240 (6.649 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.80it/s]\n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.2889 loss -1.2889 (34.173 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.2337 loss -1.2337 (7.095 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.2620 loss -1.2620 (6.952 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.2614 loss -1.2614 (7.205 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.2875 loss -1.2875 (6.832 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.2064 loss -1.2064 (7.019 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.2980 loss -1.2980 (7.106 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.2454 loss -1.2454 (6.723 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.2329 loss -1.2329 (6.437 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.2617 loss -1.2617 (6.927 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.2504 loss -1.2504 (6.593 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.2805 loss -1.2805 (6.568 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.4099 loss -1.4099 (6.873 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.3451 loss -1.3451 (6.812 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.2841 loss -1.2841 (6.496 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.2431 loss -1.2431 (6.967 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.2680 loss -1.2680 (6.735 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.3607 loss -1.3607 (6.510 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.3283 loss -1.3283 (7.014 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.3525 loss -1.3525 (6.621 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.2333 loss -1.2333 (6.652 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.2605 loss -1.2605 (6.925 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.2834 loss -1.2834 (6.521 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.2407 loss -1.2407 (6.574 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.2902 loss -1.2902 (6.863 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.3213 loss -1.3213 (6.628 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.37it/s]\n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.3061 loss -1.3061 (33.571 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.3405 loss -1.3405 (6.844 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.1869 loss -1.1869 (6.932 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.2906 loss -1.2906 (7.054 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.2901 loss -1.2901 (7.007 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.2944 loss -1.2944 (6.452 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.3233 loss -1.3233 (6.971 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.2657 loss -1.2657 (6.791 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.3265 loss -1.3265 (6.437 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.3221 loss -1.3221 (6.983 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.2867 loss -1.2867 (6.658 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.3645 loss -1.3645 (6.788 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.2560 loss -1.2560 (7.116 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.2267 loss -1.2267 (6.910 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2328 loss -1.2328 (6.860 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.2781 loss -1.2781 (7.051 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.2654 loss -1.2654 (7.185 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.2908 loss -1.2908 (6.583 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.2545 loss -1.2545 (7.041 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.3537 loss -1.3537 (7.168 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.3493 loss -1.3493 (6.531 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.2324 loss -1.2324 (6.853 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.3210 loss -1.3210 (6.810 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.2531 loss -1.2531 (6.429 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.2944 loss -1.2944 (6.705 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.3014 loss -1.3014 (6.790 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.24it/s] \n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.3330 loss -1.3330 (32.884 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.2553 loss -1.2553 (6.727 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.3599 loss -1.3599 (6.349 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.2574 loss -1.2574 (6.873 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2876 loss -1.2876 (6.659 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.3078 loss -1.3078 (6.528 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.3108 loss -1.3108 (6.862 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.2692 loss -1.2692 (6.624 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.3394 loss -1.3394 (6.420 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2423 loss -1.2423 (6.711 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.3021 loss -1.3021 (6.684 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.4194 loss -1.4194 (6.477 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.3142 loss -1.3142 (6.827 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.3075 loss -1.3075 (6.541 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.3142 loss -1.3142 (6.724 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.3374 loss -1.3374 (6.912 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.2761 loss -1.2761 (6.553 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.2322 loss -1.2322 (6.663 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.3104 loss -1.3104 (6.881 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.2945 loss -1.2945 (6.711 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.3740 loss -1.3740 (6.693 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.3091 loss -1.3091 (6.943 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.3898 loss -1.3898 (6.660 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.3555 loss -1.3555 (6.608 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.2947 loss -1.2947 (7.055 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.3126 loss -1.3126 (6.664 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.39it/s]\n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.3426 loss -1.3426 (33.192 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.1614 loss -1.1614 (6.807 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.3835 loss -1.3835 (6.651 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.3008 loss -1.3008 (6.816 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.3089 loss -1.3089 (6.629 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.3029 loss -1.3029 (6.645 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.3515 loss -1.3515 (7.122 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.2568 loss -1.2568 (6.878 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.4239 loss -1.4239 (6.924 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.2373 loss -1.2373 (7.083 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.3247 loss -1.3247 (6.807 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.3233 loss -1.3233 (6.761 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.3632 loss -1.3632 (7.133 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.3798 loss -1.3798 (7.108 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.3592 loss -1.3592 (6.647 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.3359 loss -1.3359 (7.135 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.3590 loss -1.3590 (7.002 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.2995 loss -1.2995 (6.778 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.3682 loss -1.3682 (7.112 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.3912 loss -1.3912 (7.051 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.2137 loss -1.2137 (7.221 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.3149 loss -1.3149 (7.754 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.3164 loss -1.3164 (7.607 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.3988 loss -1.3988 (7.061 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2614 loss -1.2614 (6.574 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.3286 loss -1.3286 (6.938 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.88it/s] \n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.3550 loss -1.3550 (33.013 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.2905 loss -1.2905 (7.055 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.3007 loss -1.3007 (6.947 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.3710 loss -1.3710 (7.075 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.3852 loss -1.3852 (7.202 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.3643 loss -1.3643 (7.418 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2798 loss -1.2798 (6.951 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.3143 loss -1.3143 (6.657 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.3622 loss -1.3622 (6.918 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.3356 loss -1.3356 (6.487 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.4096 loss -1.4096 (6.671 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.3942 loss -1.3942 (7.093 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.3779 loss -1.3779 (6.656 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.3987 loss -1.3987 (6.750 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.3619 loss -1.3619 (6.970 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.2868 loss -1.2868 (6.610 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.3534 loss -1.3534 (6.886 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.3574 loss -1.3574 (6.983 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.3772 loss -1.3772 (6.675 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.3484 loss -1.3484 (6.839 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.3450 loss -1.3450 (7.062 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.3108 loss -1.3108 (6.662 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.3743 loss -1.3743 (6.846 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.4088 loss -1.4088 (6.965 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.4029 loss -1.4029 (6.619 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.3419 loss -1.3419 (6.642 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.41it/s] \n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.3601 loss -1.3601 (33.553 secs)\n",
      "\n",
      "isanp:isanp-num_latents-16_rbf_100 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.3193 loss -1.3193 (6.774 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.3607 loss -1.3607 (7.209 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.2934 loss -1.2934 (6.820 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.2670 loss -1.2670 (6.865 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.4377 loss -1.4377 (7.123 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2176 loss -1.2176 (7.174 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.3545 loss -1.3545 (6.693 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.3874 loss -1.3874 (7.210 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.3456 loss -1.3456 (7.112 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.3251 loss -1.3251 (6.751 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.3397 loss -1.3397 (7.149 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.3263 loss -1.3263 (6.684 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.2910 loss -1.2910 (6.481 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.3927 loss -1.3927 (6.590 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.3546 loss -1.3546 (6.774 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.2688 loss -1.2688 (6.582 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.3621 loss -1.3621 (6.708 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.3552 loss -1.3552 (6.752 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2726 loss -1.2726 (7.004 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.3281 loss -1.3281 (6.964 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.3809 loss -1.3809 (7.193 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.4427 loss -1.4427 (7.976 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.3853 loss -1.3853 (8.015 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.3168 loss -1.3168 (7.705 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.2975 loss -1.2975 (7.682 secs)\n",
      "100%|##########| 3000/3000 [00:39<00:00, 75.60it/s]\n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.3619 loss -1.3619 (39.687 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:39<00:00, 75.44it/s]\n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.3619 loss -1.3619 (39.765 secs)\n",
      "isanp:isanp-num_latents-16_rbf_100 rbf tar_ll 1.3619 loss -1.3619 (39.765 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4289854.0 miliseconds\n",
      "Execution time: 4289.854 seconds\n",
      "Initial Memory Usage: 0.0 MB\n",
      "Final Memory Usage: 55.28955078125 MB\n",
      "Memory Usage Change: 55.28955078125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='isanp-num_latents-16_rbf_100',val_seed=100, val_l=16,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b934f2e9-1bec-45f7-9665-93595c89eb14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 16\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:08<00:00, 43.73it/s]\n",
      "isanp:isanp-num_latents-16_rbf_0 rbf tar_ll 1.3519 loss -1.3519 (68.602 secs)\n"
     ]
    }
   ],
   "source": [
    "%run gp.py --mode eval --expid isanp-num_latents-16_rbf_0 --model isanp --num_latents 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800afe27-bc68-4b1c-a7de-38f75babcc62",
   "metadata": {},
   "source": [
    "## ISANP (32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84998de7-17bf-4aec-a399-33d4139d7649",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: isanp-isanp-num_latents-32_rbf_0\n",
      "Total number of parameters: 787138\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 32\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-32_rbf_0 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6988 loss 0.6988 (25.330 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 400 lr 5.000e-04 [train_loss] tar_ll -0.6509 loss 0.6509 (22.948 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 600 lr 5.000e-04 [train_loss] tar_ll -0.5752 loss 0.5752 (21.765 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 800 lr 4.999e-04 [train_loss] tar_ll -0.5500 loss 0.5500 (9.720 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 1000 lr 4.999e-04 [train_loss] tar_ll -0.4703 loss 0.4703 (6.265 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.3950 loss 0.3950 (6.480 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 1400 lr 4.998e-04 [train_loss] tar_ll -0.2139 loss 0.2139 (6.333 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 1600 lr 4.997e-04 [train_loss] tar_ll -0.1851 loss 0.1851 (6.652 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 1800 lr 4.996e-04 [train_loss] tar_ll -0.0895 loss 0.0895 (6.985 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 2000 lr 4.995e-04 [train_loss] tar_ll 0.0013 loss -0.0013 (7.270 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.1310 loss -0.1310 (7.100 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.1530 loss -0.1530 (6.500 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1661 loss -0.1661 (6.552 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.2583 loss -0.2583 (6.458 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.2135 loss -0.2135 (6.420 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.2067 loss -0.2067 (6.345 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.2871 loss -0.2871 (6.588 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2002 loss -0.2002 (6.423 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.3028 loss -0.3028 (6.364 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.3402 loss -0.3402 (6.600 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3608 loss -0.3608 (6.570 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.2415 loss -0.2415 (6.435 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4426 loss -0.4426 (6.605 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4349 loss -0.4349 (6.470 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.1929 loss -0.1929 (6.555 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 78.38it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 0.8149 loss -0.8149 (0.015 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.3559 loss -0.3559 (6.760 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.3594 loss -0.3594 (6.510 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4545 loss -0.4545 (6.605 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.4763 loss -0.4763 (6.510 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4833 loss -0.4833 (6.480 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.5388 loss -0.5388 (6.555 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.5785 loss -0.5785 (6.940 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5322 loss -0.5322 (7.085 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.5859 loss -0.5859 (6.650 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.6227 loss -0.6227 (6.550 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.5113 loss -0.5113 (6.610 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.6012 loss -0.6012 (6.520 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.6052 loss -0.6052 (6.469 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.5724 loss -0.5724 (6.625 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.6889 loss -0.6889 (6.680 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.6178 loss -0.6178 (6.665 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.5373 loss -0.5373 (6.700 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.6639 loss -0.6639 (6.625 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.7295 loss -0.7295 (6.753 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.7015 loss -0.7015 (6.657 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.7412 loss -0.7412 (6.829 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.7397 loss -0.7397 (6.805 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.7193 loss -0.7193 (6.754 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.6892 loss -0.6892 (6.960 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.7629 loss -0.7629 (6.710 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 60.74it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 1.0162 loss -1.0162 (0.022 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.6959 loss -0.6959 (6.922 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.7987 loss -0.7987 (6.545 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.7388 loss -0.7388 (6.550 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.7154 loss -0.7154 (6.655 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.7563 loss -0.7563 (6.498 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.6677 loss -0.6677 (6.752 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.7572 loss -0.7572 (6.553 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.6999 loss -0.6999 (6.514 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.6714 loss -0.6714 (6.799 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.7850 loss -0.7850 (6.750 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.7303 loss -0.7303 (7.005 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.8196 loss -0.8196 (7.139 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.7340 loss -0.7340 (7.255 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.7822 loss -0.7822 (7.095 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.7391 loss -0.7391 (6.796 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.8480 loss -0.8480 (6.940 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.8334 loss -0.8334 (6.770 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.8724 loss -0.8724 (6.865 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.7492 loss -0.7492 (6.762 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.7625 loss -0.7625 (7.046 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.7769 loss -0.7769 (6.994 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.8120 loss -0.8120 (6.542 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.8180 loss -0.8180 (6.585 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.8383 loss -0.8383 (6.690 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.8036 loss -0.8036 (6.740 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 69.97it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 1.4615 loss -1.4615 (0.021 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.7739 loss -0.7739 (6.813 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.7605 loss -0.7605 (6.640 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.8128 loss -0.8128 (6.650 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.9066 loss -0.9066 (6.590 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.8601 loss -0.8601 (6.664 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.8204 loss -0.8204 (6.765 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.8574 loss -0.8574 (6.691 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.7958 loss -0.7958 (6.570 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.6361 loss -0.6361 (6.590 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.5149 loss -0.5149 (6.500 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.7575 loss -0.7575 (6.740 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.8257 loss -0.8257 (6.465 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.8681 loss -0.8681 (6.406 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.8211 loss -0.8211 (6.690 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.8779 loss -0.8779 (6.450 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.8717 loss -0.8717 (6.540 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.7745 loss -0.7745 (6.890 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.7045 loss -0.7045 (6.670 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.8621 loss -0.8621 (6.751 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.9247 loss -0.9247 (6.760 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.8584 loss -0.8584 (7.009 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.9480 loss -0.9480 (6.710 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.8855 loss -0.8855 (6.800 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.8266 loss -0.8266 (6.810 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.8276 loss -0.8276 (6.810 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 85.75it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 1.1012 loss -1.1012 (0.014 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.8967 loss -0.8967 (7.050 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.9175 loss -0.9175 (6.915 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.7540 loss -0.7540 (6.910 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.8895 loss -0.8895 (6.880 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.9316 loss -0.9316 (6.560 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.9226 loss -0.9226 (6.785 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.9576 loss -0.9576 (6.665 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.9593 loss -0.9593 (6.592 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.9109 loss -0.9109 (6.577 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 22000 lr 4.426e-04 [train_loss] tar_ll 1.0023 loss -1.0023 (6.570 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.9231 loss -0.9231 (6.730 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.9160 loss -0.9160 (6.770 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.7889 loss -0.7889 (6.930 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.8950 loss -0.8950 (6.770 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.8582 loss -0.8582 (6.695 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.9014 loss -0.9014 (6.947 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.9363 loss -0.9363 (6.739 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.9407 loss -0.9407 (6.913 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.9726 loss -0.9726 (6.646 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.9175 loss -0.9175 (6.660 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.9177 loss -0.9177 (6.925 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.8341 loss -0.8341 (6.695 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.9594 loss -0.9594 (6.503 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.9090 loss -0.9090 (6.586 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.9085 loss -0.9085 (6.570 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 100.56it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 1.8029 loss -1.8029 (0.015 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.9824 loss -0.9824 (6.569 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.9813 loss -0.9813 (6.665 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.8638 loss -0.8638 (6.650 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.8906 loss -0.8906 (6.595 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.8999 loss -0.8999 (6.595 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 26200 lr 4.200e-04 [train_loss] tar_ll 1.0073 loss -1.0073 (6.730 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.9538 loss -0.9538 (6.830 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.9439 loss -0.9439 (6.685 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.9112 loss -0.9112 (6.670 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 27000 lr 4.153e-04 [train_loss] tar_ll 1.0016 loss -1.0016 (6.690 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.9134 loss -0.9134 (6.730 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.9723 loss -0.9723 (6.625 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 27600 lr 4.118e-04 [train_loss] tar_ll 1.0403 loss -1.0403 (6.605 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.8916 loss -0.8916 (6.740 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.9252 loss -0.9252 (6.550 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.9645 loss -0.9645 (6.625 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.8801 loss -0.8801 (6.495 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 28600 lr 4.057e-04 [train_loss] tar_ll 1.0100 loss -1.0100 (6.466 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.9868 loss -0.9868 (6.534 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.9670 loss -0.9670 (6.640 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 29200 lr 4.020e-04 [train_loss] tar_ll 1.0041 loss -1.0041 (6.739 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.9919 loss -0.9919 (6.758 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 29600 lr 3.995e-04 [train_loss] tar_ll 1.0204 loss -1.0204 (6.702 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 29800 lr 3.982e-04 [train_loss] tar_ll 1.0419 loss -1.0419 (6.875 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 30000 lr 3.969e-04 [train_loss] tar_ll 1.0630 loss -1.0630 (6.861 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 81.60it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 1.7906 loss -1.7906 (0.015 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.9779 loss -0.9779 (6.930 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.8967 loss -0.8967 (6.759 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.8967 loss -0.8967 (6.845 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.9818 loss -0.9818 (6.918 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 31000 lr 3.905e-04 [train_loss] tar_ll 1.0189 loss -1.0189 (6.952 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.8987 loss -0.8987 (7.153 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.9960 loss -0.9960 (7.229 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 31600 lr 3.866e-04 [train_loss] tar_ll 1.0230 loss -1.0230 (6.955 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.9492 loss -0.9492 (6.908 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.9580 loss -0.9580 (7.600 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 32200 lr 3.826e-04 [train_loss] tar_ll 1.0631 loss -1.0631 (7.567 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.9569 loss -0.9569 (6.750 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 32600 lr 3.800e-04 [train_loss] tar_ll 1.0556 loss -1.0556 (6.655 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 32800 lr 3.786e-04 [train_loss] tar_ll 1.0486 loss -1.0486 (6.610 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.9701 loss -0.9701 (6.730 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.9043 loss -0.9043 (6.805 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 33400 lr 3.745e-04 [train_loss] tar_ll 1.0072 loss -1.0072 (6.675 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.9606 loss -0.9606 (6.660 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.9576 loss -0.9576 (6.640 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 34000 lr 3.704e-04 [train_loss] tar_ll 1.0263 loss -1.0263 (6.695 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 34200 lr 3.691e-04 [train_loss] tar_ll 1.0264 loss -1.0264 (6.695 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.9645 loss -0.9645 (6.730 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 34600 lr 3.663e-04 [train_loss] tar_ll 1.0547 loss -1.0547 (6.643 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 34800 lr 3.649e-04 [train_loss] tar_ll 1.0444 loss -1.0444 (6.662 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 35000 lr 3.635e-04 [train_loss] tar_ll 1.0319 loss -1.0319 (6.650 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 76.15it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 1.6867 loss -1.6867 (0.014 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 35200 lr 3.621e-04 [train_loss] tar_ll 1.0696 loss -1.0696 (6.638 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 35400 lr 3.607e-04 [train_loss] tar_ll 1.0372 loss -1.0372 (6.660 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 35600 lr 3.593e-04 [train_loss] tar_ll 1.0495 loss -1.0495 (6.645 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 35800 lr 3.579e-04 [train_loss] tar_ll 1.0039 loss -1.0039 (6.560 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.9297 loss -0.9297 (6.715 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.9853 loss -0.9853 (6.715 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.9964 loss -0.9964 (6.670 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 36600 lr 3.522e-04 [train_loss] tar_ll 1.0636 loss -1.0636 (6.690 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 36800 lr 3.507e-04 [train_loss] tar_ll 1.0843 loss -1.0843 (6.700 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 37000 lr 3.493e-04 [train_loss] tar_ll 1.0649 loss -1.0649 (6.770 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 37200 lr 3.478e-04 [train_loss] tar_ll 1.0356 loss -1.0356 (6.884 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 37400 lr 3.464e-04 [train_loss] tar_ll 1.0168 loss -1.0168 (6.586 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 37600 lr 3.449e-04 [train_loss] tar_ll 1.0906 loss -1.0906 (6.595 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 37800 lr 3.435e-04 [train_loss] tar_ll 1.0661 loss -1.0661 (6.681 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 38000 lr 3.420e-04 [train_loss] tar_ll 1.0569 loss -1.0569 (6.658 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 38200 lr 3.406e-04 [train_loss] tar_ll 1.0583 loss -1.0583 (6.706 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.9623 loss -0.9623 (6.665 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 38600 lr 3.376e-04 [train_loss] tar_ll 1.1379 loss -1.1379 (6.600 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.9668 loss -0.9668 (6.750 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 39000 lr 3.347e-04 [train_loss] tar_ll 1.0089 loss -1.0089 (6.593 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 39200 lr 3.332e-04 [train_loss] tar_ll 1.0297 loss -1.0297 (6.704 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 39400 lr 3.317e-04 [train_loss] tar_ll 1.0268 loss -1.0268 (6.571 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 39600 lr 3.302e-04 [train_loss] tar_ll 1.0484 loss -1.0484 (6.555 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 39800 lr 3.287e-04 [train_loss] tar_ll 1.1045 loss -1.1045 (6.795 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 40000 lr 3.273e-04 [train_loss] tar_ll 1.0394 loss -1.0394 (6.790 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 58.29it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 1.5288 loss -1.5288 (0.020 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 40200 lr 3.258e-04 [train_loss] tar_ll 1.0343 loss -1.0343 (6.904 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 40400 lr 3.243e-04 [train_loss] tar_ll 1.0501 loss -1.0501 (6.630 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 40600 lr 3.228e-04 [train_loss] tar_ll 1.0674 loss -1.0674 (6.930 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 40800 lr 3.213e-04 [train_loss] tar_ll 1.1553 loss -1.1553 (6.811 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 41000 lr 3.197e-04 [train_loss] tar_ll 1.1148 loss -1.1148 (6.829 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 41200 lr 3.182e-04 [train_loss] tar_ll 1.1681 loss -1.1681 (6.940 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 41400 lr 3.167e-04 [train_loss] tar_ll 1.0771 loss -1.0771 (6.810 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 41600 lr 3.152e-04 [train_loss] tar_ll 1.0844 loss -1.0844 (6.915 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 41800 lr 3.137e-04 [train_loss] tar_ll 1.1727 loss -1.1727 (6.905 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 42000 lr 3.122e-04 [train_loss] tar_ll 1.1533 loss -1.1533 (7.091 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 42200 lr 3.106e-04 [train_loss] tar_ll 1.0660 loss -1.0660 (7.273 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 42400 lr 3.091e-04 [train_loss] tar_ll 1.0679 loss -1.0679 (6.673 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 42600 lr 3.076e-04 [train_loss] tar_ll 1.1780 loss -1.1780 (6.767 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 42800 lr 3.061e-04 [train_loss] tar_ll 1.0601 loss -1.0601 (6.670 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 43000 lr 3.045e-04 [train_loss] tar_ll 1.1270 loss -1.1270 (6.760 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 43200 lr 3.030e-04 [train_loss] tar_ll 1.1150 loss -1.1150 (6.990 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 43400 lr 3.015e-04 [train_loss] tar_ll 1.0956 loss -1.0956 (6.615 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 43600 lr 2.999e-04 [train_loss] tar_ll 1.1598 loss -1.1598 (6.630 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 43800 lr 2.984e-04 [train_loss] tar_ll 1.0790 loss -1.0790 (7.004 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 44000 lr 2.968e-04 [train_loss] tar_ll 1.0208 loss -1.0208 (7.066 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 44200 lr 2.953e-04 [train_loss] tar_ll 1.1591 loss -1.1591 (7.355 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 44400 lr 2.938e-04 [train_loss] tar_ll 1.0807 loss -1.0807 (7.314 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 44600 lr 2.922e-04 [train_loss] tar_ll 1.0377 loss -1.0377 (7.590 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 44800 lr 2.907e-04 [train_loss] tar_ll 1.0656 loss -1.0656 (7.080 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.9564 loss -0.9564 (6.820 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 88.70it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 1.7728 loss -1.7728 (0.015 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 45200 lr 2.876e-04 [train_loss] tar_ll 1.2390 loss -1.2390 (6.800 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 45400 lr 2.860e-04 [train_loss] tar_ll 1.1147 loss -1.1147 (6.700 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 45600 lr 2.844e-04 [train_loss] tar_ll 1.0213 loss -1.0213 (6.790 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 45800 lr 2.829e-04 [train_loss] tar_ll 1.0834 loss -1.0834 (6.620 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 46000 lr 2.813e-04 [train_loss] tar_ll 1.1079 loss -1.1079 (6.486 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 46200 lr 2.798e-04 [train_loss] tar_ll 1.1288 loss -1.1288 (6.680 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 46400 lr 2.782e-04 [train_loss] tar_ll 1.0431 loss -1.0431 (6.570 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 46600 lr 2.767e-04 [train_loss] tar_ll 1.0790 loss -1.0790 (6.610 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 46800 lr 2.751e-04 [train_loss] tar_ll 1.0620 loss -1.0620 (6.820 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 47000 lr 2.735e-04 [train_loss] tar_ll 1.1303 loss -1.1303 (6.775 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 47200 lr 2.720e-04 [train_loss] tar_ll 1.1316 loss -1.1316 (6.905 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 47400 lr 2.704e-04 [train_loss] tar_ll 1.1004 loss -1.1004 (6.710 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 47600 lr 2.688e-04 [train_loss] tar_ll 1.1616 loss -1.1616 (6.720 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 47800 lr 2.673e-04 [train_loss] tar_ll 1.0861 loss -1.0861 (6.815 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 48000 lr 2.657e-04 [train_loss] tar_ll 1.1332 loss -1.1332 (6.700 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 48200 lr 2.641e-04 [train_loss] tar_ll 1.1688 loss -1.1688 (6.785 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 48400 lr 2.626e-04 [train_loss] tar_ll 1.1608 loss -1.1608 (6.690 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 48600 lr 2.610e-04 [train_loss] tar_ll 1.0774 loss -1.0774 (6.680 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 48800 lr 2.594e-04 [train_loss] tar_ll 1.1745 loss -1.1745 (6.565 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 49000 lr 2.579e-04 [train_loss] tar_ll 1.1018 loss -1.1018 (6.700 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 49200 lr 2.563e-04 [train_loss] tar_ll 1.1504 loss -1.1504 (6.740 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 49400 lr 2.547e-04 [train_loss] tar_ll 1.1838 loss -1.1838 (6.560 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 49600 lr 2.531e-04 [train_loss] tar_ll 1.2291 loss -1.2291 (6.660 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 49800 lr 2.516e-04 [train_loss] tar_ll 1.1291 loss -1.1291 (6.550 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 50000 lr 2.500e-04 [train_loss] tar_ll 1.1923 loss -1.1923 (6.550 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 91.91it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 2.0790 loss -2.0790 (0.016 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 50200 lr 2.484e-04 [train_loss] tar_ll 1.1481 loss -1.1481 (6.889 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 50400 lr 2.469e-04 [train_loss] tar_ll 1.1199 loss -1.1199 (6.820 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 50600 lr 2.453e-04 [train_loss] tar_ll 1.1014 loss -1.1014 (6.840 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 50800 lr 2.437e-04 [train_loss] tar_ll 1.1300 loss -1.1300 (6.825 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 51000 lr 2.421e-04 [train_loss] tar_ll 1.0687 loss -1.0687 (6.850 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 51200 lr 2.406e-04 [train_loss] tar_ll 1.2111 loss -1.2111 (6.935 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 51400 lr 2.390e-04 [train_loss] tar_ll 1.2045 loss -1.2045 (6.955 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 51600 lr 2.374e-04 [train_loss] tar_ll 1.1896 loss -1.1896 (6.750 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 51800 lr 2.359e-04 [train_loss] tar_ll 1.1805 loss -1.1805 (6.950 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 52000 lr 2.343e-04 [train_loss] tar_ll 1.1262 loss -1.1262 (6.940 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 52200 lr 2.327e-04 [train_loss] tar_ll 1.1407 loss -1.1407 (7.010 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 52400 lr 2.312e-04 [train_loss] tar_ll 1.1311 loss -1.1311 (6.825 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 52600 lr 2.296e-04 [train_loss] tar_ll 1.1036 loss -1.1036 (6.760 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 52800 lr 2.280e-04 [train_loss] tar_ll 1.1404 loss -1.1404 (6.850 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 53000 lr 2.265e-04 [train_loss] tar_ll 1.2164 loss -1.2164 (6.490 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 53200 lr 2.249e-04 [train_loss] tar_ll 1.2025 loss -1.2025 (6.510 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 53400 lr 2.233e-04 [train_loss] tar_ll 1.1394 loss -1.1394 (6.615 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 53600 lr 2.218e-04 [train_loss] tar_ll 1.2086 loss -1.2086 (6.585 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 53800 lr 2.202e-04 [train_loss] tar_ll 1.2015 loss -1.2015 (6.605 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 54000 lr 2.187e-04 [train_loss] tar_ll 1.2050 loss -1.2050 (6.630 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 54200 lr 2.171e-04 [train_loss] tar_ll 1.1286 loss -1.1286 (6.758 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 54400 lr 2.156e-04 [train_loss] tar_ll 1.1839 loss -1.1839 (6.830 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 54600 lr 2.140e-04 [train_loss] tar_ll 1.2077 loss -1.2077 (6.820 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 54800 lr 2.124e-04 [train_loss] tar_ll 1.1796 loss -1.1796 (6.609 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 55000 lr 2.109e-04 [train_loss] tar_ll 1.1768 loss -1.1768 (6.740 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 80.65it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 2.0976 loss -2.0976 (0.014 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 55200 lr 2.093e-04 [train_loss] tar_ll 1.1748 loss -1.1748 (6.881 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 55400 lr 2.078e-04 [train_loss] tar_ll 1.2025 loss -1.2025 (6.625 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 55600 lr 2.062e-04 [train_loss] tar_ll 1.2186 loss -1.2186 (6.675 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 55800 lr 2.047e-04 [train_loss] tar_ll 1.2073 loss -1.2073 (6.614 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 56000 lr 2.032e-04 [train_loss] tar_ll 1.2184 loss -1.2184 (6.645 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 56200 lr 2.016e-04 [train_loss] tar_ll 1.1545 loss -1.1545 (6.845 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 56400 lr 2.001e-04 [train_loss] tar_ll 1.2113 loss -1.2113 (6.615 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 56600 lr 1.985e-04 [train_loss] tar_ll 1.2120 loss -1.2120 (6.525 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0887 loss -1.0887 (6.640 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 57000 lr 1.955e-04 [train_loss] tar_ll 1.1012 loss -1.1012 (6.590 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 57200 lr 1.939e-04 [train_loss] tar_ll 1.2475 loss -1.2475 (7.000 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 57400 lr 1.924e-04 [train_loss] tar_ll 1.1891 loss -1.1891 (6.855 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 57600 lr 1.909e-04 [train_loss] tar_ll 1.1574 loss -1.1574 (7.000 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 57800 lr 1.894e-04 [train_loss] tar_ll 1.2946 loss -1.2946 (6.890 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 58000 lr 1.878e-04 [train_loss] tar_ll 1.2108 loss -1.2108 (6.900 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 58200 lr 1.863e-04 [train_loss] tar_ll 1.2004 loss -1.2004 (6.900 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 58400 lr 1.848e-04 [train_loss] tar_ll 1.2368 loss -1.2368 (6.890 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 58600 lr 1.833e-04 [train_loss] tar_ll 1.1942 loss -1.1942 (6.915 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 58800 lr 1.818e-04 [train_loss] tar_ll 1.2397 loss -1.2397 (6.782 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.2457 loss -1.2457 (6.770 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 59200 lr 1.787e-04 [train_loss] tar_ll 1.1320 loss -1.1320 (6.887 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 59400 lr 1.772e-04 [train_loss] tar_ll 1.1357 loss -1.1357 (6.693 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 59600 lr 1.757e-04 [train_loss] tar_ll 1.1924 loss -1.1924 (6.727 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 59800 lr 1.742e-04 [train_loss] tar_ll 1.2153 loss -1.2153 (6.675 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 60000 lr 1.727e-04 [train_loss] tar_ll 1.1446 loss -1.1446 (6.797 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 105.56it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 2.1799 loss -2.1799 (0.018 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 60200 lr 1.713e-04 [train_loss] tar_ll 1.2107 loss -1.2107 (6.772 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 60400 lr 1.698e-04 [train_loss] tar_ll 1.2033 loss -1.2033 (6.824 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.1663 loss -1.1663 (6.595 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.2406 loss -1.2406 (6.580 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 61000 lr 1.653e-04 [train_loss] tar_ll 1.2413 loss -1.2413 (6.930 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 61200 lr 1.638e-04 [train_loss] tar_ll 1.3175 loss -1.3175 (7.020 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 61400 lr 1.624e-04 [train_loss] tar_ll 1.2177 loss -1.2177 (6.945 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 61600 lr 1.609e-04 [train_loss] tar_ll 1.2606 loss -1.2606 (6.875 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.2418 loss -1.2418 (6.990 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 62000 lr 1.580e-04 [train_loss] tar_ll 1.2764 loss -1.2764 (7.037 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 62200 lr 1.565e-04 [train_loss] tar_ll 1.2980 loss -1.2980 (7.065 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.1787 loss -1.1787 (6.978 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.2159 loss -1.2159 (6.980 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.2692 loss -1.2692 (7.027 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.2893 loss -1.2893 (7.043 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.2248 loss -1.2248 (7.406 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.1982 loss -1.1982 (7.389 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.2346 loss -1.2346 (7.239 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.2233 loss -1.2233 (6.950 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.3019 loss -1.3019 (6.944 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.2707 loss -1.2707 (7.291 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.2291 loss -1.2291 (6.889 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.1816 loss -1.1816 (7.066 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.1992 loss -1.1992 (12.082 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.2605 loss -1.2605 (6.814 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 85.33it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 1.9415 loss -1.9415 (0.015 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.3029 loss -1.3029 (7.495 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.2603 loss -1.2603 (7.582 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.2969 loss -1.2969 (14.633 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.2202 loss -1.2202 (14.248 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.2920 loss -1.2920 (14.046 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.1786 loss -1.1786 (14.236 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.3412 loss -1.3412 (14.164 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.2555 loss -1.2555 (13.879 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.2625 loss -1.2625 (14.040 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.2328 loss -1.2328 (13.735 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.1844 loss -1.1844 (14.064 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.1988 loss -1.1988 (13.993 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.2416 loss -1.2416 (14.203 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.3333 loss -1.3333 (14.381 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.3162 loss -1.3162 (14.167 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.2810 loss -1.2810 (14.164 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.2728 loss -1.2728 (14.223 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.2297 loss -1.2297 (14.212 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.2804 loss -1.2804 (14.651 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.2775 loss -1.2775 (14.466 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.3175 loss -1.3175 (13.970 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.2797 loss -1.2797 (14.063 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.3457 loss -1.3457 (14.023 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.2451 loss -1.2451 (13.565 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.2852 loss -1.2852 (13.965 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 37.60it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 2.2721 loss -2.2721 (0.030 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.2955 loss -1.2955 (13.853 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.1975 loss -1.1975 (13.814 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.2760 loss -1.2760 (13.550 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.2693 loss -1.2693 (14.117 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.2435 loss -1.2435 (14.097 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.3335 loss -1.3335 (13.963 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.2396 loss -1.2396 (14.007 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.2420 loss -1.2420 (13.802 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.3243 loss -1.3243 (13.967 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.3433 loss -1.3433 (14.137 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.3009 loss -1.3009 (14.345 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.2608 loss -1.2608 (14.043 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.2698 loss -1.2698 (13.776 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.2544 loss -1.2544 (13.556 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.3187 loss -1.3187 (13.428 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.2962 loss -1.2962 (14.120 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.2456 loss -1.2456 (14.268 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.2482 loss -1.2482 (13.782 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.3350 loss -1.3350 (13.822 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.2860 loss -1.2860 (13.844 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.2871 loss -1.2871 (14.071 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.3051 loss -1.3051 (13.724 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.3788 loss -1.3788 (13.921 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.3115 loss -1.3115 (14.177 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.2909 loss -1.2909 (14.186 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 36.88it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 2.1719 loss -2.1719 (0.030 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.2455 loss -1.2455 (14.010 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.3224 loss -1.3224 (13.835 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.2847 loss -1.2847 (14.307 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.3066 loss -1.3066 (14.066 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.3803 loss -1.3803 (13.580 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.2267 loss -1.2267 (13.742 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.2918 loss -1.2918 (13.944 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.2970 loss -1.2970 (14.172 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.3351 loss -1.3351 (13.720 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.3191 loss -1.3191 (13.568 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.2668 loss -1.2668 (14.174 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.2845 loss -1.2845 (13.382 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.2904 loss -1.2904 (13.437 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.3312 loss -1.3312 (14.365 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.3540 loss -1.3540 (14.174 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.3554 loss -1.3554 (14.485 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.2971 loss -1.2971 (13.111 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.2729 loss -1.2729 (6.825 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.2990 loss -1.2990 (7.140 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.2940 loss -1.2940 (7.170 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.3385 loss -1.3385 (7.305 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.3610 loss -1.3610 (7.025 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.3081 loss -1.3081 (7.160 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.3030 loss -1.3030 (7.168 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.3559 loss -1.3559 (7.017 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 80.42it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 2.2858 loss -2.2858 (0.014 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.3183 loss -1.3183 (7.217 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.3333 loss -1.3333 (7.065 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1288 loss -1.1288 (7.075 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.3499 loss -1.3499 (7.032 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.3233 loss -1.3233 (7.118 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.3037 loss -1.3037 (7.130 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.2728 loss -1.2728 (7.070 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.3678 loss -1.3678 (7.205 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2837 loss -1.2837 (6.875 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.3535 loss -1.3535 (6.990 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.2948 loss -1.2948 (7.160 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.2981 loss -1.2981 (6.690 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.3323 loss -1.3323 (6.580 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.2743 loss -1.2743 (6.773 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.3876 loss -1.3876 (6.597 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.3238 loss -1.3238 (7.035 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.3377 loss -1.3377 (6.810 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.3156 loss -1.3156 (6.729 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.3772 loss -1.3772 (6.996 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.4081 loss -1.4081 (6.930 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.3968 loss -1.3968 (6.780 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.3603 loss -1.3603 (6.655 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.3288 loss -1.3288 (6.735 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.2534 loss -1.2534 (6.797 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.3651 loss -1.3651 (6.573 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 85.11it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 2.3626 loss -2.3626 (0.016 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.2358 loss -1.2358 (6.840 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.2959 loss -1.2959 (6.495 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.3296 loss -1.3296 (6.527 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.4240 loss -1.4240 (6.430 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.2629 loss -1.2629 (6.437 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.3464 loss -1.3464 (6.523 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.3646 loss -1.3646 (6.460 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.2919 loss -1.2919 (6.475 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.4243 loss -1.4243 (6.745 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.3667 loss -1.3667 (6.402 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.3489 loss -1.3489 (6.772 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.3725 loss -1.3725 (6.325 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.3427 loss -1.3427 (6.420 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.3157 loss -1.3157 (6.517 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.3028 loss -1.3028 (6.322 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.3727 loss -1.3727 (6.623 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.4330 loss -1.4330 (6.597 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.3484 loss -1.3484 (6.423 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.2714 loss -1.2714 (6.548 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.3304 loss -1.3304 (6.329 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.3120 loss -1.3120 (6.770 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.4199 loss -1.4199 (6.519 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.3422 loss -1.3422 (6.731 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2963 loss -1.2963 (6.475 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.3823 loss -1.3823 (6.820 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 93.67it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 2.3750 loss -2.3750 (0.014 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.3750 loss -1.3750 (6.782 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.3788 loss -1.3788 (6.782 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.3322 loss -1.3322 (6.725 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.3887 loss -1.3887 (6.540 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.3449 loss -1.3449 (6.315 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.3312 loss -1.3312 (6.685 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.3729 loss -1.3729 (6.535 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.3684 loss -1.3684 (6.570 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.3650 loss -1.3650 (6.485 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.3075 loss -1.3075 (6.587 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.4247 loss -1.4247 (6.680 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.3491 loss -1.3491 (6.675 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.3415 loss -1.3415 (6.655 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.4200 loss -1.4200 (6.385 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.3808 loss -1.3808 (6.650 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.3444 loss -1.3444 (6.590 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.3965 loss -1.3965 (6.542 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.2935 loss -1.2935 (6.539 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.3626 loss -1.3626 (6.482 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.3738 loss -1.3738 (6.723 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.4287 loss -1.4287 (7.030 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.3661 loss -1.3661 (7.110 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.2713 loss -1.2713 (6.730 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.3211 loss -1.3211 (6.670 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.4006 loss -1.4006 (6.851 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 61.85it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 2.3785 loss -2.3785 (0.019 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_0 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.4005 loss -1.4005 (6.830 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.3291 loss -1.3291 (6.650 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.3410 loss -1.3410 (6.310 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.3589 loss -1.3589 (6.620 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.3513 loss -1.3513 (6.680 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2446 loss -1.2446 (6.840 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.4225 loss -1.4225 (6.650 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.3132 loss -1.3132 (6.585 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.4349 loss -1.4349 (6.765 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.3347 loss -1.3347 (6.735 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.3559 loss -1.3559 (6.695 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.2645 loss -1.2645 (6.680 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.3563 loss -1.3563 (6.671 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.3814 loss -1.3814 (6.624 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.3985 loss -1.3985 (6.600 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.4418 loss -1.4418 (6.575 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.4142 loss -1.4142 (6.510 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.3921 loss -1.3921 (6.819 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.3160 loss -1.3160 (6.598 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.4043 loss -1.4043 (6.669 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.3810 loss -1.3810 (6.505 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.3628 loss -1.3628 (6.581 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.3151 loss -1.3151 (6.419 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.3886 loss -1.3886 (6.793 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.3286 loss -1.3286 (6.785 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 102.94it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 2.3916 loss -2.3916 (0.011 secs)\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00, 51.18it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 2.3916 loss -2.3916 (0.020 secs)\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 2.3916 loss -2.3916 (0.020 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3906930.0 miliseconds\n",
      "Execution time: 3906.93 seconds\n",
      "Initial Memory Usage: 0.0 MB\n",
      "Final Memory Usage: 61.78564453125 MB\n",
      "Memory Usage Change: 61.78564453125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='isanp-num_latents-32_rbf_0',val_seed=0, val_l=32,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f25d6e-29c5-464b-8b53-cee701586a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14efb131-a59b-4172-b14a-3d7f50f016e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 32\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:10<00:00, 42.32it/s]\n",
      "isanp:isanp-num_latents-32_rbf_0 rbf tar_ll 1.3781 loss -1.3781 (70.886 secs)\n"
     ]
    }
   ],
   "source": [
    "%run gp.py --mode eval --expid isanp-num_latents-32_rbf_0 --model isanp --num_latents 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd6f9e4e-78e3-4568-9376-4d64d1aa760f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: isanp-isanp-num_latents-32_rbf_100\n",
      "Total number of parameters: 787138\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 32\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-32_rbf_100 step 200 lr 5.000e-04 [train_loss] tar_ll -0.7034 loss 0.7034 (8.095 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 400 lr 5.000e-04 [train_loss] tar_ll -0.6863 loss 0.6863 (8.110 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 600 lr 5.000e-04 [train_loss] tar_ll -0.5424 loss 0.5424 (8.242 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 800 lr 4.999e-04 [train_loss] tar_ll -0.4592 loss 0.4592 (7.835 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 1000 lr 4.999e-04 [train_loss] tar_ll -0.3504 loss 0.3504 (8.191 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.3222 loss 0.3222 (8.017 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 1400 lr 4.998e-04 [train_loss] tar_ll -0.2244 loss 0.2244 (8.090 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 1600 lr 4.997e-04 [train_loss] tar_ll -0.1081 loss 0.1081 (7.980 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 1800 lr 4.996e-04 [train_loss] tar_ll -0.0470 loss 0.0470 (8.105 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 2000 lr 4.995e-04 [train_loss] tar_ll -0.0443 loss 0.0443 (7.961 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.0854 loss -0.0854 (8.055 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.1336 loss -0.1336 (8.098 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1804 loss -0.1804 (8.213 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.1957 loss -0.1957 (8.133 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.1379 loss -0.1379 (8.191 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.2456 loss -0.2456 (8.002 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.2898 loss -0.2898 (8.244 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2830 loss -0.2830 (7.951 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.3297 loss -0.3297 (8.162 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.2773 loss -0.2773 (8.004 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.2831 loss -0.2831 (8.229 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.2614 loss -0.2614 (7.983 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.3870 loss -0.3870 (8.090 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4603 loss -0.4603 (7.923 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.4277 loss -0.4277 (8.166 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.18it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 0.2302 loss -0.2302 (36.956 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.4030 loss -0.4030 (8.031 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.4748 loss -0.4748 (6.963 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.3964 loss -0.3964 (8.156 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.3677 loss -0.3677 (15.212 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4733 loss -0.4733 (15.056 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.4807 loss -0.4807 (15.276 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.4546 loss -0.4546 (15.336 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5141 loss -0.5141 (15.669 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.5222 loss -0.5222 (14.166 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.4568 loss -0.4568 (15.530 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.5431 loss -0.5431 (15.373 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5923 loss -0.5923 (15.558 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.4443 loss -0.4443 (14.653 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.5740 loss -0.5740 (15.089 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.5632 loss -0.5632 (14.718 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.6248 loss -0.6248 (15.108 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.5909 loss -0.5909 (14.789 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.6560 loss -0.6560 (14.742 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.6046 loss -0.6046 (14.639 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.5966 loss -0.5966 (14.458 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.6264 loss -0.6264 (14.805 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.6609 loss -0.6609 (14.753 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.6488 loss -0.6488 (14.627 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.6845 loss -0.6845 (14.590 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.6332 loss -0.6332 (14.538 secs)\n",
      "100%|##########| 3000/3000 [01:10<00:00, 42.31it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 0.7015 loss -0.7015 (70.911 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.6600 loss -0.6600 (14.895 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.5761 loss -0.5761 (14.865 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.5923 loss -0.5923 (15.045 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.6106 loss -0.6106 (14.961 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.6667 loss -0.6667 (14.846 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.6140 loss -0.6140 (15.454 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.6195 loss -0.6195 (15.835 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.6495 loss -0.6495 (14.982 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.7357 loss -0.7357 (14.917 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.6700 loss -0.6700 (15.760 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.7133 loss -0.7133 (14.483 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.7207 loss -0.7207 (15.581 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.7556 loss -0.7556 (14.615 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.7292 loss -0.7292 (14.331 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.5887 loss -0.5887 (14.642 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.4579 loss -0.4579 (15.080 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.5836 loss -0.5836 (15.403 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.7082 loss -0.7082 (7.082 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.7172 loss -0.7172 (6.769 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.6812 loss -0.6812 (6.725 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.6717 loss -0.6717 (6.530 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.7140 loss -0.7140 (6.591 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.7026 loss -0.7026 (6.459 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.7656 loss -0.7656 (6.631 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.6592 loss -0.6592 (6.519 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.73it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 0.7724 loss -0.7724 (32.705 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.7076 loss -0.7076 (6.549 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.6113 loss -0.6113 (6.643 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.7160 loss -0.7160 (6.475 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.7546 loss -0.7546 (6.580 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.7642 loss -0.7642 (6.547 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.6298 loss -0.6298 (6.596 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.6580 loss -0.6580 (6.656 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.7453 loss -0.7453 (6.753 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.7559 loss -0.7559 (6.850 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.7178 loss -0.7178 (6.645 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.8178 loss -0.8178 (6.823 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.8352 loss -0.8352 (6.671 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.7124 loss -0.7124 (6.833 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.7058 loss -0.7058 (6.672 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.6792 loss -0.6792 (6.580 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.8073 loss -0.8073 (6.698 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.8378 loss -0.8378 (6.588 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.7266 loss -0.7266 (6.609 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.7720 loss -0.7720 (6.632 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.8578 loss -0.8578 (6.478 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.7690 loss -0.7690 (6.627 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.8637 loss -0.8637 (6.534 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.7963 loss -0.7963 (6.525 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.8599 loss -0.8599 (6.494 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.8743 loss -0.8743 (6.579 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.01it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 0.8614 loss -0.8614 (33.706 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.8747 loss -0.8747 (6.869 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.8254 loss -0.8254 (6.548 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.8739 loss -0.8739 (6.914 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.7962 loss -0.7962 (6.773 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.8738 loss -0.8738 (6.842 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.8682 loss -0.8682 (6.779 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.8587 loss -0.8587 (6.716 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.8779 loss -0.8779 (6.665 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.8183 loss -0.8183 (6.407 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.9321 loss -0.9321 (7.162 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.7733 loss -0.7733 (6.462 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.8999 loss -0.8999 (6.456 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.7711 loss -0.7711 (6.373 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.8897 loss -0.8897 (6.280 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.8628 loss -0.8628 (6.560 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.8767 loss -0.8767 (6.428 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.8920 loss -0.8920 (6.452 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.9140 loss -0.9140 (6.298 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.8732 loss -0.8732 (6.318 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.8912 loss -0.8912 (6.427 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.7902 loss -0.7902 (6.394 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.6694 loss -0.6694 (6.346 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.9109 loss -0.9109 (6.318 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.8836 loss -0.8836 (6.427 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.9645 loss -0.9645 (6.912 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.22it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 0.8376 loss -0.8376 (33.252 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.8561 loss -0.8561 (6.482 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.9088 loss -0.9088 (6.713 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.9222 loss -0.9222 (6.775 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.9701 loss -0.9701 (6.601 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.8543 loss -0.8543 (6.614 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.9550 loss -0.9550 (6.543 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.9481 loss -0.9481 (6.774 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.9230 loss -0.9230 (6.779 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.9481 loss -0.9481 (6.732 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.9011 loss -0.9011 (6.569 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.8642 loss -0.8642 (6.804 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.8332 loss -0.8332 (6.471 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 27600 lr 4.118e-04 [train_loss] tar_ll 1.0133 loss -1.0133 (6.431 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.9421 loss -0.9421 (6.696 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.9628 loss -0.9628 (6.404 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.8972 loss -0.8972 (6.584 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.8472 loss -0.8472 (6.426 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.9277 loss -0.9277 (6.367 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 28800 lr 4.045e-04 [train_loss] tar_ll 1.0021 loss -1.0021 (6.610 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.9311 loss -0.9311 (6.645 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.9939 loss -0.9939 (6.774 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.8771 loss -0.8771 (6.658 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.9722 loss -0.9722 (6.656 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.8899 loss -0.8899 (6.712 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.9696 loss -0.9696 (6.686 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 93.91it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 0.9617 loss -0.9617 (31.948 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.9025 loss -0.9025 (6.545 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 30400 lr 3.944e-04 [train_loss] tar_ll 1.0292 loss -1.0292 (6.277 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.9522 loss -0.9522 (6.536 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.9625 loss -0.9625 (6.482 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.8983 loss -0.8983 (6.398 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 31200 lr 3.892e-04 [train_loss] tar_ll 1.0084 loss -1.0084 (6.580 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 31400 lr 3.879e-04 [train_loss] tar_ll 1.0037 loss -1.0037 (6.359 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 31600 lr 3.866e-04 [train_loss] tar_ll 1.0768 loss -1.0768 (6.409 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.9570 loss -0.9570 (6.456 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.9079 loss -0.9079 (6.535 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.9083 loss -0.9083 (6.514 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.9849 loss -0.9849 (6.517 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.9766 loss -0.9766 (6.641 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.9498 loss -0.9498 (6.402 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 33000 lr 3.773e-04 [train_loss] tar_ll 1.0443 loss -1.0443 (6.516 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.9234 loss -0.9234 (6.610 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.9817 loss -0.9817 (6.482 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.9404 loss -0.9404 (6.575 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.9745 loss -0.9745 (6.412 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 34000 lr 3.704e-04 [train_loss] tar_ll 1.0108 loss -1.0108 (6.416 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.9226 loss -0.9226 (6.588 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.9605 loss -0.9605 (6.385 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.9297 loss -0.9297 (6.681 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.9589 loss -0.9589 (6.542 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 35000 lr 3.635e-04 [train_loss] tar_ll 1.0673 loss -1.0673 (6.584 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 92.02it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 0.9502 loss -0.9502 (32.605 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 35200 lr 3.621e-04 [train_loss] tar_ll 1.0269 loss -1.0269 (6.801 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.9571 loss -0.9571 (6.705 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.9752 loss -0.9752 (6.533 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 35800 lr 3.579e-04 [train_loss] tar_ll 1.0509 loss -1.0509 (6.497 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 36000 lr 3.564e-04 [train_loss] tar_ll 1.0050 loss -1.0050 (6.716 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 36200 lr 3.550e-04 [train_loss] tar_ll 1.0247 loss -1.0247 (6.518 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.9858 loss -0.9858 (6.535 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 36600 lr 3.522e-04 [train_loss] tar_ll 1.0480 loss -1.0480 (6.543 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 36800 lr 3.507e-04 [train_loss] tar_ll 1.0156 loss -1.0156 (6.394 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 37000 lr 3.493e-04 [train_loss] tar_ll 1.0078 loss -1.0078 (6.563 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 37200 lr 3.478e-04 [train_loss] tar_ll 1.0394 loss -1.0394 (6.619 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 37400 lr 3.464e-04 [train_loss] tar_ll 1.0635 loss -1.0635 (6.419 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 37600 lr 3.449e-04 [train_loss] tar_ll 1.0729 loss -1.0729 (6.703 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 37800 lr 3.435e-04 [train_loss] tar_ll 1.0463 loss -1.0463 (6.601 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 38000 lr 3.420e-04 [train_loss] tar_ll 1.1085 loss -1.1085 (6.670 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 38200 lr 3.406e-04 [train_loss] tar_ll 1.0201 loss -1.0201 (6.961 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 38400 lr 3.391e-04 [train_loss] tar_ll 1.0054 loss -1.0054 (6.833 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 38600 lr 3.376e-04 [train_loss] tar_ll 1.0602 loss -1.0602 (6.714 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 38800 lr 3.362e-04 [train_loss] tar_ll 1.0241 loss -1.0241 (6.759 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 39000 lr 3.347e-04 [train_loss] tar_ll 1.0851 loss -1.0851 (6.835 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 39200 lr 3.332e-04 [train_loss] tar_ll 1.0852 loss -1.0852 (6.776 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 39400 lr 3.317e-04 [train_loss] tar_ll 1.1074 loss -1.1074 (6.795 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 39600 lr 3.302e-04 [train_loss] tar_ll 1.0113 loss -1.0113 (6.707 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 39800 lr 3.287e-04 [train_loss] tar_ll 1.0512 loss -1.0512 (6.896 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 40000 lr 3.273e-04 [train_loss] tar_ll 1.0693 loss -1.0693 (6.889 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.76it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 0.9847 loss -0.9847 (33.054 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 40200 lr 3.258e-04 [train_loss] tar_ll 1.0352 loss -1.0352 (6.551 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 40400 lr 3.243e-04 [train_loss] tar_ll 1.0700 loss -1.0700 (6.449 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 40600 lr 3.228e-04 [train_loss] tar_ll 1.0819 loss -1.0819 (6.283 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 40800 lr 3.213e-04 [train_loss] tar_ll 1.0866 loss -1.0866 (6.582 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 41000 lr 3.197e-04 [train_loss] tar_ll 1.1028 loss -1.1028 (6.286 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 41200 lr 3.182e-04 [train_loss] tar_ll 1.0057 loss -1.0057 (6.407 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 41400 lr 3.167e-04 [train_loss] tar_ll 1.1165 loss -1.1165 (6.514 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 41600 lr 3.152e-04 [train_loss] tar_ll 1.0618 loss -1.0618 (6.234 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 41800 lr 3.137e-04 [train_loss] tar_ll 1.0424 loss -1.0424 (6.517 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 42000 lr 3.122e-04 [train_loss] tar_ll 1.1213 loss -1.1213 (6.333 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 42200 lr 3.106e-04 [train_loss] tar_ll 1.0099 loss -1.0099 (6.292 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 42400 lr 3.091e-04 [train_loss] tar_ll 1.0297 loss -1.0297 (6.482 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 42600 lr 3.076e-04 [train_loss] tar_ll 1.0726 loss -1.0726 (6.315 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 42800 lr 3.061e-04 [train_loss] tar_ll 1.0412 loss -1.0412 (6.444 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 43000 lr 3.045e-04 [train_loss] tar_ll 1.0889 loss -1.0889 (6.481 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 43200 lr 3.030e-04 [train_loss] tar_ll 1.0775 loss -1.0775 (6.380 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 43400 lr 3.015e-04 [train_loss] tar_ll 1.1568 loss -1.1568 (6.619 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 43600 lr 2.999e-04 [train_loss] tar_ll 1.0406 loss -1.0406 (6.478 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 43800 lr 2.984e-04 [train_loss] tar_ll 1.1597 loss -1.1597 (6.565 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 44000 lr 2.968e-04 [train_loss] tar_ll 1.0185 loss -1.0185 (6.539 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 44200 lr 2.953e-04 [train_loss] tar_ll 1.0430 loss -1.0430 (6.712 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.9848 loss -0.9848 (6.486 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 44600 lr 2.922e-04 [train_loss] tar_ll 1.1469 loss -1.1469 (6.646 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 44800 lr 2.907e-04 [train_loss] tar_ll 1.1120 loss -1.1120 (6.623 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 45000 lr 2.891e-04 [train_loss] tar_ll 1.0950 loss -1.0950 (7.422 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.57it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 1.0613 loss -1.0613 (32.764 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 45200 lr 2.876e-04 [train_loss] tar_ll 1.0651 loss -1.0651 (6.639 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 45400 lr 2.860e-04 [train_loss] tar_ll 1.1215 loss -1.1215 (6.528 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 45600 lr 2.844e-04 [train_loss] tar_ll 1.0656 loss -1.0656 (6.599 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 45800 lr 2.829e-04 [train_loss] tar_ll 1.1179 loss -1.1179 (6.457 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 46000 lr 2.813e-04 [train_loss] tar_ll 1.0584 loss -1.0584 (6.582 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 46200 lr 2.798e-04 [train_loss] tar_ll 1.0724 loss -1.0724 (6.633 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 46400 lr 2.782e-04 [train_loss] tar_ll 1.0946 loss -1.0946 (6.404 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 46600 lr 2.767e-04 [train_loss] tar_ll 1.1182 loss -1.1182 (6.620 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 46800 lr 2.751e-04 [train_loss] tar_ll 1.0900 loss -1.0900 (6.541 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 47000 lr 2.735e-04 [train_loss] tar_ll 1.1699 loss -1.1699 (6.621 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 47200 lr 2.720e-04 [train_loss] tar_ll 1.1794 loss -1.1794 (6.848 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 47400 lr 2.704e-04 [train_loss] tar_ll 1.0930 loss -1.0930 (6.640 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 47600 lr 2.688e-04 [train_loss] tar_ll 1.0562 loss -1.0562 (6.849 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 47800 lr 2.673e-04 [train_loss] tar_ll 1.0743 loss -1.0743 (6.616 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 48000 lr 2.657e-04 [train_loss] tar_ll 1.1965 loss -1.1965 (6.660 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 48200 lr 2.641e-04 [train_loss] tar_ll 1.1247 loss -1.1247 (6.721 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 48400 lr 2.626e-04 [train_loss] tar_ll 1.1352 loss -1.1352 (6.693 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 48600 lr 2.610e-04 [train_loss] tar_ll 1.1715 loss -1.1715 (6.481 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 48800 lr 2.594e-04 [train_loss] tar_ll 1.1015 loss -1.1015 (6.304 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 49000 lr 2.579e-04 [train_loss] tar_ll 1.1135 loss -1.1135 (6.516 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 49200 lr 2.563e-04 [train_loss] tar_ll 1.0778 loss -1.0778 (6.384 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 49400 lr 2.547e-04 [train_loss] tar_ll 1.1463 loss -1.1463 (6.353 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 49600 lr 2.531e-04 [train_loss] tar_ll 1.1241 loss -1.1241 (6.631 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 49800 lr 2.516e-04 [train_loss] tar_ll 1.1537 loss -1.1537 (6.336 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 50000 lr 2.500e-04 [train_loss] tar_ll 1.1650 loss -1.1650 (6.603 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 90.95it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 1.0973 loss -1.0973 (32.987 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 50200 lr 2.484e-04 [train_loss] tar_ll 1.1336 loss -1.1336 (6.619 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 50400 lr 2.469e-04 [train_loss] tar_ll 1.1019 loss -1.1019 (6.528 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 50600 lr 2.453e-04 [train_loss] tar_ll 1.0765 loss -1.0765 (6.611 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 50800 lr 2.437e-04 [train_loss] tar_ll 1.1341 loss -1.1341 (6.482 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 51000 lr 2.421e-04 [train_loss] tar_ll 1.0637 loss -1.0637 (6.491 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 51200 lr 2.406e-04 [train_loss] tar_ll 1.1501 loss -1.1501 (7.060 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 51400 lr 2.390e-04 [train_loss] tar_ll 1.2039 loss -1.2039 (7.296 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 51600 lr 2.374e-04 [train_loss] tar_ll 1.1105 loss -1.1105 (7.233 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 51800 lr 2.359e-04 [train_loss] tar_ll 1.1035 loss -1.1035 (7.045 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 52000 lr 2.343e-04 [train_loss] tar_ll 1.1788 loss -1.1788 (6.478 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 52200 lr 2.327e-04 [train_loss] tar_ll 1.1474 loss -1.1474 (6.395 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 52400 lr 2.312e-04 [train_loss] tar_ll 1.1899 loss -1.1899 (6.527 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 52600 lr 2.296e-04 [train_loss] tar_ll 1.1548 loss -1.1548 (6.370 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 52800 lr 2.280e-04 [train_loss] tar_ll 1.1705 loss -1.1705 (6.514 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 53000 lr 2.265e-04 [train_loss] tar_ll 1.1621 loss -1.1621 (6.507 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 53200 lr 2.249e-04 [train_loss] tar_ll 1.2387 loss -1.2387 (7.556 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 53400 lr 2.233e-04 [train_loss] tar_ll 1.2313 loss -1.2313 (6.683 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 53600 lr 2.218e-04 [train_loss] tar_ll 1.1669 loss -1.1669 (6.532 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 53800 lr 2.202e-04 [train_loss] tar_ll 1.1948 loss -1.1948 (6.713 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 54000 lr 2.187e-04 [train_loss] tar_ll 1.2382 loss -1.2382 (7.294 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 54200 lr 2.171e-04 [train_loss] tar_ll 1.1556 loss -1.1556 (7.275 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 54400 lr 2.156e-04 [train_loss] tar_ll 1.0548 loss -1.0548 (7.298 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 54600 lr 2.140e-04 [train_loss] tar_ll 1.1718 loss -1.1718 (7.399 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 54800 lr 2.124e-04 [train_loss] tar_ll 1.1521 loss -1.1521 (6.483 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 55000 lr 2.109e-04 [train_loss] tar_ll 1.1832 loss -1.1832 (6.462 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 92.17it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 1.0948 loss -1.0948 (32.552 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 55200 lr 2.093e-04 [train_loss] tar_ll 1.1157 loss -1.1157 (6.598 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 55400 lr 2.078e-04 [train_loss] tar_ll 1.1150 loss -1.1150 (6.366 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 55600 lr 2.062e-04 [train_loss] tar_ll 1.1521 loss -1.1521 (6.770 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 55800 lr 2.047e-04 [train_loss] tar_ll 1.1581 loss -1.1581 (6.662 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 56000 lr 2.032e-04 [train_loss] tar_ll 1.2021 loss -1.2021 (6.736 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 56200 lr 2.016e-04 [train_loss] tar_ll 1.2107 loss -1.2107 (6.820 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 56400 lr 2.001e-04 [train_loss] tar_ll 1.2727 loss -1.2727 (6.544 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 56600 lr 1.985e-04 [train_loss] tar_ll 1.1492 loss -1.1492 (6.859 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 56800 lr 1.970e-04 [train_loss] tar_ll 1.1686 loss -1.1686 (6.621 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 57000 lr 1.955e-04 [train_loss] tar_ll 1.2358 loss -1.2358 (6.729 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 57200 lr 1.939e-04 [train_loss] tar_ll 1.1999 loss -1.1999 (6.687 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 57400 lr 1.924e-04 [train_loss] tar_ll 1.2097 loss -1.2097 (6.729 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 57600 lr 1.909e-04 [train_loss] tar_ll 1.2229 loss -1.2229 (6.857 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 57800 lr 1.894e-04 [train_loss] tar_ll 1.2567 loss -1.2567 (6.746 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 58000 lr 1.878e-04 [train_loss] tar_ll 1.2318 loss -1.2318 (6.837 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 58200 lr 1.863e-04 [train_loss] tar_ll 1.2033 loss -1.2033 (6.667 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 58400 lr 1.848e-04 [train_loss] tar_ll 1.2251 loss -1.2251 (6.448 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 58600 lr 1.833e-04 [train_loss] tar_ll 1.1808 loss -1.1808 (6.614 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 58800 lr 1.818e-04 [train_loss] tar_ll 1.1700 loss -1.1700 (6.412 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.1912 loss -1.1912 (6.843 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 59200 lr 1.787e-04 [train_loss] tar_ll 1.1843 loss -1.1843 (6.418 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 59400 lr 1.772e-04 [train_loss] tar_ll 1.1353 loss -1.1353 (6.462 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 59600 lr 1.757e-04 [train_loss] tar_ll 1.1971 loss -1.1971 (6.577 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 59800 lr 1.742e-04 [train_loss] tar_ll 1.1393 loss -1.1393 (6.441 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 60000 lr 1.727e-04 [train_loss] tar_ll 1.2166 loss -1.2166 (6.545 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.63it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 1.1828 loss -1.1828 (33.851 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 60200 lr 1.713e-04 [train_loss] tar_ll 1.2019 loss -1.2019 (7.311 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 60400 lr 1.698e-04 [train_loss] tar_ll 1.1738 loss -1.1738 (7.133 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.2460 loss -1.2460 (7.089 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.2752 loss -1.2752 (6.766 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 61000 lr 1.653e-04 [train_loss] tar_ll 1.1207 loss -1.1207 (6.642 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 61200 lr 1.638e-04 [train_loss] tar_ll 1.2311 loss -1.2311 (6.677 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 61400 lr 1.624e-04 [train_loss] tar_ll 1.1509 loss -1.1509 (6.579 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 61600 lr 1.609e-04 [train_loss] tar_ll 1.2256 loss -1.2256 (6.686 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.0531 loss -1.0531 (6.652 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 62000 lr 1.580e-04 [train_loss] tar_ll 1.2682 loss -1.2682 (6.559 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 62200 lr 1.565e-04 [train_loss] tar_ll 1.2095 loss -1.2095 (6.596 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.1710 loss -1.1710 (6.472 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.2258 loss -1.2258 (6.552 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.2734 loss -1.2734 (6.717 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.1762 loss -1.1762 (6.495 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.1468 loss -1.1468 (6.660 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.1872 loss -1.1872 (6.552 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.2663 loss -1.2663 (6.435 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.2199 loss -1.2199 (6.558 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.2465 loss -1.2465 (6.551 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.2271 loss -1.2271 (6.588 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.2576 loss -1.2576 (6.461 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.2227 loss -1.2227 (6.438 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.1845 loss -1.1845 (6.419 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.1607 loss -1.1607 (6.396 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.58it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 1.2349 loss -1.2349 (34.257 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.2826 loss -1.2826 (6.798 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.2846 loss -1.2846 (6.727 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.2201 loss -1.2201 (6.740 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.2505 loss -1.2505 (6.483 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.3033 loss -1.3033 (6.735 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.2078 loss -1.2078 (6.889 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.3046 loss -1.3046 (6.893 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.2092 loss -1.2092 (6.865 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.1564 loss -1.1564 (6.861 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.1797 loss -1.1797 (6.630 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.3259 loss -1.3259 (6.527 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.3883 loss -1.3883 (6.432 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.2234 loss -1.2234 (6.545 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.2330 loss -1.2330 (6.428 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.2604 loss -1.2604 (6.488 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.3109 loss -1.3109 (6.684 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.2917 loss -1.2917 (6.668 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.3540 loss -1.3540 (6.474 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.3321 loss -1.3321 (6.599 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.2747 loss -1.2747 (6.786 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.2946 loss -1.2946 (6.565 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.3408 loss -1.3408 (6.680 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.3381 loss -1.3381 (6.517 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.2225 loss -1.2225 (6.481 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.2242 loss -1.2242 (6.634 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.98it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 1.2880 loss -1.2880 (34.494 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.3284 loss -1.3284 (6.505 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.3075 loss -1.3075 (6.597 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.3020 loss -1.3020 (6.551 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.2629 loss -1.2629 (6.814 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.3088 loss -1.3088 (6.598 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.2171 loss -1.2171 (6.632 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.2694 loss -1.2694 (6.558 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.2794 loss -1.2794 (6.450 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.2581 loss -1.2581 (6.415 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.2761 loss -1.2761 (6.243 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.2843 loss -1.2843 (7.242 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.3889 loss -1.3889 (6.758 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.3008 loss -1.3008 (6.336 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.1063 loss -1.1063 (6.483 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.2548 loss -1.2548 (6.307 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.2517 loss -1.2517 (6.531 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.2682 loss -1.2682 (6.295 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.2865 loss -1.2865 (6.289 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.3139 loss -1.3139 (6.506 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.2154 loss -1.2154 (6.406 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.2617 loss -1.2617 (6.714 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.3393 loss -1.3393 (6.606 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.3009 loss -1.3009 (6.560 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.3708 loss -1.3708 (6.613 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.2747 loss -1.2747 (6.670 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.94it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 1.3178 loss -1.3178 (33.731 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.3048 loss -1.3048 (6.831 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.3769 loss -1.3769 (6.780 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.2836 loss -1.2836 (6.787 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.3163 loss -1.3163 (6.519 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.3754 loss -1.3754 (6.385 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.2984 loss -1.2984 (6.321 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.1786 loss -1.1786 (6.273 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.2385 loss -1.2385 (6.418 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.3551 loss -1.3551 (6.360 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.4081 loss -1.4081 (6.304 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.3702 loss -1.3702 (6.395 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.2676 loss -1.2676 (6.339 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.3453 loss -1.3453 (6.458 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.3081 loss -1.3081 (6.325 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.2125 loss -1.2125 (6.374 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.2765 loss -1.2765 (6.515 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.2681 loss -1.2681 (6.395 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.2601 loss -1.2601 (6.501 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.3171 loss -1.3171 (6.516 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.2516 loss -1.2516 (6.466 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.3205 loss -1.3205 (6.496 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.3169 loss -1.3169 (6.407 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.3082 loss -1.3082 (6.474 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.3438 loss -1.3438 (6.351 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.3739 loss -1.3739 (6.376 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 93.20it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 1.3498 loss -1.3498 (32.192 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.3697 loss -1.3697 (6.427 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.3760 loss -1.3760 (6.428 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.3419 loss -1.3419 (6.489 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.3048 loss -1.3048 (6.530 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.2737 loss -1.2737 (6.535 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.3062 loss -1.3062 (6.498 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.3106 loss -1.3106 (6.575 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.3925 loss -1.3925 (6.538 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2822 loss -1.2822 (6.459 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.3881 loss -1.3881 (6.581 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.2608 loss -1.2608 (6.382 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.3339 loss -1.3339 (6.533 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.2725 loss -1.2725 (6.442 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.2708 loss -1.2708 (6.388 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.3395 loss -1.3395 (6.488 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.3434 loss -1.3434 (6.614 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.3572 loss -1.3572 (6.559 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.3779 loss -1.3779 (6.729 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.2677 loss -1.2677 (6.653 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.3079 loss -1.3079 (6.675 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2662 loss -1.2662 (6.785 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.2796 loss -1.2796 (6.692 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.2741 loss -1.2741 (6.608 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.3777 loss -1.3777 (6.720 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.2609 loss -1.2609 (6.728 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.77it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 1.3560 loss -1.3560 (33.421 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.3438 loss -1.3438 (6.366 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.3551 loss -1.3551 (6.446 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.2548 loss -1.2548 (6.278 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.3452 loss -1.3452 (6.390 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.2370 loss -1.2370 (6.382 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.3634 loss -1.3634 (6.402 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.2646 loss -1.2646 (6.372 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.3661 loss -1.3661 (6.332 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.2942 loss -1.2942 (6.370 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.3120 loss -1.3120 (6.373 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.3735 loss -1.3735 (6.420 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.3710 loss -1.3710 (6.464 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.3651 loss -1.3651 (6.354 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.3195 loss -1.3195 (6.567 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.3855 loss -1.3855 (6.340 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.3490 loss -1.3490 (6.447 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.3350 loss -1.3350 (6.466 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.3915 loss -1.3915 (6.462 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.4221 loss -1.4221 (6.395 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.2541 loss -1.2541 (6.322 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.3101 loss -1.3101 (6.528 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.2927 loss -1.2927 (6.497 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.2463 loss -1.2463 (6.440 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.3611 loss -1.3611 (6.663 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.3352 loss -1.3352 (6.510 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.39it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 1.3635 loss -1.3635 (32.828 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.3711 loss -1.3711 (6.556 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.3457 loss -1.3457 (6.466 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.3673 loss -1.3673 (6.393 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.3675 loss -1.3675 (6.500 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.3908 loss -1.3908 (6.892 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.3886 loss -1.3886 (6.959 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.3587 loss -1.3587 (6.282 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.2398 loss -1.2398 (6.439 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.3279 loss -1.3279 (6.531 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.3719 loss -1.3719 (6.469 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.3489 loss -1.3489 (6.836 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.2886 loss -1.2886 (6.555 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.3854 loss -1.3854 (6.523 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.3371 loss -1.3371 (6.626 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.3155 loss -1.3155 (6.576 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.3804 loss -1.3804 (6.751 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.4106 loss -1.4106 (6.671 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.3292 loss -1.3292 (6.717 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.3216 loss -1.3216 (6.657 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.3915 loss -1.3915 (6.868 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.3613 loss -1.3613 (6.820 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.3684 loss -1.3684 (6.913 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.3484 loss -1.3484 (6.775 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.3041 loss -1.3041 (6.566 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.3354 loss -1.3354 (6.294 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.45it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 1.3751 loss -1.3751 (32.807 secs)\n",
      "\n",
      "isanp:isanp-num_latents-32_rbf_100 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.3831 loss -1.3831 (6.404 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.3338 loss -1.3338 (6.439 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.4404 loss -1.4404 (6.402 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.3373 loss -1.3373 (6.428 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.3992 loss -1.3992 (6.445 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.3360 loss -1.3360 (6.352 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.2672 loss -1.2672 (6.410 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.3873 loss -1.3873 (6.722 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.3283 loss -1.3283 (6.479 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.3649 loss -1.3649 (6.497 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.3795 loss -1.3795 (6.378 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.4042 loss -1.4042 (6.332 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.2940 loss -1.2940 (6.478 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.3161 loss -1.3161 (6.426 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.3099 loss -1.3099 (6.504 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.4508 loss -1.4508 (6.504 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.3884 loss -1.3884 (6.457 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.3967 loss -1.3967 (6.660 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.3720 loss -1.3720 (6.510 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.3715 loss -1.3715 (6.640 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.3129 loss -1.3129 (6.620 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2766 loss -1.2766 (6.500 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.3872 loss -1.3872 (6.693 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.2949 loss -1.2949 (6.629 secs)\n",
      "isanp:isanp-num_latents-32_rbf_100 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.3420 loss -1.3420 (6.634 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.77it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 1.3769 loss -1.3769 (33.420 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.25it/s]\n",
      "isanp:isanp-num_latents-32_rbf_100 rbf tar_ll 1.3769 loss -1.3769 (32.879 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4412963.5 miliseconds\n",
      "Execution time: 4412.9635 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 61.78564453125 MB\n",
      "Memory Usage Change: 45.53564453125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='isanp-num_latents-32_rbf_100',val_seed=100, val_l=32,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f2180-65ea-4eac-8af6-48d93a80b8d5",
   "metadata": {},
   "source": [
    "## ISANP (64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b48f3fda-f23f-4139-8186-99f4c3a6d989",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: isanp-isanp-num_latents-64_rbf_0\n",
      "Total number of parameters: 789186\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 64\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "rbf-seed0.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6940 loss 0.6940 (14.170 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 400 lr 5.000e-04 [train_loss] tar_ll -0.6573 loss 0.6573 (14.029 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 600 lr 5.000e-04 [train_loss] tar_ll -0.5307 loss 0.5307 (14.080 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 800 lr 4.999e-04 [train_loss] tar_ll -0.5028 loss 0.5028 (13.833 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 1000 lr 4.999e-04 [train_loss] tar_ll -0.4674 loss 0.4674 (14.453 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.3649 loss 0.3649 (14.143 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 1400 lr 4.998e-04 [train_loss] tar_ll -0.2073 loss 0.2073 (14.301 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 1600 lr 4.997e-04 [train_loss] tar_ll -0.1724 loss 0.1724 (14.463 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 1800 lr 4.996e-04 [train_loss] tar_ll -0.0974 loss 0.0974 (14.627 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 2000 lr 4.995e-04 [train_loss] tar_ll -0.0239 loss 0.0239 (14.935 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.0801 loss -0.0801 (14.661 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.1166 loss -0.1166 (14.773 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1413 loss -0.1413 (14.669 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.1077 loss -0.1077 (14.487 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.2168 loss -0.2168 (14.916 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.1507 loss -0.1507 (14.519 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.2446 loss -0.2446 (14.125 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2993 loss -0.2993 (13.671 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.2861 loss -0.2861 (13.866 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.3099 loss -0.3099 (13.861 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3634 loss -0.3634 (13.626 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.2817 loss -0.2817 (13.419 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.3134 loss -0.3134 (13.298 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.3529 loss -0.3529 (13.158 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.3510 loss -0.3510 (13.780 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 41.45it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 0.5669 loss -0.5669 (0.029 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.3742 loss -0.3742 (14.011 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.3873 loss -0.3873 (14.301 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4245 loss -0.4245 (14.458 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.2657 loss -0.2657 (14.318 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4748 loss -0.4748 (14.411 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.5013 loss -0.5013 (14.905 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.4869 loss -0.4869 (15.222 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5426 loss -0.5426 (14.800 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.5048 loss -0.5048 (14.452 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.4760 loss -0.4760 (14.430 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.4892 loss -0.4892 (14.377 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.4195 loss -0.4195 (13.928 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.3036 loss -0.3036 (14.029 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.3789 loss -0.3789 (14.247 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.5210 loss -0.5210 (13.494 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.5586 loss -0.5586 (13.738 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.5424 loss -0.5424 (14.672 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.3759 loss -0.3759 (14.934 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.4304 loss -0.4304 (14.856 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.3849 loss -0.3849 (14.944 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.5605 loss -0.5605 (15.489 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.4544 loss -0.4544 (14.689 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.4988 loss -0.4988 (15.062 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.5924 loss -0.5924 (14.982 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.5468 loss -0.5468 (14.117 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 47.99it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 0.6318 loss -0.6318 (0.025 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.5503 loss -0.5503 (14.133 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.5408 loss -0.5408 (14.118 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.5860 loss -0.5860 (13.770 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.7309 loss -0.7309 (13.961 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.6371 loss -0.6371 (13.600 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.5446 loss -0.5446 (13.965 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.5786 loss -0.5786 (13.995 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.4744 loss -0.4744 (13.713 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.4789 loss -0.4789 (14.354 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.6207 loss -0.6207 (14.135 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.6782 loss -0.6782 (13.894 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.6623 loss -0.6623 (13.268 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.6562 loss -0.6562 (14.410 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.6291 loss -0.6291 (14.486 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.6361 loss -0.6361 (14.886 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.6593 loss -0.6593 (15.637 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.6248 loss -0.6248 (14.274 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.6925 loss -0.6925 (14.424 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.7317 loss -0.7317 (13.969 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.4411 loss -0.4411 (14.215 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.5583 loss -0.5583 (14.275 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.4157 loss -0.4157 (14.238 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.5411 loss -0.5411 (13.895 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.6617 loss -0.6617 (13.614 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.7353 loss -0.7353 (13.986 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 38.58it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 1.2291 loss -1.2291 (0.029 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.6352 loss -0.6352 (14.830 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.6082 loss -0.6082 (14.336 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.5675 loss -0.5675 (15.259 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.6596 loss -0.6596 (15.018 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.5494 loss -0.5494 (14.426 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.3883 loss -0.3883 (14.359 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.4230 loss -0.4230 (15.520 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.5685 loss -0.5685 (15.398 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.6127 loss -0.6127 (13.620 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.7324 loss -0.7324 (13.724 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.6716 loss -0.6716 (13.812 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.7546 loss -0.7546 (13.843 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.7991 loss -0.7991 (13.629 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.6755 loss -0.6755 (13.615 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.7084 loss -0.7084 (13.728 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.7508 loss -0.7508 (14.185 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.6996 loss -0.6996 (13.987 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.6412 loss -0.6412 (14.237 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.6629 loss -0.6629 (14.518 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.6872 loss -0.6872 (14.520 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.7115 loss -0.7115 (14.831 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.7938 loss -0.7938 (14.574 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.7612 loss -0.7612 (14.946 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.7492 loss -0.7492 (14.393 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.7426 loss -0.7426 (14.360 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 41.81it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 1.0253 loss -1.0253 (0.028 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.7701 loss -0.7701 (13.764 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.4149 loss -0.4149 (14.158 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.5073 loss -0.5073 (13.992 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.6686 loss -0.6686 (14.216 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.6897 loss -0.6897 (13.860 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.7409 loss -0.7409 (13.868 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.6920 loss -0.6920 (13.920 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.6179 loss -0.6179 (14.105 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.3599 loss -0.3599 (14.303 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.5464 loss -0.5464 (14.885 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.5979 loss -0.5979 (15.335 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.6036 loss -0.6036 (14.538 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.6867 loss -0.6867 (14.917 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.7730 loss -0.7730 (14.844 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.6585 loss -0.6585 (14.605 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.7003 loss -0.7003 (14.728 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.6851 loss -0.6851 (14.157 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.7049 loss -0.7049 (13.557 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.7455 loss -0.7455 (13.677 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.6351 loss -0.6351 (13.526 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.7267 loss -0.7267 (13.990 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.6789 loss -0.6789 (14.816 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.7457 loss -0.7457 (15.013 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.6485 loss -0.6485 (15.391 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.5857 loss -0.5857 (14.375 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 35.79it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 0.9402 loss -0.9402 (0.030 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.6022 loss -0.6022 (14.469 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.5030 loss -0.5030 (14.306 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.5634 loss -0.5634 (14.366 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.5980 loss -0.5980 (14.490 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.6149 loss -0.6149 (14.769 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.6224 loss -0.6224 (14.519 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.6111 loss -0.6111 (14.792 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.4417 loss -0.4417 (14.657 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.4156 loss -0.4156 (14.364 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.3627 loss -0.3627 (14.305 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.5472 loss -0.5472 (14.505 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.5740 loss -0.5740 (14.496 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.6440 loss -0.6440 (14.402 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.4832 loss -0.4832 (14.433 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.6388 loss -0.6388 (14.193 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.6608 loss -0.6608 (14.653 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.6167 loss -0.6167 (14.239 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.6876 loss -0.6876 (14.816 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.7584 loss -0.7584 (15.620 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.7243 loss -0.7243 (14.643 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.6435 loss -0.6435 (14.755 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.6503 loss -0.6503 (14.614 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.6766 loss -0.6766 (14.487 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.7109 loss -0.7109 (14.493 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.7423 loss -0.7423 (14.461 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 38.35it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 1.4714 loss -1.4714 (0.030 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.7957 loss -0.7957 (14.459 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.6777 loss -0.6777 (14.487 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.7388 loss -0.7388 (14.665 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.7614 loss -0.7614 (14.351 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.7864 loss -0.7864 (14.464 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.8500 loss -0.8500 (14.384 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.6648 loss -0.6648 (14.360 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.8052 loss -0.8052 (14.592 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.8245 loss -0.8245 (14.624 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.8357 loss -0.8357 (14.377 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.8096 loss -0.8096 (14.652 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.7531 loss -0.7531 (14.267 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.7153 loss -0.7153 (14.630 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.6850 loss -0.6850 (14.609 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.7181 loss -0.7181 (14.461 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.6271 loss -0.6271 (14.807 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.6322 loss -0.6322 (14.650 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.6657 loss -0.6657 (14.280 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.5843 loss -0.5843 (14.188 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.7420 loss -0.7420 (14.175 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.4786 loss -0.4786 (14.610 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.6315 loss -0.6315 (14.048 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.5174 loss -0.5174 (14.599 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.6914 loss -0.6914 (14.276 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.6680 loss -0.6680 (14.380 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 34.67it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 1.1227 loss -1.1227 (0.033 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.7525 loss -0.7525 (14.675 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.7518 loss -0.7518 (14.620 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.8402 loss -0.8402 (14.507 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.8039 loss -0.8039 (14.921 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.7509 loss -0.7509 (14.458 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.5731 loss -0.5731 (14.634 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.7776 loss -0.7776 (14.695 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.5837 loss -0.5837 (14.762 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.7279 loss -0.7279 (14.358 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.8611 loss -0.8611 (14.527 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.8087 loss -0.8087 (14.401 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.8616 loss -0.8616 (14.313 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.8062 loss -0.8062 (14.257 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.8738 loss -0.8738 (14.074 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.6792 loss -0.6792 (14.083 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.9075 loss -0.9075 (14.483 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.7991 loss -0.7991 (14.570 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.8867 loss -0.8867 (14.513 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.7613 loss -0.7613 (14.482 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.7504 loss -0.7504 (14.477 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.7001 loss -0.7001 (14.611 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.7530 loss -0.7530 (14.690 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.6542 loss -0.6542 (14.585 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.7717 loss -0.7717 (14.496 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.6812 loss -0.6812 (14.401 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 43.04it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 1.1093 loss -1.1093 (0.027 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.8346 loss -0.8346 (14.539 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.7204 loss -0.7204 (14.587 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.6512 loss -0.6512 (14.259 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.8029 loss -0.8029 (14.400 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.8030 loss -0.8030 (14.256 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.7996 loss -0.7996 (14.401 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.8189 loss -0.8189 (14.220 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.7272 loss -0.7272 (14.711 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.8688 loss -0.8688 (14.584 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.8182 loss -0.8182 (14.665 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.8686 loss -0.8686 (14.576 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.8555 loss -0.8555 (14.581 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 42600 lr 3.076e-04 [train_loss] tar_ll 0.8846 loss -0.8846 (14.644 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.8838 loss -0.8838 (14.397 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.9606 loss -0.9606 (14.533 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.9666 loss -0.9666 (14.688 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.9219 loss -0.9219 (14.398 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.7927 loss -0.7927 (14.194 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.8781 loss -0.8781 (14.410 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.9745 loss -0.9745 (14.466 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.9884 loss -0.9884 (14.343 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.8582 loss -0.8582 (14.451 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.9812 loss -0.9812 (14.280 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.9379 loss -0.9379 (14.526 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.9347 loss -0.9347 (14.512 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 31.90it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 1.5338 loss -1.5338 (0.036 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.9027 loss -0.9027 (14.472 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 45400 lr 2.860e-04 [train_loss] tar_ll 1.0241 loss -1.0241 (14.797 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.9608 loss -0.9608 (14.631 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.8372 loss -0.8372 (14.505 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.7179 loss -0.7179 (14.483 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.9214 loss -0.9214 (14.762 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.9721 loss -0.9721 (14.590 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.9467 loss -0.9467 (14.526 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.9509 loss -0.9509 (14.441 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.9411 loss -0.9411 (14.291 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.8953 loss -0.8953 (14.417 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.9720 loss -0.9720 (14.222 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 47600 lr 2.688e-04 [train_loss] tar_ll 1.0122 loss -1.0122 (14.298 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.9829 loss -0.9829 (14.036 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.9634 loss -0.9634 (14.251 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.9511 loss -0.9511 (14.230 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 48400 lr 2.626e-04 [train_loss] tar_ll 0.9202 loss -0.9202 (14.641 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.9331 loss -0.9331 (14.368 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 48800 lr 2.594e-04 [train_loss] tar_ll 0.9453 loss -0.9453 (14.405 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.9665 loss -0.9665 (14.373 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 49200 lr 2.563e-04 [train_loss] tar_ll 1.0339 loss -1.0339 (14.789 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.9208 loss -0.9208 (14.418 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 49600 lr 2.531e-04 [train_loss] tar_ll 1.0845 loss -1.0845 (14.290 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.9595 loss -0.9595 (14.675 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 50000 lr 2.500e-04 [train_loss] tar_ll 1.0747 loss -1.0747 (14.341 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 37.20it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 1.7422 loss -1.7422 (0.029 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 50200 lr 2.484e-04 [train_loss] tar_ll 1.0015 loss -1.0015 (14.375 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 50400 lr 2.469e-04 [train_loss] tar_ll 0.9267 loss -0.9267 (14.512 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 50600 lr 2.453e-04 [train_loss] tar_ll 1.1075 loss -1.1075 (14.363 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 50800 lr 2.437e-04 [train_loss] tar_ll 0.9444 loss -0.9444 (14.265 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 51000 lr 2.421e-04 [train_loss] tar_ll 1.0184 loss -1.0184 (14.221 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 51200 lr 2.406e-04 [train_loss] tar_ll 0.9358 loss -0.9358 (14.592 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 51400 lr 2.390e-04 [train_loss] tar_ll 1.0439 loss -1.0439 (14.368 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 51600 lr 2.374e-04 [train_loss] tar_ll 1.0181 loss -1.0181 (14.335 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 51800 lr 2.359e-04 [train_loss] tar_ll 0.9703 loss -0.9703 (14.578 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 52000 lr 2.343e-04 [train_loss] tar_ll 1.0102 loss -1.0102 (14.440 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 52200 lr 2.327e-04 [train_loss] tar_ll 0.9305 loss -0.9305 (14.565 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 52400 lr 2.312e-04 [train_loss] tar_ll 1.1526 loss -1.1526 (14.513 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 52600 lr 2.296e-04 [train_loss] tar_ll 1.0742 loss -1.0742 (14.497 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 52800 lr 2.280e-04 [train_loss] tar_ll 1.0972 loss -1.0972 (14.383 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 53000 lr 2.265e-04 [train_loss] tar_ll 1.0913 loss -1.0913 (14.447 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 53200 lr 2.249e-04 [train_loss] tar_ll 1.0747 loss -1.0747 (14.547 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 53400 lr 2.233e-04 [train_loss] tar_ll 1.1048 loss -1.1048 (14.397 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 53600 lr 2.218e-04 [train_loss] tar_ll 1.0736 loss -1.0736 (14.302 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 53800 lr 2.202e-04 [train_loss] tar_ll 1.0465 loss -1.0465 (14.230 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 54000 lr 2.187e-04 [train_loss] tar_ll 1.0907 loss -1.0907 (14.324 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 54200 lr 2.171e-04 [train_loss] tar_ll 0.9881 loss -0.9881 (14.214 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 54400 lr 2.156e-04 [train_loss] tar_ll 1.1018 loss -1.1018 (14.528 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 54600 lr 2.140e-04 [train_loss] tar_ll 0.9637 loss -0.9637 (13.984 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 54800 lr 2.124e-04 [train_loss] tar_ll 1.0395 loss -1.0395 (14.494 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 55000 lr 2.109e-04 [train_loss] tar_ll 1.0446 loss -1.0446 (14.460 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 39.56it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 1.8112 loss -1.8112 (0.031 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 55200 lr 2.093e-04 [train_loss] tar_ll 1.1261 loss -1.1261 (15.073 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 55400 lr 2.078e-04 [train_loss] tar_ll 1.0141 loss -1.0141 (14.892 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 55600 lr 2.062e-04 [train_loss] tar_ll 0.9288 loss -0.9288 (14.744 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 55800 lr 2.047e-04 [train_loss] tar_ll 1.0902 loss -1.0902 (14.590 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 56000 lr 2.032e-04 [train_loss] tar_ll 1.1047 loss -1.1047 (14.588 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 56200 lr 2.016e-04 [train_loss] tar_ll 1.0703 loss -1.0703 (15.216 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 56400 lr 2.001e-04 [train_loss] tar_ll 1.0602 loss -1.0602 (14.555 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 56600 lr 1.985e-04 [train_loss] tar_ll 1.0348 loss -1.0348 (14.460 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0737 loss -1.0737 (13.877 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 57000 lr 1.955e-04 [train_loss] tar_ll 1.1122 loss -1.1122 (14.367 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 57200 lr 1.939e-04 [train_loss] tar_ll 1.1681 loss -1.1681 (14.412 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 57400 lr 1.924e-04 [train_loss] tar_ll 1.0028 loss -1.0028 (14.199 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 57600 lr 1.909e-04 [train_loss] tar_ll 1.1207 loss -1.1207 (14.335 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 57800 lr 1.894e-04 [train_loss] tar_ll 1.1541 loss -1.1541 (14.403 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 58000 lr 1.878e-04 [train_loss] tar_ll 1.1291 loss -1.1291 (14.144 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 58200 lr 1.863e-04 [train_loss] tar_ll 1.1026 loss -1.1026 (14.586 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 58400 lr 1.848e-04 [train_loss] tar_ll 1.0714 loss -1.0714 (14.570 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 58600 lr 1.833e-04 [train_loss] tar_ll 1.0979 loss -1.0979 (14.492 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 58800 lr 1.818e-04 [train_loss] tar_ll 1.1067 loss -1.1067 (14.605 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.0962 loss -1.0962 (14.907 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 59200 lr 1.787e-04 [train_loss] tar_ll 1.0954 loss -1.0954 (14.688 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 59400 lr 1.772e-04 [train_loss] tar_ll 1.0837 loss -1.0837 (14.504 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 59600 lr 1.757e-04 [train_loss] tar_ll 1.1053 loss -1.1053 (14.529 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 59800 lr 1.742e-04 [train_loss] tar_ll 1.1104 loss -1.1104 (14.574 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 60000 lr 1.727e-04 [train_loss] tar_ll 1.0729 loss -1.0729 (14.288 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 33.34it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 1.8977 loss -1.8977 (0.035 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 60200 lr 1.713e-04 [train_loss] tar_ll 1.1221 loss -1.1221 (14.646 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 60400 lr 1.698e-04 [train_loss] tar_ll 1.1438 loss -1.1438 (14.474 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 60600 lr 1.683e-04 [train_loss] tar_ll 0.9785 loss -0.9785 (14.504 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.0528 loss -1.0528 (14.226 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 61000 lr 1.653e-04 [train_loss] tar_ll 1.0516 loss -1.0516 (14.366 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 61200 lr 1.638e-04 [train_loss] tar_ll 1.0712 loss -1.0712 (14.340 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 61400 lr 1.624e-04 [train_loss] tar_ll 1.1741 loss -1.1741 (14.376 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 61600 lr 1.609e-04 [train_loss] tar_ll 1.1226 loss -1.1226 (14.901 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.1573 loss -1.1573 (14.665 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 62000 lr 1.580e-04 [train_loss] tar_ll 1.1777 loss -1.1777 (14.552 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 62200 lr 1.565e-04 [train_loss] tar_ll 1.1536 loss -1.1536 (14.414 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.1628 loss -1.1628 (14.647 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.1265 loss -1.1265 (14.568 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.1012 loss -1.1012 (14.621 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.1159 loss -1.1159 (14.502 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.1971 loss -1.1971 (14.514 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.1102 loss -1.1102 (14.242 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.1375 loss -1.1375 (14.181 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.1931 loss -1.1931 (14.258 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.0588 loss -1.0588 (14.513 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.0434 loss -1.0434 (14.368 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.1011 loss -1.1011 (14.389 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.1455 loss -1.1455 (14.152 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.1383 loss -1.1383 (14.218 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.1236 loss -1.1236 (14.554 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 39.25it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 1.9616 loss -1.9616 (0.030 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.2177 loss -1.2177 (14.746 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.1531 loss -1.1531 (14.770 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.1481 loss -1.1481 (14.562 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.1158 loss -1.1158 (14.525 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.1805 loss -1.1805 (14.412 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.2103 loss -1.2103 (14.675 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.1465 loss -1.1465 (14.889 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.1895 loss -1.1895 (14.063 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.2098 loss -1.2098 (14.210 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.2232 loss -1.2232 (14.419 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.1352 loss -1.1352 (14.492 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.1495 loss -1.1495 (14.316 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.1388 loss -1.1388 (14.684 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.2262 loss -1.2262 (14.355 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.1762 loss -1.1762 (14.244 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.0915 loss -1.0915 (14.415 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.2265 loss -1.2265 (14.386 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.1824 loss -1.1824 (14.530 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.2389 loss -1.2389 (14.501 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.1594 loss -1.1594 (14.833 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.1622 loss -1.1622 (14.634 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.2154 loss -1.2154 (14.537 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.2583 loss -1.2583 (14.500 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.2512 loss -1.2512 (14.429 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.2026 loss -1.2026 (14.062 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 38.66it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 2.1491 loss -2.1491 (0.030 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.2411 loss -1.2411 (14.470 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.1969 loss -1.1969 (14.308 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.2224 loss -1.2224 (14.459 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.2249 loss -1.2249 (14.483 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.2039 loss -1.2039 (14.275 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.1918 loss -1.1918 (14.387 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.2052 loss -1.2052 (14.884 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.2006 loss -1.2006 (14.839 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.2187 loss -1.2187 (14.544 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.2745 loss -1.2745 (14.533 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.2084 loss -1.2084 (14.671 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.2875 loss -1.2875 (14.419 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.2093 loss -1.2093 (14.385 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.1903 loss -1.1903 (14.600 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.3304 loss -1.3304 (14.637 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.2892 loss -1.2892 (14.437 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.2706 loss -1.2706 (14.204 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.1649 loss -1.1649 (14.242 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.2033 loss -1.2033 (14.647 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.2255 loss -1.2255 (14.419 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.2597 loss -1.2597 (14.646 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.1957 loss -1.1957 (14.178 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.2484 loss -1.2484 (14.134 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.2218 loss -1.2218 (14.181 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.2331 loss -1.2331 (14.475 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 43.42it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 2.2033 loss -2.2033 (0.026 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.3022 loss -1.3022 (14.798 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.1595 loss -1.1595 (14.723 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.2984 loss -1.2984 (14.545 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.3346 loss -1.3346 (14.429 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.2069 loss -1.2069 (14.693 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.2571 loss -1.2571 (14.685 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.3247 loss -1.3247 (14.919 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.2170 loss -1.2170 (14.391 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.2180 loss -1.2180 (14.193 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.3160 loss -1.3160 (14.402 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.1778 loss -1.1778 (14.444 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.2223 loss -1.2223 (14.124 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.3523 loss -1.3523 (14.003 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2890 loss -1.2890 (14.176 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.2395 loss -1.2395 (14.360 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.3020 loss -1.3020 (14.651 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.2990 loss -1.2990 (14.358 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.2841 loss -1.2841 (14.766 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.2927 loss -1.2927 (14.863 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.2763 loss -1.2763 (14.694 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.2249 loss -1.2249 (14.705 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.2793 loss -1.2793 (14.771 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.3037 loss -1.3037 (14.550 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.2622 loss -1.2622 (14.798 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.2766 loss -1.2766 (14.407 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 38.93it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 2.2182 loss -2.2182 (0.030 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.3461 loss -1.3461 (14.314 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.1945 loss -1.1945 (14.109 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.3446 loss -1.3446 (14.414 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2103 loss -1.2103 (14.243 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.3118 loss -1.3118 (14.222 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.2775 loss -1.2775 (14.280 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.2609 loss -1.2609 (14.408 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.3409 loss -1.3409 (14.524 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.3195 loss -1.3195 (14.563 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.3462 loss -1.3462 (14.598 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.3132 loss -1.3132 (14.386 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.2879 loss -1.2879 (14.855 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.2949 loss -1.2949 (14.685 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.2738 loss -1.2738 (14.768 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.3337 loss -1.3337 (14.796 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.2995 loss -1.2995 (14.522 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.2808 loss -1.2808 (14.765 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.2937 loss -1.2937 (14.499 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.2523 loss -1.2523 (14.278 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.4005 loss -1.4005 (14.504 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2238 loss -1.2238 (14.739 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.2545 loss -1.2545 (14.385 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.1513 loss -1.1513 (14.385 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.2998 loss -1.2998 (14.496 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.2963 loss -1.2963 (14.461 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 35.99it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 2.3446 loss -2.3446 (0.032 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.3471 loss -1.3471 (14.680 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.2295 loss -1.2295 (14.706 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.3376 loss -1.3376 (14.565 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.3163 loss -1.3163 (14.558 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.2677 loss -1.2677 (14.931 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.3339 loss -1.3339 (14.795 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.3488 loss -1.3488 (14.763 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.3338 loss -1.3338 (14.597 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.3121 loss -1.3121 (14.323 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.3398 loss -1.3398 (14.110 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.2532 loss -1.2532 (14.565 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.2774 loss -1.2774 (14.098 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.2620 loss -1.2620 (14.335 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.2766 loss -1.2766 (14.372 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.2561 loss -1.2561 (14.280 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.2841 loss -1.2841 (14.890 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.3223 loss -1.3223 (14.573 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.3135 loss -1.3135 (14.953 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.3738 loss -1.3738 (14.694 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.2952 loss -1.2952 (14.815 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.3971 loss -1.3971 (14.948 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.3955 loss -1.3955 (14.527 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.2824 loss -1.2824 (14.541 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.3108 loss -1.3108 (14.343 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.3706 loss -1.3706 (14.411 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 36.94it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 2.3377 loss -2.3377 (0.031 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.3355 loss -1.3355 (14.698 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.3474 loss -1.3474 (14.351 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.2309 loss -1.2309 (14.403 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.2591 loss -1.2591 (14.211 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.2768 loss -1.2768 (14.562 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.3790 loss -1.3790 (14.738 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2802 loss -1.2802 (14.828 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.3560 loss -1.3560 (14.498 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2874 loss -1.2874 (14.509 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.3298 loss -1.3298 (14.482 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.3752 loss -1.3752 (14.706 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.3481 loss -1.3481 (14.397 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.3273 loss -1.3273 (14.587 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.3213 loss -1.3213 (14.437 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.2942 loss -1.2942 (14.161 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.3340 loss -1.3340 (14.394 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.3044 loss -1.3044 (14.231 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.3802 loss -1.3802 (14.169 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.3572 loss -1.3572 (15.080 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.3230 loss -1.3230 (14.817 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.3066 loss -1.3066 (14.598 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.3833 loss -1.3833 (14.041 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.3926 loss -1.3926 (14.152 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.3262 loss -1.3262 (14.718 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.2968 loss -1.2968 (14.544 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 37.50it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 2.3611 loss -2.3611 (0.033 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_0 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.2417 loss -1.2417 (14.765 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.3032 loss -1.3032 (14.797 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.3952 loss -1.3952 (14.499 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.3924 loss -1.3924 (14.670 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.3331 loss -1.3331 (14.821 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2694 loss -1.2694 (14.961 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.3445 loss -1.3445 (14.268 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.2851 loss -1.2851 (14.468 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.2948 loss -1.2948 (14.521 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.3415 loss -1.3415 (14.423 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.2521 loss -1.2521 (14.578 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.4024 loss -1.4024 (14.603 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.3226 loss -1.3226 (14.451 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.3309 loss -1.3309 (14.382 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.3325 loss -1.3325 (14.810 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.2697 loss -1.2697 (14.693 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.2926 loss -1.2926 (14.563 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.4144 loss -1.4144 (14.606 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.3941 loss -1.3941 (14.845 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.4412 loss -1.4412 (14.836 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.2666 loss -1.2666 (15.157 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.3402 loss -1.3402 (14.685 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.3837 loss -1.3837 (14.248 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.3022 loss -1.3022 (14.331 secs)\n",
      "isanp:isanp-num_latents-64_rbf_0 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.3345 loss -1.3345 (14.367 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 37.74it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 2.3682 loss -2.3682 (0.032 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 38.31it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 2.3682 loss -2.3682 (0.029 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "Execution time: 7226873.0 miliseconds\n",
      "Execution time: 7226.873 seconds\n",
      "Initial Memory Usage: 28.1552734375 MB\n",
      "Final Memory Usage: 92.74951171875 MB\n",
      "Memory Usage Change: 64.59423828125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='isanp-num_latents-64_rbf_0',val_seed=0, val_l=64,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f57c7-dc46-4e70-a037-924893d86cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68d32f5f-5e67-480a-9a65-907a5c6e2add",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 64\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:10<00:00, 42.30it/s]\n",
      "isanp:isanp-num_latents-64_rbf_0 rbf tar_ll 1.3358 loss -1.3358 (70.929 secs)\n"
     ]
    }
   ],
   "source": [
    "%run gp.py --mode eval --expid isanp-num_latents-64_rbf_0 --model isanp --num_latents 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f93c11cb-67fe-4246-b6f2-f2b8a7b4b786",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: isanp-isanp-num_latents-64_rbf_100\n",
      "Total number of parameters: 789186\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 64\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-64_rbf_100 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6795 loss 0.6795 (6.772 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 400 lr 5.000e-04 [train_loss] tar_ll -0.6247 loss 0.6247 (6.649 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 600 lr 5.000e-04 [train_loss] tar_ll -0.4983 loss 0.4983 (6.589 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 800 lr 4.999e-04 [train_loss] tar_ll -0.3992 loss 0.3992 (6.587 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 1000 lr 4.999e-04 [train_loss] tar_ll -0.3044 loss 0.3044 (6.544 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.2924 loss 0.2924 (6.741 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 1400 lr 4.998e-04 [train_loss] tar_ll -0.2119 loss 0.2119 (6.702 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 1600 lr 4.997e-04 [train_loss] tar_ll -0.0853 loss 0.0853 (6.319 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 1800 lr 4.996e-04 [train_loss] tar_ll -0.0123 loss 0.0123 (6.429 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 2000 lr 4.995e-04 [train_loss] tar_ll -0.0208 loss 0.0208 (6.401 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.0109 loss -0.0109 (6.343 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.1437 loss -0.1437 (6.578 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1738 loss -0.1738 (6.466 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.1620 loss -0.1620 (6.563 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.2710 loss -0.2710 (6.275 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.2109 loss -0.2109 (6.406 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.3250 loss -0.3250 (6.429 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.1817 loss -0.1817 (6.443 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.2364 loss -0.2364 (6.421 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.2678 loss -0.2678 (6.369 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3829 loss -0.3829 (6.385 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.3153 loss -0.3153 (6.425 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4037 loss -0.4037 (6.397 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4848 loss -0.4848 (6.486 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.3939 loss -0.3939 (6.291 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 92.66it/s] \n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 0.4600 loss -0.4600 (32.379 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.3704 loss -0.3704 (6.530 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.5064 loss -0.5064 (6.388 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4626 loss -0.4626 (6.377 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.5224 loss -0.5224 (6.554 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4502 loss -0.4502 (6.358 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.4740 loss -0.4740 (6.531 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.5054 loss -0.5054 (6.484 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5401 loss -0.5401 (6.430 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.5449 loss -0.5449 (6.648 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.4821 loss -0.4821 (6.557 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.5467 loss -0.5467 (6.608 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5451 loss -0.5451 (6.527 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.5527 loss -0.5527 (6.434 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.4690 loss -0.4690 (6.634 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.4546 loss -0.4546 (6.601 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.4748 loss -0.4748 (6.620 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.4916 loss -0.4916 (6.499 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.4846 loss -0.4846 (6.502 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.5531 loss -0.5531 (6.559 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.5922 loss -0.5922 (6.480 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.4381 loss -0.4381 (6.591 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.5482 loss -0.5482 (6.366 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.5654 loss -0.5654 (6.486 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.5741 loss -0.5741 (6.586 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.5366 loss -0.5366 (6.407 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 92.64it/s] \n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 0.4645 loss -0.4645 (32.383 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.5670 loss -0.5670 (6.809 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.2045 loss -0.2045 (6.714 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.4585 loss -0.4585 (6.691 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.2380 loss -0.2380 (6.697 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.5163 loss -0.5163 (6.840 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.5526 loss -0.5526 (6.855 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.6463 loss -0.6463 (6.830 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.4514 loss -0.4514 (6.891 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.5343 loss -0.5343 (6.538 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.5223 loss -0.5223 (6.498 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.5642 loss -0.5642 (6.483 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.5108 loss -0.5108 (6.364 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.5362 loss -0.5362 (6.435 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.5141 loss -0.5141 (6.370 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.5621 loss -0.5621 (6.444 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.6589 loss -0.6589 (6.415 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.7039 loss -0.7039 (6.437 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.6552 loss -0.6552 (6.411 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.7323 loss -0.7323 (6.340 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.6778 loss -0.6778 (6.432 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.6720 loss -0.6720 (6.380 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.6866 loss -0.6866 (6.325 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.7309 loss -0.7309 (6.434 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.6112 loss -0.6112 (6.451 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.7073 loss -0.7073 (6.448 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 94.11it/s] \n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 0.6901 loss -0.6901 (31.880 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.7086 loss -0.7086 (6.458 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.7139 loss -0.7139 (6.380 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.7075 loss -0.7075 (6.520 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.6598 loss -0.6598 (6.459 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.5764 loss -0.5764 (6.564 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.6419 loss -0.6419 (6.561 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.6984 loss -0.6984 (6.442 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.6839 loss -0.6839 (6.670 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.7772 loss -0.7772 (6.484 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.6205 loss -0.6205 (6.513 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.6643 loss -0.6643 (6.506 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.7172 loss -0.7172 (6.630 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.7166 loss -0.7166 (6.473 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.5520 loss -0.5520 (6.466 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.6729 loss -0.6729 (6.620 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.7450 loss -0.7450 (6.534 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.7064 loss -0.7064 (6.545 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.7565 loss -0.7565 (6.383 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.8007 loss -0.8007 (6.452 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.5583 loss -0.5583 (6.516 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.7003 loss -0.7003 (6.645 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.6015 loss -0.6015 (6.797 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.7198 loss -0.7198 (6.643 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.6541 loss -0.6541 (6.509 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.4595 loss -0.4595 (6.738 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 92.32it/s]\n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 0.3781 loss -0.3781 (32.498 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.6566 loss -0.6566 (6.774 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.6958 loss -0.6958 (6.787 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.6317 loss -0.6317 (6.763 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.7234 loss -0.7234 (6.688 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.7138 loss -0.7138 (6.387 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.6941 loss -0.6941 (6.497 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.6778 loss -0.6778 (6.534 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.6910 loss -0.6910 (6.346 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.7712 loss -0.7712 (6.489 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.7913 loss -0.7913 (6.381 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.7312 loss -0.7312 (6.428 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.6286 loss -0.6286 (6.542 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.7242 loss -0.7242 (6.325 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.7470 loss -0.7470 (6.531 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.7156 loss -0.7156 (6.379 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.8102 loss -0.8102 (6.396 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.7783 loss -0.7783 (6.471 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.7147 loss -0.7147 (6.349 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.7779 loss -0.7779 (6.524 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.8808 loss -0.8808 (6.519 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.8043 loss -0.8043 (6.418 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.8550 loss -0.8550 (6.459 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.7676 loss -0.7676 (6.545 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.3789 loss -0.3789 (6.468 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.6997 loss -0.6997 (6.564 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 95.68it/s] \n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 0.8215 loss -0.8215 (31.358 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.7023 loss -0.7023 (6.543 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.7870 loss -0.7870 (6.583 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.7792 loss -0.7792 (6.486 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.8246 loss -0.8246 (6.719 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.8704 loss -0.8704 (6.731 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.8731 loss -0.8731 (6.882 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.8553 loss -0.8553 (6.668 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.8980 loss -0.8980 (6.521 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.9277 loss -0.9277 (6.654 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.8910 loss -0.8910 (6.576 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.8832 loss -0.8832 (6.600 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.8843 loss -0.8843 (6.523 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.8729 loss -0.8729 (6.665 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.8848 loss -0.8848 (6.620 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.6848 loss -0.6848 (6.549 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.8055 loss -0.8055 (6.901 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.8168 loss -0.8168 (6.629 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.9327 loss -0.9327 (6.908 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.8181 loss -0.8181 (6.839 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.9483 loss -0.9483 (6.741 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.6907 loss -0.6907 (6.819 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.8294 loss -0.8294 (6.743 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.9917 loss -0.9917 (6.929 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.8374 loss -0.8374 (6.810 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.6302 loss -0.6302 (6.890 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.38it/s] \n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 0.2346 loss -0.2346 (32.831 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.4017 loss -0.4017 (6.494 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.6569 loss -0.6569 (6.434 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.7769 loss -0.7769 (6.569 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.8239 loss -0.8239 (6.326 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.8155 loss -0.8155 (6.496 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.8332 loss -0.8332 (6.494 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.6416 loss -0.6416 (6.407 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.5511 loss -0.5511 (6.494 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.5156 loss -0.5156 (6.420 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.6689 loss -0.6689 (6.591 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.3436 loss -0.3436 (6.541 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.4183 loss -0.4183 (6.503 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.6263 loss -0.6263 (6.455 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.7429 loss -0.7429 (6.478 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.7801 loss -0.7801 (6.512 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.5606 loss -0.5606 (7.221 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.5457 loss -0.5457 (7.317 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.8273 loss -0.8273 (6.976 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.8277 loss -0.8277 (6.787 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.7804 loss -0.7804 (6.917 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.7482 loss -0.7482 (6.674 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.6696 loss -0.6696 (6.541 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.7100 loss -0.7100 (6.432 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.8294 loss -0.8294 (6.595 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.7234 loss -0.7234 (6.395 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.12it/s]\n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 0.8232 loss -0.8232 (34.838 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.7960 loss -0.7960 (6.584 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.7917 loss -0.7917 (6.476 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.8494 loss -0.8494 (6.457 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.5771 loss -0.5771 (6.557 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.6859 loss -0.6859 (6.414 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.7247 loss -0.7247 (6.546 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.7192 loss -0.7192 (6.460 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.7430 loss -0.7430 (6.387 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.4686 loss -0.4686 (6.455 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.5309 loss -0.5309 (6.564 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.7870 loss -0.7870 (6.815 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.7415 loss -0.7415 (6.538 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.7643 loss -0.7643 (6.656 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.7984 loss -0.7984 (6.709 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.8327 loss -0.8327 (6.595 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.8858 loss -0.8858 (6.852 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.8635 loss -0.8635 (6.682 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.9527 loss -0.9527 (6.621 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.8967 loss -0.8967 (6.649 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.9324 loss -0.9324 (6.713 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.8783 loss -0.8783 (6.764 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.8721 loss -0.8721 (6.645 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.8719 loss -0.8719 (6.549 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.9257 loss -0.9257 (6.280 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.8256 loss -0.8256 (6.380 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 96.18it/s] \n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 0.8403 loss -0.8403 (31.195 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.8672 loss -0.8672 (6.573 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.8330 loss -0.8330 (6.372 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.8288 loss -0.8288 (6.512 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.8754 loss -0.8754 (6.382 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.8353 loss -0.8353 (6.302 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.8570 loss -0.8570 (6.534 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.7644 loss -0.7644 (6.366 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.5717 loss -0.5717 (6.429 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.7056 loss -0.7056 (6.394 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.7857 loss -0.7857 (6.409 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.7651 loss -0.7651 (6.589 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.8361 loss -0.8361 (6.361 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 42600 lr 3.076e-04 [train_loss] tar_ll 0.8241 loss -0.8241 (6.293 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.8996 loss -0.8996 (6.350 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.9820 loss -0.9820 (6.569 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.9006 loss -0.9006 (6.449 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.6873 loss -0.6873 (6.435 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.8870 loss -0.8870 (6.446 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.8444 loss -0.8444 (6.459 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.9414 loss -0.9414 (6.535 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.9032 loss -0.9032 (6.496 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.9043 loss -0.9043 (6.488 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.9409 loss -0.9409 (6.455 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.8684 loss -0.8684 (6.508 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.9407 loss -0.9407 (6.445 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 94.37it/s]\n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 0.9638 loss -0.9638 (31.793 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 45200 lr 2.876e-04 [train_loss] tar_ll 1.0345 loss -1.0345 (6.383 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.9336 loss -0.9336 (6.338 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.9267 loss -0.9267 (6.585 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.9097 loss -0.9097 (6.614 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.9030 loss -0.9030 (6.733 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.9317 loss -0.9317 (6.513 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.8798 loss -0.8798 (6.467 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.7947 loss -0.7947 (6.656 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.8121 loss -0.8121 (6.524 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.6381 loss -0.6381 (6.631 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.7281 loss -0.7281 (6.612 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.8659 loss -0.8659 (6.554 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 47600 lr 2.688e-04 [train_loss] tar_ll 0.9839 loss -0.9839 (6.679 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.7962 loss -0.7962 (6.659 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.9538 loss -0.9538 (6.716 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.4178 loss -0.4178 (6.698 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 48400 lr 2.626e-04 [train_loss] tar_ll 0.5498 loss -0.5498 (6.694 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.7016 loss -0.7016 (6.579 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 48800 lr 2.594e-04 [train_loss] tar_ll 0.6642 loss -0.6642 (6.485 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.6901 loss -0.6901 (6.411 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 49200 lr 2.563e-04 [train_loss] tar_ll 0.6809 loss -0.6809 (6.437 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.8725 loss -0.8725 (6.437 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 49600 lr 2.531e-04 [train_loss] tar_ll 0.8351 loss -0.8351 (6.444 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.7778 loss -0.7778 (6.504 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 50000 lr 2.500e-04 [train_loss] tar_ll 0.7328 loss -0.7328 (6.459 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 95.79it/s] \n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 0.8149 loss -0.8149 (31.320 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 50200 lr 2.484e-04 [train_loss] tar_ll 0.7354 loss -0.7354 (6.473 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 50400 lr 2.469e-04 [train_loss] tar_ll 0.7629 loss -0.7629 (6.832 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 50600 lr 2.453e-04 [train_loss] tar_ll 0.7637 loss -0.7637 (6.624 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 50800 lr 2.437e-04 [train_loss] tar_ll 0.8022 loss -0.8022 (7.183 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 51000 lr 2.421e-04 [train_loss] tar_ll 0.8778 loss -0.8778 (7.137 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 51200 lr 2.406e-04 [train_loss] tar_ll 1.0104 loss -1.0104 (7.270 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 51400 lr 2.390e-04 [train_loss] tar_ll 0.9067 loss -0.9067 (6.786 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 51600 lr 2.374e-04 [train_loss] tar_ll 0.8735 loss -0.8735 (6.446 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 51800 lr 2.359e-04 [train_loss] tar_ll 1.0080 loss -1.0080 (6.654 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 52000 lr 2.343e-04 [train_loss] tar_ll 1.0032 loss -1.0032 (6.614 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 52200 lr 2.327e-04 [train_loss] tar_ll 1.0011 loss -1.0011 (6.521 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 52400 lr 2.312e-04 [train_loss] tar_ll 0.5793 loss -0.5793 (6.805 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 52600 lr 2.296e-04 [train_loss] tar_ll 0.4199 loss -0.4199 (6.571 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 52800 lr 2.280e-04 [train_loss] tar_ll 0.6458 loss -0.6458 (6.704 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 53000 lr 2.265e-04 [train_loss] tar_ll 0.7605 loss -0.7605 (6.573 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 53200 lr 2.249e-04 [train_loss] tar_ll 0.8330 loss -0.8330 (6.773 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 53400 lr 2.233e-04 [train_loss] tar_ll 0.8039 loss -0.8039 (6.572 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 53600 lr 2.218e-04 [train_loss] tar_ll 0.8044 loss -0.8044 (6.470 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 53800 lr 2.202e-04 [train_loss] tar_ll 0.8600 loss -0.8600 (6.547 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 54000 lr 2.187e-04 [train_loss] tar_ll 0.9249 loss -0.9249 (6.451 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 54200 lr 2.171e-04 [train_loss] tar_ll 0.8423 loss -0.8423 (6.597 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 54400 lr 2.156e-04 [train_loss] tar_ll 0.8893 loss -0.8893 (7.308 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 54600 lr 2.140e-04 [train_loss] tar_ll 0.9178 loss -0.9178 (6.644 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 54800 lr 2.124e-04 [train_loss] tar_ll 0.9435 loss -0.9435 (6.632 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 55000 lr 2.109e-04 [train_loss] tar_ll 0.8763 loss -0.8763 (6.439 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.67it/s]\n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 0.8707 loss -0.8707 (33.460 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 55200 lr 2.093e-04 [train_loss] tar_ll 1.0018 loss -1.0018 (6.932 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 55400 lr 2.078e-04 [train_loss] tar_ll 0.9617 loss -0.9617 (6.561 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 55600 lr 2.062e-04 [train_loss] tar_ll 0.9517 loss -0.9517 (6.637 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 55800 lr 2.047e-04 [train_loss] tar_ll 1.0028 loss -1.0028 (6.490 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 56000 lr 2.032e-04 [train_loss] tar_ll 1.0631 loss -1.0631 (6.636 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 56200 lr 2.016e-04 [train_loss] tar_ll 0.9673 loss -0.9673 (6.684 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 56400 lr 2.001e-04 [train_loss] tar_ll 0.9136 loss -0.9136 (6.640 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 56600 lr 1.985e-04 [train_loss] tar_ll 0.9581 loss -0.9581 (6.689 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0270 loss -1.0270 (6.661 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 57000 lr 1.955e-04 [train_loss] tar_ll 0.9416 loss -0.9416 (6.744 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 57200 lr 1.939e-04 [train_loss] tar_ll 1.0021 loss -1.0021 (6.753 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 57400 lr 1.924e-04 [train_loss] tar_ll 1.0355 loss -1.0355 (6.727 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 57600 lr 1.909e-04 [train_loss] tar_ll 1.0135 loss -1.0135 (6.526 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 57800 lr 1.894e-04 [train_loss] tar_ll 1.0432 loss -1.0432 (6.339 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 58000 lr 1.878e-04 [train_loss] tar_ll 1.0826 loss -1.0826 (6.862 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 58200 lr 1.863e-04 [train_loss] tar_ll 1.0701 loss -1.0701 (6.510 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 58400 lr 1.848e-04 [train_loss] tar_ll 1.1299 loss -1.1299 (6.422 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 58600 lr 1.833e-04 [train_loss] tar_ll 1.0462 loss -1.0462 (6.518 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 58800 lr 1.818e-04 [train_loss] tar_ll 0.9505 loss -0.9505 (6.357 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.0960 loss -1.0960 (6.445 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 59200 lr 1.787e-04 [train_loss] tar_ll 1.1147 loss -1.1147 (6.425 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 59400 lr 1.772e-04 [train_loss] tar_ll 0.9943 loss -0.9943 (6.446 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 59600 lr 1.757e-04 [train_loss] tar_ll 1.0102 loss -1.0102 (6.538 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 59800 lr 1.742e-04 [train_loss] tar_ll 0.9453 loss -0.9453 (6.369 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 60000 lr 1.727e-04 [train_loss] tar_ll 0.9273 loss -0.9273 (6.482 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.98it/s]\n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 0.9031 loss -0.9031 (33.718 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 60200 lr 1.713e-04 [train_loss] tar_ll 0.9501 loss -0.9501 (6.831 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 60400 lr 1.698e-04 [train_loss] tar_ll 0.9582 loss -0.9582 (6.677 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.0257 loss -1.0257 (6.430 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 60800 lr 1.668e-04 [train_loss] tar_ll 0.9862 loss -0.9862 (6.380 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 61000 lr 1.653e-04 [train_loss] tar_ll 0.9982 loss -0.9982 (6.552 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 61200 lr 1.638e-04 [train_loss] tar_ll 1.0999 loss -1.0999 (6.549 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 61400 lr 1.624e-04 [train_loss] tar_ll 1.0568 loss -1.0568 (6.531 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 61600 lr 1.609e-04 [train_loss] tar_ll 1.0352 loss -1.0352 (6.425 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.0240 loss -1.0240 (6.542 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 62000 lr 1.580e-04 [train_loss] tar_ll 1.0247 loss -1.0247 (6.527 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 62200 lr 1.565e-04 [train_loss] tar_ll 1.0523 loss -1.0523 (6.617 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.1196 loss -1.1196 (6.590 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.1414 loss -1.1414 (6.513 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.1080 loss -1.1080 (6.540 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.1677 loss -1.1677 (6.523 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.0295 loss -1.0295 (6.487 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.0808 loss -1.0808 (6.438 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.1176 loss -1.1176 (6.495 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.0511 loss -1.0511 (6.470 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.0582 loss -1.0582 (6.359 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.1082 loss -1.1082 (6.445 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.0297 loss -1.0297 (6.401 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.1505 loss -1.1505 (6.466 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.1619 loss -1.1619 (6.606 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.0910 loss -1.0910 (6.527 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.18it/s]\n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 1.1518 loss -1.1518 (33.641 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.1652 loss -1.1652 (6.730 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.0631 loss -1.0631 (6.749 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.1778 loss -1.1778 (6.647 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.1423 loss -1.1423 (6.757 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.1348 loss -1.1348 (6.769 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.0398 loss -1.0398 (6.831 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.0469 loss -1.0469 (6.583 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.0236 loss -1.0236 (6.341 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.1269 loss -1.1269 (6.416 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.1082 loss -1.1082 (6.415 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.1051 loss -1.1051 (6.460 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.2089 loss -1.2089 (6.276 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.1640 loss -1.1640 (6.407 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.0860 loss -1.0860 (6.352 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.1415 loss -1.1415 (6.339 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.1530 loss -1.1530 (6.488 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.1174 loss -1.1174 (6.258 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.2139 loss -1.2139 (6.321 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.0960 loss -1.0960 (6.499 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.1960 loss -1.1960 (6.295 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.0823 loss -1.0823 (6.479 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.1567 loss -1.1567 (6.257 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.1014 loss -1.1014 (6.300 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.1566 loss -1.1566 (6.549 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.2261 loss -1.2261 (6.329 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.74it/s]\n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 1.2102 loss -1.2102 (33.431 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.2597 loss -1.2597 (6.581 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.1533 loss -1.1533 (6.336 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.1206 loss -1.1206 (6.509 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.2476 loss -1.2476 (6.443 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.1389 loss -1.1389 (6.416 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.1755 loss -1.1755 (6.509 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.2167 loss -1.2167 (6.465 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.1972 loss -1.1972 (6.557 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.1983 loss -1.1983 (6.817 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.2060 loss -1.2060 (6.857 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.1174 loss -1.1174 (6.513 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.1698 loss -1.1698 (6.357 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.1432 loss -1.1432 (6.480 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.1916 loss -1.1916 (6.400 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.1189 loss -1.1189 (6.474 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.0882 loss -1.0882 (6.489 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.0736 loss -1.0736 (6.330 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.1277 loss -1.1277 (6.482 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.2339 loss -1.2339 (6.524 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.2480 loss -1.2480 (6.572 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.2049 loss -1.2049 (6.569 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.1884 loss -1.1884 (6.605 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.1861 loss -1.1861 (6.628 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.2038 loss -1.2038 (6.575 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1578 loss -1.1578 (6.711 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.25it/s]\n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 1.2540 loss -1.2540 (33.996 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1286 loss -1.1286 (6.703 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.2311 loss -1.2311 (6.781 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.2293 loss -1.2293 (6.266 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.2381 loss -1.2381 (6.345 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.1480 loss -1.1480 (6.404 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.1852 loss -1.1852 (6.375 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.2206 loss -1.2206 (6.454 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.2214 loss -1.2214 (6.248 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.2241 loss -1.2241 (6.278 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.2665 loss -1.2665 (6.492 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.3139 loss -1.3139 (6.369 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.2428 loss -1.2428 (6.421 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.2475 loss -1.2475 (6.263 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2581 loss -1.2581 (6.260 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.1877 loss -1.1877 (6.373 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.1867 loss -1.1867 (6.388 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.1797 loss -1.1797 (6.416 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.2607 loss -1.2607 (6.343 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.3437 loss -1.3437 (6.348 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.1838 loss -1.1838 (6.385 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.2077 loss -1.2077 (6.669 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.1976 loss -1.1976 (6.375 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.2557 loss -1.2557 (6.436 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.1144 loss -1.1144 (6.323 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.2009 loss -1.2009 (6.449 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.80it/s]\n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 1.2725 loss -1.2725 (32.681 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.2879 loss -1.2879 (6.587 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.2734 loss -1.2734 (6.536 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1619 loss -1.1619 (6.393 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2425 loss -1.2425 (6.452 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.1917 loss -1.1917 (6.556 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.2610 loss -1.2610 (6.385 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.1927 loss -1.1927 (6.541 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.2850 loss -1.2850 (6.417 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2489 loss -1.2489 (6.471 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.1688 loss -1.1688 (6.531 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.2491 loss -1.2491 (6.319 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.2119 loss -1.2119 (6.472 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.2789 loss -1.2789 (6.311 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.2903 loss -1.2903 (6.397 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.2242 loss -1.2242 (6.653 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.2629 loss -1.2629 (6.545 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.2624 loss -1.2624 (6.677 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.2836 loss -1.2836 (6.564 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.4044 loss -1.4044 (6.658 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.2647 loss -1.2647 (6.869 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2824 loss -1.2824 (6.637 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.2981 loss -1.2981 (6.661 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.3479 loss -1.3479 (6.748 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.2017 loss -1.2017 (6.751 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.2691 loss -1.2691 (6.669 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.12it/s]\n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 1.2936 loss -1.2936 (32.925 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.2916 loss -1.2916 (6.579 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.2960 loss -1.2960 (6.354 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.3235 loss -1.3235 (6.341 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.3012 loss -1.3012 (6.576 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.2831 loss -1.2831 (6.362 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.3304 loss -1.3304 (6.529 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.2992 loss -1.2992 (6.478 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.2297 loss -1.2297 (6.397 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.3606 loss -1.3606 (6.788 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.2580 loss -1.2580 (6.582 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.2703 loss -1.2703 (6.823 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.3409 loss -1.3409 (6.597 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.1939 loss -1.1939 (6.421 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.2803 loss -1.2803 (6.385 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.2648 loss -1.2648 (6.377 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.2199 loss -1.2199 (6.461 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.2925 loss -1.2925 (6.331 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.2923 loss -1.2923 (6.575 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.2688 loss -1.2688 (6.501 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.2088 loss -1.2088 (6.450 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.3118 loss -1.3118 (6.646 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.2847 loss -1.2847 (6.490 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.2593 loss -1.2593 (6.588 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2798 loss -1.2798 (6.550 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.2988 loss -1.2988 (6.459 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.72it/s]\n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 1.3140 loss -1.3140 (33.070 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.3407 loss -1.3407 (6.571 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.3398 loss -1.3398 (6.493 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.2869 loss -1.2869 (6.525 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.2798 loss -1.2798 (6.541 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.3832 loss -1.3832 (6.419 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.3480 loss -1.3480 (6.544 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2802 loss -1.2802 (6.400 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.2526 loss -1.2526 (6.541 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2631 loss -1.2631 (6.467 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.3065 loss -1.3065 (6.639 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.1956 loss -1.1956 (6.682 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.3773 loss -1.3773 (6.680 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.3341 loss -1.3341 (6.688 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.2451 loss -1.2451 (6.632 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.3841 loss -1.3841 (6.719 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.2658 loss -1.2658 (6.819 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.3683 loss -1.3683 (6.618 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.2470 loss -1.2470 (6.769 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.3128 loss -1.3128 (6.791 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.3146 loss -1.3146 (6.831 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.3062 loss -1.3062 (6.885 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.2935 loss -1.2935 (6.685 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.2334 loss -1.2334 (6.345 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.2243 loss -1.2243 (6.329 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.2728 loss -1.2728 (6.469 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 92.34it/s]\n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 1.3267 loss -1.3267 (32.489 secs)\n",
      "\n",
      "isanp:isanp-num_latents-64_rbf_100 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.3566 loss -1.3566 (6.508 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.1931 loss -1.1931 (6.471 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.3438 loss -1.3438 (6.390 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.2609 loss -1.2609 (6.487 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.2600 loss -1.2600 (6.455 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.3270 loss -1.3270 (6.569 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.2123 loss -1.2123 (6.479 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.2449 loss -1.2449 (6.467 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.2914 loss -1.2914 (6.425 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.4246 loss -1.4246 (6.483 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.3383 loss -1.3383 (6.517 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.2521 loss -1.2521 (6.504 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.3508 loss -1.3508 (6.350 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.3698 loss -1.3698 (6.426 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.3407 loss -1.3407 (6.589 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.3290 loss -1.3290 (6.962 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.3478 loss -1.3478 (6.972 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.3742 loss -1.3742 (6.828 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2912 loss -1.2912 (6.839 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.3040 loss -1.3040 (6.918 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.2584 loss -1.2584 (6.962 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2820 loss -1.2820 (6.694 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.3511 loss -1.3511 (6.484 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.2440 loss -1.2440 (6.597 secs)\n",
      "isanp:isanp-num_latents-64_rbf_100 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.3250 loss -1.3250 (6.416 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.87it/s]\n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 1.3287 loss -1.3287 (32.657 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.61it/s]\n",
      "isanp:isanp-num_latents-64_rbf_100 rbf tar_ll 1.3287 loss -1.3287 (33.482 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3975180.0 miliseconds\n",
      "Execution time: 3975.18 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 80.84423828125 MB\n",
      "Memory Usage Change: 64.59423828125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='isanp-num_latents-64_rbf_100',val_seed=100, val_l=64,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe4057-1d0b-4659-9571-09d654f5e2c4",
   "metadata": {},
   "source": [
    "## ISANP (128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb85a5f2-d1e7-4fe6-9191-d803391586f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: isanp-isanp-num_latents-128_rbf_0\n",
      "Total number of parameters: 793282\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_rbf_0 step 200 lr 5.000e-04 [train_loss] tar_ll -0.7041 loss 0.7041 (7.176 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 400 lr 5.000e-04 [train_loss] tar_ll -0.6576 loss 0.6576 (6.755 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 600 lr 5.000e-04 [train_loss] tar_ll -0.5860 loss 0.5860 (6.845 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 800 lr 4.999e-04 [train_loss] tar_ll -0.5665 loss 0.5665 (6.985 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 1000 lr 4.999e-04 [train_loss] tar_ll -0.5098 loss 0.5098 (7.118 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.3860 loss 0.3860 (7.230 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 1400 lr 4.998e-04 [train_loss] tar_ll -0.2280 loss 0.2280 (6.972 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 1600 lr 4.997e-04 [train_loss] tar_ll -0.1567 loss 0.1567 (6.961 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 1800 lr 4.996e-04 [train_loss] tar_ll -0.0624 loss 0.0624 (7.185 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 2000 lr 4.995e-04 [train_loss] tar_ll -0.0105 loss 0.0105 (7.280 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.1555 loss -0.1555 (7.104 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.1192 loss -0.1192 (7.035 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.2326 loss -0.2326 (6.970 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.2191 loss -0.2191 (6.790 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.3062 loss -0.3062 (7.050 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.1863 loss -0.1863 (6.895 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.2614 loss -0.2614 (7.000 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.4172 loss -0.4172 (6.810 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.3939 loss -0.3939 (6.775 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.4014 loss -0.4014 (7.000 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3450 loss -0.3450 (7.020 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.3364 loss -0.3364 (6.873 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4953 loss -0.4953 (6.958 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4514 loss -0.4514 (6.820 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.4618 loss -0.4618 (6.861 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 85.67it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 1.1217 loss -1.1217 (0.020 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.4215 loss -0.4215 (6.798 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.5230 loss -0.5230 (6.650 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.5268 loss -0.5268 (6.810 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.4133 loss -0.4133 (6.940 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4807 loss -0.4807 (6.590 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.5899 loss -0.5899 (6.675 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.4446 loss -0.4446 (6.635 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5778 loss -0.5778 (6.588 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.6645 loss -0.6645 (6.842 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.5844 loss -0.5844 (6.526 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.6344 loss -0.6344 (6.771 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5715 loss -0.5715 (6.685 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.6148 loss -0.6148 (6.638 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.5572 loss -0.5572 (6.453 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.5593 loss -0.5593 (6.522 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.5589 loss -0.5589 (6.845 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.4722 loss -0.4722 (6.450 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.6197 loss -0.6197 (6.520 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.6421 loss -0.6421 (6.525 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.6151 loss -0.6151 (6.485 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.6885 loss -0.6885 (6.685 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.7423 loss -0.7423 (6.555 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.6419 loss -0.6419 (6.515 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.6804 loss -0.6804 (6.360 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.6737 loss -0.6737 (6.427 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 84.03it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 1.1298 loss -1.1298 (0.014 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.7002 loss -0.7002 (6.401 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.7455 loss -0.7455 (6.280 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.6550 loss -0.6550 (6.915 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.6359 loss -0.6359 (6.971 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.6749 loss -0.6749 (7.014 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.6721 loss -0.6721 (6.850 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.7573 loss -0.7573 (6.810 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.7396 loss -0.7396 (6.960 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.7243 loss -0.7243 (7.050 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.6241 loss -0.6241 (6.941 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.7185 loss -0.7185 (7.069 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.7904 loss -0.7904 (6.870 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.7692 loss -0.7692 (6.910 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.7749 loss -0.7749 (6.640 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.6692 loss -0.6692 (6.965 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.7102 loss -0.7102 (6.895 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.7026 loss -0.7026 (6.950 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.7442 loss -0.7442 (7.080 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.8090 loss -0.8090 (6.852 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.8314 loss -0.8314 (6.833 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.8047 loss -0.8047 (6.835 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.6903 loss -0.6903 (6.725 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.6890 loss -0.6890 (6.737 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.8008 loss -0.8008 (6.810 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.6983 loss -0.6983 (6.665 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 80.42it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 1.3948 loss -1.3948 (0.015 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.8156 loss -0.8156 (7.229 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.7096 loss -0.7096 (8.021 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.8367 loss -0.8367 (7.364 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.8772 loss -0.8772 (6.910 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.8992 loss -0.8992 (6.790 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.8215 loss -0.8215 (7.870 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.8207 loss -0.8207 (7.948 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.8481 loss -0.8481 (6.727 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.8210 loss -0.8210 (6.495 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.8579 loss -0.8579 (6.430 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.8123 loss -0.8123 (6.448 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.7713 loss -0.7713 (6.510 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.8460 loss -0.8460 (6.540 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.7750 loss -0.7750 (6.305 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.8151 loss -0.8151 (6.445 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.8316 loss -0.8316 (6.470 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.8807 loss -0.8807 (6.545 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.7018 loss -0.7018 (6.662 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.6999 loss -0.6999 (6.697 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.7030 loss -0.7030 (6.523 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.7032 loss -0.7032 (6.755 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.8277 loss -0.8277 (6.520 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.7731 loss -0.7731 (6.880 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.7880 loss -0.7880 (6.568 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.8812 loss -0.8812 (6.850 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 73.36it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 1.6078 loss -1.6078 (0.017 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.9336 loss -0.9336 (6.820 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.8037 loss -0.8037 (6.792 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.8655 loss -0.8655 (6.530 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.9238 loss -0.9238 (6.819 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.9088 loss -0.9088 (6.570 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.9128 loss -0.9128 (6.850 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.8702 loss -0.8702 (6.985 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.8243 loss -0.8243 (6.600 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.7782 loss -0.7782 (6.817 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.8391 loss -0.8391 (6.752 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.7266 loss -0.7266 (6.770 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.8220 loss -0.8220 (8.868 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.7357 loss -0.7357 (15.224 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.8782 loss -0.8782 (11.360 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.7982 loss -0.7982 (7.030 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.8463 loss -0.8463 (7.119 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.8464 loss -0.8464 (6.740 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.8764 loss -0.8764 (6.830 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.8840 loss -0.8840 (6.670 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.7377 loss -0.7377 (6.541 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.8027 loss -0.8027 (7.009 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.8114 loss -0.8114 (6.970 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.8411 loss -0.8411 (6.963 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.8749 loss -0.8749 (7.037 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.8974 loss -0.8974 (6.870 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 72.07it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 1.7160 loss -1.7160 (0.022 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.8937 loss -0.8937 (7.237 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.8945 loss -0.8945 (6.810 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.8969 loss -0.8969 (6.885 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.9040 loss -0.9040 (6.915 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.8397 loss -0.8397 (7.370 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.8433 loss -0.8433 (7.660 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.9156 loss -0.9156 (7.314 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 26600 lr 4.177e-04 [train_loss] tar_ll 1.0600 loss -1.0600 (7.280 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.8391 loss -0.8391 (6.852 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.5905 loss -0.5905 (6.918 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.5210 loss -0.5210 (6.835 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.6427 loss -0.6427 (6.905 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.7101 loss -0.7101 (6.799 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.7437 loss -0.7437 (7.421 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.7483 loss -0.7483 (6.649 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.7425 loss -0.7425 (6.700 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.8031 loss -0.8031 (6.753 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.8617 loss -0.8617 (6.838 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.8737 loss -0.8737 (6.696 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.9887 loss -0.9887 (6.797 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.8229 loss -0.8229 (6.890 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.8785 loss -0.8785 (6.900 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.8735 loss -0.8735 (6.860 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.7096 loss -0.7096 (6.880 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.7884 loss -0.7884 (6.591 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 100.36it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 1.4402 loss -1.4402 (0.015 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.8171 loss -0.8171 (7.289 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.7748 loss -0.7748 (6.703 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.5776 loss -0.5776 (6.672 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.7710 loss -0.7710 (6.690 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.7937 loss -0.7937 (6.600 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.9195 loss -0.9195 (6.933 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.9958 loss -0.9958 (6.807 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.9109 loss -0.9109 (6.884 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.8270 loss -0.8270 (6.949 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.9564 loss -0.9564 (6.970 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.8258 loss -0.8258 (6.730 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.9209 loss -0.9209 (6.620 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.9436 loss -0.9436 (6.745 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.9214 loss -0.9214 (6.520 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.9133 loss -0.9133 (6.610 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.9409 loss -0.9409 (6.510 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.8087 loss -0.8087 (6.480 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.9390 loss -0.9390 (6.880 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.8991 loss -0.8991 (7.330 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.9437 loss -0.9437 (6.600 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.9508 loss -0.9508 (6.645 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 34400 lr 3.677e-04 [train_loss] tar_ll 1.0371 loss -1.0371 (6.605 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.9661 loss -0.9661 (6.510 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.9580 loss -0.9580 (6.620 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.8903 loss -0.8903 (6.930 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 84.36it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 1.7229 loss -1.7229 (0.015 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.9563 loss -0.9563 (6.960 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 35400 lr 3.607e-04 [train_loss] tar_ll 1.0643 loss -1.0643 (6.865 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 35600 lr 3.593e-04 [train_loss] tar_ll 1.0951 loss -1.0951 (6.711 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.9750 loss -0.9750 (6.889 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 36000 lr 3.564e-04 [train_loss] tar_ll 1.0496 loss -1.0496 (6.730 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.9485 loss -0.9485 (6.730 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.9603 loss -0.9603 (7.000 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 36600 lr 3.522e-04 [train_loss] tar_ll 1.0481 loss -1.0481 (7.020 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 36800 lr 3.507e-04 [train_loss] tar_ll 1.0297 loss -1.0297 (7.205 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.9339 loss -0.9339 (7.320 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.9877 loss -0.9877 (7.140 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.5823 loss -0.5823 (6.635 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.9498 loss -0.9498 (6.465 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.8392 loss -0.8392 (6.670 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.8675 loss -0.8675 (6.575 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 38200 lr 3.406e-04 [train_loss] tar_ll 1.0291 loss -1.0291 (6.745 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.9666 loss -0.9666 (6.465 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 38600 lr 3.376e-04 [train_loss] tar_ll 1.0692 loss -1.0692 (6.735 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.9531 loss -0.9531 (6.615 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.9840 loss -0.9840 (6.615 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 39200 lr 3.332e-04 [train_loss] tar_ll 1.0171 loss -1.0171 (6.907 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.9753 loss -0.9753 (6.683 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 39600 lr 3.302e-04 [train_loss] tar_ll 1.0226 loss -1.0226 (6.585 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 39800 lr 3.287e-04 [train_loss] tar_ll 1.0121 loss -1.0121 (6.780 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 40000 lr 3.273e-04 [train_loss] tar_ll 1.0278 loss -1.0278 (6.470 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 93.82it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 1.8375 loss -1.8375 (0.013 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.9488 loss -0.9488 (6.807 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.9504 loss -0.9504 (6.610 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.9497 loss -0.9497 (6.710 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.9637 loss -0.9637 (6.755 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.9676 loss -0.9676 (6.889 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.8759 loss -0.8759 (6.754 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.9595 loss -0.9595 (6.620 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.9861 loss -0.9861 (6.715 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 41800 lr 3.137e-04 [train_loss] tar_ll 1.0558 loss -1.0558 (6.960 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 42000 lr 3.122e-04 [train_loss] tar_ll 1.0458 loss -1.0458 (6.910 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 42200 lr 3.106e-04 [train_loss] tar_ll 1.0054 loss -1.0054 (6.980 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 42400 lr 3.091e-04 [train_loss] tar_ll 1.0075 loss -1.0075 (7.005 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 42600 lr 3.076e-04 [train_loss] tar_ll 1.0473 loss -1.0473 (6.880 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 42800 lr 3.061e-04 [train_loss] tar_ll 1.0226 loss -1.0226 (6.940 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.8372 loss -0.8372 (7.215 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.6600 loss -0.6600 (7.055 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.9245 loss -0.9245 (7.026 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.9778 loss -0.9778 (7.044 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.8787 loss -0.8787 (7.005 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.9615 loss -0.9615 (6.830 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.9886 loss -0.9886 (6.935 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.9437 loss -0.9437 (6.730 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.9529 loss -0.9529 (6.370 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.9718 loss -0.9718 (6.610 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 45000 lr 2.891e-04 [train_loss] tar_ll 1.0147 loss -1.0147 (6.470 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 85.02it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 1.9454 loss -1.9454 (0.014 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.9742 loss -0.9742 (6.515 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.9838 loss -0.9838 (6.460 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 45600 lr 2.844e-04 [train_loss] tar_ll 1.1005 loss -1.1005 (6.335 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 45800 lr 2.829e-04 [train_loss] tar_ll 1.0117 loss -1.0117 (6.715 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 46000 lr 2.813e-04 [train_loss] tar_ll 1.0572 loss -1.0572 (6.720 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 46200 lr 2.798e-04 [train_loss] tar_ll 1.0800 loss -1.0800 (7.350 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.9765 loss -0.9765 (7.559 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.9311 loss -0.9311 (7.293 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 46800 lr 2.751e-04 [train_loss] tar_ll 1.0456 loss -1.0456 (6.897 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 47000 lr 2.735e-04 [train_loss] tar_ll 1.0819 loss -1.0819 (7.070 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 47200 lr 2.720e-04 [train_loss] tar_ll 1.0558 loss -1.0558 (7.334 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 47400 lr 2.704e-04 [train_loss] tar_ll 1.0547 loss -1.0547 (7.206 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 47600 lr 2.688e-04 [train_loss] tar_ll 1.0531 loss -1.0531 (7.140 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 47800 lr 2.673e-04 [train_loss] tar_ll 1.0683 loss -1.0683 (7.024 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 48000 lr 2.657e-04 [train_loss] tar_ll 1.1418 loss -1.1418 (6.596 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.8422 loss -0.8422 (6.615 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 48400 lr 2.626e-04 [train_loss] tar_ll 1.0116 loss -1.0116 (6.630 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 48600 lr 2.610e-04 [train_loss] tar_ll 1.1649 loss -1.1649 (6.680 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 48800 lr 2.594e-04 [train_loss] tar_ll 1.0095 loss -1.0095 (6.571 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 49000 lr 2.579e-04 [train_loss] tar_ll 1.0593 loss -1.0593 (6.705 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 49200 lr 2.563e-04 [train_loss] tar_ll 1.0121 loss -1.0121 (6.795 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 49400 lr 2.547e-04 [train_loss] tar_ll 1.1094 loss -1.1094 (6.760 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 49600 lr 2.531e-04 [train_loss] tar_ll 1.0081 loss -1.0081 (6.775 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 49800 lr 2.516e-04 [train_loss] tar_ll 1.1380 loss -1.1380 (6.710 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 50000 lr 2.500e-04 [train_loss] tar_ll 1.0661 loss -1.0661 (6.690 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 67.22it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 1.9735 loss -1.9735 (0.017 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 50200 lr 2.484e-04 [train_loss] tar_ll 1.0705 loss -1.0705 (6.944 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 50400 lr 2.469e-04 [train_loss] tar_ll 1.0304 loss -1.0304 (6.655 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 50600 lr 2.453e-04 [train_loss] tar_ll 1.1284 loss -1.1284 (6.545 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 50800 lr 2.437e-04 [train_loss] tar_ll 1.0918 loss -1.0918 (6.645 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 51000 lr 2.421e-04 [train_loss] tar_ll 1.1279 loss -1.1279 (6.664 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 51200 lr 2.406e-04 [train_loss] tar_ll 1.1592 loss -1.1592 (6.626 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 51400 lr 2.390e-04 [train_loss] tar_ll 1.1076 loss -1.1076 (6.900 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 51600 lr 2.374e-04 [train_loss] tar_ll 1.1222 loss -1.1222 (6.479 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 51800 lr 2.359e-04 [train_loss] tar_ll 1.2022 loss -1.2022 (6.773 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 52000 lr 2.343e-04 [train_loss] tar_ll 1.0966 loss -1.0966 (6.672 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 52200 lr 2.327e-04 [train_loss] tar_ll 1.2098 loss -1.2098 (6.585 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 52400 lr 2.312e-04 [train_loss] tar_ll 1.0881 loss -1.0881 (6.900 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 52600 lr 2.296e-04 [train_loss] tar_ll 1.0256 loss -1.0256 (6.680 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 52800 lr 2.280e-04 [train_loss] tar_ll 1.1014 loss -1.1014 (6.764 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 53000 lr 2.265e-04 [train_loss] tar_ll 1.1664 loss -1.1664 (6.765 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 53200 lr 2.249e-04 [train_loss] tar_ll 1.1565 loss -1.1565 (6.803 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 53400 lr 2.233e-04 [train_loss] tar_ll 1.1190 loss -1.1190 (6.717 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 53600 lr 2.218e-04 [train_loss] tar_ll 1.1180 loss -1.1180 (6.750 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 53800 lr 2.202e-04 [train_loss] tar_ll 1.1262 loss -1.1262 (6.850 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 54000 lr 2.187e-04 [train_loss] tar_ll 1.0596 loss -1.0596 (6.830 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 54200 lr 2.171e-04 [train_loss] tar_ll 1.1659 loss -1.1659 (7.330 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 54400 lr 2.156e-04 [train_loss] tar_ll 1.0621 loss -1.0621 (7.140 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 54600 lr 2.140e-04 [train_loss] tar_ll 1.1887 loss -1.1887 (7.075 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 54800 lr 2.124e-04 [train_loss] tar_ll 1.1084 loss -1.1084 (7.055 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 55000 lr 2.109e-04 [train_loss] tar_ll 1.1664 loss -1.1664 (6.540 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 90.06it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 2.0676 loss -2.0676 (0.016 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 55200 lr 2.093e-04 [train_loss] tar_ll 1.1663 loss -1.1663 (6.684 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 55400 lr 2.078e-04 [train_loss] tar_ll 1.1281 loss -1.1281 (6.620 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 55600 lr 2.062e-04 [train_loss] tar_ll 1.0989 loss -1.0989 (6.498 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 55800 lr 2.047e-04 [train_loss] tar_ll 1.1862 loss -1.1862 (6.491 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 56000 lr 2.032e-04 [train_loss] tar_ll 1.2138 loss -1.2138 (6.590 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 56200 lr 2.016e-04 [train_loss] tar_ll 1.1670 loss -1.1670 (7.037 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 56400 lr 2.001e-04 [train_loss] tar_ll 1.1745 loss -1.1745 (6.763 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 56600 lr 1.985e-04 [train_loss] tar_ll 1.0912 loss -1.0912 (6.860 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 56800 lr 1.970e-04 [train_loss] tar_ll 1.2329 loss -1.2329 (6.675 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 57000 lr 1.955e-04 [train_loss] tar_ll 1.1319 loss -1.1319 (6.935 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 57200 lr 1.939e-04 [train_loss] tar_ll 1.1921 loss -1.1921 (6.710 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 57400 lr 1.924e-04 [train_loss] tar_ll 1.0884 loss -1.0884 (6.750 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 57600 lr 1.909e-04 [train_loss] tar_ll 1.1685 loss -1.1685 (6.700 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 57800 lr 1.894e-04 [train_loss] tar_ll 1.1190 loss -1.1190 (6.810 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 58000 lr 1.878e-04 [train_loss] tar_ll 1.1460 loss -1.1460 (6.903 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 58200 lr 1.863e-04 [train_loss] tar_ll 1.0982 loss -1.0982 (6.927 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 58400 lr 1.848e-04 [train_loss] tar_ll 1.1803 loss -1.1803 (6.770 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 58600 lr 1.833e-04 [train_loss] tar_ll 1.1430 loss -1.1430 (6.961 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 58800 lr 1.818e-04 [train_loss] tar_ll 1.1469 loss -1.1469 (6.678 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.1916 loss -1.1916 (6.665 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 59200 lr 1.787e-04 [train_loss] tar_ll 1.2120 loss -1.2120 (6.770 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 59400 lr 1.772e-04 [train_loss] tar_ll 1.1510 loss -1.1510 (6.822 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 59600 lr 1.757e-04 [train_loss] tar_ll 1.2090 loss -1.2090 (6.814 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 59800 lr 1.742e-04 [train_loss] tar_ll 1.1417 loss -1.1417 (6.812 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 60000 lr 1.727e-04 [train_loss] tar_ll 1.1993 loss -1.1993 (6.682 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 88.77it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 2.1111 loss -2.1111 (0.016 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 60200 lr 1.713e-04 [train_loss] tar_ll 1.1985 loss -1.1985 (6.934 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 60400 lr 1.698e-04 [train_loss] tar_ll 1.1612 loss -1.1612 (6.785 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.2633 loss -1.2633 (6.695 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.1758 loss -1.1758 (6.800 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 61000 lr 1.653e-04 [train_loss] tar_ll 1.2192 loss -1.2192 (6.930 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 61200 lr 1.638e-04 [train_loss] tar_ll 1.1019 loss -1.1019 (7.260 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 61400 lr 1.624e-04 [train_loss] tar_ll 1.1918 loss -1.1918 (7.420 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 61600 lr 1.609e-04 [train_loss] tar_ll 1.2235 loss -1.2235 (7.570 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.2305 loss -1.2305 (7.295 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 62000 lr 1.580e-04 [train_loss] tar_ll 1.1778 loss -1.1778 (7.205 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 62200 lr 1.565e-04 [train_loss] tar_ll 1.0697 loss -1.0697 (6.840 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.1618 loss -1.1618 (6.578 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.1136 loss -1.1136 (6.810 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.1412 loss -1.1412 (6.633 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.2370 loss -1.2370 (6.842 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.1666 loss -1.1666 (6.905 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.2204 loss -1.2204 (6.670 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.1594 loss -1.1594 (6.850 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.1205 loss -1.1205 (6.895 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.2464 loss -1.2464 (6.895 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.1552 loss -1.1552 (6.908 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.2223 loss -1.2223 (6.780 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.1785 loss -1.1785 (6.990 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.2508 loss -1.2508 (7.065 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.1424 loss -1.1424 (7.095 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 128.38it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 2.0074 loss -2.0074 (0.010 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.1610 loss -1.1610 (7.040 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.1842 loss -1.1842 (7.145 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.2061 loss -1.2061 (6.425 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.1739 loss -1.1739 (6.565 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.2369 loss -1.2369 (6.595 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.2845 loss -1.2845 (6.715 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.1832 loss -1.1832 (6.618 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.2224 loss -1.2224 (6.367 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.2675 loss -1.2675 (6.505 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.2092 loss -1.2092 (6.519 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.2724 loss -1.2724 (6.646 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.1775 loss -1.1775 (6.595 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.2090 loss -1.2090 (6.491 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.1914 loss -1.1914 (6.835 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.2156 loss -1.2156 (6.540 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.2478 loss -1.2478 (7.630 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.2228 loss -1.2228 (7.355 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.2187 loss -1.2187 (7.488 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.2099 loss -1.2099 (7.417 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.2160 loss -1.2160 (6.945 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.1927 loss -1.1927 (6.855 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.3142 loss -1.3142 (6.764 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.1392 loss -1.1392 (6.656 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.1832 loss -1.1832 (6.790 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.2223 loss -1.2223 (6.610 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 31.34it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 2.2206 loss -2.2206 (0.040 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.3319 loss -1.3319 (6.850 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.2180 loss -1.2180 (6.900 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.1848 loss -1.1848 (6.850 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.1599 loss -1.1599 (6.774 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.2941 loss -1.2941 (6.850 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.2037 loss -1.2037 (7.020 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.2462 loss -1.2462 (6.850 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.2623 loss -1.2623 (7.017 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.2154 loss -1.2154 (6.690 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.2201 loss -1.2201 (6.990 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.2628 loss -1.2628 (7.810 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.2944 loss -1.2944 (7.298 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.3108 loss -1.3108 (6.833 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.2831 loss -1.2831 (6.730 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.2711 loss -1.2711 (6.590 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.2602 loss -1.2602 (6.878 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.2363 loss -1.2363 (6.842 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.3835 loss -1.3835 (6.785 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.2863 loss -1.2863 (6.930 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.3441 loss -1.3441 (6.671 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.3205 loss -1.3205 (7.029 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.2280 loss -1.2280 (6.875 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.2744 loss -1.2744 (6.775 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.1970 loss -1.1970 (6.885 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.2791 loss -1.2791 (6.905 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 94.58it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 2.3189 loss -2.3189 (0.012 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1677 loss -1.1677 (7.200 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.3711 loss -1.3711 (7.085 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.3086 loss -1.3086 (7.124 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.3143 loss -1.3143 (7.103 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.2599 loss -1.2599 (6.650 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.3593 loss -1.3593 (6.715 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.2663 loss -1.2663 (6.425 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.2642 loss -1.2642 (6.431 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.3290 loss -1.3290 (6.474 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.2840 loss -1.2840 (6.495 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.3198 loss -1.3198 (6.930 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.2348 loss -1.2348 (6.715 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.3147 loss -1.3147 (6.775 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2046 loss -1.2046 (6.706 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.3386 loss -1.3386 (6.922 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.2505 loss -1.2505 (6.828 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.2565 loss -1.2565 (6.780 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.2286 loss -1.2286 (6.920 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.2883 loss -1.2883 (6.820 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.2382 loss -1.2382 (6.695 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.2706 loss -1.2706 (7.770 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.2349 loss -1.2349 (7.135 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.3071 loss -1.3071 (6.720 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.3484 loss -1.3484 (6.710 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.2828 loss -1.2828 (6.810 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 91.36it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 2.2335 loss -2.2335 (0.014 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.3347 loss -1.3347 (6.630 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.2079 loss -1.2079 (6.945 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.3054 loss -1.3054 (6.765 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.3380 loss -1.3380 (6.725 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.2971 loss -1.2971 (6.795 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.3254 loss -1.3254 (6.960 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.3396 loss -1.3396 (6.980 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.3346 loss -1.3346 (6.752 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2288 loss -1.2288 (6.885 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.3235 loss -1.3235 (6.795 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.2660 loss -1.2660 (7.245 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.2799 loss -1.2799 (7.076 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.3699 loss -1.3699 (7.384 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.2756 loss -1.2756 (7.380 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.3080 loss -1.3080 (7.025 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.2560 loss -1.2560 (6.996 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.3064 loss -1.3064 (6.751 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.2900 loss -1.2900 (6.939 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.3213 loss -1.3213 (6.985 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.2674 loss -1.2674 (7.075 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.4168 loss -1.4168 (6.885 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.3865 loss -1.3865 (6.945 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.3790 loss -1.3790 (6.995 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.1955 loss -1.1955 (6.985 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.2358 loss -1.2358 (7.000 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 84.63it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 2.3600 loss -2.3600 (0.014 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.3099 loss -1.3099 (6.905 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.2515 loss -1.2515 (7.165 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.3331 loss -1.3331 (7.121 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.2683 loss -1.2683 (7.100 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.3389 loss -1.3389 (7.046 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.3858 loss -1.3858 (7.067 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.2579 loss -1.2579 (6.767 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.2048 loss -1.2048 (6.490 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.3386 loss -1.3386 (6.560 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.3394 loss -1.3394 (6.720 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.2822 loss -1.2822 (6.710 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.2993 loss -1.2993 (6.460 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.3246 loss -1.3246 (6.480 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.3691 loss -1.3691 (6.635 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.3077 loss -1.3077 (6.525 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.3963 loss -1.3963 (7.195 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.3155 loss -1.3155 (6.725 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.2881 loss -1.2881 (6.674 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.3113 loss -1.3113 (6.751 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.3855 loss -1.3855 (6.995 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.3053 loss -1.3053 (7.340 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.3307 loss -1.3307 (7.435 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.3622 loss -1.3622 (7.360 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2748 loss -1.2748 (7.295 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.2269 loss -1.2269 (6.665 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 78.24it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 2.3801 loss -2.3801 (0.015 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.3247 loss -1.3247 (6.721 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.3096 loss -1.3096 (6.895 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.3562 loss -1.3562 (6.665 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.2979 loss -1.2979 (6.580 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.3301 loss -1.3301 (6.825 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.3976 loss -1.3976 (6.865 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.3000 loss -1.3000 (6.680 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.3205 loss -1.3205 (6.770 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2910 loss -1.2910 (6.745 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2917 loss -1.2917 (6.905 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.2825 loss -1.2825 (6.920 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.3999 loss -1.3999 (6.725 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.3190 loss -1.3190 (6.625 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.3070 loss -1.3070 (6.685 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.3353 loss -1.3353 (7.319 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.3522 loss -1.3522 (7.277 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.3384 loss -1.3384 (6.926 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.2778 loss -1.2778 (6.737 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.3306 loss -1.3306 (6.740 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.3956 loss -1.3956 (6.560 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.3143 loss -1.3143 (6.795 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.3408 loss -1.3408 (6.775 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.3124 loss -1.3124 (6.750 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.3635 loss -1.3635 (7.070 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.3481 loss -1.3481 (7.090 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 64.47it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 2.3907 loss -2.3907 (0.018 secs)\n",
      "\n",
      "isanp:isanp-num_latents-128_rbf_0 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.3061 loss -1.3061 (7.007 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.3571 loss -1.3571 (6.790 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.3609 loss -1.3609 (6.814 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.4030 loss -1.4030 (6.795 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.3478 loss -1.3478 (6.920 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.3015 loss -1.3015 (7.065 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.3298 loss -1.3298 (6.966 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.4238 loss -1.4238 (7.120 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.2823 loss -1.2823 (6.725 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.2900 loss -1.2900 (6.535 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.2879 loss -1.2879 (6.620 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.3158 loss -1.3158 (6.262 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.3588 loss -1.3588 (6.450 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.3715 loss -1.3715 (6.340 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.3530 loss -1.3530 (6.589 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.3794 loss -1.3794 (6.960 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.3002 loss -1.3002 (6.770 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.3434 loss -1.3434 (6.764 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.4052 loss -1.4052 (6.816 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.3921 loss -1.3921 (6.760 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.3628 loss -1.3628 (6.805 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.4137 loss -1.4137 (6.735 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.3658 loss -1.3658 (6.816 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.2942 loss -1.2942 (6.720 secs)\n",
      "isanp:isanp-num_latents-128_rbf_0 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.3783 loss -1.3783 (6.945 secs)\n",
      "100%|##########| 1/1 [00:00<00:00, 65.63it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 2.3952 loss -2.3952 (0.021 secs)\n",
      "\n",
      "100%|##########| 1/1 [00:00<00:00, 44.74it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 2.3952 loss -2.3952 (0.025 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3433930.75 miliseconds\n",
      "Execution time: 3433.93075 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 119.84375 MB\n",
      "Memory Usage Change: 103.59375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='isanp-num_latents-128_rbf_0',val_seed=0, val_l=128,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0a853b-2eb4-44a4-91ab-1ac364c238b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38e3cc2f-b06b-411a-8255-33be0f166c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:09<00:00, 42.99it/s]\n",
      "isanp:isanp-num_latents-128_rbf_0 rbf tar_ll 1.3508 loss -1.3508 (69.789 secs)\n"
     ]
    }
   ],
   "source": [
    "%run gp.py --mode eval --expid isanp-num_latents-128_rbf_0 --model isanp --num_latents 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90321f-ed1e-40ab-902a-30ac5cd7d494",
   "metadata": {},
   "source": [
    "## ISANP (256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac1e0f9-1e60-4070-bc9a-7ed7a6582372",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: isanp-isanp-num_latents-256_rbf_0\n",
      "Total number of parameters: 801474\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 256\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "rbf-seed0.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 200 lr 5.000e-04 [train_loss] tar_ll -0.7346 loss 0.7346 (14.983 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 400 lr 5.000e-04 [train_loss] tar_ll -0.6616 loss 0.6616 (14.055 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 600 lr 5.000e-04 [train_loss] tar_ll -0.5840 loss 0.5840 (13.924 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 800 lr 4.999e-04 [train_loss] tar_ll -0.6082 loss 0.6082 (13.934 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 1000 lr 4.999e-04 [train_loss] tar_ll -0.6255 loss 0.6255 (13.797 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.6419 loss 0.6419 (13.356 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 1400 lr 4.998e-04 [train_loss] tar_ll -0.5162 loss 0.5162 (14.016 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 1600 lr 4.997e-04 [train_loss] tar_ll -0.4856 loss 0.4856 (13.914 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 1800 lr 4.996e-04 [train_loss] tar_ll -0.3656 loss 0.3656 (13.956 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 2000 lr 4.995e-04 [train_loss] tar_ll -0.2702 loss 0.2702 (13.842 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 2200 lr 4.994e-04 [train_loss] tar_ll -0.0901 loss 0.0901 (14.446 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 2400 lr 4.993e-04 [train_loss] tar_ll -0.0454 loss 0.0454 (13.999 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.0097 loss -0.0097 (14.308 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.0889 loss -0.0889 (14.127 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.1729 loss -0.1729 (14.092 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.0970 loss -0.0970 (14.233 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.1488 loss -0.1488 (14.217 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.3080 loss -0.3080 (14.000 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.2645 loss -0.2645 (13.862 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.2812 loss -0.2812 (14.286 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3240 loss -0.3240 (14.129 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.3115 loss -0.3115 (13.999 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4589 loss -0.4589 (13.915 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.3977 loss -0.3977 (14.077 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.4376 loss -0.4376 (13.740 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 36.44it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 0.9867 loss -0.9867 (0.033 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.4110 loss -0.4110 (14.288 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.4888 loss -0.4888 (14.231 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4077 loss -0.4077 (14.060 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.3971 loss -0.3971 (14.211 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.5101 loss -0.5101 (14.159 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.5773 loss -0.5773 (14.070 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.5592 loss -0.5592 (13.936 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.4775 loss -0.4775 (13.773 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.4843 loss -0.4843 (13.579 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.5218 loss -0.5218 (13.936 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.5123 loss -0.5123 (13.984 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5834 loss -0.5834 (14.287 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.5519 loss -0.5519 (13.644 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.5867 loss -0.5867 (13.826 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.5375 loss -0.5375 (13.569 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.4742 loss -0.4742 (13.786 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.6773 loss -0.6773 (14.201 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.6041 loss -0.6041 (14.364 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.6337 loss -0.6337 (14.333 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.6301 loss -0.6301 (13.984 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.6070 loss -0.6070 (14.164 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.6334 loss -0.6334 (14.822 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.6096 loss -0.6096 (14.771 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.6860 loss -0.6860 (14.758 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.7117 loss -0.7117 (14.246 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 38.97it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 1.4558 loss -1.4558 (0.030 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.7273 loss -0.7273 (13.828 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.6973 loss -0.6973 (14.316 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.6635 loss -0.6635 (13.959 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.5498 loss -0.5498 (13.986 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.6334 loss -0.6334 (13.871 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.5931 loss -0.5931 (13.809 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.4614 loss -0.4614 (13.815 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.5875 loss -0.5875 (13.741 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.7056 loss -0.7056 (14.028 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.6934 loss -0.6934 (14.081 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.6901 loss -0.6901 (14.567 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.6565 loss -0.6565 (14.462 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.6968 loss -0.6968 (14.163 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.6625 loss -0.6625 (14.135 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.7293 loss -0.7293 (14.184 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.7077 loss -0.7077 (14.274 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.7619 loss -0.7619 (13.917 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.6658 loss -0.6658 (14.187 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.8403 loss -0.8403 (13.699 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.7895 loss -0.7895 (14.099 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.7434 loss -0.7434 (14.025 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.8147 loss -0.8147 (14.219 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.8081 loss -0.8081 (14.452 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.8547 loss -0.8547 (13.826 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.7487 loss -0.7487 (14.037 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 35.68it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 1.3563 loss -1.3563 (0.033 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.7392 loss -0.7392 (14.258 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.7632 loss -0.7632 (14.168 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.7902 loss -0.7902 (13.846 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.7060 loss -0.7060 (14.361 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.7958 loss -0.7958 (14.109 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.7500 loss -0.7500 (13.953 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.7657 loss -0.7657 (14.060 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.7461 loss -0.7461 (13.868 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.8449 loss -0.8449 (13.786 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.8072 loss -0.8072 (13.806 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.7639 loss -0.7639 (13.921 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.8213 loss -0.8213 (13.877 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.7556 loss -0.7556 (13.929 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.8647 loss -0.8647 (13.934 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.8354 loss -0.8354 (13.955 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.7641 loss -0.7641 (13.890 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.9436 loss -0.9436 (13.986 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.8574 loss -0.8574 (14.081 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.7331 loss -0.7331 (14.080 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.8191 loss -0.8191 (14.211 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.7746 loss -0.7746 (14.294 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.8735 loss -0.8735 (14.012 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.8647 loss -0.8647 (14.080 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.9256 loss -0.9256 (14.008 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.8980 loss -0.8980 (14.390 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 41.04it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 1.3737 loss -1.3737 (0.030 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.8931 loss -0.8931 (14.173 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.9261 loss -0.9261 (13.790 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.8848 loss -0.8848 (13.709 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.8447 loss -0.8447 (13.677 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.8710 loss -0.8710 (13.676 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.8315 loss -0.8315 (13.650 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.9560 loss -0.9560 (13.494 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.8541 loss -0.8541 (13.856 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.8972 loss -0.8972 (13.576 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.9025 loss -0.9025 (14.224 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.8875 loss -0.8875 (14.021 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.9092 loss -0.9092 (14.007 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.8220 loss -0.8220 (13.756 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.8749 loss -0.8749 (14.081 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.8340 loss -0.8340 (13.844 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.8887 loss -0.8887 (13.964 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.5631 loss -0.5631 (14.080 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.7453 loss -0.7453 (14.538 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.9028 loss -0.9028 (13.727 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.8176 loss -0.8176 (13.708 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.7834 loss -0.7834 (13.860 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.8916 loss -0.8916 (13.635 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.9530 loss -0.9530 (13.841 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.6265 loss -0.6265 (13.709 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.9059 loss -0.9059 (13.950 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 30.22it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 1.5126 loss -1.5126 (0.036 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.9128 loss -0.9128 (14.045 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.9456 loss -0.9456 (13.870 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.8314 loss -0.8314 (13.975 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.8231 loss -0.8231 (13.856 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.9842 loss -0.9842 (14.049 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.9928 loss -0.9928 (14.157 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 26400 lr 4.188e-04 [train_loss] tar_ll 1.0343 loss -1.0343 (14.015 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.9449 loss -0.9449 (14.034 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.8942 loss -0.8942 (13.954 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.9685 loss -0.9685 (14.079 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.9680 loss -0.9680 (13.824 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.9386 loss -0.9386 (13.438 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.9000 loss -0.9000 (13.785 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.8686 loss -0.8686 (14.144 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.9088 loss -0.9088 (13.589 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 28200 lr 4.081e-04 [train_loss] tar_ll 1.0046 loss -1.0046 (13.835 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.9352 loss -0.9352 (13.717 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.9726 loss -0.9726 (13.468 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 28800 lr 4.045e-04 [train_loss] tar_ll 1.0110 loss -1.0110 (14.049 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.9456 loss -0.9456 (13.907 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 29200 lr 4.020e-04 [train_loss] tar_ll 1.0235 loss -1.0235 (14.072 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.9391 loss -0.9391 (14.197 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.8348 loss -0.8348 (14.052 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.8734 loss -0.8734 (14.113 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.9016 loss -0.9016 (14.056 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 35.73it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 1.8557 loss -1.8557 (0.034 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.9555 loss -0.9555 (14.211 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.9353 loss -0.9353 (14.184 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 30600 lr 3.931e-04 [train_loss] tar_ll 1.0077 loss -1.0077 (14.061 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.9580 loss -0.9580 (13.800 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 31000 lr 3.905e-04 [train_loss] tar_ll 1.0969 loss -1.0969 (13.670 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.9049 loss -0.9049 (13.838 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.9741 loss -0.9741 (13.831 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 31600 lr 3.866e-04 [train_loss] tar_ll 1.0080 loss -1.0080 (13.677 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 31800 lr 3.853e-04 [train_loss] tar_ll 1.0691 loss -1.0691 (14.063 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.9987 loss -0.9987 (13.777 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 32200 lr 3.826e-04 [train_loss] tar_ll 1.0061 loss -1.0061 (14.264 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.9481 loss -0.9481 (14.131 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.9271 loss -0.9271 (14.160 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.9061 loss -0.9061 (13.991 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.9429 loss -0.9429 (13.861 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.9923 loss -0.9923 (14.099 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.9994 loss -0.9994 (13.696 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 33600 lr 3.732e-04 [train_loss] tar_ll 1.0237 loss -1.0237 (14.084 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.9851 loss -0.9851 (13.796 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.9123 loss -0.9123 (14.030 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.9835 loss -0.9835 (13.977 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.9699 loss -0.9699 (13.969 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 34600 lr 3.663e-04 [train_loss] tar_ll 1.0327 loss -1.0327 (13.707 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.9496 loss -0.9496 (13.686 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.9212 loss -0.9212 (13.871 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 41.33it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 1.7118 loss -1.7118 (0.028 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.9195 loss -0.9195 (13.729 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.9666 loss -0.9666 (13.473 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.9892 loss -0.9892 (13.679 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.9798 loss -0.9798 (13.900 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 36000 lr 3.564e-04 [train_loss] tar_ll 1.0810 loss -1.0810 (13.913 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.9491 loss -0.9491 (14.188 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 36400 lr 3.536e-04 [train_loss] tar_ll 1.0032 loss -1.0032 (14.177 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.9448 loss -0.9448 (13.851 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 36800 lr 3.507e-04 [train_loss] tar_ll 1.0092 loss -1.0092 (13.897 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 37000 lr 3.493e-04 [train_loss] tar_ll 1.0312 loss -1.0312 (13.929 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.9970 loss -0.9970 (14.080 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.9341 loss -0.9341 (13.905 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.9864 loss -0.9864 (13.705 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 37800 lr 3.435e-04 [train_loss] tar_ll 1.0541 loss -1.0541 (13.970 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.9968 loss -0.9968 (13.987 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 38200 lr 3.406e-04 [train_loss] tar_ll 1.0277 loss -1.0277 (13.833 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 38400 lr 3.391e-04 [train_loss] tar_ll 1.0106 loss -1.0106 (13.446 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.9978 loss -0.9978 (13.800 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.9753 loss -0.9753 (13.847 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 39000 lr 3.347e-04 [train_loss] tar_ll 1.0308 loss -1.0308 (13.659 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 39200 lr 3.332e-04 [train_loss] tar_ll 1.0718 loss -1.0718 (14.055 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 39400 lr 3.317e-04 [train_loss] tar_ll 1.0798 loss -1.0798 (14.048 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.9873 loss -0.9873 (14.142 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.9960 loss -0.9960 (14.073 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 40000 lr 3.273e-04 [train_loss] tar_ll 1.0446 loss -1.0446 (13.993 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 38.22it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 1.9205 loss -1.9205 (0.029 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 40200 lr 3.258e-04 [train_loss] tar_ll 1.0433 loss -1.0433 (13.990 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 40400 lr 3.243e-04 [train_loss] tar_ll 1.0437 loss -1.0437 (14.259 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 40600 lr 3.228e-04 [train_loss] tar_ll 1.0550 loss -1.0550 (14.348 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 40800 lr 3.213e-04 [train_loss] tar_ll 1.0202 loss -1.0202 (14.054 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 41000 lr 3.197e-04 [train_loss] tar_ll 1.0259 loss -1.0259 (13.586 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.9974 loss -0.9974 (14.252 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 41400 lr 3.167e-04 [train_loss] tar_ll 1.0707 loss -1.0707 (13.961 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 41600 lr 3.152e-04 [train_loss] tar_ll 1.0466 loss -1.0466 (13.839 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 41800 lr 3.137e-04 [train_loss] tar_ll 1.0730 loss -1.0730 (13.735 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 42000 lr 3.122e-04 [train_loss] tar_ll 1.0733 loss -1.0733 (13.765 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 42200 lr 3.106e-04 [train_loss] tar_ll 1.1182 loss -1.1182 (13.717 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 42400 lr 3.091e-04 [train_loss] tar_ll 1.0344 loss -1.0344 (13.441 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 42600 lr 3.076e-04 [train_loss] tar_ll 1.0720 loss -1.0720 (14.022 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 42800 lr 3.061e-04 [train_loss] tar_ll 1.0275 loss -1.0275 (13.638 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.8069 loss -0.8069 (13.975 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 43200 lr 3.030e-04 [train_loss] tar_ll 1.0675 loss -1.0675 (14.319 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 43400 lr 3.015e-04 [train_loss] tar_ll 1.0993 loss -1.0993 (14.388 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 43600 lr 2.999e-04 [train_loss] tar_ll 1.0544 loss -1.0544 (14.162 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 43800 lr 2.984e-04 [train_loss] tar_ll 1.1433 loss -1.1433 (14.208 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 44000 lr 2.968e-04 [train_loss] tar_ll 1.0515 loss -1.0515 (14.058 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 44200 lr 2.953e-04 [train_loss] tar_ll 1.0816 loss -1.0816 (14.497 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 44400 lr 2.938e-04 [train_loss] tar_ll 1.0533 loss -1.0533 (14.020 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 44600 lr 2.922e-04 [train_loss] tar_ll 1.1261 loss -1.1261 (13.834 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 44800 lr 2.907e-04 [train_loss] tar_ll 1.0602 loss -1.0602 (13.820 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 45000 lr 2.891e-04 [train_loss] tar_ll 1.1286 loss -1.1286 (13.879 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 42.15it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 2.0473 loss -2.0473 (0.027 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 45200 lr 2.876e-04 [train_loss] tar_ll 1.1269 loss -1.1269 (13.843 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 45400 lr 2.860e-04 [train_loss] tar_ll 1.0832 loss -1.0832 (13.763 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 45600 lr 2.844e-04 [train_loss] tar_ll 1.0704 loss -1.0704 (13.720 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 45800 lr 2.829e-04 [train_loss] tar_ll 1.0135 loss -1.0135 (14.106 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 46000 lr 2.813e-04 [train_loss] tar_ll 1.1698 loss -1.1698 (13.851 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 46200 lr 2.798e-04 [train_loss] tar_ll 1.0652 loss -1.0652 (14.007 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 46400 lr 2.782e-04 [train_loss] tar_ll 1.1313 loss -1.1313 (14.127 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 46600 lr 2.767e-04 [train_loss] tar_ll 1.1455 loss -1.1455 (13.880 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 46800 lr 2.751e-04 [train_loss] tar_ll 1.1787 loss -1.1787 (13.743 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 47000 lr 2.735e-04 [train_loss] tar_ll 1.1813 loss -1.1813 (13.858 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 47200 lr 2.720e-04 [train_loss] tar_ll 1.0329 loss -1.0329 (14.088 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 47400 lr 2.704e-04 [train_loss] tar_ll 1.1004 loss -1.1004 (13.706 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 47600 lr 2.688e-04 [train_loss] tar_ll 1.1114 loss -1.1114 (13.722 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 47800 lr 2.673e-04 [train_loss] tar_ll 1.1898 loss -1.1898 (13.669 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 48000 lr 2.657e-04 [train_loss] tar_ll 1.1611 loss -1.1611 (13.820 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 48200 lr 2.641e-04 [train_loss] tar_ll 1.1407 loss -1.1407 (13.658 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 48400 lr 2.626e-04 [train_loss] tar_ll 1.0891 loss -1.0891 (13.604 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 48600 lr 2.610e-04 [train_loss] tar_ll 1.1673 loss -1.1673 (13.584 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 48800 lr 2.594e-04 [train_loss] tar_ll 1.1312 loss -1.1312 (13.578 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 49000 lr 2.579e-04 [train_loss] tar_ll 1.1525 loss -1.1525 (13.538 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 49200 lr 2.563e-04 [train_loss] tar_ll 1.0824 loss -1.0824 (14.144 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 49400 lr 2.547e-04 [train_loss] tar_ll 1.0728 loss -1.0728 (13.671 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 49600 lr 2.531e-04 [train_loss] tar_ll 1.1834 loss -1.1834 (14.272 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 49800 lr 2.516e-04 [train_loss] tar_ll 1.1016 loss -1.1016 (14.182 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 50000 lr 2.500e-04 [train_loss] tar_ll 1.1392 loss -1.1392 (14.359 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 38.47it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 2.0897 loss -2.0897 (0.030 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 50200 lr 2.484e-04 [train_loss] tar_ll 1.0738 loss -1.0738 (14.002 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 50400 lr 2.469e-04 [train_loss] tar_ll 1.0649 loss -1.0649 (14.219 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 50600 lr 2.453e-04 [train_loss] tar_ll 1.2173 loss -1.2173 (14.086 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 50800 lr 2.437e-04 [train_loss] tar_ll 1.0308 loss -1.0308 (14.233 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 51000 lr 2.421e-04 [train_loss] tar_ll 1.0522 loss -1.0522 (14.332 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 51200 lr 2.406e-04 [train_loss] tar_ll 1.0310 loss -1.0310 (13.924 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 51400 lr 2.390e-04 [train_loss] tar_ll 1.2358 loss -1.2358 (13.610 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 51600 lr 2.374e-04 [train_loss] tar_ll 1.1502 loss -1.1502 (13.639 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 51800 lr 2.359e-04 [train_loss] tar_ll 1.0950 loss -1.0950 (13.851 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 52000 lr 2.343e-04 [train_loss] tar_ll 1.1466 loss -1.1466 (13.888 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 52200 lr 2.327e-04 [train_loss] tar_ll 1.2310 loss -1.2310 (13.663 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 52400 lr 2.312e-04 [train_loss] tar_ll 0.9953 loss -0.9953 (13.507 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 52600 lr 2.296e-04 [train_loss] tar_ll 1.1356 loss -1.1356 (13.591 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 52800 lr 2.280e-04 [train_loss] tar_ll 1.1048 loss -1.1048 (13.547 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 53000 lr 2.265e-04 [train_loss] tar_ll 1.1658 loss -1.1658 (13.907 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 53200 lr 2.249e-04 [train_loss] tar_ll 1.2505 loss -1.2505 (14.175 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 53400 lr 2.233e-04 [train_loss] tar_ll 1.1591 loss -1.1591 (13.857 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 53600 lr 2.218e-04 [train_loss] tar_ll 1.0728 loss -1.0728 (13.880 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 53800 lr 2.202e-04 [train_loss] tar_ll 1.1765 loss -1.1765 (14.061 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 54000 lr 2.187e-04 [train_loss] tar_ll 1.1426 loss -1.1426 (13.965 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 54200 lr 2.171e-04 [train_loss] tar_ll 1.2366 loss -1.2366 (14.174 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 54400 lr 2.156e-04 [train_loss] tar_ll 1.0847 loss -1.0847 (13.964 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 54600 lr 2.140e-04 [train_loss] tar_ll 1.2362 loss -1.2362 (13.975 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 54800 lr 2.124e-04 [train_loss] tar_ll 1.1773 loss -1.1773 (13.669 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 55000 lr 2.109e-04 [train_loss] tar_ll 1.1989 loss -1.1989 (13.955 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 40.07it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 2.0939 loss -2.0939 (0.030 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 55200 lr 2.093e-04 [train_loss] tar_ll 1.2046 loss -1.2046 (13.689 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 55400 lr 2.078e-04 [train_loss] tar_ll 1.1946 loss -1.1946 (13.749 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 55600 lr 2.062e-04 [train_loss] tar_ll 1.2119 loss -1.2119 (13.846 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 55800 lr 2.047e-04 [train_loss] tar_ll 1.1029 loss -1.1029 (14.017 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 56000 lr 2.032e-04 [train_loss] tar_ll 1.1771 loss -1.1771 (13.812 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 56200 lr 2.016e-04 [train_loss] tar_ll 1.1483 loss -1.1483 (13.581 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 56400 lr 2.001e-04 [train_loss] tar_ll 1.1787 loss -1.1787 (13.804 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 56600 lr 1.985e-04 [train_loss] tar_ll 1.2057 loss -1.2057 (13.876 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 56800 lr 1.970e-04 [train_loss] tar_ll 1.2184 loss -1.2184 (13.896 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 57000 lr 1.955e-04 [train_loss] tar_ll 1.1479 loss -1.1479 (14.056 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 57200 lr 1.939e-04 [train_loss] tar_ll 1.1767 loss -1.1767 (14.323 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 57400 lr 1.924e-04 [train_loss] tar_ll 1.2650 loss -1.2650 (14.249 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 57600 lr 1.909e-04 [train_loss] tar_ll 1.2065 loss -1.2065 (14.197 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 57800 lr 1.894e-04 [train_loss] tar_ll 1.2250 loss -1.2250 (14.035 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 58000 lr 1.878e-04 [train_loss] tar_ll 1.2142 loss -1.2142 (13.980 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 58200 lr 1.863e-04 [train_loss] tar_ll 1.2246 loss -1.2246 (13.892 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 58400 lr 1.848e-04 [train_loss] tar_ll 1.2036 loss -1.2036 (13.663 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 58600 lr 1.833e-04 [train_loss] tar_ll 1.2301 loss -1.2301 (13.881 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 58800 lr 1.818e-04 [train_loss] tar_ll 1.2376 loss -1.2376 (13.918 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.1807 loss -1.1807 (13.760 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 59200 lr 1.787e-04 [train_loss] tar_ll 1.1387 loss -1.1387 (13.809 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 59400 lr 1.772e-04 [train_loss] tar_ll 1.1727 loss -1.1727 (13.639 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 59600 lr 1.757e-04 [train_loss] tar_ll 1.1773 loss -1.1773 (13.402 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 59800 lr 1.742e-04 [train_loss] tar_ll 1.2270 loss -1.2270 (13.550 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 60000 lr 1.727e-04 [train_loss] tar_ll 1.2854 loss -1.2854 (14.023 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 36.11it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 2.1045 loss -2.1045 (0.031 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 60200 lr 1.713e-04 [train_loss] tar_ll 1.2558 loss -1.2558 (14.148 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 60400 lr 1.698e-04 [train_loss] tar_ll 1.1644 loss -1.1644 (13.998 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.2612 loss -1.2612 (13.861 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.1889 loss -1.1889 (13.840 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 61000 lr 1.653e-04 [train_loss] tar_ll 1.2622 loss -1.2622 (13.806 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 61200 lr 1.638e-04 [train_loss] tar_ll 1.2317 loss -1.2317 (13.964 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 61400 lr 1.624e-04 [train_loss] tar_ll 1.2262 loss -1.2262 (14.288 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 61600 lr 1.609e-04 [train_loss] tar_ll 1.1436 loss -1.1436 (14.306 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.1947 loss -1.1947 (13.456 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 62000 lr 1.580e-04 [train_loss] tar_ll 1.1848 loss -1.1848 (13.689 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 62200 lr 1.565e-04 [train_loss] tar_ll 1.1527 loss -1.1527 (13.795 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.2092 loss -1.2092 (13.659 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.2745 loss -1.2745 (13.751 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.2463 loss -1.2463 (13.566 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.2145 loss -1.2145 (13.605 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.2349 loss -1.2349 (13.514 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.2737 loss -1.2737 (13.981 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.1697 loss -1.1697 (13.776 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.2512 loss -1.2512 (14.156 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.1572 loss -1.1572 (14.243 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.2216 loss -1.2216 (14.198 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.1881 loss -1.1881 (13.950 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.2460 loss -1.2460 (13.848 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.2505 loss -1.2505 (13.848 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.1927 loss -1.1927 (13.929 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 43.11it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 2.1257 loss -2.1257 (0.026 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.2540 loss -1.2540 (13.896 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.2178 loss -1.2178 (13.673 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.2278 loss -1.2278 (13.880 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.2393 loss -1.2393 (14.043 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.2347 loss -1.2347 (13.829 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.1429 loss -1.1429 (13.459 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.2592 loss -1.2592 (13.724 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.2760 loss -1.2760 (14.086 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.2965 loss -1.2965 (13.845 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.2845 loss -1.2845 (13.785 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.2926 loss -1.2926 (14.028 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.2904 loss -1.2904 (14.101 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.2162 loss -1.2162 (13.969 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.2271 loss -1.2271 (13.848 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.2889 loss -1.2889 (14.078 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.2061 loss -1.2061 (13.996 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.2152 loss -1.2152 (14.067 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.2536 loss -1.2536 (13.604 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.2473 loss -1.2473 (13.547 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.2395 loss -1.2395 (13.671 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.3002 loss -1.3002 (13.836 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.3174 loss -1.3174 (13.938 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.3148 loss -1.3148 (13.769 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.2323 loss -1.2323 (14.040 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.2313 loss -1.2313 (13.525 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 46.32it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 2.2313 loss -2.2313 (0.025 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.2042 loss -1.2042 (13.547 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.2366 loss -1.2366 (14.218 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.2537 loss -1.2537 (14.191 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.2302 loss -1.2302 (14.091 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.3478 loss -1.3478 (14.078 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.2807 loss -1.2807 (14.130 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.3042 loss -1.3042 (13.889 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.2580 loss -1.2580 (13.933 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.2405 loss -1.2405 (14.135 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.2714 loss -1.2714 (13.759 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.2880 loss -1.2880 (13.834 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.2430 loss -1.2430 (13.592 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.2958 loss -1.2958 (13.877 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.2027 loss -1.2027 (13.608 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.2904 loss -1.2904 (13.559 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.2480 loss -1.2480 (13.638 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.2507 loss -1.2507 (13.741 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.2607 loss -1.2607 (13.577 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.2196 loss -1.2196 (13.781 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.1891 loss -1.1891 (14.149 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.2899 loss -1.2899 (14.092 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.2163 loss -1.2163 (14.045 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.3180 loss -1.3180 (14.035 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.1585 loss -1.1585 (13.816 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.2779 loss -1.2779 (13.781 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 43.67it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 2.2339 loss -2.2339 (0.027 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.2907 loss -1.2907 (14.118 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.2458 loss -1.2458 (13.955 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.2442 loss -1.2442 (13.666 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.2957 loss -1.2957 (13.894 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.2810 loss -1.2810 (13.537 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.2595 loss -1.2595 (13.885 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.3678 loss -1.3678 (13.731 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.3060 loss -1.3060 (13.641 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.2539 loss -1.2539 (13.597 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.2878 loss -1.2878 (13.492 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.3250 loss -1.3250 (13.700 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.3216 loss -1.3216 (14.019 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.2813 loss -1.2813 (13.938 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.3581 loss -1.3581 (14.263 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.2948 loss -1.2948 (14.046 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.2835 loss -1.2835 (13.945 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.2850 loss -1.2850 (13.923 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.3177 loss -1.3177 (13.855 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.3194 loss -1.3194 (13.801 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.1969 loss -1.1969 (13.759 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.3556 loss -1.3556 (13.465 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.2653 loss -1.2653 (13.894 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.3219 loss -1.3219 (13.849 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.2211 loss -1.2211 (13.890 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.2846 loss -1.2846 (13.576 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 38.55it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 2.2925 loss -2.2925 (0.029 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.3604 loss -1.3604 (13.836 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.2283 loss -1.2283 (13.680 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.3111 loss -1.3111 (13.771 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2865 loss -1.2865 (14.166 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.3490 loss -1.3490 (14.177 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.4081 loss -1.4081 (14.273 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.2346 loss -1.2346 (14.124 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.3503 loss -1.3503 (14.123 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.3033 loss -1.3033 (13.826 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.3661 loss -1.3661 (14.018 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.2806 loss -1.2806 (14.508 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.2610 loss -1.2610 (13.712 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.2997 loss -1.2997 (13.646 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.2973 loss -1.2973 (13.945 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.2350 loss -1.2350 (13.687 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.3147 loss -1.3147 (13.971 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.2504 loss -1.2504 (13.808 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.3013 loss -1.3013 (13.550 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.2338 loss -1.2338 (13.376 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.2936 loss -1.2936 (13.570 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2932 loss -1.2932 (14.179 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.2779 loss -1.2779 (14.012 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.2308 loss -1.2308 (14.275 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.3196 loss -1.3196 (14.129 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.2698 loss -1.2698 (14.062 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 44.45it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 2.3700 loss -2.3700 (0.025 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.2575 loss -1.2575 (14.011 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.3359 loss -1.3359 (13.966 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.2424 loss -1.2424 (13.828 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.2999 loss -1.2999 (13.936 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.3411 loss -1.3411 (14.193 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.4038 loss -1.4038 (14.139 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.3648 loss -1.3648 (13.803 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.3116 loss -1.3116 (14.077 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.2595 loss -1.2595 (13.936 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.4307 loss -1.4307 (14.142 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.2915 loss -1.2915 (14.136 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.2803 loss -1.2803 (13.826 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.4001 loss -1.4001 (14.076 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.3142 loss -1.3142 (13.885 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.2784 loss -1.2784 (13.914 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.3022 loss -1.3022 (13.963 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.4177 loss -1.4177 (13.763 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.4017 loss -1.4017 (14.016 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.3607 loss -1.3607 (13.961 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.3399 loss -1.3399 (14.335 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.3810 loss -1.3810 (14.001 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.3276 loss -1.3276 (13.564 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.2936 loss -1.2936 (13.684 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.3880 loss -1.3880 (13.569 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.2352 loss -1.2352 (13.538 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 40.80it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 2.3404 loss -2.3404 (0.029 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.2792 loss -1.2792 (13.641 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.4021 loss -1.4021 (13.857 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.3307 loss -1.3307 (13.711 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.3214 loss -1.3214 (14.074 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.3559 loss -1.3559 (14.170 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2448 loss -1.2448 (14.240 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.3353 loss -1.3353 (14.347 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.2654 loss -1.2654 (14.143 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2681 loss -1.2681 (14.396 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.4093 loss -1.4093 (13.800 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.2870 loss -1.2870 (14.177 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.3930 loss -1.3930 (14.212 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.2996 loss -1.2996 (14.168 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.3929 loss -1.3929 (13.656 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.4112 loss -1.4112 (13.625 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.3048 loss -1.3048 (13.915 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.3139 loss -1.3139 (13.710 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.2697 loss -1.2697 (13.621 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.3118 loss -1.3118 (13.707 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.4010 loss -1.4010 (13.572 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.4077 loss -1.4077 (13.716 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.3502 loss -1.3502 (13.466 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.3708 loss -1.3708 (13.829 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.3339 loss -1.3339 (13.772 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.4097 loss -1.4097 (13.949 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 46.34it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 2.4049 loss -2.4049 (0.024 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_0 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.3185 loss -1.3185 (13.927 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.3968 loss -1.3968 (13.890 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.3365 loss -1.3365 (13.911 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.3882 loss -1.3882 (14.053 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.3415 loss -1.3415 (14.509 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.3557 loss -1.3557 (13.907 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.3043 loss -1.3043 (13.922 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.3449 loss -1.3449 (13.800 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.3396 loss -1.3396 (13.687 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.4089 loss -1.4089 (13.818 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.3276 loss -1.3276 (13.974 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.3404 loss -1.3404 (13.877 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.3427 loss -1.3427 (13.793 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.2273 loss -1.2273 (13.604 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.3055 loss -1.3055 (13.938 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.3041 loss -1.3041 (14.129 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.3114 loss -1.3114 (14.052 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.4346 loss -1.4346 (14.089 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.4539 loss -1.4539 (14.225 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.3275 loss -1.3275 (14.058 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.3294 loss -1.3294 (13.917 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2649 loss -1.2649 (14.186 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.2954 loss -1.2954 (13.878 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.3984 loss -1.3984 (13.633 secs)\n",
      "isanp:isanp-num_latents-256_rbf_0 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.3671 loss -1.3671 (13.905 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 39.22it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 2.3993 loss -2.3993 (0.030 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 41.66it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 2.3993 loss -2.3993 (0.028 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "Execution time: 6968613.5 miliseconds\n",
      "Execution time: 6968.6135 seconds\n",
      "Initial Memory Usage: 159.38134765625 MB\n",
      "Final Memory Usage: 340.3271484375 MB\n",
      "Memory Usage Change: 180.94580078125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='isanp-num_latents-256_rbf_0',val_seed=0, val_l=256,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a6f77-80c1-4a0f-baba-d71536386a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d383fd11-0883-4653-a892-4d48a7de13ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 256\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:11<00:00, 41.95it/s]\n",
      "isanp:isanp-num_latents-256_rbf_0 rbf tar_ll 1.3519 loss -1.3519 (71.532 secs)\n"
     ]
    }
   ],
   "source": [
    "%run gp.py --mode eval --expid isanp-num_latents-256_rbf_0 --model isanp --num_latents 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33ff54-1527-4bda-9f4b-56b45a747abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c917c46c-b7e2-4398-b276-0112a2dca947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: isanp-isanp-num_latents-256_rbf_100\n",
      "Total number of parameters: 801474\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 256\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-256_rbf_100 step 200 lr 5.000e-04 [train_loss] tar_ll -0.7295 loss 0.7295 (13.659 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 400 lr 5.000e-04 [train_loss] tar_ll -0.6822 loss 0.6822 (13.286 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 600 lr 5.000e-04 [train_loss] tar_ll -0.5974 loss 0.5974 (6.998 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 800 lr 4.999e-04 [train_loss] tar_ll -0.5867 loss 0.5867 (7.678 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 1000 lr 4.999e-04 [train_loss] tar_ll -0.5842 loss 0.5842 (7.630 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.5778 loss 0.5778 (7.395 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 1400 lr 4.998e-04 [train_loss] tar_ll -0.4003 loss 0.4003 (7.108 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 1600 lr 4.997e-04 [train_loss] tar_ll -0.2593 loss 0.2593 (7.446 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 1800 lr 4.996e-04 [train_loss] tar_ll -0.1260 loss 0.1260 (7.504 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 2000 lr 4.995e-04 [train_loss] tar_ll -0.1337 loss 0.1337 (7.071 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.0665 loss -0.0665 (7.196 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.1037 loss -0.1037 (7.038 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1435 loss -0.1435 (7.085 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.2127 loss -0.2127 (7.257 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.1971 loss -0.1971 (7.204 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.2547 loss -0.2547 (7.319 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.1979 loss -0.1979 (7.168 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.1601 loss -0.1601 (7.166 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.3028 loss -0.3028 (7.286 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.3212 loss -0.3212 (7.306 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.2566 loss -0.2566 (7.149 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.2972 loss -0.2972 (7.262 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4150 loss -0.4150 (7.258 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.5002 loss -0.5002 (7.110 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.5090 loss -0.5090 (7.339 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.02it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 0.4862 loss -0.4862 (34.879 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.4114 loss -0.4114 (7.357 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.4470 loss -0.4470 (7.188 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.5411 loss -0.5411 (7.309 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.4779 loss -0.4779 (7.202 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.2499 loss -0.2499 (7.373 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.3868 loss -0.3868 (7.367 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.4222 loss -0.4222 (7.182 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5275 loss -0.5275 (7.197 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.5789 loss -0.5789 (7.099 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.5227 loss -0.5227 (7.173 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.4016 loss -0.4016 (7.189 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.4402 loss -0.4402 (7.176 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.6288 loss -0.6288 (7.203 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.5733 loss -0.5733 (7.186 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.5979 loss -0.5979 (7.144 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.6122 loss -0.6122 (7.305 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.6093 loss -0.6093 (7.053 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.4672 loss -0.4672 (7.242 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.5528 loss -0.5528 (7.268 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.5755 loss -0.5755 (8.023 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.4809 loss -0.4809 (7.455 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.5095 loss -0.5095 (7.365 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.4646 loss -0.4646 (7.391 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.6305 loss -0.6305 (7.507 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.3959 loss -0.3959 (7.193 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 83.73it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 0.5870 loss -0.5870 (35.834 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.4560 loss -0.4560 (7.572 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.4339 loss -0.4339 (7.541 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.5624 loss -0.5624 (7.572 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.4865 loss -0.4865 (7.505 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.5788 loss -0.5788 (7.416 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.5235 loss -0.5235 (7.452 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.4630 loss -0.4630 (7.320 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.4458 loss -0.4458 (7.470 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.5105 loss -0.5105 (7.644 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.6293 loss -0.6293 (7.453 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.6914 loss -0.6914 (7.419 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.5314 loss -0.5314 (7.504 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.5860 loss -0.5860 (7.468 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.6248 loss -0.6248 (7.543 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.5733 loss -0.5733 (7.407 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.6874 loss -0.6874 (7.636 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.6714 loss -0.6714 (7.473 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.6293 loss -0.6293 (7.491 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.6563 loss -0.6563 (7.417 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.6177 loss -0.6177 (7.486 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.6217 loss -0.6217 (7.545 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.6214 loss -0.6214 (7.353 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.5230 loss -0.5230 (7.570 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.6138 loss -0.6138 (7.488 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.5761 loss -0.5761 (7.427 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 83.52it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 0.4702 loss -0.4702 (35.920 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.5643 loss -0.5643 (7.504 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.5361 loss -0.5361 (7.343 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.6866 loss -0.6866 (7.388 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.4627 loss -0.4627 (7.466 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.3897 loss -0.3897 (7.368 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.5341 loss -0.5341 (7.600 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.5250 loss -0.5250 (7.446 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.4971 loss -0.4971 (7.373 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.4112 loss -0.4112 (7.416 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.4992 loss -0.4992 (7.359 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.6411 loss -0.6411 (7.437 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.6256 loss -0.6256 (7.427 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.6658 loss -0.6658 (7.352 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.6244 loss -0.6244 (7.452 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.7475 loss -0.7475 (7.365 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.6564 loss -0.6564 (7.711 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.6311 loss -0.6311 (7.370 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.5894 loss -0.5894 (7.506 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.6515 loss -0.6515 (7.490 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.4537 loss -0.4537 (7.417 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.5953 loss -0.5953 (7.772 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.7127 loss -0.7127 (7.448 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.7505 loss -0.7505 (7.678 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.6995 loss -0.6995 (7.404 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.7174 loss -0.7174 (7.439 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.10it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 0.2358 loss -0.2358 (36.545 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.5271 loss -0.5271 (7.484 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.6864 loss -0.6864 (7.439 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.6029 loss -0.6029 (7.508 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.6610 loss -0.6610 (7.582 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.6739 loss -0.6739 (7.471 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.7007 loss -0.7007 (7.424 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.7122 loss -0.7122 (7.418 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.7578 loss -0.7578 (7.431 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.7272 loss -0.7272 (7.461 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.5070 loss -0.5070 (7.398 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.6723 loss -0.6723 (7.458 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.7644 loss -0.7644 (7.428 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.8009 loss -0.8009 (7.420 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.6686 loss -0.6686 (7.337 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.7939 loss -0.7939 (6.994 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.6384 loss -0.6384 (7.343 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.6757 loss -0.6757 (7.115 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.4979 loss -0.4979 (7.073 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.7533 loss -0.7533 (7.300 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.6431 loss -0.6431 (7.448 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.7595 loss -0.7595 (7.514 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.6050 loss -0.6050 (7.293 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.5422 loss -0.5422 (7.428 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.7051 loss -0.7051 (7.367 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.7854 loss -0.7854 (7.499 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.67it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 0.8800 loss -0.8800 (36.293 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.6448 loss -0.6448 (7.485 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.5084 loss -0.5084 (7.384 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.6609 loss -0.6609 (7.475 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.8094 loss -0.8094 (7.528 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.7641 loss -0.7641 (7.450 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.7099 loss -0.7099 (7.531 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.7525 loss -0.7525 (7.452 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.7623 loss -0.7623 (7.378 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.4840 loss -0.4840 (7.303 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.4145 loss -0.4145 (7.421 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.6317 loss -0.6317 (7.404 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.7118 loss -0.7118 (7.462 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.7204 loss -0.7204 (7.365 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.7378 loss -0.7378 (7.486 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.4782 loss -0.4782 (7.405 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.4424 loss -0.4424 (7.455 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.6153 loss -0.6153 (7.394 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.5473 loss -0.5473 (7.420 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.6834 loss -0.6834 (7.507 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.5199 loss -0.5199 (7.340 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.7570 loss -0.7570 (7.485 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.6901 loss -0.6901 (7.596 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.7005 loss -0.7005 (7.585 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.7038 loss -0.7038 (7.398 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.8121 loss -0.8121 (7.471 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.97it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 0.7472 loss -0.7472 (36.603 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.7543 loss -0.7543 (7.441 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.8112 loss -0.8112 (7.515 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.5593 loss -0.5593 (7.140 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.5687 loss -0.5687 (7.731 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.2682 loss -0.2682 (7.257 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.7120 loss -0.7120 (7.322 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.7152 loss -0.7152 (7.413 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.7467 loss -0.7467 (7.398 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.6744 loss -0.6744 (7.291 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.7262 loss -0.7262 (7.536 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.7608 loss -0.7608 (7.650 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.7312 loss -0.7312 (7.375 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.8109 loss -0.8109 (7.174 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.9512 loss -0.9512 (7.265 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.7971 loss -0.7971 (7.519 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.8444 loss -0.8444 (7.496 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.8408 loss -0.8408 (7.432 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.8018 loss -0.8018 (7.452 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.8970 loss -0.8970 (7.578 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.8915 loss -0.8915 (7.333 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.8750 loss -0.8750 (7.500 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.7819 loss -0.7819 (7.437 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.8781 loss -0.8781 (7.447 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.8173 loss -0.8173 (7.404 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.7985 loss -0.7985 (7.458 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.92it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 0.9789 loss -0.9789 (36.624 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.2608 loss -0.2608 (7.454 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.5398 loss -0.5398 (7.874 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.7552 loss -0.7552 (7.606 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.7910 loss -0.7910 (7.565 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.7255 loss -0.7255 (7.295 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.7495 loss -0.7495 (7.423 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.8262 loss -0.8262 (7.315 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.6565 loss -0.6565 (7.323 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.7767 loss -0.7767 (16.506 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.7686 loss -0.7686 (22.428 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.7878 loss -0.7878 (22.633 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.8822 loss -0.8822 (14.054 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.9624 loss -0.9624 (7.670 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.8296 loss -0.8296 (6.769 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.9465 loss -0.9465 (6.970 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.9206 loss -0.9206 (7.018 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.7329 loss -0.7329 (7.000 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.6533 loss -0.6533 (6.781 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.8881 loss -0.8881 (6.825 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.6640 loss -0.6640 (6.864 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.8002 loss -0.8002 (6.689 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.8850 loss -0.8850 (6.950 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.8390 loss -0.8390 (6.765 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.8198 loss -0.8198 (6.848 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.8768 loss -0.8768 (6.944 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.16it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 0.8865 loss -0.8865 (34.419 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.9103 loss -0.9103 (7.043 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.9008 loss -0.9008 (6.836 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.8987 loss -0.8987 (6.858 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.9052 loss -0.9052 (6.852 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.9253 loss -0.9253 (6.996 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.8594 loss -0.8594 (6.915 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.8954 loss -0.8954 (6.947 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.9381 loss -0.9381 (7.108 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.9698 loss -0.9698 (6.963 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.9322 loss -0.9322 (7.070 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.8870 loss -0.8870 (6.983 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.9626 loss -0.9626 (7.009 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 42600 lr 3.076e-04 [train_loss] tar_ll 1.0143 loss -1.0143 (7.073 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.9060 loss -0.9060 (6.982 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.8876 loss -0.8876 (7.106 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.7868 loss -0.7868 (7.000 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.9751 loss -0.9751 (7.335 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.9318 loss -0.9318 (7.288 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.9076 loss -0.9076 (7.100 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.9772 loss -0.9772 (7.196 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.9479 loss -0.9479 (7.025 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 44400 lr 2.938e-04 [train_loss] tar_ll 1.0393 loss -1.0393 (7.102 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 44600 lr 2.922e-04 [train_loss] tar_ll 1.0724 loss -1.0724 (6.954 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.9860 loss -0.9860 (7.373 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.9022 loss -0.9022 (7.265 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.75it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 0.5023 loss -0.5023 (34.583 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.8851 loss -0.8851 (7.328 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.8441 loss -0.8441 (6.979 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.9243 loss -0.9243 (7.235 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.9489 loss -0.9489 (7.276 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.9741 loss -0.9741 (7.183 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.9323 loss -0.9323 (7.129 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.8184 loss -0.8184 (6.910 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.9679 loss -0.9679 (7.290 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.9999 loss -0.9999 (6.937 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 47000 lr 2.735e-04 [train_loss] tar_ll 1.0051 loss -1.0051 (6.954 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.9989 loss -0.9989 (7.021 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.9839 loss -0.9839 (7.042 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 47600 lr 2.688e-04 [train_loss] tar_ll 1.0198 loss -1.0198 (6.846 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.9787 loss -0.9787 (7.058 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 48000 lr 2.657e-04 [train_loss] tar_ll 1.0753 loss -1.0753 (6.865 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 48200 lr 2.641e-04 [train_loss] tar_ll 1.0135 loss -1.0135 (7.017 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 48400 lr 2.626e-04 [train_loss] tar_ll 1.0169 loss -1.0169 (7.031 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 48600 lr 2.610e-04 [train_loss] tar_ll 1.0025 loss -1.0025 (7.040 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 48800 lr 2.594e-04 [train_loss] tar_ll 1.0670 loss -1.0670 (7.040 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.8652 loss -0.8652 (6.940 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 49200 lr 2.563e-04 [train_loss] tar_ll 0.9495 loss -0.9495 (7.299 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.9864 loss -0.9864 (6.757 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 49600 lr 2.531e-04 [train_loss] tar_ll 0.9344 loss -0.9344 (7.029 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.8925 loss -0.8925 (6.941 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 50000 lr 2.500e-04 [train_loss] tar_ll 1.0376 loss -1.0376 (6.916 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.78it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 0.8454 loss -0.8454 (34.176 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 50200 lr 2.484e-04 [train_loss] tar_ll 0.9756 loss -0.9756 (6.967 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 50400 lr 2.469e-04 [train_loss] tar_ll 1.0266 loss -1.0266 (6.947 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 50600 lr 2.453e-04 [train_loss] tar_ll 0.9861 loss -0.9861 (7.090 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 50800 lr 2.437e-04 [train_loss] tar_ll 1.0443 loss -1.0443 (6.863 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 51000 lr 2.421e-04 [train_loss] tar_ll 1.0297 loss -1.0297 (6.963 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 51200 lr 2.406e-04 [train_loss] tar_ll 1.0658 loss -1.0658 (7.113 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 51400 lr 2.390e-04 [train_loss] tar_ll 1.0959 loss -1.0959 (7.219 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 51600 lr 2.374e-04 [train_loss] tar_ll 1.0060 loss -1.0060 (7.008 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 51800 lr 2.359e-04 [train_loss] tar_ll 0.9149 loss -0.9149 (7.109 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 52000 lr 2.343e-04 [train_loss] tar_ll 0.9138 loss -0.9138 (7.077 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 52200 lr 2.327e-04 [train_loss] tar_ll 0.9842 loss -0.9842 (6.969 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 52400 lr 2.312e-04 [train_loss] tar_ll 0.9790 loss -0.9790 (7.086 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 52600 lr 2.296e-04 [train_loss] tar_ll 1.0046 loss -1.0046 (6.976 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 52800 lr 2.280e-04 [train_loss] tar_ll 1.0374 loss -1.0374 (7.095 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 53000 lr 2.265e-04 [train_loss] tar_ll 1.1375 loss -1.1375 (6.974 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 53200 lr 2.249e-04 [train_loss] tar_ll 1.0867 loss -1.0867 (7.141 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 53400 lr 2.233e-04 [train_loss] tar_ll 0.8365 loss -0.8365 (7.134 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 53600 lr 2.218e-04 [train_loss] tar_ll 0.8290 loss -0.8290 (7.109 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 53800 lr 2.202e-04 [train_loss] tar_ll 0.9183 loss -0.9183 (7.202 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 54000 lr 2.187e-04 [train_loss] tar_ll 0.9951 loss -0.9951 (7.251 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 54200 lr 2.171e-04 [train_loss] tar_ll 1.0673 loss -1.0673 (7.317 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 54400 lr 2.156e-04 [train_loss] tar_ll 1.0449 loss -1.0449 (7.110 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 54600 lr 2.140e-04 [train_loss] tar_ll 0.9984 loss -0.9984 (7.024 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 54800 lr 2.124e-04 [train_loss] tar_ll 1.0980 loss -1.0980 (7.123 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 55000 lr 2.109e-04 [train_loss] tar_ll 1.0145 loss -1.0145 (7.088 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.57it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 1.0142 loss -1.0142 (34.261 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 55200 lr 2.093e-04 [train_loss] tar_ll 1.0520 loss -1.0520 (7.155 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 55400 lr 2.078e-04 [train_loss] tar_ll 1.0531 loss -1.0531 (6.712 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 55600 lr 2.062e-04 [train_loss] tar_ll 1.0293 loss -1.0293 (7.430 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 55800 lr 2.047e-04 [train_loss] tar_ll 1.0414 loss -1.0414 (7.534 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 56000 lr 2.032e-04 [train_loss] tar_ll 1.0340 loss -1.0340 (6.963 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 56200 lr 2.016e-04 [train_loss] tar_ll 1.0268 loss -1.0268 (6.992 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 56400 lr 2.001e-04 [train_loss] tar_ll 1.0108 loss -1.0108 (7.087 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 56600 lr 1.985e-04 [train_loss] tar_ll 1.0559 loss -1.0559 (6.868 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0642 loss -1.0642 (7.380 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 57000 lr 1.955e-04 [train_loss] tar_ll 1.0580 loss -1.0580 (7.384 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 57200 lr 1.939e-04 [train_loss] tar_ll 1.0885 loss -1.0885 (7.344 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 57400 lr 1.924e-04 [train_loss] tar_ll 1.0873 loss -1.0873 (6.992 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 57600 lr 1.909e-04 [train_loss] tar_ll 1.1337 loss -1.1337 (6.856 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 57800 lr 1.894e-04 [train_loss] tar_ll 1.0066 loss -1.0066 (6.718 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 58000 lr 1.878e-04 [train_loss] tar_ll 1.0735 loss -1.0735 (6.794 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 58200 lr 1.863e-04 [train_loss] tar_ll 1.0647 loss -1.0647 (6.833 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 58400 lr 1.848e-04 [train_loss] tar_ll 1.1458 loss -1.1458 (6.767 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 58600 lr 1.833e-04 [train_loss] tar_ll 1.0461 loss -1.0461 (6.802 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 58800 lr 1.818e-04 [train_loss] tar_ll 1.0745 loss -1.0745 (6.742 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.0696 loss -1.0696 (6.903 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 59200 lr 1.787e-04 [train_loss] tar_ll 0.9936 loss -0.9936 (6.695 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 59400 lr 1.772e-04 [train_loss] tar_ll 1.1277 loss -1.1277 (6.666 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 59600 lr 1.757e-04 [train_loss] tar_ll 1.0125 loss -1.0125 (6.777 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 59800 lr 1.742e-04 [train_loss] tar_ll 1.0791 loss -1.0791 (7.412 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 60000 lr 1.727e-04 [train_loss] tar_ll 1.0461 loss -1.0461 (6.721 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.13it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 1.0510 loss -1.0510 (32.922 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 60200 lr 1.713e-04 [train_loss] tar_ll 1.1095 loss -1.1095 (6.876 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 60400 lr 1.698e-04 [train_loss] tar_ll 0.9574 loss -0.9574 (6.881 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.0281 loss -1.0281 (6.814 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.0157 loss -1.0157 (6.726 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 61000 lr 1.653e-04 [train_loss] tar_ll 1.0902 loss -1.0902 (6.764 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 61200 lr 1.638e-04 [train_loss] tar_ll 1.1379 loss -1.1379 (6.906 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 61400 lr 1.624e-04 [train_loss] tar_ll 1.1202 loss -1.1202 (6.890 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 61600 lr 1.609e-04 [train_loss] tar_ll 1.1567 loss -1.1567 (6.703 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.0467 loss -1.0467 (6.887 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 62000 lr 1.580e-04 [train_loss] tar_ll 1.1950 loss -1.1950 (6.887 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 62200 lr 1.565e-04 [train_loss] tar_ll 1.0500 loss -1.0500 (7.121 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.1845 loss -1.1845 (6.924 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.1219 loss -1.1219 (6.714 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.1096 loss -1.1096 (7.275 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.0711 loss -1.0711 (6.892 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.1253 loss -1.1253 (6.779 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.1209 loss -1.1209 (7.622 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.1311 loss -1.1311 (7.892 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.1017 loss -1.1017 (7.305 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.1208 loss -1.1208 (7.176 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.0549 loss -1.0549 (7.257 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.1411 loss -1.1411 (6.968 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.1715 loss -1.1715 (7.046 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.1509 loss -1.1509 (7.046 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.0854 loss -1.0854 (7.011 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.06it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 1.1790 loss -1.1790 (34.461 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.1463 loss -1.1463 (7.043 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.1600 loss -1.1600 (7.061 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.1147 loss -1.1147 (7.026 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.1710 loss -1.1710 (7.161 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.2186 loss -1.2186 (7.038 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.1447 loss -1.1447 (6.965 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.1204 loss -1.1204 (7.204 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.2001 loss -1.2001 (6.891 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.0993 loss -1.0993 (7.120 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.1351 loss -1.1351 (7.119 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.1605 loss -1.1605 (7.227 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.1519 loss -1.1519 (7.123 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.1626 loss -1.1626 (7.013 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.2174 loss -1.2174 (7.152 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.1064 loss -1.1064 (7.317 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.2028 loss -1.2028 (7.136 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.1594 loss -1.1594 (7.267 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.1079 loss -1.1079 (7.077 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.1675 loss -1.1675 (7.001 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.0762 loss -1.0762 (7.233 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.2104 loss -1.2104 (7.140 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.1551 loss -1.1551 (7.137 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.2606 loss -1.2606 (7.109 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.1754 loss -1.1754 (7.191 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.2086 loss -1.2086 (7.069 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.04it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 1.2056 loss -1.2056 (34.469 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.1873 loss -1.1873 (7.245 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.1440 loss -1.1440 (7.295 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.1241 loss -1.1241 (7.046 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.1435 loss -1.1435 (7.302 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.1649 loss -1.1649 (7.195 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.1278 loss -1.1278 (7.026 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.2043 loss -1.2043 (7.114 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.2080 loss -1.2080 (7.084 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.2493 loss -1.2493 (7.098 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.1596 loss -1.1596 (6.982 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.1811 loss -1.1811 (6.891 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.1246 loss -1.1246 (7.277 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.2021 loss -1.2021 (7.080 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.2522 loss -1.2522 (6.949 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.1752 loss -1.1752 (7.180 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.1961 loss -1.1961 (6.907 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.2697 loss -1.2697 (7.197 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.2620 loss -1.2620 (6.966 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.1911 loss -1.1911 (7.028 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.1757 loss -1.1757 (7.155 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.2320 loss -1.2320 (7.120 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.1606 loss -1.1606 (6.951 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.2211 loss -1.2211 (6.976 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.2090 loss -1.2090 (7.125 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1329 loss -1.1329 (7.018 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.01it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 1.2373 loss -1.2373 (34.883 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1572 loss -1.1572 (7.203 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.1948 loss -1.1948 (7.135 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.1697 loss -1.1697 (7.077 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.2358 loss -1.2358 (7.076 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.2019 loss -1.2019 (7.064 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.2689 loss -1.2689 (7.123 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.2200 loss -1.2200 (7.234 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.1897 loss -1.1897 (7.011 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.1953 loss -1.1953 (7.166 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.2277 loss -1.2277 (7.116 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.2035 loss -1.2035 (7.253 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.2374 loss -1.2374 (7.245 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.1871 loss -1.1871 (7.189 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2734 loss -1.2734 (7.226 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.2194 loss -1.2194 (7.216 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.1824 loss -1.1824 (7.233 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.2246 loss -1.2246 (7.280 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.2548 loss -1.2548 (7.217 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.1740 loss -1.1740 (7.072 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.1712 loss -1.1712 (7.276 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.2627 loss -1.2627 (7.136 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.2632 loss -1.2632 (7.156 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.2536 loss -1.2536 (7.219 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.2288 loss -1.2288 (7.127 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.2200 loss -1.2200 (7.240 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.64it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 1.2423 loss -1.2423 (34.629 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.2477 loss -1.2477 (6.958 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.2144 loss -1.2144 (6.916 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1789 loss -1.1789 (6.994 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.1577 loss -1.1577 (7.002 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.2117 loss -1.2117 (7.068 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.2984 loss -1.2984 (6.914 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.2028 loss -1.2028 (6.928 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.2854 loss -1.2854 (7.002 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2234 loss -1.2234 (6.935 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.3449 loss -1.3449 (7.082 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.2730 loss -1.2730 (7.179 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.2565 loss -1.2565 (7.042 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.2078 loss -1.2078 (7.085 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.1903 loss -1.1903 (6.907 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.2348 loss -1.2348 (7.041 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.2189 loss -1.2189 (7.134 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.4007 loss -1.4007 (6.880 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.2609 loss -1.2609 (7.027 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.1571 loss -1.1571 (6.822 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.2832 loss -1.2832 (7.091 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2604 loss -1.2604 (7.077 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.2607 loss -1.2607 (7.726 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.2577 loss -1.2577 (7.295 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.2205 loss -1.2205 (7.207 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.2478 loss -1.2478 (7.040 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.95it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 1.2713 loss -1.2713 (34.504 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.2672 loss -1.2672 (7.242 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.2385 loss -1.2385 (7.166 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.2062 loss -1.2062 (7.282 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.2739 loss -1.2739 (7.215 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.2314 loss -1.2314 (7.092 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.1920 loss -1.1920 (7.504 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.2269 loss -1.2269 (7.043 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.2781 loss -1.2781 (7.204 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.2545 loss -1.2545 (7.158 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.3112 loss -1.3112 (7.114 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.2560 loss -1.2560 (7.076 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.2293 loss -1.2293 (7.204 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.3366 loss -1.3366 (7.258 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.2666 loss -1.2666 (7.175 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.2170 loss -1.2170 (7.113 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.2588 loss -1.2588 (7.222 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.2228 loss -1.2228 (7.238 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.3151 loss -1.3151 (7.249 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.3446 loss -1.3446 (7.220 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.2888 loss -1.2888 (7.250 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.2044 loss -1.2044 (7.224 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.3148 loss -1.3148 (7.010 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.3812 loss -1.3812 (7.080 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2644 loss -1.2644 (7.102 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.3443 loss -1.3443 (7.083 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.68it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 1.2948 loss -1.2948 (34.614 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.3104 loss -1.3104 (7.153 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.3031 loss -1.3031 (6.945 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.1901 loss -1.1901 (7.122 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.3123 loss -1.3123 (6.859 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.3001 loss -1.3001 (7.157 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2717 loss -1.2717 (7.178 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2640 loss -1.2640 (7.173 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.2920 loss -1.2920 (7.051 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2315 loss -1.2315 (7.075 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.3103 loss -1.3103 (7.057 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.2826 loss -1.2826 (7.074 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.2476 loss -1.2476 (7.184 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.3002 loss -1.3002 (6.969 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.2194 loss -1.2194 (7.066 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.2706 loss -1.2706 (7.078 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.3441 loss -1.3441 (7.249 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.2779 loss -1.2779 (6.998 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.3268 loss -1.3268 (7.215 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.2708 loss -1.2708 (7.025 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.2730 loss -1.2730 (7.094 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.2667 loss -1.2667 (7.219 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.3721 loss -1.3721 (7.146 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.2927 loss -1.2927 (7.315 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.3744 loss -1.3744 (6.987 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.2876 loss -1.2876 (7.374 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 85.80it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 1.3026 loss -1.3026 (34.965 secs)\n",
      "\n",
      "isanp:isanp-num_latents-256_rbf_100 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.3062 loss -1.3062 (7.203 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.2789 loss -1.2789 (7.117 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.3066 loss -1.3066 (7.061 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.2462 loss -1.2462 (7.246 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.2226 loss -1.2226 (7.132 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2694 loss -1.2694 (7.242 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.2620 loss -1.2620 (6.994 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.3235 loss -1.3235 (7.218 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.2625 loss -1.2625 (7.093 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.2044 loss -1.2044 (6.989 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.2692 loss -1.2692 (7.209 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.2170 loss -1.2170 (6.848 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.1946 loss -1.1946 (7.117 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.3025 loss -1.3025 (6.737 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.2739 loss -1.2739 (7.028 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.2688 loss -1.2688 (7.619 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.2625 loss -1.2625 (6.877 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.3257 loss -1.3257 (6.858 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2534 loss -1.2534 (7.595 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.2964 loss -1.2964 (7.501 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.3421 loss -1.3421 (7.034 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.3143 loss -1.3143 (6.991 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.3159 loss -1.3159 (7.001 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.3150 loss -1.3150 (7.002 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.2142 loss -1.2142 (6.998 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.62it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 1.3045 loss -1.3045 (34.639 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:35<00:00, 85.66it/s]\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 1.3045 loss -1.3045 (35.023 secs)\n",
      "isanp:isanp-num_latents-256_rbf_100 rbf tar_ll 1.3045 loss -1.3045 (35.023 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4407801.5 miliseconds\n",
      "Execution time: 4407.8015 seconds\n",
      "Initial Memory Usage: 0.0 MB\n",
      "Final Memory Usage: 198.203125 MB\n",
      "Memory Usage Change: 198.203125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='isanp-num_latents-256_rbf_100',val_seed=100, val_l=256,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37473e61-22d4-48b1-be2e-1f91c41e5290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1a072a1-9f44-43ea-acba-39f308e9172c",
   "metadata": {},
   "source": [
    "## ISANP2 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a030754f-08a2-4645-bfa1-f200da4690e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: isanp2-isanp2-num_latents-8_rbf_100\n",
      "Total number of parameters: 785602\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp2:isanp2-num_latents-8_rbf_100 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6711 loss 0.6711 (7.300 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 400 lr 5.000e-04 [train_loss] tar_ll -0.4930 loss 0.4930 (7.250 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 600 lr 5.000e-04 [train_loss] tar_ll -0.2236 loss 0.2236 (7.370 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 800 lr 4.999e-04 [train_loss] tar_ll -0.1238 loss 0.1238 (7.252 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 1000 lr 4.999e-04 [train_loss] tar_ll 0.0045 loss -0.0045 (7.228 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.1812 loss 0.1812 (7.186 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 1400 lr 4.998e-04 [train_loss] tar_ll -0.0098 loss 0.0098 (7.258 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 1600 lr 4.997e-04 [train_loss] tar_ll 0.1137 loss -0.1137 (7.345 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 1800 lr 4.996e-04 [train_loss] tar_ll 0.1293 loss -0.1293 (7.167 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 2000 lr 4.995e-04 [train_loss] tar_ll 0.1085 loss -0.1085 (7.144 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.2573 loss -0.2573 (7.333 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.2766 loss -0.2766 (7.011 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.2477 loss -0.2477 (7.174 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.2932 loss -0.2932 (7.086 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.3731 loss -0.3731 (7.179 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.3001 loss -0.3001 (7.224 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.3707 loss -0.3707 (7.162 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.3797 loss -0.3797 (7.332 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.4211 loss -0.4211 (7.163 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.3628 loss -0.3628 (7.017 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3681 loss -0.3681 (7.298 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.2749 loss -0.2749 (7.046 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4584 loss -0.4584 (7.200 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4448 loss -0.4448 (7.011 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.4604 loss -0.4604 (7.076 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.33it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 0.5710 loss -0.5710 (33.212 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.4625 loss -0.4625 (7.104 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.5000 loss -0.5000 (6.938 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4763 loss -0.4763 (7.000 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.5562 loss -0.5562 (7.114 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4707 loss -0.4707 (6.859 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.4975 loss -0.4975 (7.458 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.5414 loss -0.5414 (7.064 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5288 loss -0.5288 (6.988 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.3784 loss -0.3784 (6.908 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.1702 loss -0.1702 (7.065 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.1947 loss -0.1947 (7.198 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.3066 loss -0.3066 (6.933 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.4374 loss -0.4374 (7.064 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.4922 loss -0.4922 (6.938 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.5025 loss -0.5025 (6.889 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.5469 loss -0.5469 (7.194 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.4661 loss -0.4661 (6.952 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.5528 loss -0.5528 (7.079 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.5093 loss -0.5093 (6.942 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.6593 loss -0.6593 (7.081 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.4609 loss -0.4609 (7.137 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.4816 loss -0.4816 (7.051 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.4906 loss -0.4906 (7.037 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.5333 loss -0.5333 (6.963 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.6491 loss -0.6491 (7.020 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.59it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 0.5336 loss -0.5336 (33.867 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.5674 loss -0.5674 (7.333 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.6864 loss -0.6864 (7.104 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.6884 loss -0.6884 (7.090 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.6416 loss -0.6416 (7.198 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.7146 loss -0.7146 (7.192 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.5939 loss -0.5939 (7.278 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.5205 loss -0.5205 (6.964 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.5741 loss -0.5741 (7.283 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.6111 loss -0.6111 (7.121 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.4898 loss -0.4898 (7.293 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.3528 loss -0.3528 (7.140 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.6083 loss -0.6083 (7.136 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.4738 loss -0.4738 (7.223 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.5151 loss -0.5151 (7.043 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.5370 loss -0.5370 (7.031 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.5602 loss -0.5602 (7.330 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.7263 loss -0.7263 (7.116 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.7043 loss -0.7043 (6.802 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.6108 loss -0.6108 (7.142 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.7232 loss -0.7232 (6.897 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.4882 loss -0.4882 (6.963 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.5926 loss -0.5926 (6.939 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.5432 loss -0.5432 (6.894 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.6408 loss -0.6408 (7.236 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.6751 loss -0.6751 (6.963 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.48it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 0.7434 loss -0.7434 (33.529 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.5491 loss -0.5491 (7.110 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.6027 loss -0.6027 (6.991 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.1206 loss -0.1206 (7.167 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.2748 loss -0.2748 (6.892 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.1751 loss -0.1751 (6.899 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.3892 loss -0.3892 (7.132 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.5655 loss -0.5655 (6.861 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.5844 loss -0.5844 (7.155 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.6263 loss -0.6263 (6.958 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.5778 loss -0.5778 (7.059 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.7163 loss -0.7163 (7.166 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.7669 loss -0.7669 (7.226 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.6647 loss -0.6647 (6.957 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.5900 loss -0.5900 (7.138 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.4821 loss -0.4821 (7.128 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.3473 loss -0.3473 (7.183 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.4145 loss -0.4145 (7.096 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.4624 loss -0.4624 (7.300 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.1021 loss -0.1021 (7.184 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.4608 loss -0.4608 (6.942 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.4903 loss -0.4903 (7.230 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.4975 loss -0.4975 (7.253 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.5548 loss -0.5548 (7.073 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.5073 loss -0.5073 (7.249 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.5982 loss -0.5982 (7.094 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.09it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll -0.0777 loss 0.0777 (33.677 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.5160 loss -0.5160 (7.207 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.5897 loss -0.5897 (7.028 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.5856 loss -0.5856 (7.267 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.3471 loss -0.3471 (7.123 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.5574 loss -0.5574 (7.123 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.2968 loss -0.2968 (7.282 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.5322 loss -0.5322 (7.210 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.6169 loss -0.6169 (7.039 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.4312 loss -0.4312 (7.085 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.4722 loss -0.4722 (6.735 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.5573 loss -0.5573 (7.296 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.5129 loss -0.5129 (7.039 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.7397 loss -0.7397 (6.862 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.5859 loss -0.5859 (7.109 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.4761 loss -0.4761 (6.912 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.4873 loss -0.4873 (7.214 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.5996 loss -0.5996 (6.904 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.5987 loss -0.5987 (7.130 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.6945 loss -0.6945 (7.250 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.7683 loss -0.7683 (6.978 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.6573 loss -0.6573 (7.243 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.7162 loss -0.7162 (7.030 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.6565 loss -0.6565 (6.946 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.7190 loss -0.7190 (6.972 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.6846 loss -0.6846 (6.984 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.24it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 0.7529 loss -0.7529 (33.248 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.5913 loss -0.5913 (7.186 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.6652 loss -0.6652 (7.148 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.4041 loss -0.4041 (7.018 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.5355 loss -0.5355 (7.106 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.4183 loss -0.4183 (7.181 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.7009 loss -0.7009 (7.360 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.5117 loss -0.5117 (7.073 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.7773 loss -0.7773 (6.849 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.5972 loss -0.5972 (7.183 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.5794 loss -0.5794 (7.036 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.6513 loss -0.6513 (7.359 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.4257 loss -0.4257 (6.950 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.6154 loss -0.6154 (7.067 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.5604 loss -0.5604 (7.156 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.6103 loss -0.6103 (7.027 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.6173 loss -0.6173 (7.461 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.6503 loss -0.6503 (7.301 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.6817 loss -0.6817 (7.001 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.7132 loss -0.7132 (7.330 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.6710 loss -0.6710 (7.416 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.6110 loss -0.6110 (7.230 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.4674 loss -0.4674 (7.465 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.5691 loss -0.5691 (7.101 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.6587 loss -0.6587 (7.342 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.5247 loss -0.5247 (7.138 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.90it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 0.2143 loss -0.2143 (33.749 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.6169 loss -0.6169 (7.090 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.5716 loss -0.5716 (6.934 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.7009 loss -0.7009 (6.962 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.7284 loss -0.7284 (7.038 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.7669 loss -0.7669 (6.787 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.8003 loss -0.8003 (7.149 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.8509 loss -0.8509 (6.936 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.7092 loss -0.7092 (6.876 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.7009 loss -0.7009 (7.099 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.3977 loss -0.3977 (6.921 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.3851 loss -0.3851 (7.286 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.5639 loss -0.5639 (6.937 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.5418 loss -0.5418 (7.041 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.6982 loss -0.6982 (7.156 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.5497 loss -0.5497 (7.129 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.6170 loss -0.6170 (6.948 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.4869 loss -0.4869 (7.318 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.6467 loss -0.6467 (6.983 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.6172 loss -0.6172 (7.071 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.7184 loss -0.7184 (7.130 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.3162 loss -0.3162 (7.002 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.6166 loss -0.6166 (7.295 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.6366 loss -0.6366 (7.075 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.7103 loss -0.7103 (7.135 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.6859 loss -0.6859 (7.013 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.25it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 0.7928 loss -0.7928 (33.995 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.8474 loss -0.8474 (7.266 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.8549 loss -0.8549 (7.035 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.6541 loss -0.6541 (7.238 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.6756 loss -0.6756 (7.147 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.6792 loss -0.6792 (7.031 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.8219 loss -0.8219 (7.150 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.6186 loss -0.6186 (7.031 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.7439 loss -0.7439 (7.079 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.7955 loss -0.7955 (7.082 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.7520 loss -0.7520 (7.254 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.8505 loss -0.8505 (7.145 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.8844 loss -0.8844 (7.268 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.7952 loss -0.7952 (7.293 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.7487 loss -0.7487 (7.038 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.7005 loss -0.7005 (7.073 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.7116 loss -0.7116 (7.292 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.7951 loss -0.7951 (7.282 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.7931 loss -0.7931 (7.082 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.7999 loss -0.7999 (7.210 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.6107 loss -0.6107 (7.341 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.6040 loss -0.6040 (7.221 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.8140 loss -0.8140 (7.007 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.7825 loss -0.7825 (7.158 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.7575 loss -0.7575 (6.882 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.8998 loss -0.8998 (6.784 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.87it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 0.1181 loss -0.1181 (33.757 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.6633 loss -0.6633 (6.918 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.8442 loss -0.8442 (7.023 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.8879 loss -0.8879 (7.142 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.7287 loss -0.7287 (6.739 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.7477 loss -0.7477 (7.064 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.8344 loss -0.8344 (7.045 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.7977 loss -0.7977 (7.126 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.7902 loss -0.7902 (6.878 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.8374 loss -0.8374 (7.122 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.9341 loss -0.9341 (6.888 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.8088 loss -0.8088 (7.012 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.9257 loss -0.9257 (7.060 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 42600 lr 3.076e-04 [train_loss] tar_ll 0.7165 loss -0.7165 (7.059 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.9455 loss -0.9455 (7.085 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.8708 loss -0.8708 (7.114 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.8046 loss -0.8046 (7.120 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.8780 loss -0.8780 (7.156 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.8535 loss -0.8535 (6.948 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.9227 loss -0.9227 (7.055 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.8549 loss -0.8549 (7.047 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.8373 loss -0.8373 (7.143 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.8585 loss -0.8585 (6.967 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.8630 loss -0.8630 (7.036 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.9425 loss -0.9425 (6.994 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.8431 loss -0.8431 (7.358 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.75it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 0.9435 loss -0.9435 (33.428 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.8510 loss -0.8510 (7.112 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.9235 loss -0.9235 (7.176 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.8796 loss -0.8796 (6.923 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.8371 loss -0.8371 (7.151 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.8786 loss -0.8786 (7.171 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.9453 loss -0.9453 (7.061 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.6648 loss -0.6648 (7.171 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.9056 loss -0.9056 (6.951 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.8841 loss -0.8841 (7.024 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.9645 loss -0.9645 (7.031 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.9645 loss -0.9645 (7.270 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.9201 loss -0.9201 (7.204 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 47600 lr 2.688e-04 [train_loss] tar_ll 0.9132 loss -0.9132 (6.938 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.9021 loss -0.9021 (7.078 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.8729 loss -0.8729 (7.286 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.8702 loss -0.8702 (6.956 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 48400 lr 2.626e-04 [train_loss] tar_ll 0.9151 loss -0.9151 (7.235 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.9019 loss -0.9019 (7.163 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 48800 lr 2.594e-04 [train_loss] tar_ll 0.9955 loss -0.9955 (7.098 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.9847 loss -0.9847 (7.251 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 49200 lr 2.563e-04 [train_loss] tar_ll 0.9192 loss -0.9192 (7.268 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.9805 loss -0.9805 (7.067 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 49600 lr 2.531e-04 [train_loss] tar_ll 0.9726 loss -0.9726 (6.972 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 49800 lr 2.516e-04 [train_loss] tar_ll 1.0286 loss -1.0286 (6.884 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 50000 lr 2.500e-04 [train_loss] tar_ll 0.8873 loss -0.8873 (7.005 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.30it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 0.9800 loss -0.9800 (33.225 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 50200 lr 2.484e-04 [train_loss] tar_ll 0.9497 loss -0.9497 (7.055 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 50400 lr 2.469e-04 [train_loss] tar_ll 0.9561 loss -0.9561 (7.022 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 50600 lr 2.453e-04 [train_loss] tar_ll 1.0730 loss -1.0730 (6.788 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 50800 lr 2.437e-04 [train_loss] tar_ll 0.9448 loss -0.9448 (7.130 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 51000 lr 2.421e-04 [train_loss] tar_ll 0.9378 loss -0.9378 (7.233 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 51200 lr 2.406e-04 [train_loss] tar_ll 0.9274 loss -0.9274 (7.164 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 51400 lr 2.390e-04 [train_loss] tar_ll 0.8675 loss -0.8675 (7.029 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 51600 lr 2.374e-04 [train_loss] tar_ll 0.9377 loss -0.9377 (7.190 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 51800 lr 2.359e-04 [train_loss] tar_ll 1.0310 loss -1.0310 (7.042 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 52000 lr 2.343e-04 [train_loss] tar_ll 0.8380 loss -0.8380 (7.171 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 52200 lr 2.327e-04 [train_loss] tar_ll 0.9572 loss -0.9572 (7.086 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 52400 lr 2.312e-04 [train_loss] tar_ll 1.0783 loss -1.0783 (7.031 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 52600 lr 2.296e-04 [train_loss] tar_ll 0.9496 loss -0.9496 (7.123 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 52800 lr 2.280e-04 [train_loss] tar_ll 0.9560 loss -0.9560 (7.118 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 53000 lr 2.265e-04 [train_loss] tar_ll 1.0108 loss -1.0108 (7.129 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 53200 lr 2.249e-04 [train_loss] tar_ll 1.0178 loss -1.0178 (7.025 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 53400 lr 2.233e-04 [train_loss] tar_ll 1.0201 loss -1.0201 (7.229 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 53600 lr 2.218e-04 [train_loss] tar_ll 0.9853 loss -0.9853 (7.130 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 53800 lr 2.202e-04 [train_loss] tar_ll 0.9699 loss -0.9699 (6.988 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 54000 lr 2.187e-04 [train_loss] tar_ll 0.9082 loss -0.9082 (7.357 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 54200 lr 2.171e-04 [train_loss] tar_ll 0.8832 loss -0.8832 (7.992 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 54400 lr 2.156e-04 [train_loss] tar_ll 0.9229 loss -0.9229 (7.814 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 54600 lr 2.140e-04 [train_loss] tar_ll 1.0072 loss -1.0072 (7.215 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 54800 lr 2.124e-04 [train_loss] tar_ll 0.9937 loss -0.9937 (7.330 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 55000 lr 2.109e-04 [train_loss] tar_ll 1.0687 loss -1.0687 (7.288 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.51it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 1.0340 loss -1.0340 (34.679 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 55200 lr 2.093e-04 [train_loss] tar_ll 0.9342 loss -0.9342 (7.322 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 55400 lr 2.078e-04 [train_loss] tar_ll 0.9596 loss -0.9596 (7.263 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 55600 lr 2.062e-04 [train_loss] tar_ll 1.0461 loss -1.0461 (7.121 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 55800 lr 2.047e-04 [train_loss] tar_ll 1.0163 loss -1.0163 (7.124 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 56000 lr 2.032e-04 [train_loss] tar_ll 1.0075 loss -1.0075 (7.981 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 56200 lr 2.016e-04 [train_loss] tar_ll 0.9966 loss -0.9966 (7.295 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 56400 lr 2.001e-04 [train_loss] tar_ll 0.9573 loss -0.9573 (7.261 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 56600 lr 1.985e-04 [train_loss] tar_ll 0.8995 loss -0.8995 (7.218 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 56800 lr 1.970e-04 [train_loss] tar_ll 0.9897 loss -0.9897 (7.321 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 57000 lr 1.955e-04 [train_loss] tar_ll 1.0206 loss -1.0206 (7.238 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 57200 lr 1.939e-04 [train_loss] tar_ll 1.0145 loss -1.0145 (7.367 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 57400 lr 1.924e-04 [train_loss] tar_ll 1.0623 loss -1.0623 (7.394 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 57600 lr 1.909e-04 [train_loss] tar_ll 0.9958 loss -0.9958 (7.167 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 57800 lr 1.894e-04 [train_loss] tar_ll 0.6379 loss -0.6379 (7.273 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 58000 lr 1.878e-04 [train_loss] tar_ll 0.9864 loss -0.9864 (7.275 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 58200 lr 1.863e-04 [train_loss] tar_ll 0.8976 loss -0.8976 (7.445 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 58400 lr 1.848e-04 [train_loss] tar_ll 1.0139 loss -1.0139 (7.326 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 58600 lr 1.833e-04 [train_loss] tar_ll 1.0214 loss -1.0214 (7.370 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 58800 lr 1.818e-04 [train_loss] tar_ll 0.9091 loss -0.9091 (7.093 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.0871 loss -1.0871 (7.198 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 59200 lr 1.787e-04 [train_loss] tar_ll 0.9959 loss -0.9959 (7.265 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 59400 lr 1.772e-04 [train_loss] tar_ll 0.9040 loss -0.9040 (7.047 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 59600 lr 1.757e-04 [train_loss] tar_ll 1.0144 loss -1.0144 (7.299 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 59800 lr 1.742e-04 [train_loss] tar_ll 1.0477 loss -1.0477 (6.980 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 60000 lr 1.727e-04 [train_loss] tar_ll 1.0322 loss -1.0322 (7.300 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 84.69it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 1.0685 loss -1.0685 (35.425 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 60200 lr 1.713e-04 [train_loss] tar_ll 1.0499 loss -1.0499 (7.314 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 60400 lr 1.698e-04 [train_loss] tar_ll 0.8804 loss -0.8804 (7.424 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 60600 lr 1.683e-04 [train_loss] tar_ll 0.9904 loss -0.9904 (7.231 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 60800 lr 1.668e-04 [train_loss] tar_ll 0.8004 loss -0.8004 (7.149 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 61000 lr 1.653e-04 [train_loss] tar_ll 0.9573 loss -0.9573 (7.028 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 61200 lr 1.638e-04 [train_loss] tar_ll 0.9646 loss -0.9646 (7.394 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 61400 lr 1.624e-04 [train_loss] tar_ll 1.0110 loss -1.0110 (7.227 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 61600 lr 1.609e-04 [train_loss] tar_ll 1.0942 loss -1.0942 (7.175 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 61800 lr 1.594e-04 [train_loss] tar_ll 0.9955 loss -0.9955 (7.033 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 62000 lr 1.580e-04 [train_loss] tar_ll 0.8835 loss -0.8835 (7.149 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 62200 lr 1.565e-04 [train_loss] tar_ll 0.9799 loss -0.9799 (7.336 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.0417 loss -1.0417 (7.185 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.0074 loss -1.0074 (7.262 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.0735 loss -1.0735 (7.254 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.0584 loss -1.0584 (7.304 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.1241 loss -1.1241 (7.216 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.0587 loss -1.0587 (7.412 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.0581 loss -1.0581 (7.091 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.0300 loss -1.0300 (7.407 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 64000 lr 1.436e-04 [train_loss] tar_ll 0.9986 loss -0.9986 (7.140 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.0170 loss -1.0170 (7.180 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.0918 loss -1.0918 (7.117 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.0766 loss -1.0766 (6.839 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.0796 loss -1.0796 (6.979 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.0271 loss -1.0271 (6.900 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 84.68it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 1.0985 loss -1.0985 (35.430 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 65200 lr 1.351e-04 [train_loss] tar_ll 0.9877 loss -0.9877 (7.070 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.0704 loss -1.0704 (6.943 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 65600 lr 1.323e-04 [train_loss] tar_ll 0.9777 loss -0.9777 (7.102 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.0854 loss -1.0854 (6.930 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 66000 lr 1.296e-04 [train_loss] tar_ll 0.9540 loss -0.9540 (7.514 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 66200 lr 1.282e-04 [train_loss] tar_ll 0.9949 loss -0.9949 (7.212 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.0563 loss -1.0563 (7.419 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.1317 loss -1.1317 (6.993 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.0360 loss -1.0360 (7.172 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.0980 loss -1.0980 (7.143 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.1015 loss -1.1015 (7.119 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.1406 loss -1.1406 (7.775 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.0622 loss -1.0622 (6.941 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.0174 loss -1.0174 (7.222 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.0780 loss -1.0780 (7.295 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.1333 loss -1.1333 (7.652 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.0599 loss -1.0599 (7.105 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.0582 loss -1.0582 (7.411 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.0383 loss -1.0383 (7.147 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.0706 loss -1.0706 (7.101 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.0571 loss -1.0571 (7.327 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.1084 loss -1.1084 (7.266 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.0978 loss -1.0978 (7.146 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.1545 loss -1.1545 (7.237 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.1111 loss -1.1111 (7.495 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 84.17it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 1.1139 loss -1.1139 (35.643 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.0524 loss -1.0524 (7.162 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.1409 loss -1.1409 (7.550 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.1568 loss -1.1568 (7.073 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.1655 loss -1.1655 (7.211 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.1194 loss -1.1194 (7.411 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.0880 loss -1.0880 (7.471 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.0864 loss -1.0864 (7.206 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.2130 loss -1.2130 (7.487 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.1004 loss -1.1004 (7.160 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.1184 loss -1.1184 (7.422 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.0824 loss -1.0824 (7.276 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.1362 loss -1.1362 (7.309 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.1106 loss -1.1106 (7.121 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.0794 loss -1.0794 (6.900 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.0759 loss -1.0759 (7.167 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.1200 loss -1.1200 (7.148 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.0866 loss -1.0866 (7.268 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.1301 loss -1.1301 (7.072 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.1716 loss -1.1716 (7.491 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.1583 loss -1.1583 (7.170 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.0509 loss -1.0509 (7.404 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.1564 loss -1.1564 (6.968 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.1061 loss -1.1061 (7.140 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.1220 loss -1.1220 (7.392 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1506 loss -1.1506 (6.925 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 83.96it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 1.1353 loss -1.1353 (35.732 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1555 loss -1.1555 (7.325 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.1752 loss -1.1752 (7.234 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.1004 loss -1.1004 (7.297 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.2071 loss -1.2071 (7.345 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.1597 loss -1.1597 (7.294 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.1614 loss -1.1614 (7.169 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.0846 loss -1.0846 (7.297 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.0891 loss -1.0891 (7.153 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.1298 loss -1.1298 (7.148 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.1893 loss -1.1893 (7.110 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.1045 loss -1.1045 (7.432 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.1265 loss -1.1265 (7.043 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.1134 loss -1.1134 (7.323 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.1249 loss -1.1249 (7.260 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.1456 loss -1.1456 (6.941 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.1510 loss -1.1510 (7.427 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.1362 loss -1.1362 (7.096 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.0755 loss -1.0755 (7.353 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.1008 loss -1.1008 (7.023 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.0622 loss -1.0622 (7.114 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.1712 loss -1.1712 (7.269 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.1020 loss -1.1020 (7.339 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.1939 loss -1.1939 (7.167 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.1610 loss -1.1610 (7.349 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.0947 loss -1.0947 (7.273 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 84.02it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 1.1540 loss -1.1540 (35.709 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.0907 loss -1.0907 (7.367 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.1753 loss -1.1753 (6.989 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1354 loss -1.1354 (7.225 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.0885 loss -1.0885 (7.042 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.2077 loss -1.2077 (7.003 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.2696 loss -1.2696 (7.345 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.1984 loss -1.1984 (7.225 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.1244 loss -1.1244 (7.425 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2694 loss -1.2694 (7.389 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.2151 loss -1.2151 (7.631 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.1486 loss -1.1486 (7.392 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.1493 loss -1.1493 (7.317 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.1348 loss -1.1348 (6.863 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.1635 loss -1.1635 (7.007 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.2006 loss -1.2006 (7.015 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.0906 loss -1.0906 (7.140 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.1524 loss -1.1524 (6.929 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.1168 loss -1.1168 (6.941 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.2044 loss -1.2044 (7.008 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.0945 loss -1.0945 (6.952 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.1831 loss -1.1831 (7.300 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.1071 loss -1.1071 (6.983 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.1043 loss -1.1043 (7.044 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.1789 loss -1.1789 (7.197 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.0626 loss -1.0626 (7.161 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.53it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 1.1888 loss -1.1888 (34.671 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.1946 loss -1.1946 (7.210 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.1703 loss -1.1703 (7.100 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.1840 loss -1.1840 (7.179 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.0758 loss -1.0758 (7.117 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.1476 loss -1.1476 (7.289 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.2306 loss -1.2306 (7.192 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.1384 loss -1.1384 (7.140 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.1859 loss -1.1859 (7.205 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.2904 loss -1.2904 (7.062 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.0738 loss -1.0738 (7.146 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.2401 loss -1.2401 (7.285 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.1422 loss -1.1422 (7.225 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.2207 loss -1.2207 (7.067 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.2299 loss -1.2299 (7.158 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.1840 loss -1.1840 (7.256 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.2184 loss -1.2184 (7.536 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.2049 loss -1.2049 (7.111 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.2026 loss -1.2026 (7.204 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.0719 loss -1.0719 (7.113 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.2117 loss -1.2117 (7.230 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.2048 loss -1.2048 (7.290 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.2419 loss -1.2419 (7.153 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.2280 loss -1.2280 (7.265 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2438 loss -1.2438 (6.903 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.1032 loss -1.1032 (7.204 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.90it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 1.2089 loss -1.2089 (34.526 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.2028 loss -1.2028 (7.162 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.2504 loss -1.2504 (7.116 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.2332 loss -1.2332 (7.145 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.2599 loss -1.2599 (7.008 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.1713 loss -1.1713 (7.081 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.1746 loss -1.1746 (7.154 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.1770 loss -1.1770 (7.007 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.1556 loss -1.1556 (7.089 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2583 loss -1.2583 (7.000 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2088 loss -1.2088 (6.910 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.1999 loss -1.1999 (7.244 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.2355 loss -1.2355 (6.942 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.1685 loss -1.1685 (7.044 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.0829 loss -1.0829 (7.082 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.2249 loss -1.2249 (7.184 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.2239 loss -1.2239 (7.198 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.1990 loss -1.1990 (7.104 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.2323 loss -1.2323 (7.289 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.2140 loss -1.2140 (7.160 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.2809 loss -1.2809 (7.146 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.1941 loss -1.1941 (7.007 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.2252 loss -1.2252 (7.160 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.1552 loss -1.1552 (6.999 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.1683 loss -1.1683 (7.146 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.1428 loss -1.1428 (7.092 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.86it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 1.2156 loss -1.2156 (34.147 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.2408 loss -1.2408 (7.324 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.1986 loss -1.1986 (7.148 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.2187 loss -1.2187 (7.157 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.2695 loss -1.2695 (7.065 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.1783 loss -1.1783 (7.306 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.1548 loss -1.1548 (7.219 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.1733 loss -1.1733 (7.063 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.1755 loss -1.1755 (7.343 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.1908 loss -1.1908 (7.015 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.1205 loss -1.1205 (7.075 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.1476 loss -1.1476 (7.280 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.1589 loss -1.1589 (7.235 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.1154 loss -1.1154 (6.945 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.1557 loss -1.1557 (7.306 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.1660 loss -1.1660 (6.939 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.1208 loss -1.1208 (7.002 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.1838 loss -1.1838 (7.061 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.1962 loss -1.1962 (7.050 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.1987 loss -1.1987 (7.030 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.1863 loss -1.1863 (6.905 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.2711 loss -1.2711 (7.201 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2467 loss -1.2467 (7.052 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.1753 loss -1.1753 (7.027 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.1989 loss -1.1989 (6.962 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_100 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.1987 loss -1.1987 (6.876 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.47it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 1.2176 loss -1.2176 (34.299 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:34<00:00, 88.14it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_100 rbf tar_ll 1.2176 loss -1.2176 (34.039 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4307590.5 miliseconds\n",
      "Execution time: 4307.5905 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 55.90087890625 MB\n",
      "Memory Usage Change: 39.65087890625 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp2', name='isanp2-num_latents-8_rbf_100',val_seed=100, val_l=8,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6e11f-ae9b-4450-8358-dee56819ef47",
   "metadata": {},
   "source": [
    "## ISANP2 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc22ac4-d5a3-4b8b-b471-64bc4500d311",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: isanp2-isanp2-num_latents-16_rbf_100\n",
      "Total number of parameters: 786114\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 16\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp2:isanp2-num_latents-16_rbf_100 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6396 loss 0.6396 (7.103 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 400 lr 5.000e-04 [train_loss] tar_ll -0.4110 loss 0.4110 (7.319 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 600 lr 5.000e-04 [train_loss] tar_ll -0.1449 loss 0.1449 (7.058 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 800 lr 4.999e-04 [train_loss] tar_ll 0.0053 loss -0.0053 (7.246 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 1000 lr 4.999e-04 [train_loss] tar_ll 0.0276 loss -0.0276 (7.105 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 1200 lr 4.998e-04 [train_loss] tar_ll 0.0017 loss -0.0017 (7.196 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 1400 lr 4.998e-04 [train_loss] tar_ll 0.0286 loss -0.0286 (7.230 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 1600 lr 4.997e-04 [train_loss] tar_ll 0.1553 loss -0.1553 (7.115 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 1800 lr 4.996e-04 [train_loss] tar_ll 0.2110 loss -0.2110 (7.335 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 2000 lr 4.995e-04 [train_loss] tar_ll 0.1287 loss -0.1287 (7.077 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.2983 loss -0.2983 (7.209 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.3386 loss -0.3386 (7.102 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.3070 loss -0.3070 (7.276 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.3696 loss -0.3696 (7.115 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.3506 loss -0.3506 (6.982 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.3682 loss -0.3682 (7.308 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.4166 loss -0.4166 (7.277 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2879 loss -0.2879 (7.060 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.4251 loss -0.4251 (7.179 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.3393 loss -0.3393 (7.227 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3281 loss -0.3281 (7.323 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.2308 loss -0.2308 (7.179 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.3793 loss -0.3793 (7.129 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4348 loss -0.4348 (7.278 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.5436 loss -0.5436 (6.959 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.77it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 0.4455 loss -0.4455 (33.421 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.4618 loss -0.4618 (7.031 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.3437 loss -0.3437 (6.857 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.3989 loss -0.3989 (6.934 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.4978 loss -0.4978 (7.000 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.5686 loss -0.5686 (6.823 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.5155 loss -0.5155 (7.392 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.4610 loss -0.4610 (6.786 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5427 loss -0.5427 (7.262 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.5539 loss -0.5539 (6.942 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.6029 loss -0.6029 (6.929 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.5413 loss -0.5413 (7.497 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.4772 loss -0.4772 (7.027 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.5622 loss -0.5622 (6.990 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.3310 loss -0.3310 (6.859 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.2547 loss -0.2547 (7.172 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.2827 loss -0.2827 (7.054 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.2330 loss -0.2330 (6.976 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.5132 loss -0.5132 (6.790 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.4743 loss -0.4743 (6.870 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.4661 loss -0.4661 (6.931 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.3713 loss -0.3713 (7.051 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.3251 loss -0.3251 (7.049 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.2943 loss -0.2943 (6.913 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.4905 loss -0.4905 (7.090 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.6100 loss -0.6100 (7.039 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.72it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 0.6467 loss -0.6467 (33.441 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.3129 loss -0.3129 (7.379 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.6036 loss -0.6036 (7.130 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.5720 loss -0.5720 (7.005 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.5690 loss -0.5690 (7.114 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.5646 loss -0.5646 (7.156 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.5233 loss -0.5233 (7.358 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.4216 loss -0.4216 (7.185 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.3934 loss -0.3934 (7.254 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.4733 loss -0.4733 (7.116 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.3436 loss -0.3436 (7.252 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.5829 loss -0.5829 (7.182 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.5715 loss -0.5715 (7.494 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.3941 loss -0.3941 (7.217 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.5955 loss -0.5955 (7.234 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.4233 loss -0.4233 (7.271 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.4393 loss -0.4393 (7.171 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.5188 loss -0.5188 (7.203 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.5117 loss -0.5117 (7.107 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.6388 loss -0.6388 (8.215 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.5592 loss -0.5592 (7.693 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.5032 loss -0.5032 (7.420 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.5712 loss -0.5712 (7.079 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.4640 loss -0.4640 (7.102 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.5736 loss -0.5736 (6.813 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 15000 lr 4.728e-04 [train_loss] tar_ll -0.1294 loss 0.1294 (7.078 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.16it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 0.0823 loss -0.0823 (33.278 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.1060 loss -0.1060 (7.227 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.0696 loss -0.0696 (6.877 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.2835 loss -0.2835 (6.958 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.3213 loss -0.3213 (7.029 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.3083 loss -0.3083 (6.987 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.3260 loss -0.3260 (7.039 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.4364 loss -0.4364 (6.968 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.2346 loss -0.2346 (7.150 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.4251 loss -0.4251 (7.106 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.4902 loss -0.4902 (6.924 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.5683 loss -0.5683 (7.646 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.2642 loss -0.2642 (7.302 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.4656 loss -0.4656 (6.998 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.3575 loss -0.3575 (7.256 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.2575 loss -0.2575 (7.068 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.5557 loss -0.5557 (7.168 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.4661 loss -0.4661 (7.118 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.4723 loss -0.4723 (7.207 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.5883 loss -0.5883 (7.100 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.3899 loss -0.3899 (7.247 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.5818 loss -0.5818 (7.216 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.5578 loss -0.5578 (7.045 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.5370 loss -0.5370 (7.353 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.6412 loss -0.6412 (7.093 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.5925 loss -0.5925 (7.427 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.53it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 0.6646 loss -0.6646 (33.888 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.6385 loss -0.6385 (7.265 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.3690 loss -0.3690 (7.282 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 20600 lr 4.494e-04 [train_loss] tar_ll -0.0946 loss 0.0946 (7.099 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.3048 loss -0.3048 (7.364 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.5182 loss -0.5182 (7.132 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.4838 loss -0.4838 (7.377 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.5559 loss -0.5559 (7.132 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.5357 loss -0.5357 (7.412 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.4788 loss -0.4788 (7.150 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.4773 loss -0.4773 (6.972 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.5898 loss -0.5898 (7.434 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.4940 loss -0.4940 (6.976 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.5794 loss -0.5794 (7.179 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.6594 loss -0.6594 (6.971 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.6345 loss -0.6345 (7.215 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.5388 loss -0.5388 (7.202 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.4704 loss -0.4704 (7.099 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.5008 loss -0.5008 (7.064 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.4603 loss -0.4603 (7.078 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.4667 loss -0.4667 (7.007 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.5788 loss -0.5788 (7.217 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.6113 loss -0.6113 (7.161 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.6986 loss -0.6986 (7.069 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.4261 loss -0.4261 (7.248 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.6211 loss -0.6211 (6.944 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.10it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 0.5990 loss -0.5990 (33.672 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.6438 loss -0.6438 (7.320 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.6642 loss -0.6642 (6.992 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.5931 loss -0.5931 (7.086 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.5219 loss -0.5219 (7.004 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.7045 loss -0.7045 (7.165 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.6656 loss -0.6656 (7.120 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.5999 loss -0.5999 (7.240 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.7023 loss -0.7023 (7.138 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.3260 loss -0.3260 (7.039 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.4682 loss -0.4682 (7.166 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.5748 loss -0.5748 (7.104 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.6116 loss -0.6116 (7.201 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.7255 loss -0.7255 (7.182 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.5734 loss -0.5734 (7.106 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.7143 loss -0.7143 (7.091 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.3815 loss -0.3815 (7.363 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.5984 loss -0.5984 (7.133 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.6560 loss -0.6560 (7.224 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.7335 loss -0.7335 (7.169 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.7752 loss -0.7752 (7.000 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.7669 loss -0.7669 (7.332 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.6444 loss -0.6444 (7.072 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.6601 loss -0.6601 (7.220 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.6850 loss -0.6850 (7.153 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.7434 loss -0.7434 (7.055 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.52it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 0.7640 loss -0.7640 (33.893 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.8195 loss -0.8195 (7.300 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.5910 loss -0.5910 (6.981 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.4356 loss -0.4356 (7.004 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.6142 loss -0.6142 (7.048 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.7063 loss -0.7063 (7.049 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.6442 loss -0.6442 (7.159 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.3352 loss -0.3352 (7.454 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.4777 loss -0.4777 (6.968 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.5428 loss -0.5428 (7.093 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.6107 loss -0.6107 (6.943 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.6220 loss -0.6220 (7.004 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.6815 loss -0.6815 (7.103 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.6399 loss -0.6399 (6.968 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.7563 loss -0.7563 (7.094 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.7675 loss -0.7675 (6.862 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.7596 loss -0.7596 (7.439 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.4646 loss -0.4646 (7.113 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.5900 loss -0.5900 (7.139 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.7652 loss -0.7652 (6.989 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.6552 loss -0.6552 (7.116 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.7423 loss -0.7423 (7.315 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.6635 loss -0.6635 (6.821 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.5471 loss -0.5471 (7.229 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.7729 loss -0.7729 (7.241 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.6783 loss -0.6783 (7.069 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.58it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 0.6501 loss -0.6501 (33.870 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.7597 loss -0.7597 (7.230 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.7616 loss -0.7616 (7.283 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.7320 loss -0.7320 (7.242 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.3499 loss -0.3499 (7.248 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.5791 loss -0.5791 (7.059 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.5222 loss -0.5222 (7.252 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.4944 loss -0.4944 (7.188 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.7417 loss -0.7417 (7.253 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.4705 loss -0.4705 (7.033 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.6549 loss -0.6549 (7.031 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.7036 loss -0.7036 (7.329 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.7671 loss -0.7671 (7.006 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.7790 loss -0.7790 (7.411 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.5463 loss -0.5463 (7.094 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.6924 loss -0.6924 (7.132 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.7023 loss -0.7023 (7.377 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.6573 loss -0.6573 (7.308 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.6847 loss -0.6847 (7.095 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.3152 loss -0.3152 (7.060 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.7087 loss -0.7087 (7.061 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.8108 loss -0.8108 (7.323 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.7565 loss -0.7565 (6.965 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.8020 loss -0.8020 (7.205 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.8066 loss -0.8066 (7.240 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.8428 loss -0.8428 (7.042 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.97it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 0.8652 loss -0.8652 (33.347 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.8226 loss -0.8226 (7.326 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.8169 loss -0.8169 (6.740 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.8677 loss -0.8677 (7.214 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.7909 loss -0.7909 (7.477 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.8010 loss -0.8010 (9.583 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.8703 loss -0.8703 (7.750 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.8742 loss -0.8742 (7.654 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.8152 loss -0.8152 (7.629 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.8412 loss -0.8412 (7.608 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.7706 loss -0.7706 (7.474 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.8593 loss -0.8593 (7.541 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.8607 loss -0.8607 (7.799 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 42600 lr 3.076e-04 [train_loss] tar_ll 0.8651 loss -0.8651 (7.538 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.8176 loss -0.8176 (7.742 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.8826 loss -0.8826 (7.650 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.8720 loss -0.8720 (7.851 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.7588 loss -0.7588 (6.989 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.7977 loss -0.7977 (7.542 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.8881 loss -0.8881 (7.074 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.8322 loss -0.8322 (7.488 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.8760 loss -0.8760 (7.171 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.7073 loss -0.7073 (7.353 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.6390 loss -0.6390 (6.896 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.7034 loss -0.7034 (6.917 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.8399 loss -0.8399 (7.059 secs)\n",
      "100%|##########| 3000/3000 [00:37<00:00, 79.73it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 0.9191 loss -0.9191 (37.630 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.8636 loss -0.8636 (8.295 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.7479 loss -0.7479 (7.915 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.8162 loss -0.8162 (7.763 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.8683 loss -0.8683 (7.902 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.8500 loss -0.8500 (7.420 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.8923 loss -0.8923 (7.532 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.8292 loss -0.8292 (7.180 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.9465 loss -0.9465 (7.948 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.8640 loss -0.8640 (7.462 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.9242 loss -0.9242 (7.013 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.8525 loss -0.8525 (7.154 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.8367 loss -0.8367 (6.995 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 47600 lr 2.688e-04 [train_loss] tar_ll 0.8667 loss -0.8667 (6.869 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.9380 loss -0.9380 (7.297 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.8948 loss -0.8948 (6.969 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.9332 loss -0.9332 (7.276 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 48400 lr 2.626e-04 [train_loss] tar_ll 0.9177 loss -0.9177 (6.879 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.8572 loss -0.8572 (7.126 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 48800 lr 2.594e-04 [train_loss] tar_ll 0.9086 loss -0.9086 (6.981 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.8581 loss -0.8581 (6.850 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 49200 lr 2.563e-04 [train_loss] tar_ll 0.8222 loss -0.8222 (7.085 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.8826 loss -0.8826 (6.984 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 49600 lr 2.531e-04 [train_loss] tar_ll 0.9075 loss -0.9075 (7.060 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.8170 loss -0.8170 (7.041 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 50000 lr 2.500e-04 [train_loss] tar_ll 0.7467 loss -0.7467 (7.069 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.26it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 0.7016 loss -0.7016 (33.610 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 50200 lr 2.484e-04 [train_loss] tar_ll 0.7860 loss -0.7860 (7.047 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 50400 lr 2.469e-04 [train_loss] tar_ll 0.8658 loss -0.8658 (7.285 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 50600 lr 2.453e-04 [train_loss] tar_ll 0.8557 loss -0.8557 (6.916 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 50800 lr 2.437e-04 [train_loss] tar_ll 0.9738 loss -0.9738 (7.206 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 51000 lr 2.421e-04 [train_loss] tar_ll 0.8991 loss -0.8991 (7.106 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 51200 lr 2.406e-04 [train_loss] tar_ll 0.9476 loss -0.9476 (7.161 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 51400 lr 2.390e-04 [train_loss] tar_ll 0.9198 loss -0.9198 (7.015 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 51600 lr 2.374e-04 [train_loss] tar_ll 0.9357 loss -0.9357 (6.960 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 51800 lr 2.359e-04 [train_loss] tar_ll 0.5634 loss -0.5634 (7.237 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 52000 lr 2.343e-04 [train_loss] tar_ll 0.5700 loss -0.5700 (7.056 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 52200 lr 2.327e-04 [train_loss] tar_ll 0.5175 loss -0.5175 (7.303 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 52400 lr 2.312e-04 [train_loss] tar_ll 0.7229 loss -0.7229 (7.096 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 52600 lr 2.296e-04 [train_loss] tar_ll 0.6579 loss -0.6579 (7.444 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 52800 lr 2.280e-04 [train_loss] tar_ll 0.7265 loss -0.7265 (7.046 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 53000 lr 2.265e-04 [train_loss] tar_ll 0.9373 loss -0.9373 (7.049 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 53200 lr 2.249e-04 [train_loss] tar_ll 0.9145 loss -0.9145 (7.328 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 53400 lr 2.233e-04 [train_loss] tar_ll 0.9308 loss -0.9308 (7.014 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 53600 lr 2.218e-04 [train_loss] tar_ll 1.0102 loss -1.0102 (7.242 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 53800 lr 2.202e-04 [train_loss] tar_ll 0.9632 loss -0.9632 (7.104 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 54000 lr 2.187e-04 [train_loss] tar_ll 0.9595 loss -0.9595 (7.352 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 54200 lr 2.171e-04 [train_loss] tar_ll 0.8723 loss -0.8723 (7.189 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 54400 lr 2.156e-04 [train_loss] tar_ll 0.8867 loss -0.8867 (7.116 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 54600 lr 2.140e-04 [train_loss] tar_ll 1.0014 loss -1.0014 (7.192 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 54800 lr 2.124e-04 [train_loss] tar_ll 0.8787 loss -0.8787 (7.299 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 55000 lr 2.109e-04 [train_loss] tar_ll 0.8893 loss -0.8893 (7.076 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 88.06it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 0.9849 loss -0.9849 (34.068 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 55200 lr 2.093e-04 [train_loss] tar_ll 0.9168 loss -0.9168 (7.240 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 55400 lr 2.078e-04 [train_loss] tar_ll 0.9546 loss -0.9546 (7.020 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 55600 lr 2.062e-04 [train_loss] tar_ll 0.9695 loss -0.9695 (6.887 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 55800 lr 2.047e-04 [train_loss] tar_ll 0.9662 loss -0.9662 (7.292 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 56000 lr 2.032e-04 [train_loss] tar_ll 0.8843 loss -0.8843 (6.970 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 56200 lr 2.016e-04 [train_loss] tar_ll 0.9851 loss -0.9851 (7.196 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 56400 lr 2.001e-04 [train_loss] tar_ll 0.9376 loss -0.9376 (6.981 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 56600 lr 1.985e-04 [train_loss] tar_ll 0.9849 loss -0.9849 (7.081 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 56800 lr 1.970e-04 [train_loss] tar_ll 0.9837 loss -0.9837 (7.064 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 57000 lr 1.955e-04 [train_loss] tar_ll 0.9466 loss -0.9466 (7.071 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 57200 lr 1.939e-04 [train_loss] tar_ll 0.9938 loss -0.9938 (7.073 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 57400 lr 1.924e-04 [train_loss] tar_ll 0.9373 loss -0.9373 (7.018 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 57600 lr 1.909e-04 [train_loss] tar_ll 0.9380 loss -0.9380 (7.108 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 57800 lr 1.894e-04 [train_loss] tar_ll 1.0481 loss -1.0481 (6.999 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 58000 lr 1.878e-04 [train_loss] tar_ll 0.9748 loss -0.9748 (7.074 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 58200 lr 1.863e-04 [train_loss] tar_ll 0.9958 loss -0.9958 (7.144 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 58400 lr 1.848e-04 [train_loss] tar_ll 0.6662 loss -0.6662 (7.115 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 58600 lr 1.833e-04 [train_loss] tar_ll 0.7285 loss -0.7285 (6.920 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 58800 lr 1.818e-04 [train_loss] tar_ll 0.8541 loss -0.8541 (6.920 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 59000 lr 1.803e-04 [train_loss] tar_ll 0.9616 loss -0.9616 (7.045 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 59200 lr 1.787e-04 [train_loss] tar_ll 0.9689 loss -0.9689 (7.229 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 59400 lr 1.772e-04 [train_loss] tar_ll 0.9677 loss -0.9677 (7.086 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 59600 lr 1.757e-04 [train_loss] tar_ll 1.0757 loss -1.0757 (6.972 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 59800 lr 1.742e-04 [train_loss] tar_ll 0.9836 loss -0.9836 (7.435 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 60000 lr 1.727e-04 [train_loss] tar_ll 1.0000 loss -1.0000 (7.042 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.79it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 1.0016 loss -1.0016 (34.176 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 60200 lr 1.713e-04 [train_loss] tar_ll 1.0995 loss -1.0995 (7.289 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 60400 lr 1.698e-04 [train_loss] tar_ll 1.0215 loss -1.0215 (7.017 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.0110 loss -1.0110 (7.174 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.0135 loss -1.0135 (7.060 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 61000 lr 1.653e-04 [train_loss] tar_ll 1.0520 loss -1.0520 (7.254 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 61200 lr 1.638e-04 [train_loss] tar_ll 0.9639 loss -0.9639 (7.203 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 61400 lr 1.624e-04 [train_loss] tar_ll 1.0544 loss -1.0544 (7.219 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 61600 lr 1.609e-04 [train_loss] tar_ll 0.9931 loss -0.9931 (7.124 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.0216 loss -1.0216 (7.075 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 62000 lr 1.580e-04 [train_loss] tar_ll 1.0336 loss -1.0336 (7.218 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 62200 lr 1.565e-04 [train_loss] tar_ll 1.0325 loss -1.0325 (7.191 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 62400 lr 1.551e-04 [train_loss] tar_ll 0.9987 loss -0.9987 (7.103 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.0595 loss -1.0595 (7.023 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.0554 loss -1.0554 (7.212 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 63000 lr 1.507e-04 [train_loss] tar_ll 0.9659 loss -0.9659 (7.560 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.1211 loss -1.1211 (7.354 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.0788 loss -1.0788 (7.052 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 63600 lr 1.464e-04 [train_loss] tar_ll 0.9755 loss -0.9755 (7.160 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.0282 loss -1.0282 (7.057 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.0071 loss -1.0071 (7.102 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.0733 loss -1.0733 (7.292 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.0615 loss -1.0615 (6.932 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.0731 loss -1.0731 (7.218 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.0508 loss -1.0508 (7.099 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.0883 loss -1.0883 (7.283 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.59it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 1.0333 loss -1.0333 (33.866 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.0165 loss -1.0165 (7.137 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 65400 lr 1.337e-04 [train_loss] tar_ll 0.9789 loss -0.9789 (6.992 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.0151 loss -1.0151 (7.020 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.0288 loss -1.0288 (7.277 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.1134 loss -1.1134 (6.855 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.0997 loss -1.0997 (7.051 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.0847 loss -1.0847 (6.862 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.1102 loss -1.1102 (6.925 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.0877 loss -1.0877 (6.710 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.1395 loss -1.1395 (6.617 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.0459 loss -1.0459 (6.995 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.1076 loss -1.1076 (6.726 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.0421 loss -1.0421 (6.664 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.0905 loss -1.0905 (6.732 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.0186 loss -1.0186 (6.943 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.0710 loss -1.0710 (6.973 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 68400 lr 1.134e-04 [train_loss] tar_ll 0.8745 loss -0.8745 (6.698 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 68600 lr 1.121e-04 [train_loss] tar_ll 0.8515 loss -0.8515 (6.756 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 68800 lr 1.108e-04 [train_loss] tar_ll 0.9897 loss -0.9897 (6.973 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 69000 lr 1.095e-04 [train_loss] tar_ll 0.9865 loss -0.9865 (6.840 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.0508 loss -1.0508 (6.858 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.0686 loss -1.0686 (6.810 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.0853 loss -1.0853 (6.836 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.1219 loss -1.1219 (6.879 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.0770 loss -1.0770 (6.831 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.98it/s] \n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 1.0958 loss -1.0958 (32.619 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.0725 loss -1.0725 (6.951 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.0209 loss -1.0209 (7.095 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.0614 loss -1.0614 (6.793 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.0612 loss -1.0612 (7.057 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.0694 loss -1.0694 (6.827 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.1035 loss -1.1035 (6.928 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.0035 loss -1.0035 (6.939 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.1888 loss -1.1888 (6.576 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.0073 loss -1.0073 (6.878 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.1326 loss -1.1326 (7.015 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.1179 loss -1.1179 (6.809 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.1327 loss -1.1327 (6.813 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.1097 loss -1.1097 (6.709 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.1334 loss -1.1334 (6.904 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.0409 loss -1.0409 (6.668 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.0426 loss -1.0426 (6.888 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 73400 lr 8.233e-05 [train_loss] tar_ll 0.9452 loss -0.9452 (6.871 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.0737 loss -1.0737 (6.755 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 73800 lr 8.001e-05 [train_loss] tar_ll 0.9229 loss -0.9229 (6.888 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.0673 loss -1.0673 (6.903 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 74200 lr 7.772e-05 [train_loss] tar_ll 0.8481 loss -0.8481 (6.890 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 74400 lr 7.659e-05 [train_loss] tar_ll 0.9992 loss -0.9992 (6.697 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.1128 loss -1.1128 (6.494 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.1748 loss -1.1748 (6.720 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1426 loss -1.1426 (6.849 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.13it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 1.1373 loss -1.1373 (32.924 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1644 loss -1.1644 (7.107 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.1912 loss -1.1912 (7.051 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.1145 loss -1.1145 (7.233 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.1517 loss -1.1517 (7.102 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.0842 loss -1.0842 (7.452 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.1462 loss -1.1462 (7.277 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.1348 loss -1.1348 (7.475 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.2133 loss -1.2133 (7.433 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.1469 loss -1.1469 (7.403 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 77000 lr 6.247e-05 [train_loss] tar_ll 0.9623 loss -0.9623 (7.423 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.0153 loss -1.0153 (7.726 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.0534 loss -1.0534 (7.476 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.1318 loss -1.1318 (7.364 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.1232 loss -1.1232 (7.644 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.1540 loss -1.1540 (7.490 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.1154 loss -1.1154 (7.665 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.1691 loss -1.1691 (7.303 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.0702 loss -1.0702 (7.674 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.0923 loss -1.0923 (7.624 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.1763 loss -1.1763 (7.505 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.0983 loss -1.0983 (7.658 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.1411 loss -1.1411 (7.644 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.1054 loss -1.1054 (7.592 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.1827 loss -1.1827 (7.636 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.0380 loss -1.0380 (7.803 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 83.44it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 1.1520 loss -1.1520 (35.956 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.1782 loss -1.1782 (7.745 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.1804 loss -1.1804 (7.492 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1636 loss -1.1636 (7.723 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.0857 loss -1.0857 (7.471 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.0641 loss -1.0641 (8.301 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.0769 loss -1.0769 (7.789 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.1592 loss -1.1592 (7.737 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.0623 loss -1.0623 (7.410 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.1307 loss -1.1307 (7.775 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.1547 loss -1.1547 (7.703 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.0898 loss -1.0898 (7.633 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.1611 loss -1.1611 (7.884 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.0975 loss -1.0975 (7.963 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.1633 loss -1.1633 (7.865 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.1906 loss -1.1906 (7.899 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.0790 loss -1.0790 (7.760 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.1484 loss -1.1484 (7.500 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.1787 loss -1.1787 (7.708 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.1427 loss -1.1427 (7.663 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.1656 loss -1.1656 (7.648 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.0261 loss -1.0261 (7.640 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.1587 loss -1.1587 (7.583 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.1890 loss -1.1890 (7.632 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.1640 loss -1.1640 (7.704 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.0812 loss -1.0812 (7.738 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.76it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 1.1754 loss -1.1754 (36.250 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.2169 loss -1.2169 (7.830 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.1806 loss -1.1806 (7.649 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.1787 loss -1.1787 (7.740 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.1372 loss -1.1372 (7.773 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.1305 loss -1.1305 (7.678 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.2058 loss -1.2058 (7.765 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.1320 loss -1.1320 (7.769 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.1903 loss -1.1903 (7.806 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.1185 loss -1.1185 (7.702 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.1691 loss -1.1691 (7.907 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.1110 loss -1.1110 (7.914 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.1630 loss -1.1630 (7.788 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.1837 loss -1.1837 (7.874 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.1317 loss -1.1317 (7.859 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.1173 loss -1.1173 (7.800 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.1857 loss -1.1857 (7.778 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.1755 loss -1.1755 (7.630 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.1960 loss -1.1960 (7.983 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.1413 loss -1.1413 (7.904 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.1452 loss -1.1452 (7.822 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.1734 loss -1.1734 (7.769 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.1377 loss -1.1377 (7.885 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.2281 loss -1.2281 (7.811 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.1420 loss -1.1420 (7.609 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.1446 loss -1.1446 (7.776 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.64it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 1.1819 loss -1.1819 (36.306 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.1608 loss -1.1608 (8.110 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.1397 loss -1.1397 (7.828 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.2100 loss -1.2100 (7.808 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.1537 loss -1.1537 (7.676 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.2290 loss -1.2290 (7.636 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.1142 loss -1.1142 (7.891 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2448 loss -1.2448 (8.056 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.1918 loss -1.1918 (7.660 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.1313 loss -1.1313 (7.886 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2737 loss -1.2737 (7.719 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.2102 loss -1.2102 (7.866 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.1965 loss -1.1965 (7.743 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.1995 loss -1.1995 (7.899 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.0931 loss -1.0931 (7.865 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.2035 loss -1.2035 (7.830 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.1333 loss -1.1333 (7.759 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.1655 loss -1.1655 (7.903 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.1693 loss -1.1693 (7.603 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.0644 loss -1.0644 (7.654 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.1719 loss -1.1719 (7.954 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.1859 loss -1.1859 (8.071 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.1672 loss -1.1672 (7.916 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.2278 loss -1.2278 (7.810 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.2641 loss -1.2641 (8.182 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.1978 loss -1.1978 (7.950 secs)\n",
      "100%|##########| 3000/3000 [00:37<00:00, 79.81it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 1.1896 loss -1.1896 (37.593 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.2314 loss -1.2314 (8.185 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.2220 loss -1.2220 (8.179 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.2116 loss -1.2116 (7.931 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.1799 loss -1.1799 (7.958 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.1352 loss -1.1352 (7.986 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.1761 loss -1.1761 (8.016 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.1500 loss -1.1500 (7.773 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.2676 loss -1.2676 (8.013 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.2313 loss -1.2313 (7.616 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.2896 loss -1.2896 (7.931 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.1198 loss -1.1198 (7.903 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.2389 loss -1.2389 (7.827 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.2108 loss -1.2108 (7.739 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.2308 loss -1.2308 (7.880 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.1576 loss -1.1576 (8.950 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.1782 loss -1.1782 (9.355 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.1385 loss -1.1385 (9.316 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.2029 loss -1.2029 (8.277 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2258 loss -1.2258 (7.848 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.2040 loss -1.2040 (7.799 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.1726 loss -1.1726 (7.904 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.1092 loss -1.1092 (7.405 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.1776 loss -1.1776 (7.297 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.1079 loss -1.1079 (7.313 secs)\n",
      "isanp2:isanp2-num_latents-16_rbf_100 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.1318 loss -1.1318 (7.403 secs)\n",
      "100%|##########| 3000/3000 [00:37<00:00, 79.65it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 1.1921 loss -1.1921 (37.666 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.76it/s]\n",
      "isanp2:isanp2-num_latents-16_rbf_100 rbf tar_ll 1.1921 loss -1.1921 (36.693 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4390303.0 miliseconds\n",
      "Execution time: 4390.303 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 57.9521484375 MB\n",
      "Memory Usage Change: 41.7021484375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp2', name='isanp2-num_latents-16_rbf_100',val_seed=100, val_l=16,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93bd2e6-4acb-4186-bc41-5fe0b58d9543",
   "metadata": {},
   "source": [
    "## ISANP2 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9737a180-6848-4fdc-9079-0a5f25491dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: isanp2-isanp2-num_latents-32_rbf_100\n",
      "Total number of parameters: 787138\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 32\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp2:isanp2-num_latents-32_rbf_100 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6957 loss 0.6957 (7.358 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 400 lr 5.000e-04 [train_loss] tar_ll -0.5377 loss 0.5377 (7.384 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 600 lr 5.000e-04 [train_loss] tar_ll -0.2099 loss 0.2099 (7.376 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 800 lr 4.999e-04 [train_loss] tar_ll -0.0679 loss 0.0679 (7.633 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 1000 lr 4.999e-04 [train_loss] tar_ll 0.0223 loss -0.0223 (7.489 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.0376 loss 0.0376 (7.696 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 1400 lr 4.998e-04 [train_loss] tar_ll 0.0058 loss -0.0058 (7.501 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 1600 lr 4.997e-04 [train_loss] tar_ll 0.0159 loss -0.0159 (7.462 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 1800 lr 4.996e-04 [train_loss] tar_ll 0.1755 loss -0.1755 (7.540 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 2000 lr 4.995e-04 [train_loss] tar_ll 0.1540 loss -0.1540 (7.675 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.1470 loss -0.1470 (7.559 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.2362 loss -0.2362 (7.633 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.2960 loss -0.2960 (7.521 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.3072 loss -0.3072 (7.529 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.1669 loss -0.1669 (7.404 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 3200 lr 4.987e-04 [train_loss] tar_ll -0.0276 loss 0.0276 (7.498 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.2527 loss -0.2527 (7.356 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.1645 loss -0.1645 (7.389 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.1950 loss -0.1950 (7.074 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.2652 loss -0.2652 (7.232 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3229 loss -0.3229 (7.324 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.3154 loss -0.3154 (7.176 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4073 loss -0.4073 (7.495 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4052 loss -0.4052 (7.171 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.4145 loss -0.4145 (7.333 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.28it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 0.4290 loss -0.4290 (34.774 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.3639 loss -0.3639 (7.302 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.4652 loss -0.4652 (7.366 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4735 loss -0.4735 (7.327 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.3501 loss -0.3501 (7.433 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4932 loss -0.4932 (7.197 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.4778 loss -0.4778 (7.541 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.5516 loss -0.5516 (7.220 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.4686 loss -0.4686 (7.499 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.2552 loss -0.2552 (7.392 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.4641 loss -0.4641 (7.466 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.4920 loss -0.4920 (7.369 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.3770 loss -0.3770 (7.712 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.3711 loss -0.3711 (7.447 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.4503 loss -0.4503 (7.281 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.4375 loss -0.4375 (7.465 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.3101 loss -0.3101 (7.409 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.5217 loss -0.5217 (7.450 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.4289 loss -0.4289 (7.351 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.5614 loss -0.5614 (7.501 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.2381 loss -0.2381 (7.181 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.1057 loss -0.1057 (7.707 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.1716 loss -0.1716 (7.455 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.3964 loss -0.3964 (7.359 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.3845 loss -0.3845 (7.281 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.1844 loss -0.1844 (7.355 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 84.75it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll -0.0498 loss 0.0498 (35.399 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.0834 loss -0.0834 (7.404 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.1230 loss -0.1230 (7.453 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.2816 loss -0.2816 (7.470 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.1877 loss -0.1877 (7.570 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.2495 loss -0.2495 (7.390 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.3918 loss -0.3918 (7.593 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.2367 loss -0.2367 (7.196 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.4767 loss -0.4767 (7.320 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.3519 loss -0.3519 (7.137 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.4846 loss -0.4846 (7.338 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.3854 loss -0.3854 (7.496 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.4883 loss -0.4883 (7.263 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.4145 loss -0.4145 (7.520 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.3356 loss -0.3356 (7.383 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.5218 loss -0.5218 (7.632 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.4948 loss -0.4948 (7.345 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.4482 loss -0.4482 (7.464 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.3829 loss -0.3829 (7.217 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.4334 loss -0.4334 (7.293 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.3413 loss -0.3413 (7.313 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.3503 loss -0.3503 (7.431 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.4776 loss -0.4776 (7.328 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.4630 loss -0.4630 (7.302 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.2719 loss -0.2719 (7.370 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.2905 loss -0.2905 (7.162 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 84.63it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 0.3374 loss -0.3374 (35.453 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.3411 loss -0.3411 (7.443 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.4544 loss -0.4544 (7.360 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.4736 loss -0.4736 (7.506 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.4123 loss -0.4123 (7.437 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.2052 loss -0.2052 (7.492 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.4590 loss -0.4590 (7.320 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.4677 loss -0.4677 (7.608 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.4355 loss -0.4355 (7.409 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.4560 loss -0.4560 (7.787 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.4337 loss -0.4337 (7.504 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.2491 loss -0.2491 (7.559 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.1489 loss -0.1489 (7.389 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.1456 loss -0.1456 (7.501 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.0878 loss -0.0878 (7.349 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.0852 loss -0.0852 (7.480 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.3351 loss -0.3351 (7.394 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.4086 loss -0.4086 (7.634 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.3754 loss -0.3754 (7.512 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.4683 loss -0.4683 (7.384 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.4489 loss -0.4489 (7.406 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.4505 loss -0.4505 (7.540 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.2786 loss -0.2786 (7.418 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.3347 loss -0.3347 (7.277 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.2534 loss -0.2534 (7.667 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.2508 loss -0.2508 (7.298 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 84.53it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 0.2580 loss -0.2580 (35.493 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.2336 loss -0.2336 (7.612 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.4125 loss -0.4125 (7.403 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.3965 loss -0.3965 (7.736 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.6256 loss -0.6256 (7.374 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.4470 loss -0.4470 (7.602 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.5193 loss -0.5193 (7.381 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.4711 loss -0.4711 (7.503 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.4503 loss -0.4503 (7.357 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.5215 loss -0.5215 (7.518 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.5025 loss -0.5025 (7.570 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.2086 loss -0.2086 (7.640 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.5012 loss -0.5012 (7.247 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.5087 loss -0.5087 (7.456 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.4874 loss -0.4874 (7.211 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.5463 loss -0.5463 (7.346 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.5856 loss -0.5856 (7.362 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.6214 loss -0.6214 (8.501 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.3868 loss -0.3868 (8.099 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.5843 loss -0.5843 (7.658 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.5533 loss -0.5533 (7.490 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.4137 loss -0.4137 (7.560 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.5177 loss -0.5177 (7.418 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.1820 loss -0.1820 (7.383 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.4155 loss -0.4155 (7.452 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.2710 loss -0.2710 (7.288 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.77it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 0.4431 loss -0.4431 (36.689 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.5527 loss -0.5527 (7.418 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.5679 loss -0.5679 (7.360 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.5627 loss -0.5627 (7.444 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.6753 loss -0.6753 (7.409 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.6556 loss -0.6556 (7.591 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.6420 loss -0.6420 (7.348 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.5435 loss -0.5435 (7.544 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.5998 loss -0.5998 (7.473 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.7250 loss -0.7250 (7.396 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.6369 loss -0.6369 (7.470 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.6320 loss -0.6320 (7.606 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.5731 loss -0.5731 (7.242 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.6093 loss -0.6093 (7.550 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.4059 loss -0.4059 (7.289 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.5309 loss -0.5309 (7.302 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.6254 loss -0.6254 (7.033 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.4381 loss -0.4381 (7.190 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.4681 loss -0.4681 (7.244 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.5912 loss -0.5912 (7.278 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.7865 loss -0.7865 (7.354 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.6739 loss -0.6739 (7.414 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.6245 loss -0.6245 (7.655 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.5763 loss -0.5763 (7.214 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.3600 loss -0.3600 (7.482 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.1944 loss -0.1944 (7.199 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 84.83it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 0.3981 loss -0.3981 (35.367 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.4910 loss -0.4910 (7.548 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.4542 loss -0.4542 (7.227 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.5227 loss -0.5227 (7.600 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.6423 loss -0.6423 (7.196 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.5378 loss -0.5378 (7.492 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.6693 loss -0.6693 (7.239 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.7000 loss -0.7000 (7.402 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.7152 loss -0.7152 (7.491 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.7329 loss -0.7329 (7.632 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.7647 loss -0.7647 (7.419 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.6644 loss -0.6644 (7.476 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.5192 loss -0.5192 (7.499 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.5059 loss -0.5059 (7.507 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.5504 loss -0.5504 (7.468 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.2502 loss -0.2502 (7.408 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.1994 loss -0.1994 (7.663 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.2319 loss -0.2319 (7.425 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.4089 loss -0.4089 (7.641 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.6750 loss -0.6750 (7.139 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.6242 loss -0.6242 (7.399 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.6945 loss -0.6945 (7.415 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.5773 loss -0.5773 (7.523 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.5361 loss -0.5361 (7.461 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.6514 loss -0.6514 (7.726 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.7409 loss -0.7409 (7.582 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.62it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 0.6880 loss -0.6880 (36.314 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.7009 loss -0.7009 (7.814 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.7600 loss -0.7600 (7.299 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.6320 loss -0.6320 (7.309 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.7088 loss -0.7088 (7.192 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.7051 loss -0.7051 (7.267 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.5559 loss -0.5559 (7.267 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.5121 loss -0.5121 (7.540 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.7427 loss -0.7427 (7.133 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.5243 loss -0.5243 (7.283 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.6664 loss -0.6664 (7.425 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.5460 loss -0.5460 (7.353 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.3719 loss -0.3719 (7.488 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.4793 loss -0.4793 (7.182 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.6430 loss -0.6430 (7.470 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.7255 loss -0.7255 (7.360 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.6720 loss -0.6720 (7.616 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.7262 loss -0.7262 (7.227 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.6919 loss -0.6919 (7.486 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.7964 loss -0.7964 (7.332 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.7501 loss -0.7501 (7.355 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.7663 loss -0.7663 (7.338 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.7206 loss -0.7206 (7.212 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.7161 loss -0.7161 (7.358 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.8517 loss -0.8517 (7.291 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.7785 loss -0.7785 (7.432 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 83.98it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 0.7972 loss -0.7972 (35.726 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.7766 loss -0.7766 (7.551 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.3298 loss -0.3298 (7.395 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.3412 loss -0.3412 (7.377 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.6696 loss -0.6696 (7.338 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.7310 loss -0.7310 (7.457 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.7626 loss -0.7626 (7.571 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.7350 loss -0.7350 (7.490 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.8048 loss -0.8048 (7.483 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.7566 loss -0.7566 (7.326 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.5113 loss -0.5113 (7.505 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.5747 loss -0.5747 (7.414 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.6889 loss -0.6889 (7.484 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 42600 lr 3.076e-04 [train_loss] tar_ll 0.8407 loss -0.8407 (7.383 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.7659 loss -0.7659 (7.449 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.8159 loss -0.8159 (7.360 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.7231 loss -0.7231 (7.672 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.8056 loss -0.8056 (7.318 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.7991 loss -0.7991 (7.516 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.8276 loss -0.8276 (7.501 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.8783 loss -0.8783 (7.344 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.6390 loss -0.6390 (7.633 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.7795 loss -0.7795 (7.184 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.7745 loss -0.7745 (7.515 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.8482 loss -0.8482 (7.381 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.8898 loss -0.8898 (8.250 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.49it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 0.8183 loss -0.8183 (36.371 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.7471 loss -0.7471 (7.359 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.8293 loss -0.8293 (7.333 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.8858 loss -0.8858 (7.436 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.7725 loss -0.7725 (7.311 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.6941 loss -0.6941 (7.330 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.7691 loss -0.7691 (7.441 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.8008 loss -0.8008 (7.199 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.8389 loss -0.8389 (7.365 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.8140 loss -0.8140 (7.467 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.9266 loss -0.9266 (7.528 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.8640 loss -0.8640 (7.511 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.9064 loss -0.9064 (7.615 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 47600 lr 2.688e-04 [train_loss] tar_ll 0.8152 loss -0.8152 (7.310 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.8494 loss -0.8494 (7.569 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.8744 loss -0.8744 (7.574 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.8471 loss -0.8471 (7.676 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 48400 lr 2.626e-04 [train_loss] tar_ll 0.8565 loss -0.8565 (7.412 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.9453 loss -0.9453 (7.469 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 48800 lr 2.594e-04 [train_loss] tar_ll 0.8854 loss -0.8854 (7.636 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.7602 loss -0.7602 (7.353 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 49200 lr 2.563e-04 [train_loss] tar_ll 0.8015 loss -0.8015 (7.710 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.8918 loss -0.8918 (7.748 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 49600 lr 2.531e-04 [train_loss] tar_ll 0.8878 loss -0.8878 (7.560 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.8396 loss -0.8396 (7.454 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 50000 lr 2.500e-04 [train_loss] tar_ll 0.8312 loss -0.8312 (7.653 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 83.78it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 0.9489 loss -0.9489 (35.811 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 50200 lr 2.484e-04 [train_loss] tar_ll 0.9015 loss -0.9015 (7.576 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 50400 lr 2.469e-04 [train_loss] tar_ll 0.8292 loss -0.8292 (7.533 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 50600 lr 2.453e-04 [train_loss] tar_ll 0.9093 loss -0.9093 (7.698 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 50800 lr 2.437e-04 [train_loss] tar_ll 0.8573 loss -0.8573 (7.369 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 51000 lr 2.421e-04 [train_loss] tar_ll 0.9043 loss -0.9043 (7.274 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 51200 lr 2.406e-04 [train_loss] tar_ll 0.8718 loss -0.8718 (7.387 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 51400 lr 2.390e-04 [train_loss] tar_ll 0.9568 loss -0.9568 (7.412 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 51600 lr 2.374e-04 [train_loss] tar_ll 0.9401 loss -0.9401 (7.579 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 51800 lr 2.359e-04 [train_loss] tar_ll 0.7735 loss -0.7735 (7.439 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 52000 lr 2.343e-04 [train_loss] tar_ll 0.8882 loss -0.8882 (7.572 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 52200 lr 2.327e-04 [train_loss] tar_ll 0.8673 loss -0.8673 (7.293 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 52400 lr 2.312e-04 [train_loss] tar_ll 0.9437 loss -0.9437 (7.536 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 52600 lr 2.296e-04 [train_loss] tar_ll 0.8808 loss -0.8808 (7.227 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 52800 lr 2.280e-04 [train_loss] tar_ll 0.8944 loss -0.8944 (7.460 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 53000 lr 2.265e-04 [train_loss] tar_ll 0.8718 loss -0.8718 (7.385 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 53200 lr 2.249e-04 [train_loss] tar_ll 0.9459 loss -0.9459 (7.445 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 53400 lr 2.233e-04 [train_loss] tar_ll 0.9229 loss -0.9229 (7.391 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 53600 lr 2.218e-04 [train_loss] tar_ll 0.9059 loss -0.9059 (7.505 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 53800 lr 2.202e-04 [train_loss] tar_ll 0.9492 loss -0.9492 (7.439 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 54000 lr 2.187e-04 [train_loss] tar_ll 0.9875 loss -0.9875 (7.254 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 54200 lr 2.171e-04 [train_loss] tar_ll 0.9398 loss -0.9398 (7.488 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 54400 lr 2.156e-04 [train_loss] tar_ll 0.8925 loss -0.8925 (7.400 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 54600 lr 2.140e-04 [train_loss] tar_ll 0.9728 loss -0.9728 (7.497 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 54800 lr 2.124e-04 [train_loss] tar_ll 0.8206 loss -0.8206 (7.333 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 55000 lr 2.109e-04 [train_loss] tar_ll 0.9183 loss -0.9183 (7.461 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 83.97it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 0.9159 loss -0.9159 (35.728 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 55200 lr 2.093e-04 [train_loss] tar_ll 0.9189 loss -0.9189 (7.557 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 55400 lr 2.078e-04 [train_loss] tar_ll 0.9156 loss -0.9156 (7.558 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 55600 lr 2.062e-04 [train_loss] tar_ll 0.9360 loss -0.9360 (7.326 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 55800 lr 2.047e-04 [train_loss] tar_ll 0.9345 loss -0.9345 (7.729 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 56000 lr 2.032e-04 [train_loss] tar_ll 0.9184 loss -0.9184 (7.263 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 56200 lr 2.016e-04 [train_loss] tar_ll 0.8277 loss -0.8277 (7.692 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 56400 lr 2.001e-04 [train_loss] tar_ll 0.9886 loss -0.9886 (7.222 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 56600 lr 1.985e-04 [train_loss] tar_ll 0.9586 loss -0.9586 (7.551 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 56800 lr 1.970e-04 [train_loss] tar_ll 0.8906 loss -0.8906 (7.335 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 57000 lr 1.955e-04 [train_loss] tar_ll 0.9748 loss -0.9748 (7.605 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 57200 lr 1.939e-04 [train_loss] tar_ll 0.9896 loss -0.9896 (7.553 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 57400 lr 1.924e-04 [train_loss] tar_ll 0.9721 loss -0.9721 (8.056 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 57600 lr 1.909e-04 [train_loss] tar_ll 0.9869 loss -0.9869 (7.614 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 57800 lr 1.894e-04 [train_loss] tar_ll 1.0393 loss -1.0393 (7.476 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 58000 lr 1.878e-04 [train_loss] tar_ll 1.0586 loss -1.0586 (7.333 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 58200 lr 1.863e-04 [train_loss] tar_ll 1.0622 loss -1.0622 (7.358 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 58400 lr 1.848e-04 [train_loss] tar_ll 0.9807 loss -0.9807 (7.385 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 58600 lr 1.833e-04 [train_loss] tar_ll 1.0047 loss -1.0047 (7.269 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 58800 lr 1.818e-04 [train_loss] tar_ll 0.9403 loss -0.9403 (7.285 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.0241 loss -1.0241 (7.216 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 59200 lr 1.787e-04 [train_loss] tar_ll 0.9381 loss -0.9381 (7.328 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 59400 lr 1.772e-04 [train_loss] tar_ll 0.9668 loss -0.9668 (7.212 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 59600 lr 1.757e-04 [train_loss] tar_ll 0.9671 loss -0.9671 (7.318 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 59800 lr 1.742e-04 [train_loss] tar_ll 0.9453 loss -0.9453 (7.048 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 60000 lr 1.727e-04 [train_loss] tar_ll 0.9791 loss -0.9791 (7.192 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.83it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 1.0203 loss -1.0203 (34.553 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 60200 lr 1.713e-04 [train_loss] tar_ll 0.9728 loss -0.9728 (7.039 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 60400 lr 1.698e-04 [train_loss] tar_ll 0.9449 loss -0.9449 (7.441 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.0086 loss -1.0086 (7.420 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.0791 loss -1.0791 (7.628 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 61000 lr 1.653e-04 [train_loss] tar_ll 0.9617 loss -0.9617 (7.406 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 61200 lr 1.638e-04 [train_loss] tar_ll 1.0231 loss -1.0231 (7.589 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 61400 lr 1.624e-04 [train_loss] tar_ll 0.9573 loss -0.9573 (7.304 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 61600 lr 1.609e-04 [train_loss] tar_ll 1.0175 loss -1.0175 (7.380 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.0045 loss -1.0045 (7.372 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 62000 lr 1.580e-04 [train_loss] tar_ll 0.9895 loss -0.9895 (7.514 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 62200 lr 1.565e-04 [train_loss] tar_ll 0.9053 loss -0.9053 (7.451 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.0243 loss -1.0243 (7.404 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.0360 loss -1.0360 (7.424 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 62800 lr 1.522e-04 [train_loss] tar_ll 0.9812 loss -0.9812 (7.425 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.0262 loss -1.0262 (7.475 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.0414 loss -1.0414 (7.490 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 63400 lr 1.478e-04 [train_loss] tar_ll 0.9220 loss -0.9220 (7.628 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 63600 lr 1.464e-04 [train_loss] tar_ll 0.9953 loss -0.9953 (7.234 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.0584 loss -1.0584 (7.528 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 64000 lr 1.436e-04 [train_loss] tar_ll 0.9446 loss -0.9446 (7.327 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.0243 loss -1.0243 (7.668 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 64400 lr 1.407e-04 [train_loss] tar_ll 0.9563 loss -0.9563 (7.320 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.0181 loss -1.0181 (7.388 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.0549 loss -1.0549 (7.475 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.0296 loss -1.0296 (7.398 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.31it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 1.0831 loss -1.0831 (36.452 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.0427 loss -1.0427 (7.432 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 65400 lr 1.337e-04 [train_loss] tar_ll 0.9472 loss -0.9472 (7.387 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.0480 loss -1.0480 (7.352 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.0485 loss -1.0485 (7.568 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.0543 loss -1.0543 (7.256 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.0383 loss -1.0383 (7.503 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.0318 loss -1.0318 (7.426 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.0859 loss -1.0859 (7.407 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.0854 loss -1.0854 (7.650 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 67000 lr 1.227e-04 [train_loss] tar_ll 0.9576 loss -0.9576 (7.436 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.1248 loss -1.1248 (7.498 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 67400 lr 1.200e-04 [train_loss] tar_ll 0.9925 loss -0.9925 (7.409 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.1032 loss -1.1032 (7.712 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.0402 loss -1.0402 (7.427 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.0707 loss -1.0707 (7.618 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 68200 lr 1.147e-04 [train_loss] tar_ll 0.9575 loss -0.9575 (7.317 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.1089 loss -1.1089 (7.926 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.1693 loss -1.1693 (7.565 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.0490 loss -1.0490 (7.771 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.0536 loss -1.0536 (7.437 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.0103 loss -1.0103 (7.703 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.1767 loss -1.1767 (7.595 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.0556 loss -1.0556 (7.883 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.0969 loss -1.0969 (7.525 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 70000 lr 1.031e-04 [train_loss] tar_ll 0.9958 loss -0.9958 (7.782 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 84.09it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 1.0804 loss -1.0804 (35.678 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.1027 loss -1.1027 (7.215 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.1291 loss -1.1291 (7.464 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.0453 loss -1.0453 (7.186 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.0857 loss -1.0857 (7.451 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.0770 loss -1.0770 (7.406 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.0249 loss -1.0249 (7.293 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.0781 loss -1.0781 (7.471 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.0827 loss -1.0827 (7.457 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 71800 lr 9.186e-05 [train_loss] tar_ll 0.9760 loss -0.9760 (7.637 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.1116 loss -1.1116 (7.428 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.0658 loss -1.0658 (7.726 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.1176 loss -1.1176 (7.420 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.0198 loss -1.0198 (7.472 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.0165 loss -1.0165 (7.293 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.0616 loss -1.0616 (7.489 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.0942 loss -1.0942 (7.401 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.1378 loss -1.1378 (7.413 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.1073 loss -1.1073 (7.413 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.0600 loss -1.0600 (7.704 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.1874 loss -1.1874 (7.554 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.1823 loss -1.1823 (7.609 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.0703 loss -1.0703 (7.403 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.1479 loss -1.1479 (7.590 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.0348 loss -1.0348 (7.525 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.0602 loss -1.0602 (7.320 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.53it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 1.1339 loss -1.1339 (36.800 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1676 loss -1.1676 (7.572 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.1008 loss -1.1008 (7.472 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.0961 loss -1.0961 (7.490 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.0895 loss -1.0895 (7.573 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.1541 loss -1.1541 (7.388 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.0899 loss -1.0899 (7.652 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.1567 loss -1.1567 (7.258 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.1151 loss -1.1151 (7.584 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.1336 loss -1.1336 (7.524 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.1421 loss -1.1421 (7.550 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.1242 loss -1.1242 (7.468 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.1224 loss -1.1224 (7.285 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.0286 loss -1.0286 (7.843 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.0590 loss -1.0590 (7.412 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.1087 loss -1.1087 (7.563 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.1557 loss -1.1557 (7.294 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.1412 loss -1.1412 (7.463 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.1052 loss -1.1052 (7.294 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.0048 loss -1.0048 (7.483 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.0811 loss -1.0811 (7.409 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.1979 loss -1.1979 (7.483 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.1672 loss -1.1672 (7.325 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.1322 loss -1.1322 (7.453 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.0391 loss -1.0391 (7.365 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.1193 loss -1.1193 (7.651 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.05it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 1.1530 loss -1.1530 (36.567 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.1308 loss -1.1308 (7.463 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.1118 loss -1.1118 (7.481 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.0681 loss -1.0681 (7.303 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.0478 loss -1.0478 (7.533 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.1677 loss -1.1677 (7.413 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.1074 loss -1.1074 (7.566 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.1708 loss -1.1708 (7.165 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.0025 loss -1.0025 (7.450 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.1201 loss -1.1201 (7.432 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.0885 loss -1.0885 (7.384 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.1228 loss -1.1228 (7.467 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.0532 loss -1.0532 (7.391 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.1167 loss -1.1167 (7.510 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.1238 loss -1.1238 (7.363 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.1218 loss -1.1218 (7.741 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.1048 loss -1.1048 (7.452 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.1514 loss -1.1514 (7.565 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.1847 loss -1.1847 (7.566 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.1316 loss -1.1316 (7.662 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.1315 loss -1.1315 (7.583 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2019 loss -1.2019 (7.560 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.1351 loss -1.1351 (7.315 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.1220 loss -1.1220 (7.432 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.1492 loss -1.1492 (7.310 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.0285 loss -1.0285 (7.261 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.76it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 1.1640 loss -1.1640 (36.251 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.2077 loss -1.2077 (7.535 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.1893 loss -1.1893 (7.491 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.1293 loss -1.1293 (7.393 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.1165 loss -1.1165 (7.416 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.1627 loss -1.1627 (7.236 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.1599 loss -1.1599 (8.144 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.1433 loss -1.1433 (7.517 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.1388 loss -1.1388 (7.778 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.2225 loss -1.2225 (7.742 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.1980 loss -1.1980 (7.815 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.1612 loss -1.1612 (7.459 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.1150 loss -1.1150 (7.241 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.2104 loss -1.2104 (7.555 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.1486 loss -1.1486 (7.389 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.1637 loss -1.1637 (7.465 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.1364 loss -1.1364 (7.400 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.1818 loss -1.1818 (7.650 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.2203 loss -1.2203 (7.350 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.1820 loss -1.1820 (7.510 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.1452 loss -1.1452 (7.445 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.2404 loss -1.2404 (7.589 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.1379 loss -1.1379 (7.362 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.1467 loss -1.1467 (7.480 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.1618 loss -1.1618 (7.307 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.2706 loss -1.2706 (7.428 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.37it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 1.1822 loss -1.1822 (36.421 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.1380 loss -1.1380 (7.501 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.1179 loss -1.1179 (7.651 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.1414 loss -1.1414 (7.385 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.2351 loss -1.2351 (7.560 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.2826 loss -1.2826 (7.300 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2305 loss -1.2305 (7.575 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2017 loss -1.2017 (7.449 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.1635 loss -1.1635 (7.487 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2271 loss -1.2271 (7.314 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.1764 loss -1.1764 (7.651 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.2152 loss -1.2152 (7.481 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.1876 loss -1.1876 (7.383 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.0862 loss -1.0862 (7.314 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.2035 loss -1.2035 (7.312 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.0934 loss -1.0934 (7.462 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.1578 loss -1.1578 (7.436 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.2660 loss -1.2660 (7.617 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.0493 loss -1.0493 (7.547 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.2369 loss -1.2369 (7.653 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.1684 loss -1.1684 (7.418 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.1380 loss -1.1380 (7.497 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.2199 loss -1.2199 (7.220 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.2506 loss -1.2506 (7.511 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.1449 loss -1.1449 (7.247 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.2514 loss -1.2514 (7.317 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.43it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 1.1878 loss -1.1878 (36.397 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.1327 loss -1.1327 (7.415 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.1926 loss -1.1926 (7.479 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.1307 loss -1.1307 (7.686 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.2067 loss -1.2067 (7.889 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.0704 loss -1.0704 (7.816 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2208 loss -1.2208 (7.771 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.1903 loss -1.1903 (7.427 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.1622 loss -1.1622 (7.450 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.1464 loss -1.1464 (7.335 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.1430 loss -1.1430 (7.494 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.1602 loss -1.1602 (7.432 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.1344 loss -1.1344 (7.543 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.1680 loss -1.1680 (7.740 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.1742 loss -1.1742 (7.485 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.1848 loss -1.1848 (7.663 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.1710 loss -1.1710 (7.477 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.1307 loss -1.1307 (7.561 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.0477 loss -1.0477 (7.403 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2419 loss -1.2419 (7.488 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.1920 loss -1.1920 (7.486 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.1772 loss -1.1772 (8.040 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2060 loss -1.2060 (7.517 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.2190 loss -1.2190 (7.854 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.2365 loss -1.2365 (7.404 secs)\n",
      "isanp2:isanp2-num_latents-32_rbf_100 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.1479 loss -1.1479 (7.668 secs)\n",
      "100%|##########| 3000/3000 [00:37<00:00, 79.36it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 1.1897 loss -1.1897 (37.807 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:37<00:00, 78.97it/s]\n",
      "isanp2:isanp2-num_latents-32_rbf_100 rbf tar_ll 1.1897 loss -1.1897 (37.992 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4501596.5 miliseconds\n",
      "Execution time: 4501.5965 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 66.5302734375 MB\n",
      "Memory Usage Change: 50.2802734375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp2', name='isanp2-num_latents-32_rbf_100',val_seed=100, val_l=32,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e383217e-37e8-4028-b8ef-9f5fd23eb11b",
   "metadata": {},
   "source": [
    "## ISANP2 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6117fb1b-df61-4535-896d-8e08119c2307",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: isanp2-isanp2-num_latents-64_rbf_100\n",
      "Total number of parameters: 789186\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 64\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp2:isanp2-num_latents-64_rbf_100 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6548 loss 0.6548 (7.595 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 400 lr 5.000e-04 [train_loss] tar_ll -0.4752 loss 0.4752 (7.526 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 600 lr 5.000e-04 [train_loss] tar_ll -0.1644 loss 0.1644 (7.470 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 800 lr 4.999e-04 [train_loss] tar_ll -0.0460 loss 0.0460 (7.246 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 1000 lr 4.999e-04 [train_loss] tar_ll 0.0283 loss -0.0283 (7.640 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.0016 loss 0.0016 (7.370 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 1400 lr 4.998e-04 [train_loss] tar_ll 0.0917 loss -0.0917 (7.475 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 1600 lr 4.997e-04 [train_loss] tar_ll 0.1556 loss -0.1556 (7.431 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 1800 lr 4.996e-04 [train_loss] tar_ll 0.2340 loss -0.2340 (7.552 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 2000 lr 4.995e-04 [train_loss] tar_ll 0.1117 loss -0.1117 (7.360 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.2260 loss -0.2260 (7.599 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.3044 loss -0.3044 (7.206 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.3718 loss -0.3718 (7.496 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.3814 loss -0.3814 (7.310 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.3896 loss -0.3896 (7.433 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.3608 loss -0.3608 (7.723 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.3027 loss -0.3027 (7.297 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2642 loss -0.2642 (7.679 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.4148 loss -0.4148 (7.527 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.4187 loss -0.4187 (7.621 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3786 loss -0.3786 (7.409 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.2788 loss -0.2788 (7.624 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4481 loss -0.4481 (7.411 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.5056 loss -0.5056 (7.613 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.4055 loss -0.4055 (7.250 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.45it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 0.2497 loss -0.2497 (36.389 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.4195 loss -0.4195 (7.785 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.5056 loss -0.5056 (7.543 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.5105 loss -0.5105 (7.801 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.5447 loss -0.5447 (7.693 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4672 loss -0.4672 (8.040 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.5061 loss -0.5061 (7.462 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.5944 loss -0.5944 (7.481 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5036 loss -0.5036 (7.520 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.4621 loss -0.4621 (7.360 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.5358 loss -0.5358 (7.433 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.5317 loss -0.5317 (7.619 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.6369 loss -0.6369 (7.276 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.4877 loss -0.4877 (7.456 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.5840 loss -0.5840 (7.194 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.6059 loss -0.6059 (7.446 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.5319 loss -0.5319 (7.427 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.5100 loss -0.5100 (7.423 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.4078 loss -0.4078 (7.300 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.5068 loss -0.5068 (7.500 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.4065 loss -0.4065 (7.584 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.5765 loss -0.5765 (7.360 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.6766 loss -0.6766 (7.392 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.5874 loss -0.5874 (7.263 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.4742 loss -0.4742 (7.453 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.4936 loss -0.4936 (7.122 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 84.27it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 0.5770 loss -0.5770 (35.604 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.2219 loss -0.2219 (7.726 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.4426 loss -0.4426 (7.416 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.4835 loss -0.4835 (7.925 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 10800 lr 4.857e-04 [train_loss] tar_ll -0.3359 loss 0.3359 (7.454 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 11000 lr 4.852e-04 [train_loss] tar_ll -0.1435 loss 0.1435 (7.786 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.0068 loss -0.0068 (7.779 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.1851 loss -0.1851 (7.899 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.2172 loss -0.2172 (7.597 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.2837 loss -0.2837 (7.886 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.1655 loss -0.1655 (7.792 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.2765 loss -0.2765 (7.762 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.1417 loss -0.1417 (7.786 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.0045 loss -0.0045 (7.793 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 12800 lr 4.801e-04 [train_loss] tar_ll -0.0810 loss 0.0810 (7.716 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.1581 loss -0.1581 (8.070 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.1943 loss -0.1943 (7.743 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.3436 loss -0.3436 (7.921 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.1892 loss -0.1892 (7.565 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.3047 loss -0.3047 (7.590 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.4313 loss -0.4313 (7.410 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.3429 loss -0.3429 (7.464 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.3334 loss -0.3334 (7.649 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.3876 loss -0.3876 (7.353 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.2893 loss -0.2893 (7.603 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.4084 loss -0.4084 (7.299 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.73it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 0.5049 loss -0.5049 (36.267 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.4896 loss -0.4896 (7.361 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.3715 loss -0.3715 (7.333 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.3269 loss -0.3269 (7.215 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.2809 loss -0.2809 (7.398 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.4753 loss -0.4753 (7.261 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.4637 loss -0.4637 (7.493 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.4043 loss -0.4043 (7.446 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.1303 loss -0.1303 (7.536 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.0793 loss -0.0793 (7.397 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.3293 loss -0.3293 (7.290 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.3544 loss -0.3544 (7.502 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.1291 loss -0.1291 (7.494 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.2685 loss -0.2685 (7.514 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.4295 loss -0.4295 (7.334 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.4669 loss -0.4669 (7.528 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.5064 loss -0.5064 (7.366 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.0391 loss -0.0391 (7.504 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.1304 loss -0.1304 (7.355 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.3133 loss -0.3133 (7.420 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.4217 loss -0.4217 (7.436 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.4231 loss -0.4231 (7.561 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.4473 loss -0.4473 (7.492 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 19600 lr 4.541e-04 [train_loss] tar_ll -0.0502 loss 0.0502 (7.326 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.2624 loss -0.2624 (7.632 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.3757 loss -0.3757 (7.561 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 83.69it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 0.3866 loss -0.3866 (35.851 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.4518 loss -0.4518 (7.528 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.4452 loss -0.4452 (7.488 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.3505 loss -0.3505 (7.449 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.4703 loss -0.4703 (7.509 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.4904 loss -0.4904 (7.554 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.2814 loss -0.2814 (7.384 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.0453 loss -0.0453 (7.603 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.2923 loss -0.2923 (7.366 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.3578 loss -0.3578 (7.587 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.3601 loss -0.3601 (7.554 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.4145 loss -0.4145 (7.998 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.4854 loss -0.4854 (7.410 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.4618 loss -0.4618 (7.465 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.3259 loss -0.3259 (7.463 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.3570 loss -0.3570 (7.452 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.3824 loss -0.3824 (7.392 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.1605 loss -0.1605 (7.579 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.3280 loss -0.3280 (7.259 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.2253 loss -0.2253 (7.281 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.4334 loss -0.4334 (7.143 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.4951 loss -0.4951 (7.209 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.5514 loss -0.5514 (7.570 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.5467 loss -0.5467 (7.327 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.5558 loss -0.5558 (7.540 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.3552 loss -0.3552 (7.308 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 83.63it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 0.3674 loss -0.3674 (35.876 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.3404 loss -0.3404 (7.439 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.3880 loss -0.3880 (7.488 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.4133 loss -0.4133 (7.277 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.5419 loss -0.5419 (7.471 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.2081 loss -0.2081 (7.451 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.3927 loss -0.3927 (7.501 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.4916 loss -0.4916 (7.686 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.4752 loss -0.4752 (7.407 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.4870 loss -0.4870 (7.918 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.3902 loss -0.3902 (7.613 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.5098 loss -0.5098 (7.917 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.1219 loss -0.1219 (7.601 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.3768 loss -0.3768 (7.880 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.4695 loss -0.4695 (7.409 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.4399 loss -0.4399 (7.643 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.5933 loss -0.5933 (7.604 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.5832 loss -0.5832 (7.643 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.5874 loss -0.5874 (7.463 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.4935 loss -0.4935 (7.636 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.4540 loss -0.4540 (7.411 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.3536 loss -0.3536 (7.662 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.1156 loss -0.1156 (7.495 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.2713 loss -0.2713 (7.298 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.4946 loss -0.4946 (7.402 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.4749 loss -0.4749 (7.283 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.22it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 0.5125 loss -0.5125 (36.489 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.4368 loss -0.4368 (7.467 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.3676 loss -0.3676 (7.683 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.1149 loss -0.1149 (7.221 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.1564 loss -0.1564 (7.579 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.4408 loss -0.4408 (7.417 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.4013 loss -0.4013 (7.375 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.4257 loss -0.4257 (7.362 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.5175 loss -0.5175 (7.232 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.5070 loss -0.5070 (7.490 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.4512 loss -0.4512 (7.116 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.3595 loss -0.3595 (7.547 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.1436 loss -0.1436 (7.301 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 32600 lr 3.800e-04 [train_loss] tar_ll -0.3499 loss 0.3499 (7.448 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.0238 loss -0.0238 (7.293 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.1568 loss -0.1568 (7.760 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.3131 loss -0.3131 (7.315 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.3579 loss -0.3579 (7.390 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.0428 loss -0.0428 (7.240 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.1141 loss -0.1141 (7.475 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.2503 loss -0.2503 (7.380 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.3568 loss -0.3568 (7.447 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.4148 loss -0.4148 (7.610 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.4535 loss -0.4535 (7.232 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.3472 loss -0.3472 (7.370 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.2780 loss -0.2780 (7.259 secs)\n",
      "100%|##########| 3000/3000 [00:37<00:00, 80.68it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 0.4402 loss -0.4402 (37.187 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.2997 loss -0.2997 (7.618 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.4939 loss -0.4939 (7.457 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.4755 loss -0.4755 (7.487 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.4297 loss -0.4297 (7.588 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.4075 loss -0.4075 (7.424 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.4977 loss -0.4977 (7.596 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.4004 loss -0.4004 (7.498 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.4957 loss -0.4957 (7.301 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.5398 loss -0.5398 (7.550 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.3554 loss -0.3554 (7.373 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.5091 loss -0.5091 (7.688 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.5533 loss -0.5533 (7.251 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.6627 loss -0.6627 (7.486 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.2167 loss -0.2167 (7.282 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.3177 loss -0.3177 (7.408 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.3726 loss -0.3726 (7.641 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.3072 loss -0.3072 (7.440 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.3828 loss -0.3828 (7.499 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.5712 loss -0.5712 (7.646 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.4112 loss -0.4112 (7.546 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.3926 loss -0.3926 (7.398 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.4951 loss -0.4951 (7.538 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.5201 loss -0.5201 (7.445 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.5247 loss -0.5247 (7.541 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.6441 loss -0.6441 (7.353 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.42it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 0.4325 loss -0.4325 (36.403 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.4534 loss -0.4534 (7.292 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.5153 loss -0.5153 (7.889 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.6464 loss -0.6464 (7.305 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.5901 loss -0.5901 (7.367 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.6564 loss -0.6564 (7.526 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.6527 loss -0.6527 (7.355 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.6426 loss -0.6426 (7.515 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.5336 loss -0.5336 (7.320 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.6018 loss -0.6018 (7.591 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.6530 loss -0.6530 (7.426 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.6513 loss -0.6513 (7.679 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.5870 loss -0.5870 (7.331 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 42600 lr 3.076e-04 [train_loss] tar_ll 0.5743 loss -0.5743 (7.547 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.5952 loss -0.5952 (7.316 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.5156 loss -0.5156 (7.496 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.5350 loss -0.5350 (7.470 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.6716 loss -0.6716 (7.498 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.5459 loss -0.5459 (7.545 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.5825 loss -0.5825 (7.580 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.6410 loss -0.6410 (7.484 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.6751 loss -0.6751 (7.397 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.7050 loss -0.7050 (7.656 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.7224 loss -0.7224 (7.304 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.4921 loss -0.4921 (7.527 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.5140 loss -0.5140 (7.556 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.50it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 0.7044 loss -0.7044 (36.366 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.7391 loss -0.7391 (7.407 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.7618 loss -0.7618 (7.404 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.6480 loss -0.6480 (7.425 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.7263 loss -0.7263 (7.410 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.6435 loss -0.6435 (7.551 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.6962 loss -0.6962 (7.549 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.6815 loss -0.6815 (7.568 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.4445 loss -0.4445 (7.438 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.7011 loss -0.7011 (7.658 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.7724 loss -0.7724 (7.242 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.7113 loss -0.7113 (7.550 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.7061 loss -0.7061 (7.419 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 47600 lr 2.688e-04 [train_loss] tar_ll 0.6508 loss -0.6508 (7.481 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.5543 loss -0.5543 (7.159 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.7124 loss -0.7124 (7.453 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.6623 loss -0.6623 (7.339 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 48400 lr 2.626e-04 [train_loss] tar_ll 0.6850 loss -0.6850 (7.423 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.3629 loss -0.3629 (7.408 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 48800 lr 2.594e-04 [train_loss] tar_ll 0.6046 loss -0.6046 (7.285 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.7757 loss -0.7757 (7.567 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 49200 lr 2.563e-04 [train_loss] tar_ll 0.7682 loss -0.7682 (7.375 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.6441 loss -0.6441 (7.718 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 49600 lr 2.531e-04 [train_loss] tar_ll 0.6567 loss -0.6567 (7.373 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.7021 loss -0.7021 (7.450 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 50000 lr 2.500e-04 [train_loss] tar_ll 0.7733 loss -0.7733 (7.447 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.44it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 0.7415 loss -0.7415 (36.390 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 50200 lr 2.484e-04 [train_loss] tar_ll 0.7828 loss -0.7828 (7.515 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 50400 lr 2.469e-04 [train_loss] tar_ll 0.7235 loss -0.7235 (7.369 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 50600 lr 2.453e-04 [train_loss] tar_ll 0.7628 loss -0.7628 (7.363 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 50800 lr 2.437e-04 [train_loss] tar_ll 0.7376 loss -0.7376 (7.395 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 51000 lr 2.421e-04 [train_loss] tar_ll 0.7927 loss -0.7927 (7.574 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 51200 lr 2.406e-04 [train_loss] tar_ll 0.7769 loss -0.7769 (7.479 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 51400 lr 2.390e-04 [train_loss] tar_ll 0.8044 loss -0.8044 (7.677 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 51600 lr 2.374e-04 [train_loss] tar_ll 0.5245 loss -0.5245 (7.460 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 51800 lr 2.359e-04 [train_loss] tar_ll 0.7124 loss -0.7124 (7.569 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 52000 lr 2.343e-04 [train_loss] tar_ll 0.7882 loss -0.7882 (7.522 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 52200 lr 2.327e-04 [train_loss] tar_ll 0.8230 loss -0.8230 (7.773 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 52400 lr 2.312e-04 [train_loss] tar_ll 0.7730 loss -0.7730 (7.397 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 52600 lr 2.296e-04 [train_loss] tar_ll 0.7189 loss -0.7189 (7.615 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 52800 lr 2.280e-04 [train_loss] tar_ll 0.6722 loss -0.6722 (7.324 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 53000 lr 2.265e-04 [train_loss] tar_ll 0.7590 loss -0.7590 (7.775 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 53200 lr 2.249e-04 [train_loss] tar_ll 0.8240 loss -0.8240 (7.484 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 53400 lr 2.233e-04 [train_loss] tar_ll 0.8211 loss -0.8211 (7.731 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 53600 lr 2.218e-04 [train_loss] tar_ll 0.8176 loss -0.8176 (7.535 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 53800 lr 2.202e-04 [train_loss] tar_ll 0.8416 loss -0.8416 (7.683 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 54000 lr 2.187e-04 [train_loss] tar_ll 0.7905 loss -0.7905 (7.575 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 54200 lr 2.171e-04 [train_loss] tar_ll 0.7614 loss -0.7614 (7.624 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 54400 lr 2.156e-04 [train_loss] tar_ll 0.6741 loss -0.6741 (7.821 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 54600 lr 2.140e-04 [train_loss] tar_ll 0.7678 loss -0.7678 (7.613 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 54800 lr 2.124e-04 [train_loss] tar_ll 0.7839 loss -0.7839 (7.688 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 55000 lr 2.109e-04 [train_loss] tar_ll 0.8405 loss -0.8405 (7.523 secs)\n",
      "100%|##########| 3000/3000 [00:37<00:00, 81.05it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 0.8711 loss -0.8711 (37.017 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 55200 lr 2.093e-04 [train_loss] tar_ll 0.7548 loss -0.7548 (7.400 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 55400 lr 2.078e-04 [train_loss] tar_ll 0.8717 loss -0.8717 (7.399 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 55600 lr 2.062e-04 [train_loss] tar_ll 0.8506 loss -0.8506 (7.170 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 55800 lr 2.047e-04 [train_loss] tar_ll 0.8946 loss -0.8946 (7.431 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 56000 lr 2.032e-04 [train_loss] tar_ll 0.7922 loss -0.7922 (7.097 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 56200 lr 2.016e-04 [train_loss] tar_ll 0.8559 loss -0.8559 (7.519 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 56400 lr 2.001e-04 [train_loss] tar_ll 0.9449 loss -0.9449 (7.299 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 56600 lr 1.985e-04 [train_loss] tar_ll 0.7950 loss -0.7950 (7.218 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 56800 lr 1.970e-04 [train_loss] tar_ll 0.9250 loss -0.9250 (7.808 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 57000 lr 1.955e-04 [train_loss] tar_ll 0.8872 loss -0.8872 (7.581 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 57200 lr 1.939e-04 [train_loss] tar_ll 0.8672 loss -0.8672 (7.837 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 57400 lr 1.924e-04 [train_loss] tar_ll 0.9204 loss -0.9204 (7.605 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 57600 lr 1.909e-04 [train_loss] tar_ll 0.8243 loss -0.8243 (7.953 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 57800 lr 1.894e-04 [train_loss] tar_ll 0.8570 loss -0.8570 (7.592 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 58000 lr 1.878e-04 [train_loss] tar_ll 0.8158 loss -0.8158 (7.490 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 58200 lr 1.863e-04 [train_loss] tar_ll 0.8196 loss -0.8196 (7.393 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 58400 lr 1.848e-04 [train_loss] tar_ll 0.7590 loss -0.7590 (7.468 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 58600 lr 1.833e-04 [train_loss] tar_ll 0.8294 loss -0.8294 (7.413 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 58800 lr 1.818e-04 [train_loss] tar_ll 0.8744 loss -0.8744 (7.412 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 59000 lr 1.803e-04 [train_loss] tar_ll 0.8436 loss -0.8436 (7.418 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 59200 lr 1.787e-04 [train_loss] tar_ll 0.7966 loss -0.7966 (8.550 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 59400 lr 1.772e-04 [train_loss] tar_ll 0.8373 loss -0.8373 (7.810 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 59600 lr 1.757e-04 [train_loss] tar_ll 0.6168 loss -0.6168 (7.911 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 59800 lr 1.742e-04 [train_loss] tar_ll 0.6695 loss -0.6695 (7.505 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 60000 lr 1.727e-04 [train_loss] tar_ll 0.9256 loss -0.9256 (7.884 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.39it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 0.9192 loss -0.9192 (36.413 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 60200 lr 1.713e-04 [train_loss] tar_ll 0.9473 loss -0.9473 (7.657 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 60400 lr 1.698e-04 [train_loss] tar_ll 0.9128 loss -0.9128 (7.711 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 60600 lr 1.683e-04 [train_loss] tar_ll 0.8218 loss -0.8218 (7.447 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 60800 lr 1.668e-04 [train_loss] tar_ll 0.9032 loss -0.9032 (7.537 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 61000 lr 1.653e-04 [train_loss] tar_ll 0.9924 loss -0.9924 (7.370 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 61200 lr 1.638e-04 [train_loss] tar_ll 0.9650 loss -0.9650 (7.562 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 61400 lr 1.624e-04 [train_loss] tar_ll 0.9462 loss -0.9462 (7.396 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 61600 lr 1.609e-04 [train_loss] tar_ll 0.9122 loss -0.9122 (7.504 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 61800 lr 1.594e-04 [train_loss] tar_ll 0.9852 loss -0.9852 (7.347 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 62000 lr 1.580e-04 [train_loss] tar_ll 0.9489 loss -0.9489 (7.678 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 62200 lr 1.565e-04 [train_loss] tar_ll 0.9246 loss -0.9246 (7.761 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 62400 lr 1.551e-04 [train_loss] tar_ll 0.8493 loss -0.8493 (8.053 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 62600 lr 1.536e-04 [train_loss] tar_ll 0.6626 loss -0.6626 (7.739 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 62800 lr 1.522e-04 [train_loss] tar_ll 0.6830 loss -0.6830 (7.470 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 63000 lr 1.507e-04 [train_loss] tar_ll 0.8977 loss -0.8977 (7.384 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 63200 lr 1.493e-04 [train_loss] tar_ll 0.8554 loss -0.8554 (7.404 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 63400 lr 1.478e-04 [train_loss] tar_ll 0.8851 loss -0.8851 (7.580 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 63600 lr 1.464e-04 [train_loss] tar_ll 0.9341 loss -0.9341 (7.257 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 63800 lr 1.450e-04 [train_loss] tar_ll 0.9745 loss -0.9745 (7.388 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 64000 lr 1.436e-04 [train_loss] tar_ll 0.9309 loss -0.9309 (7.581 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 64200 lr 1.421e-04 [train_loss] tar_ll 0.8995 loss -0.8995 (8.661 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 64400 lr 1.407e-04 [train_loss] tar_ll 0.9291 loss -0.9291 (7.449 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 64600 lr 1.393e-04 [train_loss] tar_ll 0.9593 loss -0.9593 (7.647 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 64800 lr 1.379e-04 [train_loss] tar_ll 0.8576 loss -0.8576 (7.411 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 65000 lr 1.365e-04 [train_loss] tar_ll 0.9483 loss -0.9483 (7.305 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.76it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 0.9546 loss -0.9546 (34.189 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.0271 loss -1.0271 (7.037 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 65400 lr 1.337e-04 [train_loss] tar_ll 0.7164 loss -0.7164 (7.050 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 65600 lr 1.323e-04 [train_loss] tar_ll 0.8951 loss -0.8951 (7.084 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 65800 lr 1.309e-04 [train_loss] tar_ll 0.9001 loss -0.9001 (7.155 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 66000 lr 1.296e-04 [train_loss] tar_ll 0.9297 loss -0.9297 (6.906 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 66200 lr 1.282e-04 [train_loss] tar_ll 0.9603 loss -0.9603 (7.256 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 66400 lr 1.268e-04 [train_loss] tar_ll 0.9116 loss -0.9116 (7.043 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 66600 lr 1.255e-04 [train_loss] tar_ll 0.8980 loss -0.8980 (7.234 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 66800 lr 1.241e-04 [train_loss] tar_ll 0.9695 loss -0.9695 (7.129 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 67000 lr 1.227e-04 [train_loss] tar_ll 0.9186 loss -0.9186 (7.166 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 67200 lr 1.214e-04 [train_loss] tar_ll 0.9794 loss -0.9794 (7.290 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 67400 lr 1.200e-04 [train_loss] tar_ll 0.8426 loss -0.8426 (7.159 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 67600 lr 1.187e-04 [train_loss] tar_ll 0.9867 loss -0.9867 (7.230 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.1456 loss -1.1456 (7.100 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.0305 loss -1.0305 (7.289 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 68200 lr 1.147e-04 [train_loss] tar_ll 0.9649 loss -0.9649 (7.134 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 68400 lr 1.134e-04 [train_loss] tar_ll 0.9157 loss -0.9157 (7.197 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.0353 loss -1.0353 (7.235 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 68800 lr 1.108e-04 [train_loss] tar_ll 0.9702 loss -0.9702 (6.977 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 69000 lr 1.095e-04 [train_loss] tar_ll 0.9344 loss -0.9344 (7.221 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.0078 loss -1.0078 (7.132 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 69400 lr 1.069e-04 [train_loss] tar_ll 0.9862 loss -0.9862 (7.270 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 69600 lr 1.056e-04 [train_loss] tar_ll 0.9917 loss -0.9917 (7.112 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.0655 loss -1.0655 (7.256 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 70000 lr 1.031e-04 [train_loss] tar_ll 0.9603 loss -0.9603 (7.354 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.26it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 0.8319 loss -0.8319 (34.381 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 70200 lr 1.018e-04 [train_loss] tar_ll 0.9081 loss -0.9081 (7.325 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 70400 lr 1.005e-04 [train_loss] tar_ll 0.9555 loss -0.9555 (7.089 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 70600 lr 9.927e-05 [train_loss] tar_ll 0.9564 loss -0.9564 (7.386 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.0057 loss -1.0057 (7.202 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 71000 lr 9.677e-05 [train_loss] tar_ll 0.9961 loss -0.9961 (7.267 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.0138 loss -1.0138 (7.205 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.0497 loss -1.0497 (7.408 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.0268 loss -1.0268 (7.223 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.0471 loss -1.0471 (7.239 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 72000 lr 9.064e-05 [train_loss] tar_ll 0.9479 loss -0.9479 (7.286 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.0288 loss -1.0288 (7.250 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 72400 lr 8.824e-05 [train_loss] tar_ll 0.9795 loss -0.9795 (7.392 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 72600 lr 8.704e-05 [train_loss] tar_ll 0.9945 loss -0.9945 (7.013 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 72800 lr 8.585e-05 [train_loss] tar_ll 0.9952 loss -0.9952 (7.297 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.0481 loss -1.0481 (7.068 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 73200 lr 8.350e-05 [train_loss] tar_ll 0.9910 loss -0.9910 (7.210 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.0659 loss -1.0659 (7.154 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 73600 lr 8.117e-05 [train_loss] tar_ll 0.9877 loss -0.9877 (7.101 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.0575 loss -1.0575 (7.188 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.0647 loss -1.0647 (7.027 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.0961 loss -1.0961 (7.259 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.0314 loss -1.0314 (6.912 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.0716 loss -1.0716 (7.114 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.0797 loss -1.0797 (7.063 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.0602 loss -1.0602 (7.200 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.94it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 1.0349 loss -1.0349 (34.508 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.0189 loss -1.0189 (7.264 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.0190 loss -1.0190 (7.193 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.0284 loss -1.0284 (7.104 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.0861 loss -1.0861 (7.275 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.0068 loss -1.0068 (7.207 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.0289 loss -1.0289 (7.377 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.0447 loss -1.0447 (7.214 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.0116 loss -1.0116 (7.251 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.0470 loss -1.0470 (7.290 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.1278 loss -1.1278 (7.261 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.0383 loss -1.0383 (7.592 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 77400 lr 6.041e-05 [train_loss] tar_ll 0.9587 loss -0.9587 (7.288 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.0425 loss -1.0425 (7.317 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.1150 loss -1.1150 (7.130 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.1133 loss -1.1133 (7.471 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.0760 loss -1.0760 (7.218 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.0662 loss -1.0662 (7.095 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.0333 loss -1.0333 (7.229 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.0501 loss -1.0501 (7.117 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.0542 loss -1.0542 (7.156 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.1136 loss -1.1136 (7.289 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.0622 loss -1.0622 (7.426 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.1110 loss -1.1110 (7.217 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 79800 lr 4.867e-05 [train_loss] tar_ll 0.9643 loss -0.9643 (7.260 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.0950 loss -1.0950 (7.137 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.19it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 1.0928 loss -1.0928 (34.808 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.0425 loss -1.0425 (7.610 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.1421 loss -1.1421 (7.277 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1079 loss -1.1079 (7.556 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.0196 loss -1.0196 (7.605 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.0654 loss -1.0654 (7.604 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.1041 loss -1.1041 (7.260 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 81400 lr 4.148e-05 [train_loss] tar_ll 0.9994 loss -0.9994 (7.264 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.1145 loss -1.1145 (7.169 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.1081 loss -1.1081 (7.217 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.0435 loss -1.0435 (7.367 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.0639 loss -1.0639 (7.055 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.0461 loss -1.0461 (7.457 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.1020 loss -1.1020 (7.106 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.0254 loss -1.0254 (7.205 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.0697 loss -1.0697 (7.218 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.0740 loss -1.0740 (7.308 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.1473 loss -1.1473 (7.092 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.0808 loss -1.0808 (7.282 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.1085 loss -1.1085 (7.327 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.1165 loss -1.1165 (7.180 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.0657 loss -1.0657 (7.237 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.1158 loss -1.1158 (7.190 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.0827 loss -1.0827 (7.629 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.1212 loss -1.1212 (7.443 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.1212 loss -1.1212 (7.628 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.21it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 1.1084 loss -1.1084 (34.802 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.0307 loss -1.0307 (7.117 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.1310 loss -1.1310 (7.217 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.1209 loss -1.1209 (7.074 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.0747 loss -1.0747 (7.335 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.0512 loss -1.0512 (7.192 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.0695 loss -1.0695 (7.387 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.0524 loss -1.0524 (7.212 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.1435 loss -1.1435 (7.293 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.1448 loss -1.1448 (7.203 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.1513 loss -1.1513 (7.115 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.1542 loss -1.1542 (7.512 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.0924 loss -1.0924 (7.274 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.1518 loss -1.1518 (7.217 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.0478 loss -1.0478 (7.124 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.0554 loss -1.0554 (7.188 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.1264 loss -1.1264 (7.204 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.1736 loss -1.1736 (7.208 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.1098 loss -1.1098 (7.119 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.0432 loss -1.0432 (7.168 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 89000 lr 1.478e-05 [train_loss] tar_ll 0.9867 loss -0.9867 (7.007 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.1027 loss -1.1027 (7.036 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.1456 loss -1.1456 (7.496 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.0275 loss -1.0275 (7.009 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.0706 loss -1.0706 (7.233 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.0689 loss -1.0689 (7.080 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.16it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 1.1075 loss -1.1075 (34.423 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.1093 loss -1.1093 (7.297 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.0884 loss -1.0884 (6.966 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.1325 loss -1.1325 (7.190 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.0573 loss -1.0573 (7.030 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.0475 loss -1.0475 (7.185 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.0816 loss -1.0816 (7.195 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.1490 loss -1.1490 (7.188 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.0483 loss -1.0483 (7.115 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.0858 loss -1.0858 (7.155 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.0867 loss -1.0867 (7.324 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.1223 loss -1.1223 (7.200 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.1181 loss -1.1181 (7.252 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.1398 loss -1.1398 (7.121 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.0527 loss -1.0527 (7.334 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.1516 loss -1.1516 (7.008 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.1617 loss -1.1617 (7.359 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.0987 loss -1.0987 (7.127 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.1744 loss -1.1744 (7.114 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.1156 loss -1.1156 (7.161 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.0122 loss -1.0122 (7.031 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.1191 loss -1.1191 (7.455 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.1652 loss -1.1652 (7.024 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.1634 loss -1.1634 (7.192 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.1207 loss -1.1207 (7.136 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.1875 loss -1.1875 (7.518 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 85.58it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 1.1341 loss -1.1341 (35.057 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.1954 loss -1.1954 (7.174 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.0905 loss -1.0905 (7.245 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.1461 loss -1.1461 (7.106 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.1182 loss -1.1182 (7.453 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.0714 loss -1.0714 (7.376 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.1748 loss -1.1748 (7.538 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.1066 loss -1.1066 (7.210 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.1348 loss -1.1348 (7.117 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.1254 loss -1.1254 (7.214 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.0469 loss -1.0469 (7.101 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.1572 loss -1.1572 (7.171 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.0703 loss -1.0703 (7.257 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.1387 loss -1.1387 (7.213 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.1064 loss -1.1064 (7.003 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.1279 loss -1.1279 (7.283 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.0593 loss -1.0593 (7.047 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.1569 loss -1.1569 (7.321 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.2046 loss -1.2046 (7.059 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.1834 loss -1.1834 (7.061 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.1007 loss -1.1007 (7.367 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.1431 loss -1.1431 (7.049 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2023 loss -1.2023 (7.139 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.1446 loss -1.1446 (7.098 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.1488 loss -1.1488 (7.308 secs)\n",
      "isanp2:isanp2-num_latents-64_rbf_100 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.0139 loss -1.0139 (7.152 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.16it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 1.1368 loss -1.1368 (34.822 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:34<00:00, 85.86it/s]\n",
      "isanp2:isanp2-num_latents-64_rbf_100 rbf tar_ll 1.1368 loss -1.1368 (34.942 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4465957.5 miliseconds\n",
      "Execution time: 4465.9575 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 83.6865234375 MB\n",
      "Memory Usage Change: 67.4365234375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp2', name='isanp2-num_latents-64_rbf_100',val_seed=100, val_l=64,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74efaecf-fb76-48f9-9e1c-71ca58b55518",
   "metadata": {},
   "source": [
    "## ISANP2 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ae121f3-890e-4214-9ad6-3944645990c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: isanp2-isanp2-num_latents-128_rbf_100\n",
      "Total number of parameters: 793282\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp2:isanp2-num_latents-128_rbf_100 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6609 loss 0.6609 (7.269 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 400 lr 5.000e-04 [train_loss] tar_ll -0.4917 loss 0.4917 (7.308 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 600 lr 5.000e-04 [train_loss] tar_ll -0.2105 loss 0.2105 (7.257 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 800 lr 4.999e-04 [train_loss] tar_ll -0.1056 loss 0.1056 (7.150 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 1000 lr 4.999e-04 [train_loss] tar_ll 0.0070 loss -0.0070 (7.363 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 1200 lr 4.998e-04 [train_loss] tar_ll 0.0008 loss -0.0008 (7.282 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 1400 lr 4.998e-04 [train_loss] tar_ll 0.0495 loss -0.0495 (7.289 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 1600 lr 4.997e-04 [train_loss] tar_ll 0.1503 loss -0.1503 (7.162 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 1800 lr 4.996e-04 [train_loss] tar_ll 0.1948 loss -0.1948 (7.317 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 2000 lr 4.995e-04 [train_loss] tar_ll 0.1511 loss -0.1511 (7.286 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.2778 loss -0.2778 (7.502 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.3158 loss -0.3158 (7.301 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.3392 loss -0.3392 (7.298 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.3329 loss -0.3329 (7.344 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.3447 loss -0.3447 (7.231 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.3914 loss -0.3914 (7.531 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.4008 loss -0.4008 (7.303 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2251 loss -0.2251 (7.439 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.0167 loss -0.0167 (7.183 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.2601 loss -0.2601 (7.356 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.0506 loss -0.0506 (7.310 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.3016 loss -0.3016 (7.276 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4330 loss -0.4330 (7.304 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4455 loss -0.4455 (7.293 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.5200 loss -0.5200 (7.143 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.70it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 0.6399 loss -0.6399 (33.825 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.5092 loss -0.5092 (7.265 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.5441 loss -0.5441 (7.101 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4751 loss -0.4751 (7.209 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.4643 loss -0.4643 (7.236 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4636 loss -0.4636 (7.135 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.3864 loss -0.3864 (7.286 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.4724 loss -0.4724 (7.076 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5903 loss -0.5903 (7.322 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.5830 loss -0.5830 (7.098 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.5775 loss -0.5775 (7.258 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.4091 loss -0.4091 (7.238 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5310 loss -0.5310 (7.315 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.4271 loss -0.4271 (7.149 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.5707 loss -0.5707 (7.126 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.6244 loss -0.6244 (7.388 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.5863 loss -0.5863 (7.158 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.6683 loss -0.6683 (7.334 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.6149 loss -0.6149 (7.092 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.6626 loss -0.6626 (7.234 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.5331 loss -0.5331 (7.126 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.7528 loss -0.7528 (7.319 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.6578 loss -0.6578 (7.218 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.5744 loss -0.5744 (7.199 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.5797 loss -0.5797 (7.180 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.7162 loss -0.7162 (7.238 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 88.04it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 0.7659 loss -0.7659 (34.078 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.5983 loss -0.5983 (7.323 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.6525 loss -0.6525 (7.359 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.6986 loss -0.6986 (7.383 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.6665 loss -0.6665 (7.262 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.5730 loss -0.5730 (7.377 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.7036 loss -0.7036 (7.587 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.6053 loss -0.6053 (7.379 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.7315 loss -0.7315 (7.299 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.5531 loss -0.5531 (7.535 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.7028 loss -0.7028 (7.297 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.7617 loss -0.7617 (7.419 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.7518 loss -0.7518 (7.637 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.8559 loss -0.8559 (7.522 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.7427 loss -0.7427 (7.374 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.7781 loss -0.7781 (7.260 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.7892 loss -0.7892 (7.468 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.7156 loss -0.7156 (7.237 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.6816 loss -0.6816 (7.364 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.8068 loss -0.8068 (7.283 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.7613 loss -0.7613 (7.312 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.7044 loss -0.7044 (7.344 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.7319 loss -0.7319 (7.617 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.7661 loss -0.7661 (7.386 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.6415 loss -0.6415 (7.592 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.5374 loss -0.5374 (7.336 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.30it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 0.3478 loss -0.3478 (34.766 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.5828 loss -0.5828 (7.420 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.5714 loss -0.5714 (7.162 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.7579 loss -0.7579 (7.293 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.5559 loss -0.5559 (7.204 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.6309 loss -0.6309 (7.405 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.7018 loss -0.7018 (7.262 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.7039 loss -0.7039 (7.385 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.5312 loss -0.5312 (7.307 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.4301 loss -0.4301 (7.246 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.6477 loss -0.6477 (7.386 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.6702 loss -0.6702 (7.369 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.5553 loss -0.5553 (7.625 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.3366 loss -0.3366 (7.328 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.6346 loss -0.6346 (7.397 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.6109 loss -0.6109 (7.191 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.5292 loss -0.5292 (7.398 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.5724 loss -0.5724 (7.203 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.6529 loss -0.6529 (7.376 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.5997 loss -0.5997 (7.403 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.6264 loss -0.6264 (7.389 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.6320 loss -0.6320 (7.331 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.6965 loss -0.6965 (7.257 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.6988 loss -0.6988 (7.323 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.5783 loss -0.5783 (7.438 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.3497 loss -0.3497 (7.303 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.83it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 0.6589 loss -0.6589 (34.159 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.6307 loss -0.6307 (7.481 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.5649 loss -0.5649 (7.282 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.7193 loss -0.7193 (7.278 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.1245 loss -0.1245 (7.872 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.1995 loss -0.1995 (7.238 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.4125 loss -0.4125 (7.496 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.4320 loss -0.4320 (7.169 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.5841 loss -0.5841 (7.336 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.2872 loss -0.2872 (7.107 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.2009 loss -0.2009 (7.283 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.2981 loss -0.2981 (7.239 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.4375 loss -0.4375 (7.281 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.3669 loss -0.3669 (7.178 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.6278 loss -0.6278 (7.086 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.6372 loss -0.6372 (7.309 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.5429 loss -0.5429 (7.315 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.4883 loss -0.4883 (7.297 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.3243 loss -0.3243 (7.234 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.5330 loss -0.5330 (7.239 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.5545 loss -0.5545 (7.237 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.0030 loss -0.0030 (7.436 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.3269 loss -0.3269 (7.166 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.5075 loss -0.5075 (7.357 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.5095 loss -0.5095 (7.331 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.5268 loss -0.5268 (7.283 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 88.09it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 0.5858 loss -0.5858 (34.061 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.5839 loss -0.5839 (7.388 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.5216 loss -0.5216 (7.175 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.6863 loss -0.6863 (7.378 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.6608 loss -0.6608 (7.442 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.6328 loss -0.6328 (7.590 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.7032 loss -0.7032 (7.296 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.7514 loss -0.7514 (7.178 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.6354 loss -0.6354 (7.193 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.6710 loss -0.6710 (7.443 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.6414 loss -0.6414 (7.254 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.7458 loss -0.7458 (7.448 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.7561 loss -0.7561 (7.327 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.7052 loss -0.7052 (7.262 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.7818 loss -0.7818 (7.325 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.7255 loss -0.7255 (7.338 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.7380 loss -0.7380 (7.430 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.6458 loss -0.6458 (7.279 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.6048 loss -0.6048 (7.377 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.6792 loss -0.6792 (7.232 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.6537 loss -0.6537 (7.347 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.6839 loss -0.6839 (7.251 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.5183 loss -0.5183 (7.333 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.6963 loss -0.6963 (7.055 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.7059 loss -0.7059 (7.213 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.7452 loss -0.7452 (7.248 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.83it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 0.8637 loss -0.8637 (33.775 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.7608 loss -0.7608 (7.461 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.5322 loss -0.5322 (7.054 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.6884 loss -0.6884 (7.400 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.7608 loss -0.7608 (7.275 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.4812 loss -0.4812 (7.063 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.6704 loss -0.6704 (7.297 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.7524 loss -0.7524 (7.098 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.8111 loss -0.8111 (7.451 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.5674 loss -0.5674 (7.099 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.5024 loss -0.5024 (7.302 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.7073 loss -0.7073 (7.170 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.5706 loss -0.5706 (7.251 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.7325 loss -0.7325 (7.397 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.7289 loss -0.7289 (7.190 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.8061 loss -0.8061 (7.379 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.7207 loss -0.7207 (7.329 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.4328 loss -0.4328 (7.300 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.7666 loss -0.7666 (7.291 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.4491 loss -0.4491 (7.361 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.5363 loss -0.5363 (7.344 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.6416 loss -0.6416 (7.391 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.6770 loss -0.6770 (7.369 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.6996 loss -0.6996 (7.326 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.6532 loss -0.6532 (7.210 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.9131 loss -0.9131 (7.332 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.47it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 0.8240 loss -0.8240 (34.298 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.6779 loss -0.6779 (7.434 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.8489 loss -0.8489 (7.427 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.8749 loss -0.8749 (7.403 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.7611 loss -0.7611 (7.746 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.8827 loss -0.8827 (7.517 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.5979 loss -0.5979 (7.345 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.7216 loss -0.7216 (7.261 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.6636 loss -0.6636 (7.275 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.7604 loss -0.7604 (7.312 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.9070 loss -0.9070 (7.197 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.8620 loss -0.8620 (7.636 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.8770 loss -0.8770 (7.448 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.9118 loss -0.9118 (7.600 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.8587 loss -0.8587 (7.498 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.8252 loss -0.8252 (7.602 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.8734 loss -0.8734 (7.662 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.7658 loss -0.7658 (7.316 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.7905 loss -0.7905 (7.271 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.9570 loss -0.9570 (7.697 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.8563 loss -0.8563 (7.524 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.8746 loss -0.8746 (7.495 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.9315 loss -0.9315 (7.166 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.9282 loss -0.9282 (7.243 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.7650 loss -0.7650 (7.406 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.8636 loss -0.8636 (7.572 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.22it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 0.8856 loss -0.8856 (34.398 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.8790 loss -0.8790 (7.205 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.8360 loss -0.8360 (7.223 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.8298 loss -0.8298 (7.450 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.8932 loss -0.8932 (7.307 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.9172 loss -0.9172 (7.421 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.9401 loss -0.9401 (7.269 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.8121 loss -0.8121 (7.452 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.7479 loss -0.7479 (7.369 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.8690 loss -0.8690 (7.519 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.7312 loss -0.7312 (7.435 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.8135 loss -0.8135 (7.559 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.7883 loss -0.7883 (7.273 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 42600 lr 3.076e-04 [train_loss] tar_ll 0.8730 loss -0.8730 (7.218 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.8240 loss -0.8240 (7.344 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.8692 loss -0.8692 (7.191 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.7737 loss -0.7737 (7.368 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.7777 loss -0.7777 (7.276 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.8935 loss -0.8935 (7.415 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.8886 loss -0.8886 (7.335 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.8118 loss -0.8118 (7.218 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.9881 loss -0.9881 (7.480 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.8289 loss -0.8289 (7.277 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.9749 loss -0.9749 (7.319 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.8828 loss -0.8828 (7.531 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.9307 loss -0.9307 (7.231 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.61it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 1.0163 loss -1.0163 (34.243 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.9820 loss -0.9820 (7.414 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.9600 loss -0.9600 (7.277 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.8901 loss -0.8901 (7.390 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.9063 loss -0.9063 (7.206 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.8469 loss -0.8469 (7.656 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.8953 loss -0.8953 (7.319 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.9311 loss -0.9311 (7.252 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.8662 loss -0.8662 (7.212 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.9535 loss -0.9535 (7.131 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.8897 loss -0.8897 (7.406 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.8964 loss -0.8964 (7.370 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.9723 loss -0.9723 (7.535 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 47600 lr 2.688e-04 [train_loss] tar_ll 0.9252 loss -0.9252 (7.179 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.9130 loss -0.9130 (7.217 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.9635 loss -0.9635 (7.249 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.9141 loss -0.9141 (7.271 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 48400 lr 2.626e-04 [train_loss] tar_ll 0.8802 loss -0.8802 (7.197 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.8044 loss -0.8044 (7.232 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 48800 lr 2.594e-04 [train_loss] tar_ll 0.9587 loss -0.9587 (7.345 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.9059 loss -0.9059 (7.182 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 49200 lr 2.563e-04 [train_loss] tar_ll 1.0065 loss -1.0065 (7.453 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.9831 loss -0.9831 (7.338 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 49600 lr 2.531e-04 [train_loss] tar_ll 0.9586 loss -0.9586 (7.304 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.9520 loss -0.9520 (7.334 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 50000 lr 2.500e-04 [train_loss] tar_ll 1.0051 loss -1.0051 (7.265 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.28it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 0.7700 loss -0.7700 (33.984 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 50200 lr 2.484e-04 [train_loss] tar_ll 0.8929 loss -0.8929 (7.400 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 50400 lr 2.469e-04 [train_loss] tar_ll 0.9455 loss -0.9455 (7.365 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 50600 lr 2.453e-04 [train_loss] tar_ll 1.0260 loss -1.0260 (7.328 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 50800 lr 2.437e-04 [train_loss] tar_ll 0.9130 loss -0.9130 (7.497 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 51000 lr 2.421e-04 [train_loss] tar_ll 0.8173 loss -0.8173 (7.170 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 51200 lr 2.406e-04 [train_loss] tar_ll 1.0028 loss -1.0028 (7.236 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 51400 lr 2.390e-04 [train_loss] tar_ll 0.9778 loss -0.9778 (7.213 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 51600 lr 2.374e-04 [train_loss] tar_ll 1.0103 loss -1.0103 (7.171 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 51800 lr 2.359e-04 [train_loss] tar_ll 0.9656 loss -0.9656 (7.454 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 52000 lr 2.343e-04 [train_loss] tar_ll 1.0596 loss -1.0596 (7.414 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 52200 lr 2.327e-04 [train_loss] tar_ll 0.9590 loss -0.9590 (7.302 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 52400 lr 2.312e-04 [train_loss] tar_ll 0.9836 loss -0.9836 (7.463 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 52600 lr 2.296e-04 [train_loss] tar_ll 0.9691 loss -0.9691 (7.385 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 52800 lr 2.280e-04 [train_loss] tar_ll 1.0213 loss -1.0213 (7.388 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 53000 lr 2.265e-04 [train_loss] tar_ll 1.0080 loss -1.0080 (7.391 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 53200 lr 2.249e-04 [train_loss] tar_ll 1.0372 loss -1.0372 (7.401 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 53400 lr 2.233e-04 [train_loss] tar_ll 0.9765 loss -0.9765 (7.375 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 53600 lr 2.218e-04 [train_loss] tar_ll 1.0425 loss -1.0425 (7.275 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 53800 lr 2.202e-04 [train_loss] tar_ll 0.9897 loss -0.9897 (7.330 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 54000 lr 2.187e-04 [train_loss] tar_ll 1.0271 loss -1.0271 (7.446 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 54200 lr 2.171e-04 [train_loss] tar_ll 0.9679 loss -0.9679 (7.307 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 54400 lr 2.156e-04 [train_loss] tar_ll 1.0144 loss -1.0144 (7.178 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 54600 lr 2.140e-04 [train_loss] tar_ll 1.0531 loss -1.0531 (7.211 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 54800 lr 2.124e-04 [train_loss] tar_ll 0.9071 loss -0.9071 (7.371 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 55000 lr 2.109e-04 [train_loss] tar_ll 0.9351 loss -0.9351 (7.213 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.67it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 1.0165 loss -1.0165 (33.837 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 55200 lr 2.093e-04 [train_loss] tar_ll 0.9217 loss -0.9217 (7.436 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 55400 lr 2.078e-04 [train_loss] tar_ll 1.0249 loss -1.0249 (7.094 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 55600 lr 2.062e-04 [train_loss] tar_ll 0.9901 loss -0.9901 (7.300 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 55800 lr 2.047e-04 [train_loss] tar_ll 1.0065 loss -1.0065 (7.191 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 56000 lr 2.032e-04 [train_loss] tar_ll 0.9108 loss -0.9108 (7.233 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 56200 lr 2.016e-04 [train_loss] tar_ll 1.0250 loss -1.0250 (7.237 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 56400 lr 2.001e-04 [train_loss] tar_ll 1.0673 loss -1.0673 (7.258 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 56600 lr 1.985e-04 [train_loss] tar_ll 1.0181 loss -1.0181 (7.202 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0485 loss -1.0485 (7.260 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 57000 lr 1.955e-04 [train_loss] tar_ll 0.9279 loss -0.9279 (7.291 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 57200 lr 1.939e-04 [train_loss] tar_ll 1.0072 loss -1.0072 (7.352 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 57400 lr 1.924e-04 [train_loss] tar_ll 0.9953 loss -0.9953 (7.770 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 57600 lr 1.909e-04 [train_loss] tar_ll 0.9432 loss -0.9432 (8.500 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 57800 lr 1.894e-04 [train_loss] tar_ll 0.9021 loss -0.9021 (7.855 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 58000 lr 1.878e-04 [train_loss] tar_ll 0.9060 loss -0.9060 (8.143 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 58200 lr 1.863e-04 [train_loss] tar_ll 0.9034 loss -0.9034 (8.081 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 58400 lr 1.848e-04 [train_loss] tar_ll 0.9879 loss -0.9879 (7.443 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 58600 lr 1.833e-04 [train_loss] tar_ll 0.8828 loss -0.8828 (7.644 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 58800 lr 1.818e-04 [train_loss] tar_ll 0.9160 loss -0.9160 (7.586 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.0323 loss -1.0323 (7.524 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 59200 lr 1.787e-04 [train_loss] tar_ll 1.0305 loss -1.0305 (7.787 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 59400 lr 1.772e-04 [train_loss] tar_ll 1.0055 loss -1.0055 (7.733 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 59600 lr 1.757e-04 [train_loss] tar_ll 0.9692 loss -0.9692 (7.560 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 59800 lr 1.742e-04 [train_loss] tar_ll 1.1270 loss -1.1270 (7.621 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 60000 lr 1.727e-04 [train_loss] tar_ll 1.0606 loss -1.0606 (7.467 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.55it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 1.0710 loss -1.0710 (36.346 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 60200 lr 1.713e-04 [train_loss] tar_ll 1.0928 loss -1.0928 (7.645 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 60400 lr 1.698e-04 [train_loss] tar_ll 0.9910 loss -0.9910 (7.705 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.0634 loss -1.0634 (7.636 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.0738 loss -1.0738 (7.517 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 61000 lr 1.653e-04 [train_loss] tar_ll 1.1125 loss -1.1125 (7.601 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 61200 lr 1.638e-04 [train_loss] tar_ll 1.0564 loss -1.0564 (7.790 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 61400 lr 1.624e-04 [train_loss] tar_ll 1.0404 loss -1.0404 (7.698 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 61600 lr 1.609e-04 [train_loss] tar_ll 0.9706 loss -0.9706 (7.545 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 61800 lr 1.594e-04 [train_loss] tar_ll 0.9916 loss -0.9916 (7.614 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 62000 lr 1.580e-04 [train_loss] tar_ll 0.9904 loss -0.9904 (7.303 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 62200 lr 1.565e-04 [train_loss] tar_ll 1.0926 loss -1.0926 (7.517 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.0741 loss -1.0741 (7.296 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.1241 loss -1.1241 (7.697 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.0851 loss -1.0851 (7.629 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.0444 loss -1.0444 (7.596 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.0841 loss -1.0841 (7.610 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 63400 lr 1.478e-04 [train_loss] tar_ll 0.9876 loss -0.9876 (7.640 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.1177 loss -1.1177 (7.512 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.0875 loss -1.0875 (7.675 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 64000 lr 1.436e-04 [train_loss] tar_ll 0.9668 loss -0.9668 (7.630 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.1696 loss -1.1696 (7.448 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.1702 loss -1.1702 (7.645 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.0044 loss -1.0044 (7.391 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.0705 loss -1.0705 (7.633 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.0924 loss -1.0924 (7.364 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.15it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 1.1103 loss -1.1103 (36.521 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.0307 loss -1.0307 (7.629 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.1489 loss -1.1489 (7.575 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.0128 loss -1.0128 (7.572 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.0883 loss -1.0883 (7.713 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 66000 lr 1.296e-04 [train_loss] tar_ll 0.9920 loss -0.9920 (7.537 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.1015 loss -1.1015 (7.711 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.0875 loss -1.0875 (7.377 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.0289 loss -1.0289 (7.484 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.0568 loss -1.0568 (7.561 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.0992 loss -1.0992 (7.456 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.0832 loss -1.0832 (7.651 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.1179 loss -1.1179 (7.463 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.0065 loss -1.0065 (7.591 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.1095 loss -1.1095 (7.567 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.1406 loss -1.1406 (7.475 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.0317 loss -1.0317 (7.574 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.1023 loss -1.1023 (7.590 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.0867 loss -1.0867 (7.599 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.0347 loss -1.0347 (7.632 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.1565 loss -1.1565 (7.552 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.1291 loss -1.1291 (7.706 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.0458 loss -1.0458 (7.629 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.0569 loss -1.0569 (7.702 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.0928 loss -1.0928 (7.490 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.1127 loss -1.1127 (7.459 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 84.48it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 1.1472 loss -1.1472 (35.513 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.0464 loss -1.0464 (7.498 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.1430 loss -1.1430 (7.450 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.0528 loss -1.0528 (7.308 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.0976 loss -1.0976 (7.663 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.1144 loss -1.1144 (7.696 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.2048 loss -1.2048 (8.161 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.1624 loss -1.1624 (7.515 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.1377 loss -1.1377 (7.477 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.1428 loss -1.1428 (7.457 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.1417 loss -1.1417 (7.415 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.1112 loss -1.1112 (7.569 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.1244 loss -1.1244 (7.463 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.1111 loss -1.1111 (7.509 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.1049 loss -1.1049 (7.334 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.2092 loss -1.2092 (7.503 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.1748 loss -1.1748 (7.506 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.1651 loss -1.1651 (7.658 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.0050 loss -1.0050 (7.582 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.1016 loss -1.1016 (7.677 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.1056 loss -1.1056 (7.616 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.0510 loss -1.0510 (7.603 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.0909 loss -1.0909 (7.683 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.1900 loss -1.1900 (7.562 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.0237 loss -1.0237 (7.443 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1447 loss -1.1447 (7.499 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 83.07it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 1.1482 loss -1.1482 (36.118 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1555 loss -1.1555 (7.520 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.1197 loss -1.1197 (7.533 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.1943 loss -1.1943 (7.638 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.0986 loss -1.0986 (7.535 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.1076 loss -1.1076 (7.654 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.1495 loss -1.1495 (7.584 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.0961 loss -1.0961 (7.503 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.0761 loss -1.0761 (7.704 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.0973 loss -1.0973 (7.643 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.0987 loss -1.0987 (7.600 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.1239 loss -1.1239 (7.776 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.1088 loss -1.1088 (7.706 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.2108 loss -1.2108 (7.566 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.1790 loss -1.1790 (7.588 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.1495 loss -1.1495 (7.317 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.1283 loss -1.1283 (7.465 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.1428 loss -1.1428 (7.464 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.1154 loss -1.1154 (7.499 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.1379 loss -1.1379 (7.586 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.1598 loss -1.1598 (7.366 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.0969 loss -1.0969 (7.729 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.1277 loss -1.1277 (7.547 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.1494 loss -1.1494 (7.677 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.1572 loss -1.1572 (7.422 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.2114 loss -1.2114 (7.639 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.89it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 1.1556 loss -1.1556 (36.635 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.1439 loss -1.1439 (7.730 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.0887 loss -1.0887 (7.408 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1766 loss -1.1766 (7.530 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.1683 loss -1.1683 (7.635 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.2134 loss -1.2134 (7.587 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.0937 loss -1.0937 (7.707 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.1537 loss -1.1537 (7.403 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.2480 loss -1.2480 (7.660 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.1283 loss -1.1283 (7.899 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.1742 loss -1.1742 (7.834 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.1893 loss -1.1893 (7.494 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.1622 loss -1.1622 (7.511 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.1643 loss -1.1643 (7.444 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.1346 loss -1.1346 (7.479 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.1210 loss -1.1210 (7.531 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.1690 loss -1.1690 (7.724 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.1465 loss -1.1465 (7.470 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.2219 loss -1.2219 (7.527 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.1252 loss -1.1252 (7.535 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.2294 loss -1.2294 (7.533 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.1731 loss -1.1731 (7.703 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.1164 loss -1.1164 (7.517 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.1873 loss -1.1873 (7.576 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.1860 loss -1.1860 (7.538 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.0974 loss -1.0974 (7.570 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.87it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 1.1989 loss -1.1989 (36.645 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.0699 loss -1.0699 (7.781 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.1477 loss -1.1477 (7.297 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.1877 loss -1.1877 (7.552 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.1649 loss -1.1649 (7.275 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.1827 loss -1.1827 (7.483 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.1216 loss -1.1216 (7.411 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.1639 loss -1.1639 (7.332 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.1681 loss -1.1681 (7.473 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.1776 loss -1.1776 (7.440 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.1577 loss -1.1577 (7.594 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.2291 loss -1.2291 (7.493 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.1649 loss -1.1649 (7.745 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.2096 loss -1.2096 (7.408 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.1975 loss -1.1975 (7.516 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.1632 loss -1.1632 (7.550 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.1422 loss -1.1422 (7.649 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.1694 loss -1.1694 (7.544 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.1027 loss -1.1027 (7.530 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.2034 loss -1.2034 (7.537 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.2119 loss -1.2119 (7.627 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.1787 loss -1.1787 (7.536 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.1583 loss -1.1583 (7.583 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.1507 loss -1.1507 (7.613 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2381 loss -1.2381 (7.486 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.1430 loss -1.1430 (7.540 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.66it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 1.2051 loss -1.2051 (36.741 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.1321 loss -1.1321 (7.721 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.1843 loss -1.1843 (7.474 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.2245 loss -1.2245 (7.590 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.1790 loss -1.1790 (7.478 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.2289 loss -1.2289 (7.613 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2120 loss -1.2120 (7.686 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.1580 loss -1.1580 (7.663 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.2083 loss -1.2083 (7.614 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2043 loss -1.2043 (7.514 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.1664 loss -1.1664 (7.584 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.2215 loss -1.2215 (7.624 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.1547 loss -1.1547 (7.581 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.2309 loss -1.2309 (7.595 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.2040 loss -1.2040 (7.682 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.2076 loss -1.2076 (7.566 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.1674 loss -1.1674 (7.631 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.1866 loss -1.1866 (7.559 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.2287 loss -1.2287 (7.571 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.2069 loss -1.2069 (7.567 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.1991 loss -1.1991 (7.325 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.2223 loss -1.2223 (7.369 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.1353 loss -1.1353 (7.537 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.1510 loss -1.1510 (7.345 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.1887 loss -1.1887 (7.571 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.1341 loss -1.1341 (7.455 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.73it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 1.2108 loss -1.2108 (36.706 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.1345 loss -1.1345 (7.643 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.1848 loss -1.1848 (7.516 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.1712 loss -1.1712 (7.528 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.2096 loss -1.2096 (7.452 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.2366 loss -1.2366 (7.588 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.1818 loss -1.1818 (7.495 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.2116 loss -1.2116 (7.551 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.1668 loss -1.1668 (7.373 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.2185 loss -1.2185 (7.471 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.1063 loss -1.1063 (7.514 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.1829 loss -1.1829 (7.815 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.1207 loss -1.1207 (7.570 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.1974 loss -1.1974 (7.516 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.2373 loss -1.2373 (7.659 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.1279 loss -1.1279 (7.544 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.1608 loss -1.1608 (7.686 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.1242 loss -1.1242 (7.565 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.2256 loss -1.2256 (7.564 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2064 loss -1.2064 (7.553 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.2254 loss -1.2254 (7.567 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.2144 loss -1.2144 (7.593 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2922 loss -1.2922 (7.664 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.2530 loss -1.2530 (7.523 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.2003 loss -1.2003 (7.514 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_100 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.2697 loss -1.2697 (7.742 secs)\n",
      "100%|##########| 3000/3000 [00:37<00:00, 80.87it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 1.2135 loss -1.2135 (37.098 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.67it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_100 rbf tar_ll 1.2135 loss -1.2135 (36.737 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4470936.5 miliseconds\n",
      "Execution time: 4470.9365 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 119.4990234375 MB\n",
      "Memory Usage Change: 103.2490234375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp2', name='isanp2-num_latents-128_rbf_100',val_seed=100, val_l=128,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d86424-24df-4ac9-8dc3-10e0881f8403",
   "metadata": {},
   "source": [
    "## ISANP2 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37fdfed-67df-4c19-ae23-b5da11608a06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: isanp2-isanp2-num_latents-256_rbf_100\n",
      "Total number of parameters: 801474\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 256\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp2:isanp2-num_latents-256_rbf_100 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6545 loss 0.6545 (7.585 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 400 lr 5.000e-04 [train_loss] tar_ll -0.4711 loss 0.4711 (7.635 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 600 lr 5.000e-04 [train_loss] tar_ll -0.1600 loss 0.1600 (7.370 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 800 lr 4.999e-04 [train_loss] tar_ll -0.0287 loss 0.0287 (7.504 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 1000 lr 4.999e-04 [train_loss] tar_ll 0.0029 loss -0.0029 (7.374 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.0866 loss 0.0866 (7.579 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 1400 lr 4.998e-04 [train_loss] tar_ll 0.0303 loss -0.0303 (7.568 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 1600 lr 4.997e-04 [train_loss] tar_ll 0.1392 loss -0.1392 (7.689 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 1800 lr 4.996e-04 [train_loss] tar_ll 0.2240 loss -0.2240 (7.637 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 2000 lr 4.995e-04 [train_loss] tar_ll 0.1965 loss -0.1965 (7.596 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.2381 loss -0.2381 (7.483 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.2634 loss -0.2634 (7.533 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.3109 loss -0.3109 (7.577 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.3411 loss -0.3411 (7.556 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.3620 loss -0.3620 (7.467 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.3556 loss -0.3556 (7.579 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.3775 loss -0.3775 (7.477 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.1680 loss -0.1680 (7.605 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.3283 loss -0.3283 (7.603 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.4086 loss -0.4086 (7.476 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.4255 loss -0.4255 (7.590 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.4017 loss -0.4017 (7.428 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4907 loss -0.4907 (7.634 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4879 loss -0.4879 (7.640 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.5043 loss -0.5043 (7.691 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.19it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 0.5396 loss -0.5396 (36.500 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.3577 loss -0.3577 (7.598 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.4112 loss -0.4112 (7.545 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4382 loss -0.4382 (7.634 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.5023 loss -0.5023 (7.747 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.6011 loss -0.6011 (7.438 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.4627 loss -0.4627 (7.662 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.1912 loss -0.1912 (7.540 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5143 loss -0.5143 (7.896 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.3914 loss -0.3914 (7.614 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.4772 loss -0.4772 (7.911 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.5682 loss -0.5682 (7.731 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5802 loss -0.5802 (7.842 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.6110 loss -0.6110 (7.585 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.5575 loss -0.5575 (7.560 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.3500 loss -0.3500 (7.395 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.5350 loss -0.5350 (7.640 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.6119 loss -0.6119 (7.611 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.5265 loss -0.5265 (7.543 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.5292 loss -0.5292 (7.732 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.5189 loss -0.5189 (7.609 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.3902 loss -0.3902 (7.588 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.3226 loss -0.3226 (7.483 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.4648 loss -0.4648 (7.416 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.0561 loss -0.0561 (7.676 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.3484 loss -0.3484 (7.407 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 83.89it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 0.4449 loss -0.4449 (35.763 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.4614 loss -0.4614 (7.662 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.5443 loss -0.5443 (7.405 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.4595 loss -0.4595 (7.548 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.5774 loss -0.5774 (7.499 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.4308 loss -0.4308 (7.742 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.2567 loss -0.2567 (7.577 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.0242 loss -0.0242 (7.574 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.0822 loss -0.0822 (7.554 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.2883 loss -0.2883 (7.550 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.4682 loss -0.4682 (7.571 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.4684 loss -0.4684 (7.942 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.5213 loss -0.5213 (7.812 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.4999 loss -0.4999 (7.818 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.4907 loss -0.4907 (7.613 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.6724 loss -0.6724 (7.710 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.5538 loss -0.5538 (7.779 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.6027 loss -0.6027 (7.585 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.6277 loss -0.6277 (7.627 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.5916 loss -0.5916 (7.514 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.5082 loss -0.5082 (7.862 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.2273 loss -0.2273 (7.794 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.2837 loss -0.2837 (7.821 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.3785 loss -0.3785 (7.641 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.3673 loss -0.3673 (7.699 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.4305 loss -0.4305 (7.642 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.75it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 0.5338 loss -0.5338 (36.700 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.5386 loss -0.5386 (7.740 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.3199 loss -0.3199 (7.789 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.4686 loss -0.4686 (7.634 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.5029 loss -0.5029 (7.512 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.4175 loss -0.4175 (7.482 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.4451 loss -0.4451 (7.665 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.4859 loss -0.4859 (7.485 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.5472 loss -0.5472 (7.452 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 16800 lr 4.660e-04 [train_loss] tar_ll -0.0244 loss 0.0244 (7.508 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.3113 loss -0.3113 (7.332 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.4366 loss -0.4366 (7.659 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.5098 loss -0.5098 (7.581 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.4609 loss -0.4609 (7.599 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.4823 loss -0.4823 (7.390 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.5320 loss -0.5320 (7.675 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.4709 loss -0.4709 (7.561 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.1481 loss -0.1481 (7.603 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.3502 loss -0.3502 (7.518 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.5407 loss -0.5407 (7.540 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.5644 loss -0.5644 (7.516 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.3739 loss -0.3739 (7.646 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.5394 loss -0.5394 (7.684 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.4940 loss -0.4940 (7.444 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.4403 loss -0.4403 (7.657 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.5226 loss -0.5226 (7.465 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.47it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll -0.0915 loss 0.0915 (36.824 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.4925 loss -0.4925 (7.687 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.6151 loss -0.6151 (7.732 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.5887 loss -0.5887 (7.548 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.5721 loss -0.5721 (7.625 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.3685 loss -0.3685 (7.610 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.3718 loss -0.3718 (7.568 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.4449 loss -0.4449 (7.503 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.4329 loss -0.4329 (7.611 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.3956 loss -0.3956 (7.474 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.6053 loss -0.6053 (7.665 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.5787 loss -0.5787 (7.622 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.4880 loss -0.4880 (7.646 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.1207 loss -0.1207 (7.650 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 22800 lr 4.386e-04 [train_loss] tar_ll -0.2948 loss 0.2948 (7.624 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 23000 lr 4.375e-04 [train_loss] tar_ll -0.0519 loss 0.0519 (7.645 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.1275 loss -0.1275 (7.518 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.2368 loss -0.2368 (7.722 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.3187 loss -0.3187 (7.693 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.2781 loss -0.2781 (7.776 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.4058 loss -0.4058 (7.575 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.5673 loss -0.5673 (7.665 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.5780 loss -0.5780 (7.753 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.4279 loss -0.4279 (7.663 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.2429 loss -0.2429 (7.528 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.6195 loss -0.6195 (7.486 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 85.37it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll -0.2248 loss 0.2248 (35.146 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.2791 loss -0.2791 (7.342 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.0907 loss -0.0907 (7.349 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.3394 loss -0.3394 (7.343 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.3406 loss -0.3406 (7.556 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.1454 loss -0.1454 (7.474 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.2273 loss -0.2273 (7.750 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.4250 loss -0.4250 (7.514 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.4354 loss -0.4354 (7.596 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.5667 loss -0.5667 (7.496 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.4771 loss -0.4771 (7.598 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.4028 loss -0.4028 (7.536 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.4276 loss -0.4276 (7.508 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.4268 loss -0.4268 (7.410 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 27800 lr 4.106e-04 [train_loss] tar_ll -0.1782 loss 0.1782 (7.495 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.1674 loss -0.1674 (7.546 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.2805 loss -0.2805 (7.642 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 28400 lr 4.069e-04 [train_loss] tar_ll -0.0029 loss 0.0029 (7.545 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.2554 loss -0.2554 (7.499 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.2949 loss -0.2949 (7.634 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.3971 loss -0.3971 (7.524 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.4171 loss -0.4171 (7.724 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.2924 loss -0.2924 (7.616 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.1873 loss -0.1873 (7.456 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.3236 loss -0.3236 (7.418 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.1957 loss -0.1957 (7.679 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 83.70it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 0.3482 loss -0.3482 (35.844 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.3589 loss -0.3589 (7.600 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.2451 loss -0.2451 (7.557 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.4237 loss -0.4237 (7.526 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.4729 loss -0.4729 (7.676 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.4647 loss -0.4647 (7.619 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.4910 loss -0.4910 (7.584 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.2994 loss -0.2994 (7.688 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.1930 loss -0.1930 (7.675 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.2691 loss -0.2691 (7.774 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.2299 loss -0.2299 (8.885 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.3732 loss -0.3732 (7.903 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.5070 loss -0.5070 (7.980 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 32600 lr 3.800e-04 [train_loss] tar_ll -0.2016 loss 0.2016 (7.798 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.1780 loss -0.1780 (7.968 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.1594 loss -0.1594 (7.607 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.3355 loss -0.3355 (7.953 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.3303 loss -0.3303 (7.746 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.4130 loss -0.4130 (7.981 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.1281 loss -0.1281 (7.548 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.0543 loss -0.0543 (7.689 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 34200 lr 3.691e-04 [train_loss] tar_ll -0.1145 loss 0.1145 (7.906 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.0127 loss -0.0127 (7.698 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.2652 loss -0.2652 (7.667 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.3372 loss -0.3372 (7.640 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.4160 loss -0.4160 (7.496 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.46it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 0.4331 loss -0.4331 (36.827 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.5218 loss -0.5218 (7.690 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.4971 loss -0.4971 (7.550 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.3757 loss -0.3757 (7.595 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.3267 loss -0.3267 (7.555 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.4759 loss -0.4759 (7.697 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 36200 lr 3.550e-04 [train_loss] tar_ll -0.0486 loss 0.0486 (7.710 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 36400 lr 3.536e-04 [train_loss] tar_ll -0.0782 loss 0.0782 (7.764 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.3901 loss -0.3901 (7.661 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.2365 loss -0.2365 (7.575 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.4054 loss -0.4054 (7.546 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.5493 loss -0.5493 (7.725 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.5005 loss -0.5005 (7.617 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.1440 loss -0.1440 (7.827 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.2573 loss -0.2573 (7.696 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.4022 loss -0.4022 (7.713 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.4517 loss -0.4517 (7.579 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.5349 loss -0.5349 (7.617 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.3654 loss -0.3654 (7.576 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.4968 loss -0.4968 (7.744 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.5670 loss -0.5670 (7.859 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.4322 loss -0.4322 (7.706 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.5321 loss -0.5321 (7.699 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.4344 loss -0.4344 (7.727 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.5145 loss -0.5145 (7.886 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.5409 loss -0.5409 (7.681 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 83.36it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 0.6440 loss -0.6440 (35.991 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.4792 loss -0.4792 (7.508 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.6090 loss -0.6090 (7.915 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.5870 loss -0.5870 (7.449 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.4387 loss -0.4387 (7.732 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.4855 loss -0.4855 (7.441 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.6703 loss -0.6703 (7.705 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.1036 loss -0.1036 (7.555 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.2324 loss -0.2324 (7.581 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.5044 loss -0.5044 (7.589 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.4800 loss -0.4800 (7.707 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.5653 loss -0.5653 (7.440 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.5545 loss -0.5545 (7.620 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 42600 lr 3.076e-04 [train_loss] tar_ll 0.6700 loss -0.6700 (7.536 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.6877 loss -0.6877 (7.549 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.5310 loss -0.5310 (7.733 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.5420 loss -0.5420 (7.517 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.6213 loss -0.6213 (7.548 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.5885 loss -0.5885 (7.522 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.6404 loss -0.6404 (7.566 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.5873 loss -0.5873 (7.625 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.6077 loss -0.6077 (7.747 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.7084 loss -0.7084 (7.522 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.6325 loss -0.6325 (7.600 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.5357 loss -0.5357 (7.519 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.7027 loss -0.7027 (7.664 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.94it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 0.8226 loss -0.8226 (36.614 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.4420 loss -0.4420 (7.576 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.2835 loss -0.2835 (7.574 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.5356 loss -0.5356 (7.626 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.6866 loss -0.6866 (7.540 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.6541 loss -0.6541 (7.521 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.6423 loss -0.6423 (7.808 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.6992 loss -0.6992 (7.534 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.7094 loss -0.7094 (7.576 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.7833 loss -0.7833 (7.534 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.6050 loss -0.6050 (7.607 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.6492 loss -0.6492 (7.598 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.6796 loss -0.6796 (7.822 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 47600 lr 2.688e-04 [train_loss] tar_ll 0.7347 loss -0.7347 (7.751 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.5498 loss -0.5498 (7.539 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.5859 loss -0.5859 (7.601 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.6636 loss -0.6636 (7.713 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 48400 lr 2.626e-04 [train_loss] tar_ll 0.8380 loss -0.8380 (7.404 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.7354 loss -0.7354 (7.613 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 48800 lr 2.594e-04 [train_loss] tar_ll 0.5608 loss -0.5608 (7.186 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.2622 loss -0.2622 (7.492 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 49200 lr 2.563e-04 [train_loss] tar_ll 0.7023 loss -0.7023 (7.482 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.5549 loss -0.5549 (7.367 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 49600 lr 2.531e-04 [train_loss] tar_ll 0.7810 loss -0.7810 (7.551 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.5728 loss -0.5728 (7.472 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 50000 lr 2.500e-04 [train_loss] tar_ll 0.6083 loss -0.6083 (7.609 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.17it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 0.7578 loss -0.7578 (36.511 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 50200 lr 2.484e-04 [train_loss] tar_ll 0.7253 loss -0.7253 (7.595 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 50400 lr 2.469e-04 [train_loss] tar_ll 0.5099 loss -0.5099 (7.484 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 50600 lr 2.453e-04 [train_loss] tar_ll 0.8042 loss -0.8042 (7.661 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 50800 lr 2.437e-04 [train_loss] tar_ll 0.7613 loss -0.7613 (7.638 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 51000 lr 2.421e-04 [train_loss] tar_ll 0.7503 loss -0.7503 (7.485 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 51200 lr 2.406e-04 [train_loss] tar_ll 0.7709 loss -0.7709 (7.460 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 51400 lr 2.390e-04 [train_loss] tar_ll 0.6327 loss -0.6327 (7.578 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 51600 lr 2.374e-04 [train_loss] tar_ll 0.7490 loss -0.7490 (7.641 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 51800 lr 2.359e-04 [train_loss] tar_ll 0.8019 loss -0.8019 (7.581 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 52000 lr 2.343e-04 [train_loss] tar_ll 0.6996 loss -0.6996 (7.777 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 52200 lr 2.327e-04 [train_loss] tar_ll 0.7855 loss -0.7855 (7.693 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 52400 lr 2.312e-04 [train_loss] tar_ll 0.4297 loss -0.4297 (7.649 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 52600 lr 2.296e-04 [train_loss] tar_ll 0.2231 loss -0.2231 (7.729 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 52800 lr 2.280e-04 [train_loss] tar_ll 0.5658 loss -0.5658 (7.733 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 53000 lr 2.265e-04 [train_loss] tar_ll 0.7102 loss -0.7102 (7.570 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 53200 lr 2.249e-04 [train_loss] tar_ll 0.4737 loss -0.4737 (7.699 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 53400 lr 2.233e-04 [train_loss] tar_ll 0.6333 loss -0.6333 (7.559 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 53600 lr 2.218e-04 [train_loss] tar_ll 0.5363 loss -0.5363 (7.508 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 53800 lr 2.202e-04 [train_loss] tar_ll 0.7283 loss -0.7283 (7.495 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 54000 lr 2.187e-04 [train_loss] tar_ll 0.8913 loss -0.8913 (7.645 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 54200 lr 2.171e-04 [train_loss] tar_ll 0.8013 loss -0.8013 (7.610 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 54400 lr 2.156e-04 [train_loss] tar_ll 0.7370 loss -0.7370 (7.661 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 54600 lr 2.140e-04 [train_loss] tar_ll 0.8538 loss -0.8538 (7.934 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 54800 lr 2.124e-04 [train_loss] tar_ll 0.9246 loss -0.9246 (8.433 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 55000 lr 2.109e-04 [train_loss] tar_ll 0.7981 loss -0.7981 (8.068 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 85.28it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 0.8433 loss -0.8433 (35.182 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 55200 lr 2.093e-04 [train_loss] tar_ll 0.8144 loss -0.8144 (7.467 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 55400 lr 2.078e-04 [train_loss] tar_ll 0.7451 loss -0.7451 (7.514 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 55600 lr 2.062e-04 [train_loss] tar_ll 0.7609 loss -0.7609 (7.542 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 55800 lr 2.047e-04 [train_loss] tar_ll 0.8424 loss -0.8424 (7.233 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 56000 lr 2.032e-04 [train_loss] tar_ll 0.7307 loss -0.7307 (7.297 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 56200 lr 2.016e-04 [train_loss] tar_ll 0.6626 loss -0.6626 (7.223 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 56400 lr 2.001e-04 [train_loss] tar_ll 0.7885 loss -0.7885 (7.419 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 56600 lr 1.985e-04 [train_loss] tar_ll 0.8184 loss -0.8184 (7.112 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 56800 lr 1.970e-04 [train_loss] tar_ll 0.4345 loss -0.4345 (7.323 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 57000 lr 1.955e-04 [train_loss] tar_ll 0.5282 loss -0.5282 (7.280 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 57200 lr 1.939e-04 [train_loss] tar_ll 0.7722 loss -0.7722 (7.228 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 57400 lr 1.924e-04 [train_loss] tar_ll 0.8616 loss -0.8616 (7.405 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 57600 lr 1.909e-04 [train_loss] tar_ll 0.8581 loss -0.8581 (7.226 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 57800 lr 1.894e-04 [train_loss] tar_ll 0.8357 loss -0.8357 (7.299 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 58000 lr 1.878e-04 [train_loss] tar_ll 0.7964 loss -0.7964 (7.135 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 58200 lr 1.863e-04 [train_loss] tar_ll 0.8833 loss -0.8833 (7.422 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 58400 lr 1.848e-04 [train_loss] tar_ll 0.9190 loss -0.9190 (7.245 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 58600 lr 1.833e-04 [train_loss] tar_ll 0.8910 loss -0.8910 (7.114 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 58800 lr 1.818e-04 [train_loss] tar_ll 0.8684 loss -0.8684 (7.073 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 59000 lr 1.803e-04 [train_loss] tar_ll 0.9567 loss -0.9567 (7.324 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 59200 lr 1.787e-04 [train_loss] tar_ll 0.8858 loss -0.8858 (7.378 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 59400 lr 1.772e-04 [train_loss] tar_ll 0.8507 loss -0.8507 (7.242 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 59600 lr 1.757e-04 [train_loss] tar_ll 0.9592 loss -0.9592 (7.325 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 59800 lr 1.742e-04 [train_loss] tar_ll 0.9666 loss -0.9666 (7.303 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 60000 lr 1.727e-04 [train_loss] tar_ll 0.8748 loss -0.8748 (7.244 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.36it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 0.9636 loss -0.9636 (34.741 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 60200 lr 1.713e-04 [train_loss] tar_ll 0.8794 loss -0.8794 (7.328 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 60400 lr 1.698e-04 [train_loss] tar_ll 0.5410 loss -0.5410 (7.297 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 60600 lr 1.683e-04 [train_loss] tar_ll 0.7224 loss -0.7224 (7.400 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 60800 lr 1.668e-04 [train_loss] tar_ll 0.8364 loss -0.8364 (7.622 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 61000 lr 1.653e-04 [train_loss] tar_ll 0.8945 loss -0.8945 (7.261 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 61200 lr 1.638e-04 [train_loss] tar_ll 0.7633 loss -0.7633 (7.497 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 61400 lr 1.624e-04 [train_loss] tar_ll 0.8704 loss -0.8704 (7.145 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 61600 lr 1.609e-04 [train_loss] tar_ll 0.8049 loss -0.8049 (7.456 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 61800 lr 1.594e-04 [train_loss] tar_ll 0.8795 loss -0.8795 (7.246 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 62000 lr 1.580e-04 [train_loss] tar_ll 0.9356 loss -0.9356 (7.549 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 62200 lr 1.565e-04 [train_loss] tar_ll 0.9295 loss -0.9295 (7.389 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 62400 lr 1.551e-04 [train_loss] tar_ll 0.9755 loss -0.9755 (7.463 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 62600 lr 1.536e-04 [train_loss] tar_ll 0.9075 loss -0.9075 (7.333 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 62800 lr 1.522e-04 [train_loss] tar_ll 0.9028 loss -0.9028 (7.127 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 63000 lr 1.507e-04 [train_loss] tar_ll 0.9339 loss -0.9339 (7.276 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 63200 lr 1.493e-04 [train_loss] tar_ll 0.8122 loss -0.8122 (7.176 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 63400 lr 1.478e-04 [train_loss] tar_ll 0.8370 loss -0.8370 (7.196 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 63600 lr 1.464e-04 [train_loss] tar_ll 0.9056 loss -0.9056 (7.208 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.0504 loss -1.0504 (7.139 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 64000 lr 1.436e-04 [train_loss] tar_ll 0.9315 loss -0.9315 (7.200 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 64200 lr 1.421e-04 [train_loss] tar_ll 0.8884 loss -0.8884 (7.245 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 64400 lr 1.407e-04 [train_loss] tar_ll 0.8982 loss -0.8982 (7.119 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 64600 lr 1.393e-04 [train_loss] tar_ll 0.9813 loss -0.9813 (7.088 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.0236 loss -1.0236 (7.153 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 65000 lr 1.365e-04 [train_loss] tar_ll 0.7139 loss -0.7139 (7.025 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.83it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 0.8722 loss -0.8722 (34.159 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 65200 lr 1.351e-04 [train_loss] tar_ll 0.9149 loss -0.9149 (7.048 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 65400 lr 1.337e-04 [train_loss] tar_ll 0.9721 loss -0.9721 (7.171 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 65600 lr 1.323e-04 [train_loss] tar_ll 0.9571 loss -0.9571 (7.194 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.0289 loss -1.0289 (7.337 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 66000 lr 1.296e-04 [train_loss] tar_ll 0.9826 loss -0.9826 (7.411 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.0282 loss -1.0282 (7.215 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.0643 loss -1.0643 (7.352 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 66600 lr 1.255e-04 [train_loss] tar_ll 0.9961 loss -0.9961 (7.290 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 66800 lr 1.241e-04 [train_loss] tar_ll 0.9854 loss -0.9854 (7.337 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 67000 lr 1.227e-04 [train_loss] tar_ll 0.8677 loss -0.8677 (7.198 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 67200 lr 1.214e-04 [train_loss] tar_ll 0.8927 loss -0.8927 (7.282 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.0149 loss -1.0149 (7.279 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 67600 lr 1.187e-04 [train_loss] tar_ll 0.9898 loss -0.9898 (7.221 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.1026 loss -1.1026 (7.341 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 68000 lr 1.160e-04 [train_loss] tar_ll 0.9435 loss -0.9435 (7.244 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.0138 loss -1.0138 (7.403 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.0232 loss -1.0232 (7.352 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.0069 loss -1.0069 (7.437 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.0604 loss -1.0604 (7.297 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.0380 loss -1.0380 (7.304 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.0225 loss -1.0225 (7.382 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.0248 loss -1.0248 (7.382 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.0204 loss -1.0204 (7.254 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.0233 loss -1.0233 (7.344 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.0917 loss -1.0917 (7.311 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.64it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 1.0504 loss -1.0504 (34.630 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 70200 lr 1.018e-04 [train_loss] tar_ll 0.9589 loss -0.9589 (7.516 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.0950 loss -1.0950 (7.263 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.0517 loss -1.0517 (7.491 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.0338 loss -1.0338 (7.276 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.0311 loss -1.0311 (7.299 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 71200 lr 9.554e-05 [train_loss] tar_ll 0.9189 loss -0.9189 (7.667 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 71400 lr 9.430e-05 [train_loss] tar_ll 0.9426 loss -0.9426 (7.294 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.0706 loss -1.0706 (7.363 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.0144 loss -1.0144 (7.415 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 72000 lr 9.064e-05 [train_loss] tar_ll 0.9515 loss -0.9515 (7.273 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.0294 loss -1.0294 (7.471 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.0369 loss -1.0369 (7.310 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.1154 loss -1.1154 (7.414 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.0657 loss -1.0657 (7.271 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 73000 lr 8.467e-05 [train_loss] tar_ll 0.9201 loss -0.9201 (7.253 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.0544 loss -1.0544 (7.332 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.0623 loss -1.0623 (7.428 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.0585 loss -1.0585 (7.162 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.0700 loss -1.0700 (7.216 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.0935 loss -1.0935 (7.316 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.0932 loss -1.0932 (7.344 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.0770 loss -1.0770 (7.324 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.1271 loss -1.1271 (7.275 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.0858 loss -1.0858 (7.313 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.0281 loss -1.0281 (7.286 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.87it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 1.1008 loss -1.1008 (34.536 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1330 loss -1.1330 (7.242 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.0398 loss -1.0398 (7.406 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.0863 loss -1.0863 (7.234 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.0161 loss -1.0161 (7.223 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.0646 loss -1.0646 (7.401 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.0511 loss -1.0511 (7.481 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.0601 loss -1.0601 (7.362 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.0145 loss -1.0145 (7.275 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.0499 loss -1.0499 (7.358 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.0642 loss -1.0642 (7.413 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.0894 loss -1.0894 (7.428 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.0862 loss -1.0862 (7.288 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.1281 loss -1.1281 (7.366 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.0581 loss -1.0581 (7.265 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.0339 loss -1.0339 (7.297 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.0032 loss -1.0032 (7.346 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.0962 loss -1.0962 (7.408 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.1253 loss -1.1253 (7.328 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.0883 loss -1.0883 (7.477 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.1375 loss -1.1375 (7.341 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.1356 loss -1.1356 (7.562 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.1839 loss -1.1839 (7.246 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.0745 loss -1.0745 (7.354 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.1016 loss -1.1016 (7.530 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.1182 loss -1.1182 (7.551 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 84.90it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 1.1301 loss -1.1301 (35.337 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.0761 loss -1.0761 (7.342 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.0133 loss -1.0133 (7.342 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1292 loss -1.1292 (7.371 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.1493 loss -1.1493 (7.242 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.1602 loss -1.1602 (7.445 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 81200 lr 4.235e-05 [train_loss] tar_ll 0.9811 loss -0.9811 (7.231 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.1311 loss -1.1311 (7.287 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.1396 loss -1.1396 (7.138 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.1896 loss -1.1896 (7.436 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.1534 loss -1.1534 (7.140 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.1201 loss -1.1201 (7.339 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.0449 loss -1.0449 (7.207 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.1027 loss -1.1027 (7.244 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.0429 loss -1.0429 (7.267 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.0456 loss -1.0456 (7.226 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.1973 loss -1.1973 (7.355 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.0922 loss -1.0922 (7.256 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.1574 loss -1.1574 (7.317 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.1480 loss -1.1480 (7.281 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.1146 loss -1.1146 (7.339 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.1054 loss -1.1054 (7.268 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.1422 loss -1.1422 (7.325 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.1078 loss -1.1078 (7.290 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.0486 loss -1.0486 (7.202 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.0955 loss -1.0955 (7.432 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 85.88it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 1.1255 loss -1.1255 (34.934 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.2029 loss -1.2029 (7.455 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.0822 loss -1.0822 (7.385 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.1126 loss -1.1126 (7.174 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.1505 loss -1.1505 (7.270 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.0897 loss -1.0897 (7.287 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.0530 loss -1.0530 (7.353 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.1515 loss -1.1515 (7.259 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.1664 loss -1.1664 (7.395 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.0969 loss -1.0969 (7.221 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.1194 loss -1.1194 (7.338 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.1404 loss -1.1404 (7.395 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.1339 loss -1.1339 (7.396 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.1366 loss -1.1366 (7.424 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.0792 loss -1.0792 (7.391 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.1300 loss -1.1300 (7.278 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.1175 loss -1.1175 (7.448 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.0816 loss -1.0816 (7.386 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.1609 loss -1.1609 (7.409 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.1110 loss -1.1110 (7.374 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.1526 loss -1.1526 (7.340 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.1636 loss -1.1636 (7.476 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.1431 loss -1.1431 (7.264 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.1616 loss -1.1616 (7.298 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.1568 loss -1.1568 (7.292 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.0978 loss -1.0978 (7.216 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.40it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 1.1562 loss -1.1562 (34.724 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.1277 loss -1.1277 (7.456 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.1464 loss -1.1464 (7.217 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.1038 loss -1.1038 (7.288 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.1836 loss -1.1836 (7.116 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.0887 loss -1.0887 (7.285 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.1096 loss -1.1096 (7.412 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2222 loss -1.2222 (7.231 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.1704 loss -1.1704 (7.306 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.0654 loss -1.0654 (7.245 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2021 loss -1.2021 (7.307 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.0494 loss -1.0494 (7.331 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.2025 loss -1.2025 (7.345 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.1734 loss -1.1734 (7.354 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.2192 loss -1.2192 (7.249 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.1175 loss -1.1175 (7.482 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.0837 loss -1.0837 (7.362 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.1132 loss -1.1132 (7.518 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.1315 loss -1.1315 (7.215 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.1869 loss -1.1869 (7.437 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.1261 loss -1.1261 (7.227 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.0565 loss -1.0565 (7.422 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.1776 loss -1.1776 (7.300 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.1760 loss -1.1760 (7.240 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.1465 loss -1.1465 (7.364 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.1739 loss -1.1739 (7.134 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 85.69it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 1.1623 loss -1.1623 (35.012 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.1204 loss -1.1204 (7.355 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.1606 loss -1.1606 (7.554 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.2018 loss -1.2018 (7.332 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.1440 loss -1.1440 (7.415 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.0956 loss -1.0956 (7.495 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.1846 loss -1.1846 (7.544 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.0748 loss -1.0748 (7.393 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.0574 loss -1.0574 (7.399 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.1129 loss -1.1129 (7.427 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.1248 loss -1.1248 (7.366 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.1440 loss -1.1440 (7.391 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.1359 loss -1.1359 (7.229 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.1156 loss -1.1156 (7.382 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.1513 loss -1.1513 (7.199 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.1560 loss -1.1560 (7.265 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.1697 loss -1.1697 (7.652 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.1078 loss -1.1078 (7.078 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.0721 loss -1.0721 (7.346 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.1904 loss -1.1904 (7.347 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.1494 loss -1.1494 (7.320 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.2118 loss -1.2118 (7.327 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.1603 loss -1.1603 (7.355 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.1089 loss -1.1089 (7.362 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.1767 loss -1.1767 (7.299 secs)\n",
      "isanp2:isanp2-num_latents-256_rbf_100 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.1979 loss -1.1979 (7.254 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.40it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 1.1646 loss -1.1646 (34.722 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.81it/s]\n",
      "isanp2:isanp2-num_latents-256_rbf_100 rbf tar_ll 1.1646 loss -1.1646 (36.232 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4505499.0 miliseconds\n",
      "Execution time: 4505.499 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 193.22021484375 MB\n",
      "Memory Usage Change: 176.97021484375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp2', name='isanp2-num_latents-256_rbf_100',val_seed=100, val_l=256,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be420458-4c36-4fa4-aa3f-9034045c2cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2740730e-2851-41f1-b9b6-9b66e5a92faf",
   "metadata": {},
   "source": [
    "## LBANP (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61bc957-6d3d-49ba-bb8c-015704e0685c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: lbanp-lbanp-num_latents-8_rbf_0\n",
      "Total number of parameters: 784834\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_rbf_0 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6998 loss 0.6998 (7.957 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 400 lr 5.000e-04 [train_loss] tar_ll -0.6550 loss 0.6550 (8.227 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 600 lr 5.000e-04 [train_loss] tar_ll -0.5798 loss 0.5798 (7.973 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 800 lr 4.999e-04 [train_loss] tar_ll -0.6080 loss 0.6080 (8.196 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 1000 lr 4.999e-04 [train_loss] tar_ll -0.6241 loss 0.6241 (7.825 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.6352 loss 0.6352 (8.088 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 1400 lr 4.998e-04 [train_loss] tar_ll -0.5010 loss 0.5010 (7.728 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 1600 lr 4.997e-04 [train_loss] tar_ll -0.4618 loss 0.4618 (8.119 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 1800 lr 4.996e-04 [train_loss] tar_ll -0.3707 loss 0.3707 (7.775 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 2000 lr 4.995e-04 [train_loss] tar_ll -0.2690 loss 0.2690 (7.989 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 2200 lr 4.994e-04 [train_loss] tar_ll -0.1124 loss 0.1124 (7.612 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 2400 lr 4.993e-04 [train_loss] tar_ll -0.0945 loss 0.0945 (8.008 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 2600 lr 4.992e-04 [train_loss] tar_ll -0.0551 loss 0.0551 (7.739 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.0124 loss -0.0124 (8.190 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.0995 loss -0.0995 (8.733 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.0375 loss -0.0375 (8.647 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.0614 loss -0.0614 (8.640 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.1048 loss -0.1048 (8.545 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.1073 loss -0.1073 (7.682 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.1285 loss -0.1285 (7.994 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.1520 loss -0.1520 (7.845 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.1375 loss -0.1375 (8.205 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.2428 loss -0.2428 (7.754 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.2388 loss -0.2388 (8.259 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.2135 loss -0.2135 (7.801 secs)\n",
      "100%|##########| 3000/3000 [00:40<00:00, 74.24it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 0.1776 loss -0.1776 (40.409 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.2416 loss -0.2416 (8.133 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.2156 loss -0.2156 (8.280 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.1748 loss -0.1748 (8.714 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.2968 loss -0.2968 (8.221 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.2852 loss -0.2852 (8.442 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.2943 loss -0.2943 (8.192 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.2434 loss -0.2434 (8.279 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.2413 loss -0.2413 (7.795 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.3096 loss -0.3096 (8.136 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.3035 loss -0.3035 (7.732 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.3427 loss -0.3427 (8.129 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.3668 loss -0.3668 (7.735 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.3082 loss -0.3082 (7.930 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.3661 loss -0.3661 (7.802 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.4215 loss -0.4215 (8.322 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.4113 loss -0.4113 (7.822 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.3576 loss -0.3576 (8.020 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.3340 loss -0.3340 (8.012 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.4523 loss -0.4523 (8.580 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.4217 loss -0.4217 (8.290 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.4069 loss -0.4069 (8.564 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.3406 loss -0.3406 (8.364 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.1266 loss -0.1266 (8.722 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.3271 loss -0.3271 (7.842 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.4092 loss -0.4092 (7.871 secs)\n",
      "100%|##########| 3000/3000 [00:42<00:00, 71.15it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 0.4330 loss -0.4330 (42.166 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.4504 loss -0.4504 (8.928 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.4589 loss -0.4589 (8.696 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.3651 loss -0.3651 (8.675 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.4907 loss -0.4907 (8.731 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.4857 loss -0.4857 (8.447 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.4243 loss -0.4243 (8.853 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.4650 loss -0.4650 (7.630 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.4163 loss -0.4163 (8.048 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.4697 loss -0.4697 (7.529 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.4839 loss -0.4839 (7.988 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.4886 loss -0.4886 (8.131 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.4344 loss -0.4344 (8.119 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.5515 loss -0.5515 (8.315 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.5508 loss -0.5508 (8.255 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.5895 loss -0.5895 (7.875 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.5190 loss -0.5190 (8.008 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.4909 loss -0.4909 (7.831 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.4855 loss -0.4855 (8.849 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.4425 loss -0.4425 (8.381 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.4632 loss -0.4632 (8.389 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.4397 loss -0.4397 (8.290 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.5752 loss -0.5752 (8.229 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.5292 loss -0.5292 (7.564 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.4960 loss -0.4960 (7.872 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.4271 loss -0.4271 (7.577 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 72.05it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 0.5287 loss -0.5287 (41.641 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.4624 loss -0.4624 (8.472 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.3717 loss -0.3717 (8.504 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.4811 loss -0.4811 (8.792 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.5425 loss -0.5425 (8.209 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.5255 loss -0.5255 (8.058 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.6164 loss -0.6164 (8.008 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.5674 loss -0.5674 (8.559 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.6189 loss -0.6189 (8.316 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.4105 loss -0.4105 (8.954 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.3590 loss -0.3590 (8.272 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.3820 loss -0.3820 (8.851 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.5285 loss -0.5285 (8.339 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.5467 loss -0.5467 (8.298 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.6102 loss -0.6102 (8.258 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.5541 loss -0.5541 (8.611 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.5235 loss -0.5235 (7.783 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.5633 loss -0.5633 (8.032 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.4908 loss -0.4908 (7.656 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.5552 loss -0.5552 (8.066 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.5575 loss -0.5575 (8.143 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.3831 loss -0.3831 (8.126 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.5652 loss -0.5652 (7.930 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.5526 loss -0.5526 (7.867 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.5195 loss -0.5195 (8.083 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.6138 loss -0.6138 (8.382 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 71.72it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 0.5944 loss -0.5944 (41.831 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.5768 loss -0.5768 (8.468 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.5464 loss -0.5464 (8.476 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.6375 loss -0.6375 (8.041 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.5858 loss -0.5858 (8.046 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.6026 loss -0.6026 (8.226 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.5619 loss -0.5619 (8.674 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.5618 loss -0.5618 (8.088 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.6187 loss -0.6187 (8.150 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.5766 loss -0.5766 (8.087 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.6130 loss -0.6130 (8.056 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.6553 loss -0.6553 (8.085 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.6436 loss -0.6436 (8.287 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.6547 loss -0.6547 (7.783 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.6520 loss -0.6520 (8.529 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.6478 loss -0.6478 (7.946 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.5954 loss -0.5954 (8.270 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.5618 loss -0.5618 (8.483 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.6143 loss -0.6143 (8.255 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.5698 loss -0.5698 (8.571 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.6698 loss -0.6698 (8.701 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.6714 loss -0.6714 (8.399 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.6612 loss -0.6612 (8.268 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.6371 loss -0.6371 (8.573 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.5968 loss -0.5968 (8.648 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.6703 loss -0.6703 (8.792 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 72.48it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 0.7315 loss -0.7315 (41.396 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.6013 loss -0.6013 (8.168 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.6626 loss -0.6626 (8.526 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.5969 loss -0.5969 (8.147 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.6556 loss -0.6556 (7.746 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.6727 loss -0.6727 (8.218 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.6793 loss -0.6793 (8.034 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.6151 loss -0.6151 (8.138 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.7728 loss -0.7728 (7.998 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.6827 loss -0.6827 (8.317 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.6028 loss -0.6028 (8.255 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.7178 loss -0.7178 (8.630 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.7135 loss -0.7135 (8.265 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.6579 loss -0.6579 (8.446 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.6381 loss -0.6381 (8.162 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.6520 loss -0.6520 (8.272 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.6531 loss -0.6531 (8.029 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.7327 loss -0.7327 (8.206 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.7179 loss -0.7179 (8.209 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.6672 loss -0.6672 (8.584 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.7661 loss -0.7661 (8.085 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.7361 loss -0.7361 (8.578 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.7236 loss -0.7236 (7.973 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.7270 loss -0.7270 (8.249 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.6538 loss -0.6538 (8.257 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.8143 loss -0.8143 (8.348 secs)\n",
      "100%|##########| 3000/3000 [00:42<00:00, 71.33it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 0.7273 loss -0.7273 (42.059 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.7044 loss -0.7044 (8.482 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.6616 loss -0.6616 (8.488 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.7717 loss -0.7717 (8.049 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.7936 loss -0.7936 (8.368 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.6939 loss -0.6939 (7.923 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.7669 loss -0.7669 (8.502 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.7676 loss -0.7676 (7.790 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.6829 loss -0.6829 (8.572 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.7163 loss -0.7163 (8.910 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.7523 loss -0.7523 (9.240 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.7630 loss -0.7630 (8.530 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.7614 loss -0.7614 (8.317 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.7588 loss -0.7588 (7.768 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.6662 loss -0.6662 (8.477 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.7413 loss -0.7413 (8.165 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.7430 loss -0.7430 (8.097 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.6504 loss -0.6504 (8.126 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.6830 loss -0.6830 (8.140 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.7654 loss -0.7654 (7.984 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.8546 loss -0.8546 (8.932 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.7878 loss -0.7878 (8.603 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.7481 loss -0.7481 (8.705 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.7092 loss -0.7092 (8.716 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.8127 loss -0.8127 (8.234 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.8137 loss -0.8137 (8.919 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 71.70it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 0.7537 loss -0.7537 (41.846 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.8456 loss -0.8456 (8.485 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.7909 loss -0.7909 (8.462 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.7860 loss -0.7860 (8.420 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.6733 loss -0.6733 (8.341 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.7842 loss -0.7842 (8.190 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.8250 loss -0.8250 (8.675 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.7369 loss -0.7369 (8.329 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.7864 loss -0.7864 (8.478 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.8255 loss -0.8255 (8.512 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.7229 loss -0.7229 (9.139 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.6854 loss -0.6854 (8.471 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.6927 loss -0.6927 (8.849 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.7970 loss -0.7970 (8.624 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.8221 loss -0.8221 (9.031 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.7730 loss -0.7730 (8.623 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.7986 loss -0.7986 (8.926 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.8116 loss -0.8116 (8.410 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.6703 loss -0.6703 (8.290 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.7491 loss -0.7491 (9.093 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.8064 loss -0.8064 (8.997 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.7619 loss -0.7619 (8.700 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.7752 loss -0.7752 (8.777 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.8119 loss -0.8119 (8.830 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.7906 loss -0.7906 (8.985 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.7830 loss -0.7830 (8.968 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 71.95it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 0.7961 loss -0.7961 (41.698 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.7681 loss -0.7681 (7.816 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.7729 loss -0.7729 (8.079 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.7282 loss -0.7282 (8.548 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.7766 loss -0.7766 (8.450 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.8561 loss -0.8561 (8.316 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.8689 loss -0.8689 (8.772 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.8536 loss -0.8536 (8.314 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.8674 loss -0.8674 (8.673 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.7271 loss -0.7271 (8.076 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.9214 loss -0.9214 (8.538 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.8044 loss -0.8044 (8.005 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.8693 loss -0.8693 (8.221 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 42600 lr 3.076e-04 [train_loss] tar_ll 0.7910 loss -0.7910 (7.872 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.8027 loss -0.8027 (8.580 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.7980 loss -0.7980 (8.423 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.9365 loss -0.9365 (8.505 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.8022 loss -0.8022 (7.868 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.7804 loss -0.7804 (7.991 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.8955 loss -0.8955 (7.828 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.8528 loss -0.8528 (8.468 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.8033 loss -0.8033 (8.532 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.7612 loss -0.7612 (8.949 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.8533 loss -0.8533 (8.326 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.9117 loss -0.9117 (7.881 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.8434 loss -0.8434 (8.170 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 72.39it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 0.8954 loss -0.8954 (41.445 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.7757 loss -0.7757 (8.460 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.8586 loss -0.8586 (8.051 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.8947 loss -0.8947 (8.425 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.9027 loss -0.9027 (8.338 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.9381 loss -0.9381 (8.355 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.9281 loss -0.9281 (7.717 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.9591 loss -0.9591 (8.168 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.8230 loss -0.8230 (7.833 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.9032 loss -0.9032 (8.170 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.8300 loss -0.8300 (7.868 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.8346 loss -0.8346 (8.167 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.8635 loss -0.8635 (7.773 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 47600 lr 2.688e-04 [train_loss] tar_ll 0.8480 loss -0.8480 (8.309 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.9168 loss -0.9168 (7.997 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.8858 loss -0.8858 (9.094 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.8729 loss -0.8729 (8.376 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 48400 lr 2.626e-04 [train_loss] tar_ll 0.8654 loss -0.8654 (9.020 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.8070 loss -0.8070 (8.684 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 48800 lr 2.594e-04 [train_loss] tar_ll 0.8713 loss -0.8713 (9.063 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.8512 loss -0.8512 (8.478 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 49200 lr 2.563e-04 [train_loss] tar_ll 0.8734 loss -0.8734 (8.790 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.8989 loss -0.8989 (9.007 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 49600 lr 2.531e-04 [train_loss] tar_ll 0.8752 loss -0.8752 (8.448 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.9342 loss -0.9342 (8.096 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 50000 lr 2.500e-04 [train_loss] tar_ll 0.9406 loss -0.9406 (7.855 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 72.36it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 0.9137 loss -0.9137 (41.463 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 50200 lr 2.484e-04 [train_loss] tar_ll 0.8510 loss -0.8510 (8.353 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 50400 lr 2.469e-04 [train_loss] tar_ll 0.9769 loss -0.9769 (8.398 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 50600 lr 2.453e-04 [train_loss] tar_ll 0.8803 loss -0.8803 (8.437 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 50800 lr 2.437e-04 [train_loss] tar_ll 0.8728 loss -0.8728 (8.450 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 51000 lr 2.421e-04 [train_loss] tar_ll 0.9218 loss -0.9218 (7.927 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 51200 lr 2.406e-04 [train_loss] tar_ll 0.8711 loss -0.8711 (7.978 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 51400 lr 2.390e-04 [train_loss] tar_ll 0.7999 loss -0.7999 (8.086 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 51600 lr 2.374e-04 [train_loss] tar_ll 0.9483 loss -0.9483 (8.480 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 51800 lr 2.359e-04 [train_loss] tar_ll 0.9707 loss -0.9707 (8.421 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 52000 lr 2.343e-04 [train_loss] tar_ll 0.9556 loss -0.9556 (8.324 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 52200 lr 2.327e-04 [train_loss] tar_ll 0.9039 loss -0.9039 (8.810 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 52400 lr 2.312e-04 [train_loss] tar_ll 0.8448 loss -0.8448 (8.546 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 52600 lr 2.296e-04 [train_loss] tar_ll 0.9268 loss -0.9268 (8.126 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 52800 lr 2.280e-04 [train_loss] tar_ll 0.9278 loss -0.9278 (7.900 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 53000 lr 2.265e-04 [train_loss] tar_ll 0.9737 loss -0.9737 (8.193 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 53200 lr 2.249e-04 [train_loss] tar_ll 0.9583 loss -0.9583 (7.954 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 53400 lr 2.233e-04 [train_loss] tar_ll 0.8856 loss -0.8856 (8.111 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 53600 lr 2.218e-04 [train_loss] tar_ll 1.0331 loss -1.0331 (7.904 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 53800 lr 2.202e-04 [train_loss] tar_ll 0.9166 loss -0.9166 (8.158 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 54000 lr 2.187e-04 [train_loss] tar_ll 0.9570 loss -0.9570 (8.893 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 54200 lr 2.171e-04 [train_loss] tar_ll 0.8873 loss -0.8873 (8.003 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 54400 lr 2.156e-04 [train_loss] tar_ll 0.9992 loss -0.9992 (7.684 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 54600 lr 2.140e-04 [train_loss] tar_ll 0.9335 loss -0.9335 (7.952 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 54800 lr 2.124e-04 [train_loss] tar_ll 0.9848 loss -0.9848 (7.815 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 55000 lr 2.109e-04 [train_loss] tar_ll 0.9535 loss -0.9535 (7.965 secs)\n",
      "100%|##########| 3000/3000 [00:39<00:00, 75.10it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 0.9288 loss -0.9288 (39.948 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 55200 lr 2.093e-04 [train_loss] tar_ll 0.9697 loss -0.9697 (8.204 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 55400 lr 2.078e-04 [train_loss] tar_ll 0.9824 loss -0.9824 (7.788 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 55600 lr 2.062e-04 [train_loss] tar_ll 0.9813 loss -0.9813 (8.597 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 55800 lr 2.047e-04 [train_loss] tar_ll 0.8847 loss -0.8847 (8.303 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 56000 lr 2.032e-04 [train_loss] tar_ll 0.9995 loss -0.9995 (8.207 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 56200 lr 2.016e-04 [train_loss] tar_ll 0.9913 loss -0.9913 (7.931 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 56400 lr 2.001e-04 [train_loss] tar_ll 0.9508 loss -0.9508 (8.200 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 56600 lr 1.985e-04 [train_loss] tar_ll 1.0144 loss -1.0144 (8.300 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 56800 lr 1.970e-04 [train_loss] tar_ll 0.9890 loss -0.9890 (8.841 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 57000 lr 1.955e-04 [train_loss] tar_ll 0.9739 loss -0.9739 (8.209 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 57200 lr 1.939e-04 [train_loss] tar_ll 1.0041 loss -1.0041 (7.993 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 57400 lr 1.924e-04 [train_loss] tar_ll 1.0385 loss -1.0385 (8.509 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 57600 lr 1.909e-04 [train_loss] tar_ll 1.0100 loss -1.0100 (8.316 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 57800 lr 1.894e-04 [train_loss] tar_ll 0.9373 loss -0.9373 (8.417 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 58000 lr 1.878e-04 [train_loss] tar_ll 1.0520 loss -1.0520 (8.406 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 58200 lr 1.863e-04 [train_loss] tar_ll 0.9918 loss -0.9918 (10.398 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 58400 lr 1.848e-04 [train_loss] tar_ll 0.9537 loss -0.9537 (9.236 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 58600 lr 1.833e-04 [train_loss] tar_ll 0.9844 loss -0.9844 (9.194 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 58800 lr 1.818e-04 [train_loss] tar_ll 1.0082 loss -1.0082 (8.712 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 59000 lr 1.803e-04 [train_loss] tar_ll 0.9671 loss -0.9671 (8.429 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 59200 lr 1.787e-04 [train_loss] tar_ll 0.9684 loss -0.9684 (8.183 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 59400 lr 1.772e-04 [train_loss] tar_ll 1.0940 loss -1.0940 (9.017 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 59600 lr 1.757e-04 [train_loss] tar_ll 0.9295 loss -0.9295 (8.798 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 59800 lr 1.742e-04 [train_loss] tar_ll 1.0198 loss -1.0198 (8.206 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 60000 lr 1.727e-04 [train_loss] tar_ll 0.9497 loss -0.9497 (8.009 secs)\n",
      "100%|##########| 3000/3000 [00:40<00:00, 73.30it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 1.0397 loss -1.0397 (40.929 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 60200 lr 1.713e-04 [train_loss] tar_ll 0.9523 loss -0.9523 (9.041 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 60400 lr 1.698e-04 [train_loss] tar_ll 0.9467 loss -0.9467 (8.305 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.0807 loss -1.0807 (8.705 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 60800 lr 1.668e-04 [train_loss] tar_ll 0.9178 loss -0.9178 (8.308 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 61000 lr 1.653e-04 [train_loss] tar_ll 1.0080 loss -1.0080 (9.106 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 61200 lr 1.638e-04 [train_loss] tar_ll 0.9947 loss -0.9947 (7.840 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 61400 lr 1.624e-04 [train_loss] tar_ll 0.9508 loss -0.9508 (8.117 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 61600 lr 1.609e-04 [train_loss] tar_ll 0.9784 loss -0.9784 (8.054 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.0369 loss -1.0369 (8.107 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 62000 lr 1.580e-04 [train_loss] tar_ll 0.9591 loss -0.9591 (7.769 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 62200 lr 1.565e-04 [train_loss] tar_ll 0.9834 loss -0.9834 (8.231 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.0428 loss -1.0428 (8.365 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.0323 loss -1.0323 (9.106 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.0424 loss -1.0424 (8.900 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.0068 loss -1.0068 (9.082 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.0296 loss -1.0296 (8.319 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.0207 loss -1.0207 (8.751 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.0111 loss -1.0111 (8.736 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.0849 loss -1.0849 (7.949 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.0000 loss -1.0000 (8.787 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.0773 loss -1.0773 (8.540 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 64400 lr 1.407e-04 [train_loss] tar_ll 0.9679 loss -0.9679 (8.780 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.0449 loss -1.0449 (7.915 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.0561 loss -1.0561 (8.859 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 65000 lr 1.365e-04 [train_loss] tar_ll 0.9673 loss -0.9673 (8.139 secs)\n",
      "100%|##########| 3000/3000 [00:42<00:00, 70.44it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 1.0476 loss -1.0476 (42.592 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.1151 loss -1.1151 (8.795 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.0400 loss -1.0400 (8.404 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.0298 loss -1.0298 (8.675 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 65800 lr 1.309e-04 [train_loss] tar_ll 0.9446 loss -0.9446 (8.587 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.0209 loss -1.0209 (8.306 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.0493 loss -1.0493 (8.399 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.0976 loss -1.0976 (8.266 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.0416 loss -1.0416 (7.618 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.0908 loss -1.0908 (8.579 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.1289 loss -1.1289 (8.851 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.0226 loss -1.0226 (8.643 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.0832 loss -1.0832 (7.993 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.0306 loss -1.0306 (8.276 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.0537 loss -1.0537 (8.942 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.1052 loss -1.1052 (8.945 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.0846 loss -1.0846 (8.520 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.0582 loss -1.0582 (8.715 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.0812 loss -1.0812 (8.628 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.1130 loss -1.1130 (7.791 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.0539 loss -1.0539 (8.010 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.0767 loss -1.0767 (7.729 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.0304 loss -1.0304 (7.815 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.0919 loss -1.0919 (7.500 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.0221 loss -1.0221 (7.858 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 70000 lr 1.031e-04 [train_loss] tar_ll 0.9669 loss -0.9669 (8.333 secs)\n",
      "100%|##########| 3000/3000 [00:43<00:00, 68.83it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 1.0908 loss -1.0908 (43.588 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.0898 loss -1.0898 (8.519 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.0560 loss -1.0560 (8.186 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.1348 loss -1.1348 (8.440 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.0802 loss -1.0802 (8.321 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.0253 loss -1.0253 (7.802 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.0440 loss -1.0440 (8.343 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.0568 loss -1.0568 (7.929 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.1203 loss -1.1203 (8.104 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 71800 lr 9.186e-05 [train_loss] tar_ll 0.9795 loss -0.9795 (7.996 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.1084 loss -1.1084 (8.516 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.0685 loss -1.0685 (8.008 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.0962 loss -1.0962 (8.738 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.1485 loss -1.1485 (8.540 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.1239 loss -1.1239 (8.098 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.0319 loss -1.0319 (8.674 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.0543 loss -1.0543 (8.776 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.1236 loss -1.1236 (8.752 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.0816 loss -1.0816 (8.265 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.0800 loss -1.0800 (8.742 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.0433 loss -1.0433 (8.164 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.0968 loss -1.0968 (8.606 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.0595 loss -1.0595 (8.225 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.1223 loss -1.1223 (8.626 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.0870 loss -1.0870 (8.311 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1023 loss -1.1023 (8.619 secs)\n",
      "100%|##########| 3000/3000 [00:44<00:00, 67.69it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 1.1150 loss -1.1150 (44.319 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.0306 loss -1.0306 (8.217 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.1767 loss -1.1767 (8.530 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.1832 loss -1.1832 (8.118 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.0969 loss -1.0969 (10.668 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.1183 loss -1.1183 (8.883 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.1592 loss -1.1592 (8.036 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.0988 loss -1.0988 (8.592 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.1321 loss -1.1321 (7.944 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.0997 loss -1.0997 (8.501 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.0890 loss -1.0890 (8.180 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.0891 loss -1.0891 (8.944 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.1604 loss -1.1604 (8.120 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.1326 loss -1.1326 (8.643 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.0422 loss -1.0422 (8.427 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.1259 loss -1.1259 (8.504 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.1642 loss -1.1642 (8.248 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.1024 loss -1.1024 (8.320 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.0388 loss -1.0388 (8.587 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.2153 loss -1.2153 (8.399 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.0986 loss -1.0986 (8.198 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.1095 loss -1.1095 (8.395 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.1277 loss -1.1277 (8.165 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.0770 loss -1.0770 (8.472 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.1559 loss -1.1559 (8.124 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.1464 loss -1.1464 (8.365 secs)\n",
      "100%|##########| 3000/3000 [00:42<00:00, 70.37it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 1.1491 loss -1.1491 (42.633 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.1256 loss -1.1256 (7.794 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.1575 loss -1.1575 (8.227 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1453 loss -1.1453 (8.243 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2045 loss -1.2045 (8.230 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.1672 loss -1.1672 (8.039 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.1920 loss -1.1920 (9.174 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.1374 loss -1.1374 (8.505 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.0699 loss -1.0699 (8.659 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.1867 loss -1.1867 (8.974 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.1249 loss -1.1249 (8.462 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.1745 loss -1.1745 (7.968 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.1693 loss -1.1693 (8.234 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.1017 loss -1.1017 (7.736 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.0394 loss -1.0394 (8.112 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.1939 loss -1.1939 (8.000 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.1112 loss -1.1112 (8.144 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.1835 loss -1.1835 (8.100 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.1144 loss -1.1144 (8.986 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.1722 loss -1.1722 (7.855 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.0188 loss -1.0188 (7.973 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.1036 loss -1.1036 (8.207 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.2503 loss -1.2503 (8.027 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.1192 loss -1.1192 (8.882 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.1949 loss -1.1949 (9.212 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.1565 loss -1.1565 (8.342 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 73.17it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 1.1786 loss -1.1786 (41.004 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.1814 loss -1.1814 (7.904 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.1096 loss -1.1096 (8.217 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.2117 loss -1.2117 (7.882 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.2272 loss -1.2272 (7.792 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.2076 loss -1.2076 (8.272 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.1392 loss -1.1392 (8.092 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.1822 loss -1.1822 (8.016 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.0298 loss -1.0298 (8.203 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.2374 loss -1.2374 (8.113 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.1634 loss -1.1634 (7.955 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.1590 loss -1.1590 (8.357 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.1675 loss -1.1675 (8.354 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.1822 loss -1.1822 (8.340 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.2036 loss -1.2036 (7.914 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.1885 loss -1.1885 (8.042 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.1156 loss -1.1156 (7.911 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.1364 loss -1.1364 (8.137 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.2342 loss -1.2342 (8.440 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.1996 loss -1.1996 (8.717 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.2549 loss -1.2549 (8.216 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.2429 loss -1.2429 (8.281 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.1615 loss -1.1615 (9.110 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.1980 loss -1.1980 (8.466 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2082 loss -1.2082 (8.209 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.1935 loss -1.1935 (8.070 secs)\n",
      "100%|##########| 3000/3000 [00:40<00:00, 73.61it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 1.1864 loss -1.1864 (40.756 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.1461 loss -1.1461 (8.220 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.2052 loss -1.2052 (8.048 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.1616 loss -1.1616 (7.961 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.1503 loss -1.1503 (8.562 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.1872 loss -1.1872 (8.303 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2348 loss -1.2348 (8.481 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.0918 loss -1.0918 (7.885 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.1527 loss -1.1527 (8.786 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.1423 loss -1.1423 (8.630 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2019 loss -1.2019 (8.610 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.1497 loss -1.1497 (8.725 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.1667 loss -1.1667 (7.977 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.1637 loss -1.1637 (8.050 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.2050 loss -1.2050 (8.279 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.1525 loss -1.1525 (8.325 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.2048 loss -1.2048 (8.165 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.1810 loss -1.1810 (8.088 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.1068 loss -1.1068 (8.343 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.1596 loss -1.1596 (8.139 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.1902 loss -1.1902 (8.209 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.1416 loss -1.1416 (8.228 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.1961 loss -1.1961 (8.326 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.1602 loss -1.1602 (8.576 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.1535 loss -1.1535 (8.332 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.2282 loss -1.2282 (8.412 secs)\n",
      "100%|##########| 3000/3000 [00:42<00:00, 69.82it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 1.1972 loss -1.1972 (42.969 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.1302 loss -1.1302 (8.801 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.1783 loss -1.1783 (9.057 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.1939 loss -1.1939 (8.460 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.1950 loss -1.1950 (9.124 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.1902 loss -1.1902 (8.347 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2216 loss -1.2216 (8.946 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.1667 loss -1.1667 (7.760 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.2232 loss -1.2232 (8.136 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.1745 loss -1.1745 (7.905 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.2173 loss -1.2173 (8.125 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.1317 loss -1.1317 (7.924 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.1811 loss -1.1811 (8.114 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.2044 loss -1.2044 (7.751 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.2098 loss -1.2098 (8.388 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.1139 loss -1.1139 (7.821 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.2144 loss -1.2144 (8.500 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.2074 loss -1.2074 (8.176 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.1450 loss -1.1450 (8.565 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2398 loss -1.2398 (8.356 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.1541 loss -1.1541 (8.441 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.1773 loss -1.1773 (8.165 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.1916 loss -1.1916 (8.517 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.2292 loss -1.2292 (8.071 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.2258 loss -1.2258 (8.197 secs)\n",
      "lbanp:lbanp-num_latents-8_rbf_0 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.2077 loss -1.2077 (7.890 secs)\n",
      "100%|##########| 3000/3000 [00:42<00:00, 70.78it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 1.1993 loss -1.1993 (42.388 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:42<00:00, 70.60it/s]\n",
      "lbanp:lbanp-num_latents-8_rbf_0 rbf tar_ll 1.1993 loss -1.1993 (42.498 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 5056346.0 miliseconds\n",
      "Execution time: 5056.346 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 55.17529296875 MB\n",
      "Memory Usage Change: 38.92529296875 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='lbanp', name='lbanp-num_latents-8_rbf_0',val_seed=0, val_l=8,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83568d76-16ba-4e3e-9fa3-ecbac07218a0",
   "metadata": {},
   "source": [
    "## LBANP (128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "515a5500-9aa1-4934-b421-6a50a876dac5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: lbanp-lbanp-num_latents-128_rbf_0\n",
      "Total number of parameters: 792514\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_rbf_0 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6982 loss 0.6982 (8.954 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 400 lr 5.000e-04 [train_loss] tar_ll -0.6600 loss 0.6600 (8.595 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 600 lr 5.000e-04 [train_loss] tar_ll -0.5784 loss 0.5784 (8.561 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 800 lr 4.999e-04 [train_loss] tar_ll -0.5840 loss 0.5840 (8.683 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 1000 lr 4.999e-04 [train_loss] tar_ll -0.5319 loss 0.5319 (8.977 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 1200 lr 4.998e-04 [train_loss] tar_ll -0.4520 loss 0.4520 (8.665 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 1400 lr 4.998e-04 [train_loss] tar_ll -0.2569 loss 0.2569 (8.689 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 1600 lr 4.997e-04 [train_loss] tar_ll -0.1914 loss 0.1914 (9.838 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 1800 lr 4.996e-04 [train_loss] tar_ll -0.1037 loss 0.1037 (10.058 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 2000 lr 4.995e-04 [train_loss] tar_ll -0.0688 loss 0.0688 (8.404 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.0878 loss -0.0878 (9.375 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.0915 loss -0.0915 (9.373 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1347 loss -0.1347 (8.457 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.1671 loss -0.1671 (9.116 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.2506 loss -0.2506 (9.257 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.0467 loss -0.0467 (9.108 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.2207 loss -0.2207 (8.610 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2733 loss -0.2733 (9.217 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.2708 loss -0.2708 (8.002 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.3313 loss -0.3313 (8.385 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.2914 loss -0.2914 (8.353 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.3225 loss -0.3225 (8.297 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4308 loss -0.4308 (8.549 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.3946 loss -0.3946 (8.273 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.3835 loss -0.3835 (8.498 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 71.85it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 0.3083 loss -0.3083 (41.755 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.3772 loss -0.3772 (8.720 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.4230 loss -0.4230 (8.550 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4100 loss -0.4100 (8.574 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.3994 loss -0.3994 (8.719 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.3592 loss -0.3592 (8.503 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.4299 loss -0.4299 (8.057 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.4917 loss -0.4917 (8.415 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.3914 loss -0.3914 (8.280 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.5054 loss -0.5054 (8.338 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.4754 loss -0.4754 (9.111 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.5102 loss -0.5102 (9.097 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5236 loss -0.5236 (8.595 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.4197 loss -0.4197 (8.670 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.4974 loss -0.4974 (8.583 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.5956 loss -0.5956 (8.681 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.4878 loss -0.4878 (8.694 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.5661 loss -0.5661 (8.115 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.6129 loss -0.6129 (8.137 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.5677 loss -0.5677 (8.518 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.5429 loss -0.5429 (8.604 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.5801 loss -0.5801 (9.266 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.5643 loss -0.5643 (8.440 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.6760 loss -0.6760 (8.846 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.6423 loss -0.6423 (8.553 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.5827 loss -0.5827 (8.581 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 73.14it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 0.6781 loss -0.6781 (41.021 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.6124 loss -0.6124 (8.301 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.5600 loss -0.5600 (7.996 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.6979 loss -0.6979 (8.481 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.6795 loss -0.6795 (8.033 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.5708 loss -0.5708 (8.323 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.6207 loss -0.6207 (8.702 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.5968 loss -0.5968 (8.512 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.6738 loss -0.6738 (8.303 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.7022 loss -0.7022 (8.924 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.8068 loss -0.8068 (8.639 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.2183 loss -0.2183 (8.830 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.5246 loss -0.5246 (8.886 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.5748 loss -0.5748 (8.761 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.6809 loss -0.6809 (8.662 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.6627 loss -0.6627 (8.040 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.7416 loss -0.7416 (8.392 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.7359 loss -0.7359 (8.413 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.5875 loss -0.5875 (8.515 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.7178 loss -0.7178 (7.970 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.6299 loss -0.6299 (8.340 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.5772 loss -0.5772 (8.269 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.6225 loss -0.6225 (8.408 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.6159 loss -0.6159 (8.181 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.7448 loss -0.7448 (8.777 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.6943 loss -0.6943 (8.660 secs)\n",
      "100%|##########| 3000/3000 [00:42<00:00, 69.84it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 0.7612 loss -0.7612 (42.957 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.7617 loss -0.7617 (9.381 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.6572 loss -0.6572 (8.566 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.7120 loss -0.7120 (9.155 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.6686 loss -0.6686 (8.780 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.6384 loss -0.6384 (9.270 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.7348 loss -0.7348 (8.851 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.7862 loss -0.7862 (9.125 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.7604 loss -0.7604 (9.312 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.7291 loss -0.7291 (8.026 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.7679 loss -0.7679 (8.435 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.7184 loss -0.7184 (8.089 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.7088 loss -0.7088 (8.369 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.7450 loss -0.7450 (7.981 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.7076 loss -0.7076 (8.710 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.6462 loss -0.6462 (8.110 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.7985 loss -0.7985 (8.339 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.7546 loss -0.7546 (7.939 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.7874 loss -0.7874 (8.555 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.7966 loss -0.7966 (8.497 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.8158 loss -0.8158 (8.601 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.8075 loss -0.8075 (8.990 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.8378 loss -0.8378 (8.818 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.7006 loss -0.7006 (8.958 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.6958 loss -0.6958 (8.865 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.6907 loss -0.6907 (8.808 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 71.55it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 0.8409 loss -0.8409 (41.929 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.6656 loss -0.6656 (8.696 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.2884 loss -0.2884 (8.646 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.5311 loss -0.5311 (8.623 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.6736 loss -0.6736 (8.913 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.6168 loss -0.6168 (8.492 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.7947 loss -0.7947 (8.480 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.7185 loss -0.7185 (8.319 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.7319 loss -0.7319 (8.835 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.7729 loss -0.7729 (8.611 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.7031 loss -0.7031 (9.097 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.7641 loss -0.7641 (8.651 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.5523 loss -0.5523 (8.699 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.4939 loss -0.4939 (8.650 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.5872 loss -0.5872 (8.864 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.7085 loss -0.7085 (9.101 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.6990 loss -0.6990 (9.152 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.7596 loss -0.7596 (8.595 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.8049 loss -0.8049 (8.079 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.8357 loss -0.8357 (8.421 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.8350 loss -0.8350 (8.290 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.7148 loss -0.7148 (8.488 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.8213 loss -0.8213 (8.281 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.7950 loss -0.7950 (8.611 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.6488 loss -0.6488 (8.113 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.8021 loss -0.8021 (8.403 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 71.47it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 0.7708 loss -0.7708 (41.980 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.5820 loss -0.5820 (8.560 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.7591 loss -0.7591 (8.606 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.2216 loss -0.2216 (8.046 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.2525 loss -0.2525 (9.147 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.5131 loss -0.5131 (8.450 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.2806 loss -0.2806 (8.446 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.4535 loss -0.4535 (8.243 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.6416 loss -0.6416 (8.375 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.6041 loss -0.6041 (8.116 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.0393 loss -0.0393 (8.229 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 27200 lr 4.141e-04 [train_loss] tar_ll -0.0058 loss 0.0058 (8.178 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.3463 loss -0.3463 (8.273 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.5612 loss -0.5612 (8.454 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.6832 loss -0.6832 (8.534 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.7553 loss -0.7553 (8.157 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.8085 loss -0.8085 (8.870 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.7853 loss -0.7853 (9.280 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.8069 loss -0.8069 (8.983 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.7158 loss -0.7158 (9.107 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.8228 loss -0.8228 (8.985 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.6485 loss -0.6485 (9.122 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.7524 loss -0.7524 (9.358 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.7942 loss -0.7942 (8.830 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.7419 loss -0.7419 (8.568 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.6964 loss -0.6964 (8.918 secs)\n",
      "100%|##########| 3000/3000 [00:42<00:00, 70.17it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 0.8071 loss -0.8071 (42.756 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.6052 loss -0.6052 (8.688 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.5879 loss -0.5879 (8.895 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.7056 loss -0.7056 (8.794 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.6534 loss -0.6534 (9.295 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.5972 loss -0.5972 (9.002 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.6995 loss -0.6995 (8.106 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.7996 loss -0.7996 (8.278 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.7731 loss -0.7731 (8.082 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.7354 loss -0.7354 (8.434 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.5918 loss -0.5918 (8.102 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.6844 loss -0.6844 (8.732 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.7788 loss -0.7788 (8.405 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.7440 loss -0.7440 (8.954 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.8181 loss -0.8181 (8.715 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.8047 loss -0.8047 (8.514 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.8410 loss -0.8410 (8.305 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.7242 loss -0.7242 (9.071 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.7333 loss -0.7333 (8.632 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.5612 loss -0.5612 (8.521 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.2557 loss -0.2557 (8.461 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.5510 loss -0.5510 (8.762 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.4375 loss -0.4375 (8.374 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.5483 loss -0.5483 (8.279 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.6609 loss -0.6609 (9.473 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.6383 loss -0.6383 (9.085 secs)\n",
      "100%|##########| 3000/3000 [00:42<00:00, 69.78it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 0.2177 loss -0.2177 (42.998 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.5647 loss -0.5647 (8.689 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.7592 loss -0.7592 (8.311 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.8336 loss -0.8336 (8.854 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.7601 loss -0.7601 (8.292 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.7484 loss -0.7484 (8.440 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.9129 loss -0.9129 (8.622 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.8596 loss -0.8596 (8.557 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.8579 loss -0.8579 (8.391 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.7705 loss -0.7705 (8.693 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.6146 loss -0.6146 (9.154 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.7361 loss -0.7361 (8.965 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.7857 loss -0.7857 (8.888 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.4745 loss -0.4745 (8.648 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.3310 loss -0.3310 (8.318 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.6322 loss -0.6322 (8.090 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.6391 loss -0.6391 (8.648 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.6685 loss -0.6685 (8.079 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.6781 loss -0.6781 (8.466 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.5845 loss -0.5845 (8.064 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.2561 loss -0.2561 (8.365 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.6080 loss -0.6080 (8.246 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.6372 loss -0.6372 (8.512 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.7368 loss -0.7368 (8.612 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.6803 loss -0.6803 (8.583 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.5717 loss -0.5717 (8.179 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 71.60it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 0.4795 loss -0.4795 (41.903 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.5363 loss -0.5363 (8.645 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.3260 loss -0.3260 (8.715 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.4285 loss -0.4285 (8.762 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.4994 loss -0.4994 (8.619 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.7691 loss -0.7691 (9.073 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.6398 loss -0.6398 (8.783 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.5261 loss -0.5261 (8.767 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.6379 loss -0.6379 (8.762 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.6197 loss -0.6197 (8.846 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.7984 loss -0.7984 (8.319 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.5891 loss -0.5891 (8.699 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.7961 loss -0.7961 (8.434 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 42600 lr 3.076e-04 [train_loss] tar_ll 0.7607 loss -0.7607 (8.471 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.8249 loss -0.8249 (8.442 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.7991 loss -0.7991 (8.309 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.8298 loss -0.8298 (8.481 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.9823 loss -0.9823 (8.404 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.8832 loss -0.8832 (8.972 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.9070 loss -0.9070 (8.528 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.8577 loss -0.8577 (8.820 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.9378 loss -0.9378 (9.270 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.8201 loss -0.8201 (9.112 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.9215 loss -0.9215 (8.814 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.8750 loss -0.8750 (8.473 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.8698 loss -0.8698 (8.269 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 72.82it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 0.7418 loss -0.7418 (41.201 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.9069 loss -0.9069 (8.234 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.8571 loss -0.8571 (8.029 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.7368 loss -0.7368 (8.391 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.7727 loss -0.7727 (8.084 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.9049 loss -0.9049 (8.231 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.7030 loss -0.7030 (8.617 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.8071 loss -0.8071 (8.547 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.8097 loss -0.8097 (8.354 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.8760 loss -0.8760 (9.116 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.8890 loss -0.8890 (8.435 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.9502 loss -0.9502 (8.542 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.8251 loss -0.8251 (8.076 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 47600 lr 2.688e-04 [train_loss] tar_ll 0.9347 loss -0.9347 (8.794 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.9121 loss -0.9121 (8.656 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.8676 loss -0.8676 (8.067 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.8747 loss -0.8747 (8.426 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 48400 lr 2.626e-04 [train_loss] tar_ll 0.8365 loss -0.8365 (8.276 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.8016 loss -0.8016 (8.700 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 48800 lr 2.594e-04 [train_loss] tar_ll 0.7425 loss -0.7425 (8.418 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.9302 loss -0.9302 (8.657 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 49200 lr 2.563e-04 [train_loss] tar_ll 0.8102 loss -0.8102 (8.548 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.6966 loss -0.6966 (8.837 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 49600 lr 2.531e-04 [train_loss] tar_ll 0.8235 loss -0.8235 (8.540 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.8721 loss -0.8721 (8.476 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 50000 lr 2.500e-04 [train_loss] tar_ll 0.7491 loss -0.7491 (8.057 secs)\n",
      "100%|##########| 3000/3000 [00:42<00:00, 71.21it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 0.8324 loss -0.8324 (42.134 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 50200 lr 2.484e-04 [train_loss] tar_ll 0.6418 loss -0.6418 (8.535 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 50400 lr 2.469e-04 [train_loss] tar_ll 0.7549 loss -0.7549 (8.096 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 50600 lr 2.453e-04 [train_loss] tar_ll 0.7712 loss -0.7712 (8.990 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 50800 lr 2.437e-04 [train_loss] tar_ll 0.7335 loss -0.7335 (8.581 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 51000 lr 2.421e-04 [train_loss] tar_ll 0.7522 loss -0.7522 (9.265 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 51200 lr 2.406e-04 [train_loss] tar_ll 0.7894 loss -0.7894 (9.168 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 51400 lr 2.390e-04 [train_loss] tar_ll 0.7330 loss -0.7330 (9.921 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 51600 lr 2.374e-04 [train_loss] tar_ll 0.8305 loss -0.8305 (9.724 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 51800 lr 2.359e-04 [train_loss] tar_ll 0.8548 loss -0.8548 (8.180 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 52000 lr 2.343e-04 [train_loss] tar_ll 0.7587 loss -0.7587 (8.642 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 52200 lr 2.327e-04 [train_loss] tar_ll 0.8322 loss -0.8322 (8.018 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 52400 lr 2.312e-04 [train_loss] tar_ll 0.7949 loss -0.7949 (8.345 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 52600 lr 2.296e-04 [train_loss] tar_ll 0.9002 loss -0.9002 (7.958 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 52800 lr 2.280e-04 [train_loss] tar_ll 0.9051 loss -0.9051 (8.095 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 53000 lr 2.265e-04 [train_loss] tar_ll 0.8557 loss -0.8557 (8.090 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 53200 lr 2.249e-04 [train_loss] tar_ll 0.9122 loss -0.9122 (8.218 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 53400 lr 2.233e-04 [train_loss] tar_ll 0.9199 loss -0.9199 (8.641 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 53600 lr 2.218e-04 [train_loss] tar_ll 0.9831 loss -0.9831 (8.467 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 53800 lr 2.202e-04 [train_loss] tar_ll 0.8775 loss -0.8775 (7.991 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 54000 lr 2.187e-04 [train_loss] tar_ll 0.9807 loss -0.9807 (8.334 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 54200 lr 2.171e-04 [train_loss] tar_ll 0.9533 loss -0.9533 (8.180 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 54400 lr 2.156e-04 [train_loss] tar_ll 0.9411 loss -0.9411 (8.748 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 54600 lr 2.140e-04 [train_loss] tar_ll 0.8589 loss -0.8589 (7.960 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 54800 lr 2.124e-04 [train_loss] tar_ll 1.0164 loss -1.0164 (8.217 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 55000 lr 2.109e-04 [train_loss] tar_ll 0.8972 loss -0.8972 (7.867 secs)\n",
      "100%|##########| 3000/3000 [00:43<00:00, 68.74it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 0.7923 loss -0.7923 (43.643 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 55200 lr 2.093e-04 [train_loss] tar_ll 0.7429 loss -0.7429 (9.352 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 55400 lr 2.078e-04 [train_loss] tar_ll 0.7194 loss -0.7194 (8.773 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 55600 lr 2.062e-04 [train_loss] tar_ll 0.9217 loss -0.9217 (8.918 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 55800 lr 2.047e-04 [train_loss] tar_ll 0.8996 loss -0.8996 (8.723 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 56000 lr 2.032e-04 [train_loss] tar_ll 0.8573 loss -0.8573 (8.847 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 56200 lr 2.016e-04 [train_loss] tar_ll 0.7618 loss -0.7618 (8.974 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 56400 lr 2.001e-04 [train_loss] tar_ll 0.7910 loss -0.7910 (9.007 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 56600 lr 1.985e-04 [train_loss] tar_ll 0.8896 loss -0.8896 (8.672 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 56800 lr 1.970e-04 [train_loss] tar_ll 0.8370 loss -0.8370 (8.651 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 57000 lr 1.955e-04 [train_loss] tar_ll 0.9256 loss -0.9256 (8.876 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 57200 lr 1.939e-04 [train_loss] tar_ll 0.9439 loss -0.9439 (8.324 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 57400 lr 1.924e-04 [train_loss] tar_ll 0.9057 loss -0.9057 (9.059 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 57600 lr 1.909e-04 [train_loss] tar_ll 0.9283 loss -0.9283 (8.839 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 57800 lr 1.894e-04 [train_loss] tar_ll 1.0167 loss -1.0167 (8.998 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 58000 lr 1.878e-04 [train_loss] tar_ll 0.9670 loss -0.9670 (9.177 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 58200 lr 1.863e-04 [train_loss] tar_ll 0.9928 loss -0.9928 (9.267 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 58400 lr 1.848e-04 [train_loss] tar_ll 0.9259 loss -0.9259 (8.914 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 58600 lr 1.833e-04 [train_loss] tar_ll 0.9277 loss -0.9277 (8.669 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 58800 lr 1.818e-04 [train_loss] tar_ll 0.9607 loss -0.9607 (9.417 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.0755 loss -1.0755 (8.069 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 59200 lr 1.787e-04 [train_loss] tar_ll 1.0715 loss -1.0715 (8.599 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 59400 lr 1.772e-04 [train_loss] tar_ll 1.0688 loss -1.0688 (8.327 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 59600 lr 1.757e-04 [train_loss] tar_ll 1.0592 loss -1.0592 (8.540 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 59800 lr 1.742e-04 [train_loss] tar_ll 0.8664 loss -0.8664 (7.973 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 60000 lr 1.727e-04 [train_loss] tar_ll 0.9262 loss -0.9262 (8.211 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 71.94it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 1.0334 loss -1.0334 (41.707 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 60200 lr 1.713e-04 [train_loss] tar_ll 1.0052 loss -1.0052 (8.865 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 60400 lr 1.698e-04 [train_loss] tar_ll 0.9673 loss -0.9673 (8.359 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.0559 loss -1.0559 (8.608 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.0966 loss -1.0966 (8.944 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 61000 lr 1.653e-04 [train_loss] tar_ll 1.0340 loss -1.0340 (8.095 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 61200 lr 1.638e-04 [train_loss] tar_ll 0.9451 loss -0.9451 (8.742 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 61400 lr 1.624e-04 [train_loss] tar_ll 1.0439 loss -1.0439 (8.492 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 61600 lr 1.609e-04 [train_loss] tar_ll 0.9768 loss -0.9768 (8.376 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 61800 lr 1.594e-04 [train_loss] tar_ll 0.9740 loss -0.9740 (8.366 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 62000 lr 1.580e-04 [train_loss] tar_ll 1.1171 loss -1.1171 (8.450 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 62200 lr 1.565e-04 [train_loss] tar_ll 1.0648 loss -1.0648 (8.187 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.0222 loss -1.0222 (8.406 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.1170 loss -1.1170 (8.392 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.0901 loss -1.0901 (8.867 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.0676 loss -1.0676 (8.124 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 63200 lr 1.493e-04 [train_loss] tar_ll 0.9733 loss -0.9733 (8.801 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 63400 lr 1.478e-04 [train_loss] tar_ll 0.9984 loss -0.9984 (8.697 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 63600 lr 1.464e-04 [train_loss] tar_ll 0.9544 loss -0.9544 (9.082 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 63800 lr 1.450e-04 [train_loss] tar_ll 0.9869 loss -0.9869 (9.436 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.0466 loss -1.0466 (8.403 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 64200 lr 1.421e-04 [train_loss] tar_ll 0.9874 loss -0.9874 (9.262 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.0488 loss -1.0488 (8.542 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 64600 lr 1.393e-04 [train_loss] tar_ll 0.9760 loss -0.9760 (8.317 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 64800 lr 1.379e-04 [train_loss] tar_ll 0.9830 loss -0.9830 (8.130 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.0321 loss -1.0321 (8.443 secs)\n",
      "100%|##########| 3000/3000 [00:42<00:00, 69.87it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 1.0477 loss -1.0477 (42.938 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.0674 loss -1.0674 (8.630 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.0304 loss -1.0304 (8.527 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.0446 loss -1.0446 (8.131 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.1491 loss -1.1491 (8.416 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.0237 loss -1.0237 (8.245 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.0451 loss -1.0451 (8.426 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.0566 loss -1.0566 (7.913 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 66600 lr 1.255e-04 [train_loss] tar_ll 0.9918 loss -0.9918 (8.436 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.0237 loss -1.0237 (9.414 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.0200 loss -1.0200 (10.406 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.0851 loss -1.0851 (8.782 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.0235 loss -1.0235 (8.805 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.0981 loss -1.0981 (8.896 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.0331 loss -1.0331 (8.205 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.1642 loss -1.1642 (8.590 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 68200 lr 1.147e-04 [train_loss] tar_ll 0.9862 loss -0.9862 (8.226 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.0322 loss -1.0322 (8.981 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.1364 loss -1.1364 (8.169 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.0379 loss -1.0379 (8.419 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.1409 loss -1.1409 (8.524 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.0260 loss -1.0260 (8.749 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.0372 loss -1.0372 (8.319 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 69600 lr 1.056e-04 [train_loss] tar_ll 0.9754 loss -0.9754 (8.749 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.0243 loss -1.0243 (8.973 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.0384 loss -1.0384 (8.807 secs)\n",
      "100%|##########| 3000/3000 [00:43<00:00, 69.40it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 1.0622 loss -1.0622 (43.227 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.0543 loss -1.0543 (8.947 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.0096 loss -1.0096 (8.797 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.0735 loss -1.0735 (8.648 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.0928 loss -1.0928 (8.595 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.0416 loss -1.0416 (8.499 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.0900 loss -1.0900 (9.072 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.1517 loss -1.1517 (9.536 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.0585 loss -1.0585 (9.500 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.0722 loss -1.0722 (9.613 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.0526 loss -1.0526 (8.731 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.0652 loss -1.0652 (8.432 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.0586 loss -1.0586 (8.108 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.1277 loss -1.1277 (8.420 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.0912 loss -1.0912 (8.185 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.1314 loss -1.1314 (8.199 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.0987 loss -1.0987 (8.330 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.0074 loss -1.0074 (8.098 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.1546 loss -1.1546 (8.324 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.1332 loss -1.1332 (7.995 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.1139 loss -1.1139 (8.442 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.1544 loss -1.1544 (8.326 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.1697 loss -1.1697 (8.794 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.1624 loss -1.1624 (8.936 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.0544 loss -1.0544 (8.604 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1915 loss -1.1915 (8.399 secs)\n",
      "100%|##########| 3000/3000 [00:43<00:00, 68.69it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 1.1515 loss -1.1515 (43.674 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.0819 loss -1.0819 (9.123 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.2252 loss -1.2252 (8.129 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.1447 loss -1.1447 (8.245 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.1086 loss -1.1086 (8.523 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.0580 loss -1.0580 (8.413 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.1467 loss -1.1467 (8.897 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.0973 loss -1.0973 (8.423 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.1847 loss -1.1847 (8.627 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.1331 loss -1.1331 (8.788 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.0809 loss -1.0809 (8.684 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.1654 loss -1.1654 (8.882 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.0784 loss -1.0784 (8.900 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 77600 lr 5.939e-05 [train_loss] tar_ll 0.9741 loss -0.9741 (8.462 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.0969 loss -1.0969 (8.412 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.0789 loss -1.0789 (8.523 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.1416 loss -1.1416 (9.034 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.1025 loss -1.1025 (8.143 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.1386 loss -1.1386 (8.836 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.0508 loss -1.0508 (8.306 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.1537 loss -1.1537 (8.881 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.1783 loss -1.1783 (8.064 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.0507 loss -1.0507 (8.449 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.1793 loss -1.1793 (8.235 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.2549 loss -1.2549 (8.493 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.1005 loss -1.1005 (8.032 secs)\n",
      "100%|##########| 3000/3000 [00:42<00:00, 71.39it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 1.1506 loss -1.1506 (42.028 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.2081 loss -1.2081 (8.404 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.2039 loss -1.2039 (8.198 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.2848 loss -1.2848 (8.350 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2451 loss -1.2451 (8.156 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.1088 loss -1.1088 (9.142 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.1523 loss -1.1523 (8.671 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.1848 loss -1.1848 (8.686 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.1348 loss -1.1348 (8.386 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2363 loss -1.2363 (8.645 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.1169 loss -1.1169 (8.683 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.2435 loss -1.2435 (8.637 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.1802 loss -1.1802 (8.080 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.2634 loss -1.2634 (8.616 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.1581 loss -1.1581 (7.996 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.2179 loss -1.2179 (8.496 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.1784 loss -1.1784 (8.889 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.2235 loss -1.2235 (8.386 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.1679 loss -1.1679 (8.596 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.2024 loss -1.2024 (8.284 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.1558 loss -1.1558 (8.730 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2250 loss -1.2250 (8.550 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.1763 loss -1.1763 (8.775 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.2430 loss -1.2430 (8.185 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.2597 loss -1.2597 (8.644 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.1738 loss -1.1738 (8.119 secs)\n",
      "100%|##########| 3000/3000 [00:43<00:00, 69.68it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 1.2233 loss -1.2233 (43.057 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.2299 loss -1.2299 (8.655 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.2478 loss -1.2478 (8.624 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.1523 loss -1.1523 (8.790 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.1323 loss -1.1323 (8.399 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.2030 loss -1.2030 (8.794 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.0862 loss -1.0862 (8.082 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.1761 loss -1.1761 (9.251 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.2783 loss -1.2783 (8.932 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.1949 loss -1.1949 (8.971 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.3457 loss -1.3457 (8.418 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.1815 loss -1.1815 (8.784 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.2466 loss -1.2466 (8.620 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.1998 loss -1.1998 (8.842 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.1831 loss -1.1831 (8.750 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.1536 loss -1.1536 (8.288 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.2521 loss -1.2521 (8.652 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.2035 loss -1.2035 (8.272 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.2588 loss -1.2588 (8.915 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.2249 loss -1.2249 (8.651 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.2028 loss -1.2028 (9.145 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.2344 loss -1.2344 (8.723 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.2181 loss -1.2181 (8.714 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.2474 loss -1.2474 (9.383 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2561 loss -1.2561 (8.718 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.2354 loss -1.2354 (8.726 secs)\n",
      "100%|##########| 3000/3000 [00:42<00:00, 70.04it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 1.2377 loss -1.2377 (42.833 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.2335 loss -1.2335 (8.696 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.3357 loss -1.3357 (9.162 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.2830 loss -1.2830 (8.575 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.1466 loss -1.1466 (8.933 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.2554 loss -1.2554 (8.719 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2487 loss -1.2487 (8.779 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2850 loss -1.2850 (8.374 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.2937 loss -1.2937 (8.694 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2572 loss -1.2572 (8.524 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2405 loss -1.2405 (8.458 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.2956 loss -1.2956 (8.176 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.3012 loss -1.3012 (8.930 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.2311 loss -1.2311 (8.810 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.1704 loss -1.1704 (8.803 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.2049 loss -1.2049 (9.248 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.2441 loss -1.2441 (8.538 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.2321 loss -1.2321 (8.919 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.1973 loss -1.1973 (8.765 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.1800 loss -1.1800 (8.455 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.2622 loss -1.2622 (8.149 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.2071 loss -1.2071 (8.467 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.3202 loss -1.3202 (8.055 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.2713 loss -1.2713 (8.307 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.2393 loss -1.2393 (8.027 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.2685 loss -1.2685 (8.398 secs)\n",
      "100%|##########| 3000/3000 [00:44<00:00, 68.05it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 1.2528 loss -1.2528 (44.085 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.2401 loss -1.2401 (8.947 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.2145 loss -1.2145 (8.992 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.2707 loss -1.2707 (8.865 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.2900 loss -1.2900 (9.164 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.3139 loss -1.3139 (8.691 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2864 loss -1.2864 (8.825 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.2636 loss -1.2636 (8.428 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.2963 loss -1.2963 (8.476 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.2548 loss -1.2548 (8.896 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.2571 loss -1.2571 (8.683 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.2018 loss -1.2018 (8.859 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.2683 loss -1.2683 (8.189 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.2889 loss -1.2889 (8.581 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.3040 loss -1.3040 (8.618 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.2244 loss -1.2244 (8.927 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.2122 loss -1.2122 (8.305 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.2785 loss -1.2785 (8.635 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.1924 loss -1.1924 (8.802 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2954 loss -1.2954 (9.002 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.3187 loss -1.3187 (9.239 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.2712 loss -1.2712 (9.245 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2447 loss -1.2447 (8.747 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.3094 loss -1.3094 (9.104 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.2984 loss -1.2984 (9.131 secs)\n",
      "lbanp:lbanp-num_latents-128_rbf_0 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.2538 loss -1.2538 (8.610 secs)\n",
      "100%|##########| 3000/3000 [00:44<00:00, 66.72it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 1.2540 loss -1.2540 (44.967 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:43<00:00, 69.25it/s]\n",
      "lbanp:lbanp-num_latents-128_rbf_0 rbf tar_ll 1.2540 loss -1.2540 (43.322 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 5219727.0 miliseconds\n",
      "Execution time: 5219.727 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 177.02978515625 MB\n",
      "Memory Usage Change: 160.77978515625 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='lbanp', name='lbanp-num_latents-128_rbf_0',val_seed=0, val_l=128,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d818f75-b331-4daf-a6a3-715dee99696d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:31<00:00, 95.31it/s] \n",
      "lbanp:lbanp-num_latents-128_rbf rbf tar_ll 1.0833 loss -1.0833 (31.478 secs)\n"
     ]
    }
   ],
   "source": [
    "%run gp.py --mode eval --expid lbanp-num_latents-128_rbf --model lbanp --num_latents 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1f9b6f-9dfe-4ae7-81cd-7ed39208ce1e",
   "metadata": {},
   "source": [
    "## TNP-ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c698da0b-f88d-4b5d-8213-1b54c985faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mod(modelo='tnpnd', name='tnpnd_rbf',val_seed=100, val_l=None,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b7186d-6720-47b0-a7da-920a6741ae8e",
   "metadata": {},
   "source": [
    "## TNP-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed4d1e-9cf4-4ff7-8e55-e45ae17f2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mod(modelo='tnpa', name='tnpa_rbf',val_seed=100, val_l=None,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07901c61-b325-4eb0-b949-2b4e190afb8c",
   "metadata": {},
   "source": [
    "## LBANP (16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e1559-0245-4045-a8a7-0e0f4320ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mod(modelo='lbanp', name='lbanp-num_latents-16_rbf',val_seed=100, val_l=16,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dabb3f2-d44d-4646-8013-67c1196488c0",
   "metadata": {},
   "source": [
    "## LBANP (32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af2229-3ab0-4903-8872-1b200f9e56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mod(modelo='lbanp', name='lbanp-num_latents-32_rbf',val_seed=100, val_l=32,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03edf553-fd2a-405d-9b30-a0ef0a50bd1f",
   "metadata": {},
   "source": [
    "## LBANP (64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9875e000-c2c4-41b5-aeb5-210c8f7be806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: lbanp-lbanp-num_latents-64_rbf\n",
      "Total number of parameters: 788418\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 64\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-64_rbf step 200 lr 5.000e-04 [train_loss] tar_ll -0.6975 loss 0.6975 (16.051 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 400 lr 5.000e-04 [train_loss] tar_ll -0.6571 loss 0.6571 (15.247 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 600 lr 5.000e-04 [train_loss] tar_ll -0.5794 loss 0.5794 (6.573 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 800 lr 4.999e-04 [train_loss] tar_ll -0.6048 loss 0.6048 (6.137 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 1000 lr 4.999e-04 [train_loss] tar_ll -0.6057 loss 0.6057 (5.888 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 1200 lr 4.998e-04 [train_loss] tar_ll -0.5173 loss 0.5173 (6.197 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 1400 lr 4.998e-04 [train_loss] tar_ll -0.3183 loss 0.3183 (6.077 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 1600 lr 4.997e-04 [train_loss] tar_ll -0.2420 loss 0.2420 (6.400 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 1800 lr 4.996e-04 [train_loss] tar_ll -0.1573 loss 0.1573 (6.117 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 2000 lr 4.995e-04 [train_loss] tar_ll -0.0828 loss 0.0828 (6.002 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 2200 lr 4.994e-04 [train_loss] tar_ll 0.0122 loss -0.0122 (6.335 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 2400 lr 4.993e-04 [train_loss] tar_ll 0.0672 loss -0.0672 (7.509 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 2600 lr 4.992e-04 [train_loss] tar_ll 0.0769 loss -0.0769 (7.931 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 2800 lr 4.990e-04 [train_loss] tar_ll 0.1074 loss -0.1074 (7.394 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 3000 lr 4.989e-04 [train_loss] tar_ll 0.1992 loss -0.1992 (6.451 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 3200 lr 4.987e-04 [train_loss] tar_ll 0.0909 loss -0.0909 (6.604 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 3400 lr 4.986e-04 [train_loss] tar_ll 0.1494 loss -0.1494 (6.933 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2393 loss -0.2393 (6.417 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 3800 lr 4.982e-04 [train_loss] tar_ll 0.2232 loss -0.2232 (6.275 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 4000 lr 4.980e-04 [train_loss] tar_ll 0.2058 loss -0.2058 (6.678 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 4200 lr 4.978e-04 [train_loss] tar_ll 0.2331 loss -0.2331 (6.437 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 4400 lr 4.976e-04 [train_loss] tar_ll 0.2826 loss -0.2826 (6.163 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 4600 lr 4.974e-04 [train_loss] tar_ll 0.3472 loss -0.3472 (6.264 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 4800 lr 4.972e-04 [train_loss] tar_ll 0.3311 loss -0.3311 (6.245 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 5000 lr 4.969e-04 [train_loss] tar_ll 0.1937 loss -0.1937 (6.326 secs)\n",
      "100%|##########| 3000/3000 [00:30<00:00, 97.56it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 0.2347 loss -0.2347 (30.754 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 5200 lr 4.967e-04 [train_loss] tar_ll 0.2506 loss -0.2506 (6.388 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 5400 lr 4.964e-04 [train_loss] tar_ll 0.3529 loss -0.3529 (6.415 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 5600 lr 4.961e-04 [train_loss] tar_ll 0.3564 loss -0.3564 (6.464 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 5800 lr 4.959e-04 [train_loss] tar_ll 0.3568 loss -0.3568 (6.351 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 6000 lr 4.956e-04 [train_loss] tar_ll 0.2704 loss -0.2704 (6.732 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 6200 lr 4.953e-04 [train_loss] tar_ll 0.3417 loss -0.3417 (6.615 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 6400 lr 4.950e-04 [train_loss] tar_ll 0.1793 loss -0.1793 (6.494 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 6600 lr 4.946e-04 [train_loss] tar_ll 0.1477 loss -0.1477 (6.613 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 6800 lr 4.943e-04 [train_loss] tar_ll 0.1806 loss -0.1806 (6.504 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 7000 lr 4.940e-04 [train_loss] tar_ll 0.2996 loss -0.2996 (6.629 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 7200 lr 4.936e-04 [train_loss] tar_ll 0.3312 loss -0.3312 (6.399 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 7400 lr 4.933e-04 [train_loss] tar_ll 0.3890 loss -0.3890 (6.508 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 7600 lr 4.929e-04 [train_loss] tar_ll 0.4748 loss -0.4748 (6.449 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 7800 lr 4.925e-04 [train_loss] tar_ll 0.4754 loss -0.4754 (6.594 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 8000 lr 4.921e-04 [train_loss] tar_ll 0.4085 loss -0.4085 (6.837 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 8200 lr 4.918e-04 [train_loss] tar_ll 0.5023 loss -0.5023 (6.476 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 8400 lr 4.913e-04 [train_loss] tar_ll 0.4363 loss -0.4363 (6.798 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 8600 lr 4.909e-04 [train_loss] tar_ll 0.5390 loss -0.5390 (6.550 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 8800 lr 4.905e-04 [train_loss] tar_ll 0.5172 loss -0.5172 (6.605 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 9000 lr 4.901e-04 [train_loss] tar_ll 0.5025 loss -0.5025 (6.667 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 9200 lr 4.896e-04 [train_loss] tar_ll 0.4070 loss -0.4070 (6.578 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 9400 lr 4.892e-04 [train_loss] tar_ll 0.5117 loss -0.5117 (6.909 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 9600 lr 4.887e-04 [train_loss] tar_ll 0.5193 loss -0.5193 (6.854 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 9800 lr 4.882e-04 [train_loss] tar_ll 0.5473 loss -0.5473 (6.808 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 10000 lr 4.878e-04 [train_loss] tar_ll 0.5573 loss -0.5573 (6.763 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 95.09it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 0.1466 loss -0.1466 (31.552 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 10200 lr 4.873e-04 [train_loss] tar_ll 0.5445 loss -0.5445 (6.353 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 10400 lr 4.868e-04 [train_loss] tar_ll 0.6151 loss -0.6151 (6.400 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 10600 lr 4.863e-04 [train_loss] tar_ll 0.4963 loss -0.4963 (6.688 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 10800 lr 4.857e-04 [train_loss] tar_ll 0.5269 loss -0.5269 (6.375 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 11000 lr 4.852e-04 [train_loss] tar_ll 0.4460 loss -0.4460 (6.336 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 11200 lr 4.847e-04 [train_loss] tar_ll 0.5430 loss -0.5430 (6.506 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 11400 lr 4.841e-04 [train_loss] tar_ll 0.5623 loss -0.5623 (6.400 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 11600 lr 4.836e-04 [train_loss] tar_ll 0.6272 loss -0.6272 (6.566 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 11800 lr 4.830e-04 [train_loss] tar_ll 0.4969 loss -0.4969 (6.843 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 12000 lr 4.824e-04 [train_loss] tar_ll 0.5897 loss -0.5897 (6.806 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 12200 lr 4.819e-04 [train_loss] tar_ll 0.5345 loss -0.5345 (6.819 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 12400 lr 4.813e-04 [train_loss] tar_ll 0.5804 loss -0.5804 (6.624 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 12600 lr 4.807e-04 [train_loss] tar_ll 0.6134 loss -0.6134 (6.771 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 12800 lr 4.801e-04 [train_loss] tar_ll 0.6181 loss -0.6181 (6.477 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 13000 lr 4.794e-04 [train_loss] tar_ll 0.5457 loss -0.5457 (6.296 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 13200 lr 4.788e-04 [train_loss] tar_ll 0.6190 loss -0.6190 (6.408 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 13400 lr 4.782e-04 [train_loss] tar_ll 0.6380 loss -0.6380 (6.394 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 13600 lr 4.775e-04 [train_loss] tar_ll 0.6213 loss -0.6213 (6.282 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 13800 lr 4.769e-04 [train_loss] tar_ll 0.5767 loss -0.5767 (6.437 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 14000 lr 4.762e-04 [train_loss] tar_ll 0.5903 loss -0.5903 (6.304 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 14200 lr 4.755e-04 [train_loss] tar_ll 0.6262 loss -0.6262 (6.418 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 14400 lr 4.749e-04 [train_loss] tar_ll 0.5724 loss -0.5724 (6.408 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 14600 lr 4.742e-04 [train_loss] tar_ll 0.6733 loss -0.6733 (6.278 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 14800 lr 4.735e-04 [train_loss] tar_ll 0.4688 loss -0.4688 (6.511 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 15000 lr 4.728e-04 [train_loss] tar_ll 0.3598 loss -0.3598 (6.440 secs)\n",
      "100%|##########| 3000/3000 [00:30<00:00, 96.98it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 0.5218 loss -0.5218 (30.937 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 15200 lr 4.720e-04 [train_loss] tar_ll 0.5379 loss -0.5379 (6.967 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 15400 lr 4.713e-04 [train_loss] tar_ll 0.6072 loss -0.6072 (6.493 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 15600 lr 4.706e-04 [train_loss] tar_ll 0.6780 loss -0.6780 (6.374 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 15800 lr 4.698e-04 [train_loss] tar_ll 0.5604 loss -0.5604 (6.530 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 16000 lr 4.691e-04 [train_loss] tar_ll 0.6251 loss -0.6251 (6.374 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 16200 lr 4.683e-04 [train_loss] tar_ll 0.6433 loss -0.6433 (6.415 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 16400 lr 4.675e-04 [train_loss] tar_ll 0.6039 loss -0.6039 (6.446 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 16600 lr 4.668e-04 [train_loss] tar_ll 0.7063 loss -0.7063 (6.458 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 16800 lr 4.660e-04 [train_loss] tar_ll 0.6542 loss -0.6542 (6.631 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 17000 lr 4.652e-04 [train_loss] tar_ll 0.7166 loss -0.7166 (6.575 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 17200 lr 4.644e-04 [train_loss] tar_ll 0.6169 loss -0.6169 (6.649 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 17400 lr 4.636e-04 [train_loss] tar_ll 0.4821 loss -0.4821 (6.670 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 17600 lr 4.627e-04 [train_loss] tar_ll 0.6182 loss -0.6182 (6.658 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 17800 lr 4.619e-04 [train_loss] tar_ll 0.7228 loss -0.7228 (6.680 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 18000 lr 4.611e-04 [train_loss] tar_ll 0.6451 loss -0.6451 (6.621 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 18200 lr 4.602e-04 [train_loss] tar_ll 0.7202 loss -0.7202 (6.715 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 18400 lr 4.594e-04 [train_loss] tar_ll 0.6708 loss -0.6708 (6.670 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 18600 lr 4.585e-04 [train_loss] tar_ll 0.7749 loss -0.7749 (6.724 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 18800 lr 4.576e-04 [train_loss] tar_ll 0.7216 loss -0.7216 (6.840 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 19000 lr 4.568e-04 [train_loss] tar_ll 0.7944 loss -0.7944 (6.859 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 19200 lr 4.559e-04 [train_loss] tar_ll 0.8184 loss -0.8184 (6.717 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 19400 lr 4.550e-04 [train_loss] tar_ll 0.6686 loss -0.6686 (6.348 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 19600 lr 4.541e-04 [train_loss] tar_ll 0.6743 loss -0.6743 (6.434 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 19800 lr 4.532e-04 [train_loss] tar_ll 0.7404 loss -0.7404 (6.299 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 20000 lr 4.523e-04 [train_loss] tar_ll 0.6699 loss -0.6699 (6.431 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 96.11it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 0.8239 loss -0.8239 (31.218 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 20200 lr 4.513e-04 [train_loss] tar_ll 0.7552 loss -0.7552 (6.668 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 20400 lr 4.504e-04 [train_loss] tar_ll 0.7221 loss -0.7221 (6.390 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 20600 lr 4.494e-04 [train_loss] tar_ll 0.6302 loss -0.6302 (6.529 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 20800 lr 4.485e-04 [train_loss] tar_ll 0.6029 loss -0.6029 (6.925 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 21000 lr 4.475e-04 [train_loss] tar_ll 0.5949 loss -0.5949 (6.784 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 21200 lr 4.466e-04 [train_loss] tar_ll 0.6855 loss -0.6855 (6.926 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 21400 lr 4.456e-04 [train_loss] tar_ll 0.7659 loss -0.7659 (6.853 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 21600 lr 4.446e-04 [train_loss] tar_ll 0.7495 loss -0.7495 (6.716 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 21800 lr 4.436e-04 [train_loss] tar_ll 0.6948 loss -0.6948 (6.742 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 22000 lr 4.426e-04 [train_loss] tar_ll 0.6785 loss -0.6785 (6.382 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 22200 lr 4.416e-04 [train_loss] tar_ll 0.7257 loss -0.7257 (6.354 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 22400 lr 4.406e-04 [train_loss] tar_ll 0.7231 loss -0.7231 (6.214 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 22600 lr 4.396e-04 [train_loss] tar_ll 0.7100 loss -0.7100 (6.350 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 22800 lr 4.386e-04 [train_loss] tar_ll 0.6995 loss -0.6995 (6.458 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 23000 lr 4.375e-04 [train_loss] tar_ll 0.7020 loss -0.7020 (6.342 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 23200 lr 4.365e-04 [train_loss] tar_ll 0.7493 loss -0.7493 (6.492 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 23400 lr 4.354e-04 [train_loss] tar_ll 0.7191 loss -0.7191 (6.443 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 23600 lr 4.344e-04 [train_loss] tar_ll 0.7276 loss -0.7276 (6.419 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 23800 lr 4.333e-04 [train_loss] tar_ll 0.7486 loss -0.7486 (6.533 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 24000 lr 4.322e-04 [train_loss] tar_ll 0.7222 loss -0.7222 (6.531 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 24200 lr 4.312e-04 [train_loss] tar_ll 0.7897 loss -0.7897 (6.542 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 24400 lr 4.301e-04 [train_loss] tar_ll 0.7105 loss -0.7105 (6.542 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 24600 lr 4.290e-04 [train_loss] tar_ll 0.7418 loss -0.7418 (6.470 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 24800 lr 4.279e-04 [train_loss] tar_ll 0.8125 loss -0.8125 (6.407 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 25000 lr 4.268e-04 [train_loss] tar_ll 0.8327 loss -0.8327 (6.473 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 96.48it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 0.8803 loss -0.8803 (31.099 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 25200 lr 4.257e-04 [train_loss] tar_ll 0.8056 loss -0.8056 (6.465 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 25400 lr 4.245e-04 [train_loss] tar_ll 0.6357 loss -0.6357 (6.288 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 25600 lr 4.234e-04 [train_loss] tar_ll 0.7256 loss -0.7256 (6.724 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 25800 lr 4.223e-04 [train_loss] tar_ll 0.7644 loss -0.7644 (6.566 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 26000 lr 4.211e-04 [train_loss] tar_ll 0.6489 loss -0.6489 (6.637 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 26200 lr 4.200e-04 [train_loss] tar_ll 0.7195 loss -0.7195 (6.616 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 26400 lr 4.188e-04 [train_loss] tar_ll 0.7090 loss -0.7090 (6.500 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 26600 lr 4.177e-04 [train_loss] tar_ll 0.7714 loss -0.7714 (6.712 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 26800 lr 4.165e-04 [train_loss] tar_ll 0.7180 loss -0.7180 (6.600 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 27000 lr 4.153e-04 [train_loss] tar_ll 0.7645 loss -0.7645 (6.602 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 27200 lr 4.141e-04 [train_loss] tar_ll 0.7615 loss -0.7615 (6.687 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 27400 lr 4.130e-04 [train_loss] tar_ll 0.7940 loss -0.7940 (6.928 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 27600 lr 4.118e-04 [train_loss] tar_ll 0.9284 loss -0.9284 (6.938 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 27800 lr 4.106e-04 [train_loss] tar_ll 0.7388 loss -0.7388 (6.886 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 28000 lr 4.094e-04 [train_loss] tar_ll 0.8078 loss -0.8078 (6.883 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 28200 lr 4.081e-04 [train_loss] tar_ll 0.8635 loss -0.8635 (6.807 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 28400 lr 4.069e-04 [train_loss] tar_ll 0.8715 loss -0.8715 (6.495 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 28600 lr 4.057e-04 [train_loss] tar_ll 0.7904 loss -0.7904 (6.618 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 28800 lr 4.045e-04 [train_loss] tar_ll 0.8057 loss -0.8057 (6.433 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 29000 lr 4.032e-04 [train_loss] tar_ll 0.8052 loss -0.8052 (6.442 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 29200 lr 4.020e-04 [train_loss] tar_ll 0.9231 loss -0.9231 (6.462 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 29400 lr 4.007e-04 [train_loss] tar_ll 0.8065 loss -0.8065 (6.511 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 29600 lr 3.995e-04 [train_loss] tar_ll 0.9115 loss -0.9115 (6.457 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 29800 lr 3.982e-04 [train_loss] tar_ll 0.7705 loss -0.7705 (6.435 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 30000 lr 3.969e-04 [train_loss] tar_ll 0.8378 loss -0.8378 (6.401 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 96.74it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 0.8527 loss -0.8527 (31.011 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 30200 lr 3.957e-04 [train_loss] tar_ll 0.7669 loss -0.7669 (6.730 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 30400 lr 3.944e-04 [train_loss] tar_ll 0.7434 loss -0.7434 (6.633 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 30600 lr 3.931e-04 [train_loss] tar_ll 0.7901 loss -0.7901 (6.393 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 30800 lr 3.918e-04 [train_loss] tar_ll 0.7261 loss -0.7261 (6.380 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 31000 lr 3.905e-04 [train_loss] tar_ll 0.8478 loss -0.8478 (6.418 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 31200 lr 3.892e-04 [train_loss] tar_ll 0.7599 loss -0.7599 (6.432 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 31400 lr 3.879e-04 [train_loss] tar_ll 0.8401 loss -0.8401 (6.468 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 31600 lr 3.866e-04 [train_loss] tar_ll 0.8526 loss -0.8526 (6.316 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 31800 lr 3.853e-04 [train_loss] tar_ll 0.8303 loss -0.8303 (6.337 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 32000 lr 3.840e-04 [train_loss] tar_ll 0.9155 loss -0.9155 (6.537 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 32200 lr 3.826e-04 [train_loss] tar_ll 0.8201 loss -0.8201 (7.026 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 32400 lr 3.813e-04 [train_loss] tar_ll 0.8781 loss -0.8781 (6.774 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 32600 lr 3.800e-04 [train_loss] tar_ll 0.8975 loss -0.8975 (6.920 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 32800 lr 3.786e-04 [train_loss] tar_ll 0.8533 loss -0.8533 (6.765 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 33000 lr 3.773e-04 [train_loss] tar_ll 0.8094 loss -0.8094 (6.478 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 33200 lr 3.759e-04 [train_loss] tar_ll 0.8365 loss -0.8365 (6.566 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 33400 lr 3.745e-04 [train_loss] tar_ll 0.8791 loss -0.8791 (6.448 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 33600 lr 3.732e-04 [train_loss] tar_ll 0.9155 loss -0.9155 (6.479 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 33800 lr 3.718e-04 [train_loss] tar_ll 0.9352 loss -0.9352 (6.424 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 34000 lr 3.704e-04 [train_loss] tar_ll 0.9190 loss -0.9190 (6.440 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 34200 lr 3.691e-04 [train_loss] tar_ll 0.8470 loss -0.8470 (6.395 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 34400 lr 3.677e-04 [train_loss] tar_ll 0.8682 loss -0.8682 (6.912 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 34600 lr 3.663e-04 [train_loss] tar_ll 0.8324 loss -0.8324 (6.996 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 34800 lr 3.649e-04 [train_loss] tar_ll 0.9183 loss -0.9183 (6.676 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 35000 lr 3.635e-04 [train_loss] tar_ll 0.8677 loss -0.8677 (6.394 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 95.93it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 0.8403 loss -0.8403 (31.274 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 35200 lr 3.621e-04 [train_loss] tar_ll 0.8547 loss -0.8547 (6.563 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 35400 lr 3.607e-04 [train_loss] tar_ll 0.8167 loss -0.8167 (6.556 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 35600 lr 3.593e-04 [train_loss] tar_ll 0.8973 loss -0.8973 (6.701 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 35800 lr 3.579e-04 [train_loss] tar_ll 0.8504 loss -0.8504 (6.715 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 36000 lr 3.564e-04 [train_loss] tar_ll 0.8889 loss -0.8889 (6.695 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 36200 lr 3.550e-04 [train_loss] tar_ll 0.8387 loss -0.8387 (6.531 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 36400 lr 3.536e-04 [train_loss] tar_ll 0.8996 loss -0.8996 (6.858 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 36600 lr 3.522e-04 [train_loss] tar_ll 0.8703 loss -0.8703 (7.166 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 36800 lr 3.507e-04 [train_loss] tar_ll 0.7830 loss -0.7830 (6.992 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 37000 lr 3.493e-04 [train_loss] tar_ll 0.8217 loss -0.8217 (6.982 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 37200 lr 3.478e-04 [train_loss] tar_ll 0.7115 loss -0.7115 (7.788 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 37400 lr 3.464e-04 [train_loss] tar_ll 0.9010 loss -0.9010 (7.073 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 37600 lr 3.449e-04 [train_loss] tar_ll 0.6837 loss -0.6837 (6.959 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 37800 lr 3.435e-04 [train_loss] tar_ll 0.7247 loss -0.7247 (7.092 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 38000 lr 3.420e-04 [train_loss] tar_ll 0.7726 loss -0.7726 (7.286 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 38200 lr 3.406e-04 [train_loss] tar_ll 0.9105 loss -0.9105 (6.999 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 38400 lr 3.391e-04 [train_loss] tar_ll 0.8638 loss -0.8638 (6.908 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 38600 lr 3.376e-04 [train_loss] tar_ll 0.9032 loss -0.9032 (7.308 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 38800 lr 3.362e-04 [train_loss] tar_ll 0.8475 loss -0.8475 (7.072 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 39000 lr 3.347e-04 [train_loss] tar_ll 0.8827 loss -0.8827 (7.092 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 39200 lr 3.332e-04 [train_loss] tar_ll 0.8707 loss -0.8707 (7.259 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 39400 lr 3.317e-04 [train_loss] tar_ll 0.8287 loss -0.8287 (7.342 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 39600 lr 3.302e-04 [train_loss] tar_ll 0.9078 loss -0.9078 (6.753 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 39800 lr 3.287e-04 [train_loss] tar_ll 0.9903 loss -0.9903 (7.219 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 40000 lr 3.273e-04 [train_loss] tar_ll 0.9230 loss -0.9230 (7.504 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.52it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 0.9965 loss -0.9965 (33.143 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 40200 lr 3.258e-04 [train_loss] tar_ll 0.8208 loss -0.8208 (7.388 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 40400 lr 3.243e-04 [train_loss] tar_ll 1.0575 loss -1.0575 (6.862 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 40600 lr 3.228e-04 [train_loss] tar_ll 0.9654 loss -0.9654 (7.048 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 40800 lr 3.213e-04 [train_loss] tar_ll 0.9057 loss -0.9057 (7.303 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 41000 lr 3.197e-04 [train_loss] tar_ll 1.0678 loss -1.0678 (7.127 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 41200 lr 3.182e-04 [train_loss] tar_ll 0.8633 loss -0.8633 (7.001 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 41400 lr 3.167e-04 [train_loss] tar_ll 0.9413 loss -0.9413 (7.160 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 41600 lr 3.152e-04 [train_loss] tar_ll 0.9179 loss -0.9179 (7.281 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 41800 lr 3.137e-04 [train_loss] tar_ll 0.8660 loss -0.8660 (6.934 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 42000 lr 3.122e-04 [train_loss] tar_ll 0.8871 loss -0.8871 (6.916 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 42200 lr 3.106e-04 [train_loss] tar_ll 0.9380 loss -0.9380 (7.302 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 42400 lr 3.091e-04 [train_loss] tar_ll 0.8816 loss -0.8816 (7.098 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 42600 lr 3.076e-04 [train_loss] tar_ll 0.9269 loss -0.9269 (6.790 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 42800 lr 3.061e-04 [train_loss] tar_ll 0.8451 loss -0.8451 (7.231 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 43000 lr 3.045e-04 [train_loss] tar_ll 0.8324 loss -0.8324 (7.199 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 43200 lr 3.030e-04 [train_loss] tar_ll 0.8148 loss -0.8148 (7.185 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 43400 lr 3.015e-04 [train_loss] tar_ll 1.0071 loss -1.0071 (7.108 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 43600 lr 2.999e-04 [train_loss] tar_ll 0.9358 loss -0.9358 (7.109 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 43800 lr 2.984e-04 [train_loss] tar_ll 0.9954 loss -0.9954 (6.962 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 44000 lr 2.968e-04 [train_loss] tar_ll 0.8955 loss -0.8955 (7.007 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 44200 lr 2.953e-04 [train_loss] tar_ll 0.9428 loss -0.9428 (7.388 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 44400 lr 2.938e-04 [train_loss] tar_ll 0.9170 loss -0.9170 (7.238 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 44600 lr 2.922e-04 [train_loss] tar_ll 0.8573 loss -0.8573 (7.147 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 44800 lr 2.907e-04 [train_loss] tar_ll 0.9946 loss -0.9946 (7.357 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 45000 lr 2.891e-04 [train_loss] tar_ll 1.0275 loss -1.0275 (7.341 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.59it/s]\n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 1.0055 loss -1.0055 (33.866 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 45200 lr 2.876e-04 [train_loss] tar_ll 0.9698 loss -0.9698 (7.596 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 45400 lr 2.860e-04 [train_loss] tar_ll 0.8859 loss -0.8859 (7.203 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 45600 lr 2.844e-04 [train_loss] tar_ll 0.9268 loss -0.9268 (6.872 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 45800 lr 2.829e-04 [train_loss] tar_ll 0.9648 loss -0.9648 (6.981 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 46000 lr 2.813e-04 [train_loss] tar_ll 0.9767 loss -0.9767 (7.218 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 46200 lr 2.798e-04 [train_loss] tar_ll 1.0035 loss -1.0035 (6.829 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 46400 lr 2.782e-04 [train_loss] tar_ll 0.9796 loss -0.9796 (7.090 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 46600 lr 2.767e-04 [train_loss] tar_ll 0.9086 loss -0.9086 (7.194 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 46800 lr 2.751e-04 [train_loss] tar_ll 1.0180 loss -1.0180 (6.897 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 47000 lr 2.735e-04 [train_loss] tar_ll 1.0119 loss -1.0119 (7.000 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 47200 lr 2.720e-04 [train_loss] tar_ll 1.0237 loss -1.0237 (7.638 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 47400 lr 2.704e-04 [train_loss] tar_ll 0.9723 loss -0.9723 (7.744 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 47600 lr 2.688e-04 [train_loss] tar_ll 0.9092 loss -0.9092 (7.260 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 47800 lr 2.673e-04 [train_loss] tar_ll 0.9966 loss -0.9966 (7.412 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 48000 lr 2.657e-04 [train_loss] tar_ll 1.0330 loss -1.0330 (7.221 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 48200 lr 2.641e-04 [train_loss] tar_ll 1.0259 loss -1.0259 (7.406 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 48400 lr 2.626e-04 [train_loss] tar_ll 1.0032 loss -1.0032 (6.867 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 48600 lr 2.610e-04 [train_loss] tar_ll 1.0225 loss -1.0225 (7.264 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 48800 lr 2.594e-04 [train_loss] tar_ll 0.8877 loss -0.8877 (7.204 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 49000 lr 2.579e-04 [train_loss] tar_ll 0.8392 loss -0.8392 (6.830 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 49200 lr 2.563e-04 [train_loss] tar_ll 1.0034 loss -1.0034 (7.203 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 49400 lr 2.547e-04 [train_loss] tar_ll 0.9349 loss -0.9349 (7.146 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 49600 lr 2.531e-04 [train_loss] tar_ll 0.9491 loss -0.9491 (6.946 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 49800 lr 2.516e-04 [train_loss] tar_ll 0.9150 loss -0.9150 (7.332 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 50000 lr 2.500e-04 [train_loss] tar_ll 0.9316 loss -0.9316 (7.564 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.90it/s]\n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 1.0128 loss -1.0128 (33.750 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 50200 lr 2.484e-04 [train_loss] tar_ll 0.9648 loss -0.9648 (7.415 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 50400 lr 2.469e-04 [train_loss] tar_ll 0.9957 loss -0.9957 (7.382 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 50600 lr 2.453e-04 [train_loss] tar_ll 1.0486 loss -1.0486 (6.916 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 50800 lr 2.437e-04 [train_loss] tar_ll 1.0239 loss -1.0239 (6.919 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 51000 lr 2.421e-04 [train_loss] tar_ll 1.0179 loss -1.0179 (7.460 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 51200 lr 2.406e-04 [train_loss] tar_ll 1.0224 loss -1.0224 (6.634 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 51400 lr 2.390e-04 [train_loss] tar_ll 0.9941 loss -0.9941 (6.668 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 51600 lr 2.374e-04 [train_loss] tar_ll 0.9798 loss -0.9798 (7.286 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 51800 lr 2.359e-04 [train_loss] tar_ll 1.0029 loss -1.0029 (7.272 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 52000 lr 2.343e-04 [train_loss] tar_ll 1.0807 loss -1.0807 (6.921 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 52200 lr 2.327e-04 [train_loss] tar_ll 0.9762 loss -0.9762 (7.514 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 52400 lr 2.312e-04 [train_loss] tar_ll 1.0415 loss -1.0415 (7.617 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 52600 lr 2.296e-04 [train_loss] tar_ll 1.0561 loss -1.0561 (7.352 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 52800 lr 2.280e-04 [train_loss] tar_ll 0.9558 loss -0.9558 (6.993 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 53000 lr 2.265e-04 [train_loss] tar_ll 1.0219 loss -1.0219 (7.380 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 53200 lr 2.249e-04 [train_loss] tar_ll 1.0412 loss -1.0412 (7.450 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 53400 lr 2.233e-04 [train_loss] tar_ll 1.1124 loss -1.1124 (6.831 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 53600 lr 2.218e-04 [train_loss] tar_ll 1.0267 loss -1.0267 (6.959 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 53800 lr 2.202e-04 [train_loss] tar_ll 0.9667 loss -0.9667 (7.271 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 54000 lr 2.187e-04 [train_loss] tar_ll 1.1250 loss -1.1250 (7.093 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 54200 lr 2.171e-04 [train_loss] tar_ll 1.0143 loss -1.0143 (6.762 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 54400 lr 2.156e-04 [train_loss] tar_ll 0.9936 loss -0.9936 (7.305 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 54600 lr 2.140e-04 [train_loss] tar_ll 1.0779 loss -1.0779 (7.034 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 54800 lr 2.124e-04 [train_loss] tar_ll 1.0246 loss -1.0246 (6.890 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 55000 lr 2.109e-04 [train_loss] tar_ll 1.0387 loss -1.0387 (7.028 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.20it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 1.1064 loss -1.1064 (33.260 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 55200 lr 2.093e-04 [train_loss] tar_ll 1.1205 loss -1.1205 (7.461 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 55400 lr 2.078e-04 [train_loss] tar_ll 1.0767 loss -1.0767 (7.380 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 55600 lr 2.062e-04 [train_loss] tar_ll 1.0562 loss -1.0562 (6.944 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 55800 lr 2.047e-04 [train_loss] tar_ll 1.0821 loss -1.0821 (7.269 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 56000 lr 2.032e-04 [train_loss] tar_ll 1.0567 loss -1.0567 (7.490 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 56200 lr 2.016e-04 [train_loss] tar_ll 1.0732 loss -1.0732 (7.329 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 56400 lr 2.001e-04 [train_loss] tar_ll 1.0691 loss -1.0691 (6.691 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 56600 lr 1.985e-04 [train_loss] tar_ll 1.0280 loss -1.0280 (7.236 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0246 loss -1.0246 (7.194 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 57000 lr 1.955e-04 [train_loss] tar_ll 1.0594 loss -1.0594 (6.979 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 57200 lr 1.939e-04 [train_loss] tar_ll 1.1709 loss -1.1709 (6.885 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 57400 lr 1.924e-04 [train_loss] tar_ll 0.9572 loss -0.9572 (7.254 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 57600 lr 1.909e-04 [train_loss] tar_ll 1.0739 loss -1.0739 (7.104 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 57800 lr 1.894e-04 [train_loss] tar_ll 1.0350 loss -1.0350 (7.117 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 58000 lr 1.878e-04 [train_loss] tar_ll 1.0597 loss -1.0597 (6.585 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 58200 lr 1.863e-04 [train_loss] tar_ll 1.0866 loss -1.0866 (7.408 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 58400 lr 1.848e-04 [train_loss] tar_ll 1.1165 loss -1.1165 (6.860 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 58600 lr 1.833e-04 [train_loss] tar_ll 1.0225 loss -1.0225 (7.172 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 58800 lr 1.818e-04 [train_loss] tar_ll 0.9949 loss -0.9949 (7.403 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 59000 lr 1.803e-04 [train_loss] tar_ll 1.0096 loss -1.0096 (6.825 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 59200 lr 1.787e-04 [train_loss] tar_ll 1.0697 loss -1.0697 (7.099 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 59400 lr 1.772e-04 [train_loss] tar_ll 1.0829 loss -1.0829 (7.343 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 59600 lr 1.757e-04 [train_loss] tar_ll 1.1111 loss -1.1111 (7.352 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 59800 lr 1.742e-04 [train_loss] tar_ll 1.1356 loss -1.1356 (7.093 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 60000 lr 1.727e-04 [train_loss] tar_ll 1.1026 loss -1.1026 (6.515 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.46it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 1.0542 loss -1.0542 (34.702 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 60200 lr 1.713e-04 [train_loss] tar_ll 1.0794 loss -1.0794 (7.488 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 60400 lr 1.698e-04 [train_loss] tar_ll 1.0295 loss -1.0295 (6.757 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 60600 lr 1.683e-04 [train_loss] tar_ll 1.0587 loss -1.0587 (7.264 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 60800 lr 1.668e-04 [train_loss] tar_ll 1.1040 loss -1.1040 (7.343 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 61000 lr 1.653e-04 [train_loss] tar_ll 1.1239 loss -1.1239 (7.159 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 61200 lr 1.638e-04 [train_loss] tar_ll 1.0814 loss -1.0814 (7.140 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 61400 lr 1.624e-04 [train_loss] tar_ll 1.0645 loss -1.0645 (7.407 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 61600 lr 1.609e-04 [train_loss] tar_ll 1.1079 loss -1.1079 (7.101 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 61800 lr 1.594e-04 [train_loss] tar_ll 1.0823 loss -1.0823 (7.127 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 62000 lr 1.580e-04 [train_loss] tar_ll 1.0061 loss -1.0061 (7.419 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 62200 lr 1.565e-04 [train_loss] tar_ll 1.0484 loss -1.0484 (7.439 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 62400 lr 1.551e-04 [train_loss] tar_ll 1.1616 loss -1.1616 (7.220 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 62600 lr 1.536e-04 [train_loss] tar_ll 1.1534 loss -1.1534 (7.090 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 62800 lr 1.522e-04 [train_loss] tar_ll 1.0657 loss -1.0657 (6.730 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 63000 lr 1.507e-04 [train_loss] tar_ll 1.0497 loss -1.0497 (6.885 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 63200 lr 1.493e-04 [train_loss] tar_ll 1.0722 loss -1.0722 (6.914 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 63400 lr 1.478e-04 [train_loss] tar_ll 1.1742 loss -1.1742 (7.188 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 63600 lr 1.464e-04 [train_loss] tar_ll 1.1230 loss -1.1230 (6.998 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 63800 lr 1.450e-04 [train_loss] tar_ll 1.1079 loss -1.1079 (6.797 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 64000 lr 1.436e-04 [train_loss] tar_ll 1.1370 loss -1.1370 (7.125 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 64200 lr 1.421e-04 [train_loss] tar_ll 1.1472 loss -1.1472 (7.641 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 64400 lr 1.407e-04 [train_loss] tar_ll 1.0973 loss -1.0973 (6.771 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 64600 lr 1.393e-04 [train_loss] tar_ll 1.1143 loss -1.1143 (7.041 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 64800 lr 1.379e-04 [train_loss] tar_ll 1.1132 loss -1.1132 (7.135 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 65000 lr 1.365e-04 [train_loss] tar_ll 1.1020 loss -1.1020 (6.921 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.62it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 1.1410 loss -1.1410 (33.108 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 65200 lr 1.351e-04 [train_loss] tar_ll 1.2432 loss -1.2432 (7.094 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 65400 lr 1.337e-04 [train_loss] tar_ll 1.1537 loss -1.1537 (6.824 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 65600 lr 1.323e-04 [train_loss] tar_ll 1.1491 loss -1.1491 (7.125 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 65800 lr 1.309e-04 [train_loss] tar_ll 1.1862 loss -1.1862 (7.145 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 66000 lr 1.296e-04 [train_loss] tar_ll 1.1322 loss -1.1322 (7.176 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 66200 lr 1.282e-04 [train_loss] tar_ll 1.1033 loss -1.1033 (7.130 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 66400 lr 1.268e-04 [train_loss] tar_ll 1.1120 loss -1.1120 (7.321 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 66600 lr 1.255e-04 [train_loss] tar_ll 1.1649 loss -1.1649 (6.838 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 66800 lr 1.241e-04 [train_loss] tar_ll 1.0896 loss -1.0896 (7.033 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 67000 lr 1.227e-04 [train_loss] tar_ll 1.1521 loss -1.1521 (7.259 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 67200 lr 1.214e-04 [train_loss] tar_ll 1.1851 loss -1.1851 (7.253 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 67400 lr 1.200e-04 [train_loss] tar_ll 1.2152 loss -1.2152 (6.771 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 67600 lr 1.187e-04 [train_loss] tar_ll 1.1296 loss -1.1296 (7.088 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 67800 lr 1.174e-04 [train_loss] tar_ll 1.1519 loss -1.1519 (7.305 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 68000 lr 1.160e-04 [train_loss] tar_ll 1.2033 loss -1.2033 (7.033 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 68200 lr 1.147e-04 [train_loss] tar_ll 1.0849 loss -1.0849 (7.192 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 68400 lr 1.134e-04 [train_loss] tar_ll 1.1951 loss -1.1951 (7.114 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 68600 lr 1.121e-04 [train_loss] tar_ll 1.1954 loss -1.1954 (7.132 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 68800 lr 1.108e-04 [train_loss] tar_ll 1.1604 loss -1.1604 (6.790 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 69000 lr 1.095e-04 [train_loss] tar_ll 1.1768 loss -1.1768 (7.209 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 69200 lr 1.082e-04 [train_loss] tar_ll 1.1167 loss -1.1167 (7.051 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 69400 lr 1.069e-04 [train_loss] tar_ll 1.2073 loss -1.2073 (6.818 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 69600 lr 1.056e-04 [train_loss] tar_ll 1.1192 loss -1.1192 (7.287 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 69800 lr 1.043e-04 [train_loss] tar_ll 1.1042 loss -1.1042 (7.349 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 70000 lr 1.031e-04 [train_loss] tar_ll 1.1049 loss -1.1049 (7.104 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.50it/s]\n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 1.1626 loss -1.1626 (33.902 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 70200 lr 1.018e-04 [train_loss] tar_ll 1.1104 loss -1.1104 (7.217 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 70400 lr 1.005e-04 [train_loss] tar_ll 1.2149 loss -1.2149 (7.217 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 70600 lr 9.927e-05 [train_loss] tar_ll 1.1432 loss -1.1432 (7.386 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 70800 lr 9.802e-05 [train_loss] tar_ll 1.1458 loss -1.1458 (7.429 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 71000 lr 9.677e-05 [train_loss] tar_ll 1.1256 loss -1.1256 (6.788 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 71200 lr 9.554e-05 [train_loss] tar_ll 1.2482 loss -1.2482 (7.147 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 71400 lr 9.430e-05 [train_loss] tar_ll 1.1787 loss -1.1787 (7.009 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 71600 lr 9.308e-05 [train_loss] tar_ll 1.2532 loss -1.2532 (6.846 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 71800 lr 9.186e-05 [train_loss] tar_ll 1.1455 loss -1.1455 (6.911 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 72000 lr 9.064e-05 [train_loss] tar_ll 1.1936 loss -1.1936 (7.118 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 72200 lr 8.944e-05 [train_loss] tar_ll 1.1210 loss -1.1210 (7.075 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 72400 lr 8.824e-05 [train_loss] tar_ll 1.2131 loss -1.2131 (6.624 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 72600 lr 8.704e-05 [train_loss] tar_ll 1.0916 loss -1.0916 (7.291 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 72800 lr 8.585e-05 [train_loss] tar_ll 1.0913 loss -1.0913 (7.114 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 73000 lr 8.467e-05 [train_loss] tar_ll 1.2367 loss -1.2367 (6.804 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 73200 lr 8.350e-05 [train_loss] tar_ll 1.1664 loss -1.1664 (7.217 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 73400 lr 8.233e-05 [train_loss] tar_ll 1.1400 loss -1.1400 (6.887 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 73600 lr 8.117e-05 [train_loss] tar_ll 1.1633 loss -1.1633 (6.888 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 73800 lr 8.001e-05 [train_loss] tar_ll 1.2163 loss -1.2163 (6.964 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 74000 lr 7.886e-05 [train_loss] tar_ll 1.2117 loss -1.2117 (7.176 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 74200 lr 7.772e-05 [train_loss] tar_ll 1.1774 loss -1.1774 (6.860 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 74400 lr 7.659e-05 [train_loss] tar_ll 1.2148 loss -1.2148 (7.366 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 74600 lr 7.546e-05 [train_loss] tar_ll 1.2252 loss -1.2252 (7.775 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 74800 lr 7.434e-05 [train_loss] tar_ll 1.1721 loss -1.1721 (6.891 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 75000 lr 7.322e-05 [train_loss] tar_ll 1.2446 loss -1.2446 (7.250 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.09it/s]\n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 1.1942 loss -1.1942 (33.677 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1720 loss -1.1720 (7.182 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 75400 lr 7.102e-05 [train_loss] tar_ll 1.1516 loss -1.1516 (7.333 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 75600 lr 6.992e-05 [train_loss] tar_ll 1.1961 loss -1.1961 (6.930 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 75800 lr 6.884e-05 [train_loss] tar_ll 1.2104 loss -1.2104 (7.022 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 76000 lr 6.776e-05 [train_loss] tar_ll 1.1523 loss -1.1523 (7.097 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 76200 lr 6.669e-05 [train_loss] tar_ll 1.1771 loss -1.1771 (7.024 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 76400 lr 6.562e-05 [train_loss] tar_ll 1.2140 loss -1.2140 (6.961 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 76600 lr 6.456e-05 [train_loss] tar_ll 1.2329 loss -1.2329 (7.270 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 76800 lr 6.351e-05 [train_loss] tar_ll 1.1440 loss -1.1440 (7.211 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 77000 lr 6.247e-05 [train_loss] tar_ll 1.1431 loss -1.1431 (6.911 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 77200 lr 6.144e-05 [train_loss] tar_ll 1.2096 loss -1.2096 (7.096 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 77400 lr 6.041e-05 [train_loss] tar_ll 1.2363 loss -1.2363 (7.319 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 77600 lr 5.939e-05 [train_loss] tar_ll 1.2392 loss -1.2392 (7.136 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2170 loss -1.2170 (7.028 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 78000 lr 5.737e-05 [train_loss] tar_ll 1.2254 loss -1.2254 (8.225 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 78200 lr 5.637e-05 [train_loss] tar_ll 1.0923 loss -1.0923 (6.780 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 78400 lr 5.538e-05 [train_loss] tar_ll 1.2415 loss -1.2415 (6.673 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 78600 lr 5.440e-05 [train_loss] tar_ll 1.2601 loss -1.2601 (7.148 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 78800 lr 5.343e-05 [train_loss] tar_ll 1.1908 loss -1.1908 (7.203 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 79000 lr 5.246e-05 [train_loss] tar_ll 1.2302 loss -1.2302 (6.714 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 79200 lr 5.150e-05 [train_loss] tar_ll 1.2027 loss -1.2027 (7.148 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 79400 lr 5.055e-05 [train_loss] tar_ll 1.2176 loss -1.2176 (7.155 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 79600 lr 4.961e-05 [train_loss] tar_ll 1.2266 loss -1.2266 (6.844 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 79800 lr 4.867e-05 [train_loss] tar_ll 1.1532 loss -1.1532 (8.292 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 80000 lr 4.775e-05 [train_loss] tar_ll 1.0671 loss -1.0671 (7.165 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 84.68it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 1.2150 loss -1.2150 (35.428 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 80200 lr 4.683e-05 [train_loss] tar_ll 1.2490 loss -1.2490 (8.152 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 80400 lr 4.592e-05 [train_loss] tar_ll 1.2568 loss -1.2568 (7.465 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 80600 lr 4.501e-05 [train_loss] tar_ll 1.2456 loss -1.2456 (7.149 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2688 loss -1.2688 (6.747 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 81000 lr 4.323e-05 [train_loss] tar_ll 1.1953 loss -1.1953 (6.433 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 81200 lr 4.235e-05 [train_loss] tar_ll 1.2210 loss -1.2210 (6.390 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 81400 lr 4.148e-05 [train_loss] tar_ll 1.2264 loss -1.2264 (6.413 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 81600 lr 4.062e-05 [train_loss] tar_ll 1.1787 loss -1.1787 (6.328 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2392 loss -1.2392 (6.287 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 82000 lr 3.892e-05 [train_loss] tar_ll 1.2383 loss -1.2383 (6.299 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 82200 lr 3.808e-05 [train_loss] tar_ll 1.2750 loss -1.2750 (6.625 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 82400 lr 3.725e-05 [train_loss] tar_ll 1.2253 loss -1.2253 (6.582 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 82600 lr 3.643e-05 [train_loss] tar_ll 1.1713 loss -1.1713 (6.616 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 82800 lr 3.562e-05 [train_loss] tar_ll 1.1437 loss -1.1437 (7.366 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 83000 lr 3.481e-05 [train_loss] tar_ll 1.1964 loss -1.1964 (6.883 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 83200 lr 3.402e-05 [train_loss] tar_ll 1.1824 loss -1.1824 (6.706 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 83400 lr 3.323e-05 [train_loss] tar_ll 1.2211 loss -1.2211 (7.347 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 83600 lr 3.245e-05 [train_loss] tar_ll 1.2505 loss -1.2505 (7.000 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 83800 lr 3.168e-05 [train_loss] tar_ll 1.1650 loss -1.1650 (6.749 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 84000 lr 3.092e-05 [train_loss] tar_ll 1.2658 loss -1.2658 (6.873 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2006 loss -1.2006 (7.290 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 84400 lr 2.943e-05 [train_loss] tar_ll 1.2788 loss -1.2788 (6.734 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 84600 lr 2.869e-05 [train_loss] tar_ll 1.2565 loss -1.2565 (6.900 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 84800 lr 2.797e-05 [train_loss] tar_ll 1.2123 loss -1.2123 (7.118 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 85000 lr 2.725e-05 [train_loss] tar_ll 1.3133 loss -1.3133 (6.734 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 92.38it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 1.2443 loss -1.2443 (32.478 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 85200 lr 2.654e-05 [train_loss] tar_ll 1.2874 loss -1.2874 (6.954 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 85400 lr 2.584e-05 [train_loss] tar_ll 1.2438 loss -1.2438 (7.057 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 85600 lr 2.515e-05 [train_loss] tar_ll 1.1841 loss -1.1841 (7.353 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 85800 lr 2.447e-05 [train_loss] tar_ll 1.2415 loss -1.2415 (6.950 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 86000 lr 2.379e-05 [train_loss] tar_ll 1.2403 loss -1.2403 (7.070 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 86200 lr 2.313e-05 [train_loss] tar_ll 1.1882 loss -1.1882 (7.315 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 86400 lr 2.247e-05 [train_loss] tar_ll 1.2833 loss -1.2833 (7.057 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 86600 lr 2.183e-05 [train_loss] tar_ll 1.2162 loss -1.2162 (6.713 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 86800 lr 2.119e-05 [train_loss] tar_ll 1.2677 loss -1.2677 (7.150 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 87000 lr 2.056e-05 [train_loss] tar_ll 1.1884 loss -1.1884 (7.333 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 87200 lr 1.994e-05 [train_loss] tar_ll 1.3131 loss -1.3131 (6.721 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 87400 lr 1.933e-05 [train_loss] tar_ll 1.2174 loss -1.2174 (7.159 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 87600 lr 1.873e-05 [train_loss] tar_ll 1.2926 loss -1.2926 (7.276 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 87800 lr 1.814e-05 [train_loss] tar_ll 1.1899 loss -1.1899 (7.252 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 88000 lr 1.756e-05 [train_loss] tar_ll 1.3283 loss -1.3283 (7.093 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 88200 lr 1.698e-05 [train_loss] tar_ll 1.2874 loss -1.2874 (7.696 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 88400 lr 1.642e-05 [train_loss] tar_ll 1.2187 loss -1.2187 (7.562 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 88600 lr 1.586e-05 [train_loss] tar_ll 1.2454 loss -1.2454 (7.217 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 88800 lr 1.532e-05 [train_loss] tar_ll 1.2726 loss -1.2726 (6.990 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 89000 lr 1.478e-05 [train_loss] tar_ll 1.3016 loss -1.3016 (7.192 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 89200 lr 1.425e-05 [train_loss] tar_ll 1.2550 loss -1.2550 (7.092 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 89400 lr 1.373e-05 [train_loss] tar_ll 1.2533 loss -1.2533 (6.622 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 89600 lr 1.323e-05 [train_loss] tar_ll 1.1534 loss -1.1534 (7.413 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2079 loss -1.2079 (7.490 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 90000 lr 1.224e-05 [train_loss] tar_ll 1.3061 loss -1.3061 (6.715 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.94it/s]\n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 1.2609 loss -1.2609 (34.507 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 90200 lr 1.176e-05 [train_loss] tar_ll 1.3551 loss -1.3551 (7.154 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 90400 lr 1.128e-05 [train_loss] tar_ll 1.2769 loss -1.2769 (6.959 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 90600 lr 1.082e-05 [train_loss] tar_ll 1.1892 loss -1.1892 (7.173 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 90800 lr 1.037e-05 [train_loss] tar_ll 1.2306 loss -1.2306 (6.983 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 91000 lr 9.927e-06 [train_loss] tar_ll 1.2579 loss -1.2579 (6.793 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2962 loss -1.2962 (7.182 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2515 loss -1.2515 (7.159 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 91600 lr 8.655e-06 [train_loss] tar_ll 1.2708 loss -1.2708 (6.680 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2702 loss -1.2702 (7.112 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2430 loss -1.2430 (7.296 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 92200 lr 7.468e-06 [train_loss] tar_ll 1.2654 loss -1.2654 (7.122 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 92400 lr 7.092e-06 [train_loss] tar_ll 1.2623 loss -1.2623 (6.978 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 92600 lr 6.725e-06 [train_loss] tar_ll 1.2590 loss -1.2590 (7.336 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 92800 lr 6.368e-06 [train_loss] tar_ll 1.1831 loss -1.1831 (7.199 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 93000 lr 6.021e-06 [train_loss] tar_ll 1.4167 loss -1.4167 (6.768 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 93200 lr 5.683e-06 [train_loss] tar_ll 1.1907 loss -1.1907 (7.009 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 93400 lr 5.355e-06 [train_loss] tar_ll 1.2444 loss -1.2444 (7.457 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 93600 lr 5.036e-06 [train_loss] tar_ll 1.3208 loss -1.3208 (6.745 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 93800 lr 4.727e-06 [train_loss] tar_ll 1.2350 loss -1.2350 (7.060 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 94000 lr 4.428e-06 [train_loss] tar_ll 1.1839 loss -1.1839 (7.408 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 94200 lr 4.139e-06 [train_loss] tar_ll 1.2238 loss -1.2238 (6.787 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 94400 lr 3.859e-06 [train_loss] tar_ll 1.2709 loss -1.2709 (6.734 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 94600 lr 3.589e-06 [train_loss] tar_ll 1.1833 loss -1.1833 (7.267 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 94800 lr 3.329e-06 [train_loss] tar_ll 1.2718 loss -1.2718 (6.836 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 95000 lr 3.078e-06 [train_loss] tar_ll 1.1865 loss -1.1865 (6.757 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 88.22it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 1.2723 loss -1.2723 (34.004 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-64_rbf step 95200 lr 2.837e-06 [train_loss] tar_ll 1.2208 loss -1.2208 (6.928 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 95400 lr 2.606e-06 [train_loss] tar_ll 1.3012 loss -1.3012 (7.322 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 95600 lr 2.385e-06 [train_loss] tar_ll 1.2255 loss -1.2255 (7.148 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 95800 lr 2.173e-06 [train_loss] tar_ll 1.2723 loss -1.2723 (7.155 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 96000 lr 1.971e-06 [train_loss] tar_ll 1.2697 loss -1.2697 (7.458 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2630 loss -1.2630 (7.589 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 96400 lr 1.597e-06 [train_loss] tar_ll 1.3032 loss -1.3032 (7.277 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 96600 lr 1.425e-06 [train_loss] tar_ll 1.2227 loss -1.2227 (6.570 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 96800 lr 1.262e-06 [train_loss] tar_ll 1.3292 loss -1.3292 (6.997 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 97000 lr 1.110e-06 [train_loss] tar_ll 1.3379 loss -1.3379 (7.186 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 97200 lr 9.666e-07 [train_loss] tar_ll 1.2683 loss -1.2683 (7.051 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 97400 lr 8.335e-07 [train_loss] tar_ll 1.1790 loss -1.1790 (7.368 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 97600 lr 7.103e-07 [train_loss] tar_ll 1.2947 loss -1.2947 (6.959 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 97800 lr 5.969e-07 [train_loss] tar_ll 1.2981 loss -1.2981 (6.800 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 98000 lr 4.933e-07 [train_loss] tar_ll 1.3507 loss -1.3507 (6.928 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 98200 lr 3.996e-07 [train_loss] tar_ll 1.2473 loss -1.2473 (6.500 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 98400 lr 3.158e-07 [train_loss] tar_ll 1.2585 loss -1.2585 (6.665 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 98600 lr 2.418e-07 [train_loss] tar_ll 1.2548 loss -1.2548 (6.466 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2081 loss -1.2081 (6.490 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 99000 lr 1.234e-07 [train_loss] tar_ll 1.2846 loss -1.2846 (6.695 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 99200 lr 7.895e-08 [train_loss] tar_ll 1.3427 loss -1.3427 (6.684 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 99400 lr 4.441e-08 [train_loss] tar_ll 1.3031 loss -1.3031 (6.723 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 99600 lr 1.974e-08 [train_loss] tar_ll 1.3066 loss -1.3066 (6.877 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 99800 lr 4.935e-09 [train_loss] tar_ll 1.2324 loss -1.2324 (6.870 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf step 100000 lr 0.000e+00 [train_loss] tar_ll 1.2269 loss -1.2269 (7.327 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.23it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 1.2739 loss -1.2739 (33.250 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.99it/s] \n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 1.2739 loss -1.2739 (34.098 secs)\n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 1.2739 loss -1.2739 (34.098 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4172813.25 miliseconds\n",
      "Execution time: 4172.81325 seconds\n",
      "Initial Memory Usage: 0.0 MB\n",
      "Final Memory Usage: 103.130859375 MB\n",
      "Memory Usage Change: 103.130859375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='lbanp', name='lbanp-num_latents-64_rbf',val_seed=0, val_l=64,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0bf182-c33f-46d4-8ea7-daf978b5e5b7",
   "metadata": {},
   "source": [
    "## LBANP (256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a501ef-ad2c-4d3c-8492-3dd8adf91b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: lbanp-lbanp-num_latents-256_rbf_\n",
      "Total number of parameters: 800706\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 256\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-256_rbf_ step 200 lr 5.000e-04 [train_loss] tar_ll -0.7011 loss 0.7011 (8.607 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 400 lr 5.000e-04 [train_loss] tar_ll -0.6514 loss 0.6514 (8.215 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 600 lr 5.000e-04 [train_loss] tar_ll -0.5647 loss 0.5647 (9.374 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 800 lr 4.999e-04 [train_loss] tar_ll -0.5407 loss 0.5407 (8.591 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 1000 lr 4.999e-04 [train_loss] tar_ll -0.5170 loss 0.5170 (8.797 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 1200 lr 4.998e-04 [train_loss] tar_ll -0.4261 loss 0.4261 (8.996 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 1400 lr 4.998e-04 [train_loss] tar_ll -0.2571 loss 0.2571 (9.289 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 1600 lr 4.997e-04 [train_loss] tar_ll -0.2389 loss 0.2389 (8.759 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 1800 lr 4.996e-04 [train_loss] tar_ll -0.1661 loss 0.1661 (8.858 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 2000 lr 4.995e-04 [train_loss] tar_ll -0.0626 loss 0.0626 (8.870 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 2200 lr 4.994e-04 [train_loss] tar_ll 0.0379 loss -0.0379 (8.409 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 2400 lr 4.993e-04 [train_loss] tar_ll 0.0387 loss -0.0387 (8.890 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 2600 lr 4.992e-04 [train_loss] tar_ll 0.0496 loss -0.0496 (8.718 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 2800 lr 4.990e-04 [train_loss] tar_ll 0.0696 loss -0.0696 (8.665 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 3000 lr 4.989e-04 [train_loss] tar_ll 0.0654 loss -0.0654 (8.996 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 3200 lr 4.987e-04 [train_loss] tar_ll -0.0020 loss 0.0020 (9.270 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 3400 lr 4.986e-04 [train_loss] tar_ll 0.1543 loss -0.1543 (8.952 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 3600 lr 4.984e-04 [train_loss] tar_ll 0.0478 loss -0.0478 (8.888 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 3800 lr 4.982e-04 [train_loss] tar_ll 0.1549 loss -0.1549 (8.837 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 4000 lr 4.980e-04 [train_loss] tar_ll 0.1717 loss -0.1717 (8.604 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 4200 lr 4.978e-04 [train_loss] tar_ll 0.1230 loss -0.1230 (8.869 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 4400 lr 4.976e-04 [train_loss] tar_ll -0.1335 loss 0.1335 (8.865 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 4600 lr 4.974e-04 [train_loss] tar_ll 0.1211 loss -0.1211 (10.127 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 4800 lr 4.972e-04 [train_loss] tar_ll 0.0727 loss -0.0727 (9.222 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 5000 lr 4.969e-04 [train_loss] tar_ll 0.1104 loss -0.1104 (8.726 secs)\n",
      "100%|##########| 3000/3000 [00:43<00:00, 68.88it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.0980 loss -0.0980 (43.553 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 5200 lr 4.967e-04 [train_loss] tar_ll 0.2258 loss -0.2258 (8.969 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 5400 lr 4.964e-04 [train_loss] tar_ll 0.3122 loss -0.3122 (9.513 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 5600 lr 4.961e-04 [train_loss] tar_ll 0.0230 loss -0.0230 (8.618 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 5800 lr 4.959e-04 [train_loss] tar_ll 0.0692 loss -0.0692 (9.039 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 6000 lr 4.956e-04 [train_loss] tar_ll 0.0841 loss -0.0841 (8.779 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 6200 lr 4.953e-04 [train_loss] tar_ll 0.2131 loss -0.2131 (8.608 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 6400 lr 4.950e-04 [train_loss] tar_ll 0.2525 loss -0.2525 (8.556 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 6600 lr 4.946e-04 [train_loss] tar_ll 0.2787 loss -0.2787 (8.311 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 6800 lr 4.943e-04 [train_loss] tar_ll -0.1235 loss 0.1235 (8.570 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 7000 lr 4.940e-04 [train_loss] tar_ll -0.1456 loss 0.1456 (8.242 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 7200 lr 4.936e-04 [train_loss] tar_ll -0.3191 loss 0.3191 (8.602 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 7400 lr 4.933e-04 [train_loss] tar_ll -0.1148 loss 0.1148 (8.530 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 7600 lr 4.929e-04 [train_loss] tar_ll -0.1563 loss 0.1563 (9.213 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 7800 lr 4.925e-04 [train_loss] tar_ll -0.0817 loss 0.0817 (8.211 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 8000 lr 4.921e-04 [train_loss] tar_ll 0.0339 loss -0.0339 (8.481 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 8200 lr 4.918e-04 [train_loss] tar_ll 0.0844 loss -0.0844 (8.297 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 8400 lr 4.913e-04 [train_loss] tar_ll 0.2233 loss -0.2233 (8.571 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 8600 lr 4.909e-04 [train_loss] tar_ll 0.3092 loss -0.3092 (8.705 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 8800 lr 4.905e-04 [train_loss] tar_ll 0.2192 loss -0.2192 (9.110 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 9000 lr 4.901e-04 [train_loss] tar_ll -0.0011 loss 0.0011 (9.455 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 9200 lr 4.896e-04 [train_loss] tar_ll 0.0513 loss -0.0513 (8.777 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 9400 lr 4.892e-04 [train_loss] tar_ll 0.1281 loss -0.1281 (9.361 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 9600 lr 4.887e-04 [train_loss] tar_ll -0.0646 loss 0.0646 (8.690 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 9800 lr 4.882e-04 [train_loss] tar_ll -0.0732 loss 0.0732 (8.770 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 10000 lr 4.878e-04 [train_loss] tar_ll -0.2703 loss 0.2703 (8.475 secs)\n",
      "100%|##########| 3000/3000 [00:43<00:00, 68.90it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll -0.1033 loss 0.1033 (43.542 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 10200 lr 4.873e-04 [train_loss] tar_ll -0.1106 loss 0.1106 (9.093 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 10400 lr 4.868e-04 [train_loss] tar_ll -0.2439 loss 0.2439 (8.610 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 10600 lr 4.863e-04 [train_loss] tar_ll -0.0822 loss 0.0822 (8.887 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 10800 lr 4.857e-04 [train_loss] tar_ll 0.0037 loss -0.0037 (8.500 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 11000 lr 4.852e-04 [train_loss] tar_ll -0.2954 loss 0.2954 (8.740 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 11200 lr 4.847e-04 [train_loss] tar_ll -0.2920 loss 0.2920 (8.428 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 11400 lr 4.841e-04 [train_loss] tar_ll -0.2860 loss 0.2860 (8.450 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 11600 lr 4.836e-04 [train_loss] tar_ll -0.2232 loss 0.2232 (8.241 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 11800 lr 4.830e-04 [train_loss] tar_ll -0.1773 loss 0.1773 (8.428 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 12000 lr 4.824e-04 [train_loss] tar_ll -0.0889 loss 0.0889 (8.675 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 12200 lr 4.819e-04 [train_loss] tar_ll -0.0644 loss 0.0644 (8.733 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 12400 lr 4.813e-04 [train_loss] tar_ll -0.1066 loss 0.1066 (9.624 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 12600 lr 4.807e-04 [train_loss] tar_ll -0.0843 loss 0.0843 (9.055 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 12800 lr 4.801e-04 [train_loss] tar_ll -0.0921 loss 0.0921 (9.278 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 13000 lr 4.794e-04 [train_loss] tar_ll -0.1263 loss 0.1263 (8.402 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 13200 lr 4.788e-04 [train_loss] tar_ll -0.0869 loss 0.0869 (8.566 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 13400 lr 4.782e-04 [train_loss] tar_ll -0.0679 loss 0.0679 (8.274 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 13600 lr 4.775e-04 [train_loss] tar_ll -0.0370 loss 0.0370 (8.772 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 13800 lr 4.769e-04 [train_loss] tar_ll -0.2283 loss 0.2283 (8.253 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 14000 lr 4.762e-04 [train_loss] tar_ll -0.1576 loss 0.1576 (8.528 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 14200 lr 4.755e-04 [train_loss] tar_ll -0.1055 loss 0.1055 (8.371 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 14400 lr 4.749e-04 [train_loss] tar_ll -0.0295 loss 0.0295 (8.285 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 14600 lr 4.742e-04 [train_loss] tar_ll 0.0112 loss -0.0112 (8.288 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 14800 lr 4.735e-04 [train_loss] tar_ll -0.0546 loss 0.0546 (8.458 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 15000 lr 4.728e-04 [train_loss] tar_ll -0.0604 loss 0.0604 (8.827 secs)\n",
      "100%|##########| 3000/3000 [00:42<00:00, 70.38it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll -0.0653 loss 0.0653 (42.629 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 15200 lr 4.720e-04 [train_loss] tar_ll -0.0804 loss 0.0804 (8.995 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 15400 lr 4.713e-04 [train_loss] tar_ll -0.0227 loss 0.0227 (9.269 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 15600 lr 4.706e-04 [train_loss] tar_ll 0.1093 loss -0.1093 (9.246 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 15800 lr 4.698e-04 [train_loss] tar_ll -0.2415 loss 0.2415 (9.336 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 16000 lr 4.691e-04 [train_loss] tar_ll -0.1703 loss 0.1703 (8.915 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 16200 lr 4.683e-04 [train_loss] tar_ll -0.1238 loss 0.1238 (8.952 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 16400 lr 4.675e-04 [train_loss] tar_ll -0.0546 loss 0.0546 (8.770 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 16600 lr 4.668e-04 [train_loss] tar_ll -0.1012 loss 0.1012 (8.682 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 16800 lr 4.660e-04 [train_loss] tar_ll -0.0478 loss 0.0478 (8.717 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 17000 lr 4.652e-04 [train_loss] tar_ll -0.0440 loss 0.0440 (8.843 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 17200 lr 4.644e-04 [train_loss] tar_ll -0.0008 loss 0.0008 (8.681 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 17400 lr 4.636e-04 [train_loss] tar_ll 0.0099 loss -0.0099 (8.774 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 17600 lr 4.627e-04 [train_loss] tar_ll 0.0405 loss -0.0405 (9.471 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 17800 lr 4.619e-04 [train_loss] tar_ll 0.0134 loss -0.0134 (8.873 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 18000 lr 4.611e-04 [train_loss] tar_ll -0.0216 loss 0.0216 (8.879 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 18200 lr 4.602e-04 [train_loss] tar_ll 0.0584 loss -0.0584 (8.719 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 18400 lr 4.594e-04 [train_loss] tar_ll 0.0773 loss -0.0773 (8.958 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 18600 lr 4.585e-04 [train_loss] tar_ll -0.0091 loss 0.0091 (8.688 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 18800 lr 4.576e-04 [train_loss] tar_ll 0.1058 loss -0.1058 (8.842 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 19000 lr 4.568e-04 [train_loss] tar_ll 0.1084 loss -0.1084 (8.093 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 19200 lr 4.559e-04 [train_loss] tar_ll 0.0971 loss -0.0971 (8.710 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 19400 lr 4.550e-04 [train_loss] tar_ll 0.1416 loss -0.1416 (8.842 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 19600 lr 4.541e-04 [train_loss] tar_ll 0.0783 loss -0.0783 (8.482 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 19800 lr 4.532e-04 [train_loss] tar_ll 0.1140 loss -0.1140 (8.658 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 20000 lr 4.523e-04 [train_loss] tar_ll 0.1010 loss -0.1010 (9.392 secs)\n",
      "100%|##########| 3000/3000 [00:43<00:00, 68.32it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.0692 loss -0.0692 (43.914 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 20200 lr 4.513e-04 [train_loss] tar_ll 0.1243 loss -0.1243 (8.656 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 20400 lr 4.504e-04 [train_loss] tar_ll 0.0763 loss -0.0763 (8.077 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 20600 lr 4.494e-04 [train_loss] tar_ll 0.1002 loss -0.1002 (8.407 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 20800 lr 4.485e-04 [train_loss] tar_ll 0.0841 loss -0.0841 (8.141 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 21000 lr 4.475e-04 [train_loss] tar_ll 0.1204 loss -0.1204 (8.851 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 21200 lr 4.466e-04 [train_loss] tar_ll 0.0767 loss -0.0767 (8.812 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 21400 lr 4.456e-04 [train_loss] tar_ll 0.1265 loss -0.1265 (8.838 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 21600 lr 4.446e-04 [train_loss] tar_ll 0.1115 loss -0.1115 (8.404 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 21800 lr 4.436e-04 [train_loss] tar_ll 0.1755 loss -0.1755 (9.034 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 22000 lr 4.426e-04 [train_loss] tar_ll 0.1948 loss -0.1948 (8.593 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 22200 lr 4.416e-04 [train_loss] tar_ll 0.1353 loss -0.1353 (9.009 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 22400 lr 4.406e-04 [train_loss] tar_ll 0.2115 loss -0.2115 (8.713 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 22600 lr 4.396e-04 [train_loss] tar_ll 0.0915 loss -0.0915 (8.658 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 22800 lr 4.386e-04 [train_loss] tar_ll 0.0553 loss -0.0553 (8.744 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 23000 lr 4.375e-04 [train_loss] tar_ll 0.0949 loss -0.0949 (8.594 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 23200 lr 4.365e-04 [train_loss] tar_ll 0.1491 loss -0.1491 (8.858 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 23400 lr 4.354e-04 [train_loss] tar_ll 0.1364 loss -0.1364 (8.449 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 23600 lr 4.344e-04 [train_loss] tar_ll 0.1550 loss -0.1550 (8.967 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 23800 lr 4.333e-04 [train_loss] tar_ll 0.1455 loss -0.1455 (8.488 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 24000 lr 4.322e-04 [train_loss] tar_ll 0.1823 loss -0.1823 (8.489 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 24200 lr 4.312e-04 [train_loss] tar_ll 0.1555 loss -0.1555 (8.019 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 24400 lr 4.301e-04 [train_loss] tar_ll 0.1864 loss -0.1864 (8.429 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 24600 lr 4.290e-04 [train_loss] tar_ll 0.1749 loss -0.1749 (8.592 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 24800 lr 4.279e-04 [train_loss] tar_ll 0.1648 loss -0.1648 (9.167 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 25000 lr 4.268e-04 [train_loss] tar_ll 0.1899 loss -0.1899 (8.883 secs)\n",
      "100%|##########| 3000/3000 [00:45<00:00, 66.07it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.2577 loss -0.2577 (45.409 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 25200 lr 4.257e-04 [train_loss] tar_ll 0.2256 loss -0.2256 (8.780 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 25400 lr 4.245e-04 [train_loss] tar_ll 0.2105 loss -0.2105 (9.092 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 25600 lr 4.234e-04 [train_loss] tar_ll 0.1674 loss -0.1674 (9.363 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 25800 lr 4.223e-04 [train_loss] tar_ll 0.2426 loss -0.2426 (10.141 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 26000 lr 4.211e-04 [train_loss] tar_ll 0.2655 loss -0.2655 (10.176 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 26200 lr 4.200e-04 [train_loss] tar_ll 0.2487 loss -0.2487 (9.815 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 26400 lr 4.188e-04 [train_loss] tar_ll 0.2389 loss -0.2389 (11.172 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 26600 lr 4.177e-04 [train_loss] tar_ll 0.2379 loss -0.2379 (8.324 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 26800 lr 4.165e-04 [train_loss] tar_ll 0.2067 loss -0.2067 (8.989 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 27000 lr 4.153e-04 [train_loss] tar_ll 0.2181 loss -0.2181 (8.168 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 27200 lr 4.141e-04 [train_loss] tar_ll 0.1869 loss -0.1869 (8.644 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 27400 lr 4.130e-04 [train_loss] tar_ll 0.2563 loss -0.2563 (8.292 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 27600 lr 4.118e-04 [train_loss] tar_ll 0.2418 loss -0.2418 (8.491 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 27800 lr 4.106e-04 [train_loss] tar_ll 0.2292 loss -0.2292 (8.127 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 28000 lr 4.094e-04 [train_loss] tar_ll 0.2126 loss -0.2126 (8.749 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 28200 lr 4.081e-04 [train_loss] tar_ll 0.2583 loss -0.2583 (8.702 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 28400 lr 4.069e-04 [train_loss] tar_ll 0.2572 loss -0.2572 (8.719 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 28600 lr 4.057e-04 [train_loss] tar_ll 0.2779 loss -0.2779 (9.082 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 28800 lr 4.045e-04 [train_loss] tar_ll 0.2321 loss -0.2321 (8.491 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 29000 lr 4.032e-04 [train_loss] tar_ll 0.3037 loss -0.3037 (8.828 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 29200 lr 4.020e-04 [train_loss] tar_ll 0.3004 loss -0.3004 (8.657 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 29400 lr 4.007e-04 [train_loss] tar_ll 0.2889 loss -0.2889 (9.110 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 29600 lr 3.995e-04 [train_loss] tar_ll 0.2728 loss -0.2728 (8.368 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 29800 lr 3.982e-04 [train_loss] tar_ll 0.2590 loss -0.2590 (8.754 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 30000 lr 3.969e-04 [train_loss] tar_ll 0.2164 loss -0.2164 (8.637 secs)\n",
      "100%|##########| 3000/3000 [00:44<00:00, 67.50it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.2932 loss -0.2932 (44.447 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 30200 lr 3.957e-04 [train_loss] tar_ll 0.3365 loss -0.3365 (9.472 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 30400 lr 3.944e-04 [train_loss] tar_ll 0.2344 loss -0.2344 (9.118 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 30600 lr 3.931e-04 [train_loss] tar_ll 0.3374 loss -0.3374 (8.805 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 30800 lr 3.918e-04 [train_loss] tar_ll 0.3379 loss -0.3379 (8.832 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 31000 lr 3.905e-04 [train_loss] tar_ll 0.3196 loss -0.3196 (8.620 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 31200 lr 3.892e-04 [train_loss] tar_ll 0.2675 loss -0.2675 (9.029 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 31400 lr 3.879e-04 [train_loss] tar_ll 0.3466 loss -0.3466 (8.269 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 31600 lr 3.866e-04 [train_loss] tar_ll 0.3946 loss -0.3946 (8.840 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 31800 lr 3.853e-04 [train_loss] tar_ll 0.3455 loss -0.3455 (8.367 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 32000 lr 3.840e-04 [train_loss] tar_ll 0.3040 loss -0.3040 (9.202 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 32200 lr 3.826e-04 [train_loss] tar_ll 0.3278 loss -0.3278 (8.313 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 32400 lr 3.813e-04 [train_loss] tar_ll 0.1268 loss -0.1268 (8.665 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 32600 lr 3.800e-04 [train_loss] tar_ll 0.1783 loss -0.1783 (8.613 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 32800 lr 3.786e-04 [train_loss] tar_ll 0.1798 loss -0.1798 (8.582 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 33000 lr 3.773e-04 [train_loss] tar_ll 0.2992 loss -0.2992 (8.200 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 33200 lr 3.759e-04 [train_loss] tar_ll 0.3620 loss -0.3620 (8.763 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 33400 lr 3.745e-04 [train_loss] tar_ll 0.3401 loss -0.3401 (8.469 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 33600 lr 3.732e-04 [train_loss] tar_ll 0.3943 loss -0.3943 (9.070 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 33800 lr 3.718e-04 [train_loss] tar_ll 0.3229 loss -0.3229 (8.651 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 34000 lr 3.704e-04 [train_loss] tar_ll 0.2374 loss -0.2374 (9.125 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 34200 lr 3.691e-04 [train_loss] tar_ll 0.2922 loss -0.2922 (9.520 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 34400 lr 3.677e-04 [train_loss] tar_ll 0.3123 loss -0.3123 (8.842 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 34600 lr 3.663e-04 [train_loss] tar_ll 0.3108 loss -0.3108 (8.785 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 34800 lr 3.649e-04 [train_loss] tar_ll 0.4132 loss -0.4132 (8.083 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 35000 lr 3.635e-04 [train_loss] tar_ll 0.3835 loss -0.3835 (8.743 secs)\n",
      "100%|##########| 3000/3000 [00:43<00:00, 69.26it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.4169 loss -0.4169 (43.321 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 35200 lr 3.621e-04 [train_loss] tar_ll 0.3907 loss -0.3907 (8.548 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 35400 lr 3.607e-04 [train_loss] tar_ll 0.3882 loss -0.3882 (9.277 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 35600 lr 3.593e-04 [train_loss] tar_ll 0.3348 loss -0.3348 (8.636 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 35800 lr 3.579e-04 [train_loss] tar_ll 0.3701 loss -0.3701 (8.920 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 36000 lr 3.564e-04 [train_loss] tar_ll 0.3538 loss -0.3538 (8.735 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 36200 lr 3.550e-04 [train_loss] tar_ll 0.3960 loss -0.3960 (9.294 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 36400 lr 3.536e-04 [train_loss] tar_ll 0.3721 loss -0.3721 (8.221 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 36600 lr 3.522e-04 [train_loss] tar_ll 0.4133 loss -0.4133 (8.380 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 36800 lr 3.507e-04 [train_loss] tar_ll 0.4157 loss -0.4157 (8.170 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 37000 lr 3.493e-04 [train_loss] tar_ll 0.3886 loss -0.3886 (9.089 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 37200 lr 3.478e-04 [train_loss] tar_ll 0.3721 loss -0.3721 (9.025 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 37400 lr 3.464e-04 [train_loss] tar_ll 0.3058 loss -0.3058 (8.404 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 37600 lr 3.449e-04 [train_loss] tar_ll 0.2835 loss -0.2835 (8.540 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 37800 lr 3.435e-04 [train_loss] tar_ll 0.3321 loss -0.3321 (8.558 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 38000 lr 3.420e-04 [train_loss] tar_ll 0.3285 loss -0.3285 (8.975 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 38200 lr 3.406e-04 [train_loss] tar_ll 0.3371 loss -0.3371 (9.149 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 38400 lr 3.391e-04 [train_loss] tar_ll 0.4659 loss -0.4659 (8.934 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 38600 lr 3.376e-04 [train_loss] tar_ll 0.3845 loss -0.3845 (8.293 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 38800 lr 3.362e-04 [train_loss] tar_ll 0.3747 loss -0.3747 (9.153 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 39000 lr 3.347e-04 [train_loss] tar_ll 0.4260 loss -0.4260 (8.782 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 39200 lr 3.332e-04 [train_loss] tar_ll 0.3954 loss -0.3954 (8.788 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 39400 lr 3.317e-04 [train_loss] tar_ll 0.4331 loss -0.4331 (8.755 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 39600 lr 3.302e-04 [train_loss] tar_ll 0.4452 loss -0.4452 (8.928 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 39800 lr 3.287e-04 [train_loss] tar_ll 0.3862 loss -0.3862 (8.922 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 40000 lr 3.273e-04 [train_loss] tar_ll 0.3685 loss -0.3685 (8.722 secs)\n",
      "100%|##########| 3000/3000 [00:43<00:00, 69.49it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.4721 loss -0.4721 (43.176 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 40200 lr 3.258e-04 [train_loss] tar_ll 0.4030 loss -0.4030 (9.727 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 40400 lr 3.243e-04 [train_loss] tar_ll 0.4334 loss -0.4334 (9.132 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 40600 lr 3.228e-04 [train_loss] tar_ll 0.4579 loss -0.4579 (9.173 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 40800 lr 3.213e-04 [train_loss] tar_ll 0.4515 loss -0.4515 (8.713 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 41000 lr 3.197e-04 [train_loss] tar_ll 0.3740 loss -0.3740 (8.536 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 41200 lr 3.182e-04 [train_loss] tar_ll 0.4359 loss -0.4359 (8.256 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 41400 lr 3.167e-04 [train_loss] tar_ll 0.4716 loss -0.4716 (9.716 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 41600 lr 3.152e-04 [train_loss] tar_ll 0.4398 loss -0.4398 (8.757 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 41800 lr 3.137e-04 [train_loss] tar_ll 0.4652 loss -0.4652 (8.628 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 42000 lr 3.122e-04 [train_loss] tar_ll 0.4381 loss -0.4381 (8.788 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 42200 lr 3.106e-04 [train_loss] tar_ll 0.4837 loss -0.4837 (8.856 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 42400 lr 3.091e-04 [train_loss] tar_ll 0.4176 loss -0.4176 (9.156 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 42600 lr 3.076e-04 [train_loss] tar_ll 0.4557 loss -0.4557 (8.973 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 42800 lr 3.061e-04 [train_loss] tar_ll 0.4101 loss -0.4101 (8.915 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 43000 lr 3.045e-04 [train_loss] tar_ll 0.4373 loss -0.4373 (8.402 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 43200 lr 3.030e-04 [train_loss] tar_ll 0.4396 loss -0.4396 (8.799 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 43400 lr 3.015e-04 [train_loss] tar_ll 0.4342 loss -0.4342 (8.582 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 43600 lr 2.999e-04 [train_loss] tar_ll 0.4368 loss -0.4368 (8.274 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 43800 lr 2.984e-04 [train_loss] tar_ll 0.3929 loss -0.3929 (8.808 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 44000 lr 2.968e-04 [train_loss] tar_ll 0.5321 loss -0.5321 (8.714 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 44200 lr 2.953e-04 [train_loss] tar_ll 0.4909 loss -0.4909 (9.076 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 44400 lr 2.938e-04 [train_loss] tar_ll 0.5005 loss -0.5005 (8.550 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 44600 lr 2.922e-04 [train_loss] tar_ll 0.5120 loss -0.5120 (8.840 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 44800 lr 2.907e-04 [train_loss] tar_ll 0.5041 loss -0.5041 (8.603 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 45000 lr 2.891e-04 [train_loss] tar_ll 0.4332 loss -0.4332 (8.787 secs)\n",
      "100%|##########| 3000/3000 [00:44<00:00, 67.04it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.4416 loss -0.4416 (44.755 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 45200 lr 2.876e-04 [train_loss] tar_ll 0.4641 loss -0.4641 (8.846 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 45400 lr 2.860e-04 [train_loss] tar_ll 0.4790 loss -0.4790 (8.943 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 45600 lr 2.844e-04 [train_loss] tar_ll 0.5191 loss -0.5191 (8.201 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 45800 lr 2.829e-04 [train_loss] tar_ll 0.4563 loss -0.4563 (8.737 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 46000 lr 2.813e-04 [train_loss] tar_ll 0.5445 loss -0.5445 (8.711 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 46200 lr 2.798e-04 [train_loss] tar_ll 0.5070 loss -0.5070 (9.068 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 46400 lr 2.782e-04 [train_loss] tar_ll 0.4923 loss -0.4923 (9.171 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 46600 lr 2.767e-04 [train_loss] tar_ll 0.4835 loss -0.4835 (8.596 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 46800 lr 2.751e-04 [train_loss] tar_ll 0.5058 loss -0.5058 (8.531 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 47000 lr 2.735e-04 [train_loss] tar_ll 0.5055 loss -0.5055 (8.239 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 47200 lr 2.720e-04 [train_loss] tar_ll 0.5502 loss -0.5502 (8.664 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 47400 lr 2.704e-04 [train_loss] tar_ll 0.5013 loss -0.5013 (8.403 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 47600 lr 2.688e-04 [train_loss] tar_ll 0.4563 loss -0.4563 (8.648 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 47800 lr 2.673e-04 [train_loss] tar_ll 0.5204 loss -0.5204 (8.267 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 48000 lr 2.657e-04 [train_loss] tar_ll 0.5161 loss -0.5161 (8.500 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 48200 lr 2.641e-04 [train_loss] tar_ll 0.5301 loss -0.5301 (8.207 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 48400 lr 2.626e-04 [train_loss] tar_ll 0.5122 loss -0.5122 (8.581 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 48600 lr 2.610e-04 [train_loss] tar_ll 0.4817 loss -0.4817 (8.188 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 48800 lr 2.594e-04 [train_loss] tar_ll 0.4920 loss -0.4920 (8.830 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 49000 lr 2.579e-04 [train_loss] tar_ll 0.4512 loss -0.4512 (8.344 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 49200 lr 2.563e-04 [train_loss] tar_ll 0.5388 loss -0.5388 (9.011 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 49400 lr 2.547e-04 [train_loss] tar_ll 0.5835 loss -0.5835 (9.397 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 49600 lr 2.531e-04 [train_loss] tar_ll 0.5694 loss -0.5694 (8.852 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 49800 lr 2.516e-04 [train_loss] tar_ll 0.5472 loss -0.5472 (8.982 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 50000 lr 2.500e-04 [train_loss] tar_ll 0.5231 loss -0.5231 (8.627 secs)\n",
      "100%|##########| 3000/3000 [00:44<00:00, 68.00it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.5600 loss -0.5600 (44.118 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 50200 lr 2.484e-04 [train_loss] tar_ll 0.5474 loss -0.5474 (8.810 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 50400 lr 2.469e-04 [train_loss] tar_ll 0.5775 loss -0.5775 (8.446 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 50600 lr 2.453e-04 [train_loss] tar_ll 0.5431 loss -0.5431 (8.583 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 50800 lr 2.437e-04 [train_loss] tar_ll 0.4675 loss -0.4675 (8.279 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 51000 lr 2.421e-04 [train_loss] tar_ll 0.5174 loss -0.5174 (9.636 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 51200 lr 2.406e-04 [train_loss] tar_ll 0.5398 loss -0.5398 (8.890 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 51400 lr 2.390e-04 [train_loss] tar_ll 0.5307 loss -0.5307 (8.793 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 51600 lr 2.374e-04 [train_loss] tar_ll 0.5445 loss -0.5445 (8.781 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 51800 lr 2.359e-04 [train_loss] tar_ll 0.5547 loss -0.5547 (9.257 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 52000 lr 2.343e-04 [train_loss] tar_ll 0.5516 loss -0.5516 (9.057 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 52200 lr 2.327e-04 [train_loss] tar_ll 0.5610 loss -0.5610 (8.524 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 52400 lr 2.312e-04 [train_loss] tar_ll 0.5419 loss -0.5419 (8.524 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 52600 lr 2.296e-04 [train_loss] tar_ll 0.5131 loss -0.5131 (8.417 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 52800 lr 2.280e-04 [train_loss] tar_ll 0.5101 loss -0.5101 (8.937 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 53000 lr 2.265e-04 [train_loss] tar_ll 0.5523 loss -0.5523 (8.446 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 53200 lr 2.249e-04 [train_loss] tar_ll 0.5379 loss -0.5379 (9.533 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 53400 lr 2.233e-04 [train_loss] tar_ll 0.5944 loss -0.5944 (8.899 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 53600 lr 2.218e-04 [train_loss] tar_ll 0.5877 loss -0.5877 (9.357 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 53800 lr 2.202e-04 [train_loss] tar_ll 0.6334 loss -0.6334 (9.187 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 54000 lr 2.187e-04 [train_loss] tar_ll 0.4778 loss -0.4778 (8.512 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 54200 lr 2.171e-04 [train_loss] tar_ll 0.5069 loss -0.5069 (9.174 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 54400 lr 2.156e-04 [train_loss] tar_ll 0.5747 loss -0.5747 (8.307 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 54600 lr 2.140e-04 [train_loss] tar_ll 0.5593 loss -0.5593 (8.731 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 54800 lr 2.124e-04 [train_loss] tar_ll 0.5779 loss -0.5779 (8.298 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 55000 lr 2.109e-04 [train_loss] tar_ll 0.5573 loss -0.5573 (8.569 secs)\n",
      "100%|##########| 3000/3000 [00:42<00:00, 70.43it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.6217 loss -0.6217 (42.599 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 55200 lr 2.093e-04 [train_loss] tar_ll 0.5425 loss -0.5425 (9.011 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 55400 lr 2.078e-04 [train_loss] tar_ll 0.5127 loss -0.5127 (9.218 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 55600 lr 2.062e-04 [train_loss] tar_ll 0.6360 loss -0.6360 (8.938 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 55800 lr 2.047e-04 [train_loss] tar_ll 0.5816 loss -0.5816 (9.057 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 56000 lr 2.032e-04 [train_loss] tar_ll 0.5952 loss -0.5952 (8.951 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 56200 lr 2.016e-04 [train_loss] tar_ll 0.5971 loss -0.5971 (9.544 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 56400 lr 2.001e-04 [train_loss] tar_ll 0.5371 loss -0.5371 (9.030 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 56600 lr 1.985e-04 [train_loss] tar_ll 0.5784 loss -0.5784 (9.309 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 56800 lr 1.970e-04 [train_loss] tar_ll 0.5484 loss -0.5484 (9.280 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 57000 lr 1.955e-04 [train_loss] tar_ll 0.5889 loss -0.5889 (8.788 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 57200 lr 1.939e-04 [train_loss] tar_ll 0.5229 loss -0.5229 (8.940 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 57400 lr 1.924e-04 [train_loss] tar_ll 0.5615 loss -0.5615 (8.370 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 57600 lr 1.909e-04 [train_loss] tar_ll 0.6022 loss -0.6022 (8.928 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 57800 lr 1.894e-04 [train_loss] tar_ll 0.6001 loss -0.6001 (9.232 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 58000 lr 1.878e-04 [train_loss] tar_ll 0.5847 loss -0.5847 (9.251 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 58200 lr 1.863e-04 [train_loss] tar_ll 0.6069 loss -0.6069 (9.063 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 58400 lr 1.848e-04 [train_loss] tar_ll 0.6214 loss -0.6214 (9.404 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 58600 lr 1.833e-04 [train_loss] tar_ll 0.5667 loss -0.5667 (9.267 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 58800 lr 1.818e-04 [train_loss] tar_ll 0.6054 loss -0.6054 (8.667 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 59000 lr 1.803e-04 [train_loss] tar_ll 0.6135 loss -0.6135 (8.977 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 59200 lr 1.787e-04 [train_loss] tar_ll 0.5107 loss -0.5107 (8.696 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 59400 lr 1.772e-04 [train_loss] tar_ll 0.5850 loss -0.5850 (8.847 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 59600 lr 1.757e-04 [train_loss] tar_ll 0.7117 loss -0.7117 (8.276 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 59800 lr 1.742e-04 [train_loss] tar_ll 0.6264 loss -0.6264 (8.717 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 60000 lr 1.727e-04 [train_loss] tar_ll 0.6362 loss -0.6362 (8.852 secs)\n",
      "100%|##########| 3000/3000 [00:43<00:00, 68.60it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.6488 loss -0.6488 (43.732 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 60200 lr 1.713e-04 [train_loss] tar_ll 0.5896 loss -0.5896 (8.612 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 60400 lr 1.698e-04 [train_loss] tar_ll 0.6583 loss -0.6583 (8.274 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 60600 lr 1.683e-04 [train_loss] tar_ll 0.6497 loss -0.6497 (8.763 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 60800 lr 1.668e-04 [train_loss] tar_ll 0.6212 loss -0.6212 (8.542 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 61000 lr 1.653e-04 [train_loss] tar_ll 0.6774 loss -0.6774 (8.640 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 61200 lr 1.638e-04 [train_loss] tar_ll 0.5952 loss -0.5952 (8.673 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 61400 lr 1.624e-04 [train_loss] tar_ll 0.6529 loss -0.6529 (8.230 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 61600 lr 1.609e-04 [train_loss] tar_ll 0.6760 loss -0.6760 (8.612 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 61800 lr 1.594e-04 [train_loss] tar_ll 0.5866 loss -0.5866 (8.205 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 62000 lr 1.580e-04 [train_loss] tar_ll 0.6378 loss -0.6378 (8.915 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 62200 lr 1.565e-04 [train_loss] tar_ll 0.6373 loss -0.6373 (8.977 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 62400 lr 1.551e-04 [train_loss] tar_ll 0.6066 loss -0.6066 (8.798 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 62600 lr 1.536e-04 [train_loss] tar_ll 0.7022 loss -0.7022 (8.584 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 62800 lr 1.522e-04 [train_loss] tar_ll 0.6109 loss -0.6109 (8.917 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 63000 lr 1.507e-04 [train_loss] tar_ll 0.6828 loss -0.6828 (8.463 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 63200 lr 1.493e-04 [train_loss] tar_ll 0.6845 loss -0.6845 (9.003 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 63400 lr 1.478e-04 [train_loss] tar_ll 0.6450 loss -0.6450 (8.542 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 63600 lr 1.464e-04 [train_loss] tar_ll 0.7055 loss -0.7055 (9.218 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 63800 lr 1.450e-04 [train_loss] tar_ll 0.6204 loss -0.6204 (8.747 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 64000 lr 1.436e-04 [train_loss] tar_ll 0.6251 loss -0.6251 (8.459 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 64200 lr 1.421e-04 [train_loss] tar_ll 0.6952 loss -0.6952 (8.589 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 64400 lr 1.407e-04 [train_loss] tar_ll 0.6621 loss -0.6621 (8.526 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 64600 lr 1.393e-04 [train_loss] tar_ll 0.7238 loss -0.7238 (8.508 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 64800 lr 1.379e-04 [train_loss] tar_ll 0.6502 loss -0.6502 (8.724 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 65000 lr 1.365e-04 [train_loss] tar_ll 0.6994 loss -0.6994 (8.980 secs)\n",
      "100%|##########| 3000/3000 [00:46<00:00, 64.99it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.6688 loss -0.6688 (46.163 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 65200 lr 1.351e-04 [train_loss] tar_ll 0.7513 loss -0.7513 (9.337 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 65400 lr 1.337e-04 [train_loss] tar_ll 0.6891 loss -0.6891 (9.230 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 65600 lr 1.323e-04 [train_loss] tar_ll 0.7128 loss -0.7128 (8.728 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 65800 lr 1.309e-04 [train_loss] tar_ll 0.6700 loss -0.6700 (9.111 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 66000 lr 1.296e-04 [train_loss] tar_ll 0.6872 loss -0.6872 (9.318 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 66200 lr 1.282e-04 [train_loss] tar_ll 0.6748 loss -0.6748 (8.808 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 66400 lr 1.268e-04 [train_loss] tar_ll 0.6632 loss -0.6632 (9.156 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 66600 lr 1.255e-04 [train_loss] tar_ll 0.6512 loss -0.6512 (8.665 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 66800 lr 1.241e-04 [train_loss] tar_ll 0.7213 loss -0.7213 (8.766 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 67000 lr 1.227e-04 [train_loss] tar_ll 0.7123 loss -0.7123 (8.936 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 67200 lr 1.214e-04 [train_loss] tar_ll 0.6405 loss -0.6405 (9.756 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 67400 lr 1.200e-04 [train_loss] tar_ll 0.6576 loss -0.6576 (8.635 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 67600 lr 1.187e-04 [train_loss] tar_ll 0.6928 loss -0.6928 (8.370 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 67800 lr 1.174e-04 [train_loss] tar_ll 0.6993 loss -0.6993 (8.644 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 68000 lr 1.160e-04 [train_loss] tar_ll 0.6829 loss -0.6829 (8.303 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 68200 lr 1.147e-04 [train_loss] tar_ll 0.7015 loss -0.7015 (8.743 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 68400 lr 1.134e-04 [train_loss] tar_ll 0.6284 loss -0.6284 (8.196 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 68600 lr 1.121e-04 [train_loss] tar_ll 0.7782 loss -0.7782 (8.450 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 68800 lr 1.108e-04 [train_loss] tar_ll 0.7411 loss -0.7411 (8.118 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 69000 lr 1.095e-04 [train_loss] tar_ll 0.6185 loss -0.6185 (8.398 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 69200 lr 1.082e-04 [train_loss] tar_ll 0.7105 loss -0.7105 (8.892 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 69400 lr 1.069e-04 [train_loss] tar_ll 0.6617 loss -0.6617 (9.175 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 69600 lr 1.056e-04 [train_loss] tar_ll 0.6759 loss -0.6759 (9.135 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 69800 lr 1.043e-04 [train_loss] tar_ll 0.6593 loss -0.6593 (8.916 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 70000 lr 1.031e-04 [train_loss] tar_ll 0.7212 loss -0.7212 (9.178 secs)\n",
      "100%|##########| 3000/3000 [00:44<00:00, 66.93it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.7329 loss -0.7329 (44.824 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 70200 lr 1.018e-04 [train_loss] tar_ll 0.7358 loss -0.7358 (8.800 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 70400 lr 1.005e-04 [train_loss] tar_ll 0.6492 loss -0.6492 (9.117 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 70600 lr 9.927e-05 [train_loss] tar_ll 0.6741 loss -0.6741 (8.102 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 70800 lr 9.802e-05 [train_loss] tar_ll 0.7063 loss -0.7063 (8.802 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 71000 lr 9.677e-05 [train_loss] tar_ll 0.7013 loss -0.7013 (8.297 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 71200 lr 9.554e-05 [train_loss] tar_ll 0.6909 loss -0.6909 (8.604 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 71400 lr 9.430e-05 [train_loss] tar_ll 0.6419 loss -0.6419 (8.124 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 71600 lr 9.308e-05 [train_loss] tar_ll 0.7643 loss -0.7643 (8.690 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 71800 lr 9.186e-05 [train_loss] tar_ll 0.7207 loss -0.7207 (8.280 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 72000 lr 9.064e-05 [train_loss] tar_ll 0.7847 loss -0.7847 (9.267 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 72200 lr 8.944e-05 [train_loss] tar_ll 0.6621 loss -0.6621 (9.323 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 72400 lr 8.824e-05 [train_loss] tar_ll 0.7231 loss -0.7231 (10.198 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 72600 lr 8.704e-05 [train_loss] tar_ll 0.7722 loss -0.7722 (9.374 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 72800 lr 8.585e-05 [train_loss] tar_ll 0.7371 loss -0.7371 (8.527 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 73000 lr 8.467e-05 [train_loss] tar_ll 0.6616 loss -0.6616 (8.893 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 73200 lr 8.350e-05 [train_loss] tar_ll 0.6594 loss -0.6594 (8.294 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 73400 lr 8.233e-05 [train_loss] tar_ll 0.7439 loss -0.7439 (8.765 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 73600 lr 8.117e-05 [train_loss] tar_ll 0.6683 loss -0.6683 (9.180 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 73800 lr 8.001e-05 [train_loss] tar_ll 0.7568 loss -0.7568 (9.302 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 74000 lr 7.886e-05 [train_loss] tar_ll 0.6888 loss -0.6888 (9.093 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 74200 lr 7.772e-05 [train_loss] tar_ll 0.7289 loss -0.7289 (8.786 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 74400 lr 7.659e-05 [train_loss] tar_ll 0.7136 loss -0.7136 (9.077 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 74600 lr 7.546e-05 [train_loss] tar_ll 0.7897 loss -0.7897 (8.921 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 74800 lr 7.434e-05 [train_loss] tar_ll 0.7842 loss -0.7842 (9.300 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 75000 lr 7.322e-05 [train_loss] tar_ll 0.7924 loss -0.7924 (9.270 secs)\n",
      "100%|##########| 3000/3000 [00:47<00:00, 63.47it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.7699 loss -0.7699 (47.274 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 75200 lr 7.212e-05 [train_loss] tar_ll 0.7151 loss -0.7151 (8.923 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 75400 lr 7.102e-05 [train_loss] tar_ll 0.8014 loss -0.8014 (8.915 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 75600 lr 6.992e-05 [train_loss] tar_ll 0.7550 loss -0.7550 (8.857 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 75800 lr 6.884e-05 [train_loss] tar_ll 0.7838 loss -0.7838 (9.272 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 76000 lr 6.776e-05 [train_loss] tar_ll 0.7160 loss -0.7160 (9.009 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 76200 lr 6.669e-05 [train_loss] tar_ll 0.6994 loss -0.6994 (9.120 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 76400 lr 6.562e-05 [train_loss] tar_ll 0.7349 loss -0.7349 (8.504 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 76600 lr 6.456e-05 [train_loss] tar_ll 0.7466 loss -0.7466 (9.303 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 76800 lr 6.351e-05 [train_loss] tar_ll 0.7263 loss -0.7263 (9.187 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 77000 lr 6.247e-05 [train_loss] tar_ll 0.7248 loss -0.7248 (9.301 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 77200 lr 6.144e-05 [train_loss] tar_ll 0.7454 loss -0.7454 (8.793 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 77400 lr 6.041e-05 [train_loss] tar_ll 0.7543 loss -0.7543 (8.356 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 77600 lr 5.939e-05 [train_loss] tar_ll 0.7481 loss -0.7481 (8.631 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 77800 lr 5.838e-05 [train_loss] tar_ll 0.8149 loss -0.8149 (8.729 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 78000 lr 5.737e-05 [train_loss] tar_ll 0.7863 loss -0.7863 (9.088 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 78200 lr 5.637e-05 [train_loss] tar_ll 0.8334 loss -0.8334 (8.901 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 78400 lr 5.538e-05 [train_loss] tar_ll 0.7464 loss -0.7464 (9.092 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 78600 lr 5.440e-05 [train_loss] tar_ll 0.7450 loss -0.7450 (8.621 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 78800 lr 5.343e-05 [train_loss] tar_ll 0.7994 loss -0.7994 (8.992 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 79000 lr 5.246e-05 [train_loss] tar_ll 0.7886 loss -0.7886 (8.700 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 79200 lr 5.150e-05 [train_loss] tar_ll 0.7411 loss -0.7411 (8.483 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 79400 lr 5.055e-05 [train_loss] tar_ll 0.7412 loss -0.7412 (8.651 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 79600 lr 4.961e-05 [train_loss] tar_ll 0.7729 loss -0.7729 (8.397 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 79800 lr 4.867e-05 [train_loss] tar_ll 0.7639 loss -0.7639 (8.660 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 80000 lr 4.775e-05 [train_loss] tar_ll 0.7261 loss -0.7261 (8.065 secs)\n",
      "100%|##########| 3000/3000 [00:43<00:00, 69.46it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.7725 loss -0.7725 (43.192 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 80200 lr 4.683e-05 [train_loss] tar_ll 0.8104 loss -0.8104 (8.241 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 80400 lr 4.592e-05 [train_loss] tar_ll 0.7881 loss -0.7881 (8.517 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 80600 lr 4.501e-05 [train_loss] tar_ll 0.7644 loss -0.7644 (8.347 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 80800 lr 4.412e-05 [train_loss] tar_ll 0.7643 loss -0.7643 (8.539 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 81000 lr 4.323e-05 [train_loss] tar_ll 0.7820 loss -0.7820 (9.721 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 81200 lr 4.235e-05 [train_loss] tar_ll 0.7396 loss -0.7396 (9.076 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 81400 lr 4.148e-05 [train_loss] tar_ll 0.7989 loss -0.7989 (8.988 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 81600 lr 4.062e-05 [train_loss] tar_ll 0.8087 loss -0.8087 (8.206 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 81800 lr 3.976e-05 [train_loss] tar_ll 0.7848 loss -0.7848 (8.435 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 82000 lr 3.892e-05 [train_loss] tar_ll 0.7765 loss -0.7765 (8.107 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 82200 lr 3.808e-05 [train_loss] tar_ll 0.7678 loss -0.7678 (8.549 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 82400 lr 3.725e-05 [train_loss] tar_ll 0.7792 loss -0.7792 (8.548 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 82600 lr 3.643e-05 [train_loss] tar_ll 0.6975 loss -0.6975 (8.882 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 82800 lr 3.562e-05 [train_loss] tar_ll 0.7412 loss -0.7412 (8.441 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 83000 lr 3.481e-05 [train_loss] tar_ll 0.8283 loss -0.8283 (9.079 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 83200 lr 3.402e-05 [train_loss] tar_ll 0.7555 loss -0.7555 (9.180 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 83400 lr 3.323e-05 [train_loss] tar_ll 0.7465 loss -0.7465 (8.924 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 83600 lr 3.245e-05 [train_loss] tar_ll 0.7112 loss -0.7112 (10.280 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 83800 lr 3.168e-05 [train_loss] tar_ll 0.7366 loss -0.7366 (9.365 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 84000 lr 3.092e-05 [train_loss] tar_ll 0.7832 loss -0.7832 (9.616 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 84200 lr 3.017e-05 [train_loss] tar_ll 0.8182 loss -0.8182 (9.371 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 84400 lr 2.943e-05 [train_loss] tar_ll 0.7792 loss -0.7792 (8.346 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 84600 lr 2.869e-05 [train_loss] tar_ll 0.8199 loss -0.8199 (8.847 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 84800 lr 2.797e-05 [train_loss] tar_ll 0.8203 loss -0.8203 (8.054 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 85000 lr 2.725e-05 [train_loss] tar_ll 0.7839 loss -0.7839 (8.655 secs)\n",
      "100%|##########| 3000/3000 [00:45<00:00, 66.28it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.7867 loss -0.7867 (45.266 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 85200 lr 2.654e-05 [train_loss] tar_ll 0.6760 loss -0.6760 (8.838 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 85400 lr 2.584e-05 [train_loss] tar_ll 0.7813 loss -0.7813 (8.542 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 85600 lr 2.515e-05 [train_loss] tar_ll 0.7705 loss -0.7705 (8.262 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 85800 lr 2.447e-05 [train_loss] tar_ll 0.7790 loss -0.7790 (8.833 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 86000 lr 2.379e-05 [train_loss] tar_ll 0.7500 loss -0.7500 (9.178 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 86200 lr 2.313e-05 [train_loss] tar_ll 0.7408 loss -0.7408 (8.932 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 86400 lr 2.247e-05 [train_loss] tar_ll 0.8473 loss -0.8473 (8.306 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 86600 lr 2.183e-05 [train_loss] tar_ll 0.7802 loss -0.7802 (8.860 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 86800 lr 2.119e-05 [train_loss] tar_ll 0.8104 loss -0.8104 (9.271 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 87000 lr 2.056e-05 [train_loss] tar_ll 0.7810 loss -0.7810 (9.278 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 87200 lr 1.994e-05 [train_loss] tar_ll 0.8228 loss -0.8228 (9.622 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 87400 lr 1.933e-05 [train_loss] tar_ll 0.7961 loss -0.7961 (9.390 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 87600 lr 1.873e-05 [train_loss] tar_ll 0.8813 loss -0.8813 (9.278 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 87800 lr 1.814e-05 [train_loss] tar_ll 0.7728 loss -0.7728 (8.778 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 88000 lr 1.756e-05 [train_loss] tar_ll 0.9031 loss -0.9031 (9.840 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 88200 lr 1.698e-05 [train_loss] tar_ll 0.8052 loss -0.8052 (11.768 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 88400 lr 1.642e-05 [train_loss] tar_ll 0.8244 loss -0.8244 (10.251 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 88600 lr 1.586e-05 [train_loss] tar_ll 0.7755 loss -0.7755 (9.240 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 88800 lr 1.532e-05 [train_loss] tar_ll 0.7002 loss -0.7002 (9.205 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 89000 lr 1.478e-05 [train_loss] tar_ll 0.8632 loss -0.8632 (9.807 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 89200 lr 1.425e-05 [train_loss] tar_ll 0.8254 loss -0.8254 (10.125 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 89400 lr 1.373e-05 [train_loss] tar_ll 0.8788 loss -0.8788 (9.388 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 89600 lr 1.323e-05 [train_loss] tar_ll 0.7952 loss -0.7952 (9.735 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 89800 lr 1.273e-05 [train_loss] tar_ll 0.8151 loss -0.8151 (9.409 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 90000 lr 1.224e-05 [train_loss] tar_ll 0.7783 loss -0.7783 (8.377 secs)\n",
      "100%|##########| 3000/3000 [00:46<00:00, 63.88it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.8128 loss -0.8128 (46.966 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 90200 lr 1.176e-05 [train_loss] tar_ll 0.8015 loss -0.8015 (9.148 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 90400 lr 1.128e-05 [train_loss] tar_ll 0.7823 loss -0.7823 (8.707 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 90600 lr 1.082e-05 [train_loss] tar_ll 0.7802 loss -0.7802 (8.879 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 90800 lr 1.037e-05 [train_loss] tar_ll 0.8089 loss -0.8089 (8.662 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 91000 lr 9.927e-06 [train_loss] tar_ll 0.7523 loss -0.7523 (9.289 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 91200 lr 9.493e-06 [train_loss] tar_ll 0.8126 loss -0.8126 (9.043 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 91400 lr 9.069e-06 [train_loss] tar_ll 0.7547 loss -0.7547 (9.037 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 91600 lr 8.655e-06 [train_loss] tar_ll 0.7772 loss -0.7772 (9.471 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 91800 lr 8.250e-06 [train_loss] tar_ll 0.8746 loss -0.8746 (8.795 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 92000 lr 7.854e-06 [train_loss] tar_ll 0.8162 loss -0.8162 (9.827 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 92200 lr 7.468e-06 [train_loss] tar_ll 0.7975 loss -0.7975 (9.168 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 92400 lr 7.092e-06 [train_loss] tar_ll 0.8025 loss -0.8025 (9.211 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 92600 lr 6.725e-06 [train_loss] tar_ll 0.8274 loss -0.8274 (9.011 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 92800 lr 6.368e-06 [train_loss] tar_ll 0.8261 loss -0.8261 (9.299 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 93000 lr 6.021e-06 [train_loss] tar_ll 0.8229 loss -0.8229 (9.102 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 93200 lr 5.683e-06 [train_loss] tar_ll 0.7625 loss -0.7625 (8.882 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 93400 lr 5.355e-06 [train_loss] tar_ll 0.8138 loss -0.8138 (9.631 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 93600 lr 5.036e-06 [train_loss] tar_ll 0.8489 loss -0.8489 (9.248 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 93800 lr 4.727e-06 [train_loss] tar_ll 0.8069 loss -0.8069 (9.449 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 94000 lr 4.428e-06 [train_loss] tar_ll 0.8500 loss -0.8500 (9.769 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 94200 lr 4.139e-06 [train_loss] tar_ll 0.7887 loss -0.7887 (8.729 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 94400 lr 3.859e-06 [train_loss] tar_ll 0.8096 loss -0.8096 (8.784 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 94600 lr 3.589e-06 [train_loss] tar_ll 0.8166 loss -0.8166 (8.467 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 94800 lr 3.329e-06 [train_loss] tar_ll 0.7303 loss -0.7303 (9.079 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 95000 lr 3.078e-06 [train_loss] tar_ll 0.8407 loss -0.8407 (9.283 secs)\n",
      "100%|##########| 3000/3000 [00:43<00:00, 68.21it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.8211 loss -0.8211 (43.984 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 95200 lr 2.837e-06 [train_loss] tar_ll 0.8948 loss -0.8948 (9.240 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 95400 lr 2.606e-06 [train_loss] tar_ll 0.8548 loss -0.8548 (8.590 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 95600 lr 2.385e-06 [train_loss] tar_ll 0.8468 loss -0.8468 (8.988 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 95800 lr 2.173e-06 [train_loss] tar_ll 0.8378 loss -0.8378 (8.323 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 96000 lr 1.971e-06 [train_loss] tar_ll 0.8701 loss -0.8701 (8.745 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 96200 lr 1.779e-06 [train_loss] tar_ll 0.9008 loss -0.9008 (8.397 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 96400 lr 1.597e-06 [train_loss] tar_ll 0.8601 loss -0.8601 (8.703 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 96600 lr 1.425e-06 [train_loss] tar_ll 0.7895 loss -0.7895 (8.947 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 96800 lr 1.262e-06 [train_loss] tar_ll 0.7793 loss -0.7793 (8.767 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 97000 lr 1.110e-06 [train_loss] tar_ll 0.8258 loss -0.8258 (8.792 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 97200 lr 9.666e-07 [train_loss] tar_ll 0.7915 loss -0.7915 (8.966 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 97400 lr 8.335e-07 [train_loss] tar_ll 0.8514 loss -0.8514 (8.911 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 97600 lr 7.103e-07 [train_loss] tar_ll 0.7899 loss -0.7899 (8.888 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 97800 lr 5.969e-07 [train_loss] tar_ll 0.8952 loss -0.8952 (9.409 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 98000 lr 4.933e-07 [train_loss] tar_ll 0.7783 loss -0.7783 (8.787 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 98200 lr 3.996e-07 [train_loss] tar_ll 0.7370 loss -0.7370 (8.870 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 98400 lr 3.158e-07 [train_loss] tar_ll 0.8451 loss -0.8451 (9.443 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 98600 lr 2.418e-07 [train_loss] tar_ll 0.8367 loss -0.8367 (9.444 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 98800 lr 1.776e-07 [train_loss] tar_ll 0.8391 loss -0.8391 (9.078 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 99000 lr 1.234e-07 [train_loss] tar_ll 0.8159 loss -0.8159 (8.903 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 99200 lr 7.895e-08 [train_loss] tar_ll 0.7973 loss -0.7973 (8.904 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 99400 lr 4.441e-08 [train_loss] tar_ll 0.7866 loss -0.7866 (8.396 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 99600 lr 1.974e-08 [train_loss] tar_ll 0.7461 loss -0.7461 (8.626 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 99800 lr 4.935e-09 [train_loss] tar_ll 0.8239 loss -0.8239 (8.385 secs)\n",
      "lbanp:lbanp-num_latents-256_rbf_ step 100000 lr 0.000e+00 [train_loss] tar_ll 0.7506 loss -0.7506 (9.142 secs)\n",
      "100%|##########| 3000/3000 [00:45<00:00, 65.29it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.8236 loss -0.8236 (45.953 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:46<00:00, 65.19it/s]\n",
      "lbanp:lbanp-num_latents-256_rbf_ rbf tar_ll 0.8236 loss -0.8236 (46.022 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 5373011.5 miliseconds\n",
      "Execution time: 5373.0115 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 367.52880859375 MB\n",
      "Memory Usage Change: 351.27880859375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='lbanp', name='lbanp-num_latents-256_rbf_',val_seed=0, val_l=256,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611665c0-576d-41b3-af10-410773ca2919",
   "metadata": {},
   "source": [
    "## ISANP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b96ac41-72c9-4b41-97ff-11d9c8821fd3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "generating evaluation sets...\n",
      "Generating Evaluation Sets with rbf kernel\n",
      "Generating Evaluation sets with context and target points:   None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:07<00:00, 417.14it/s]\n",
      "Experiment: isanp-isanp-num_latents-8_rbf\n",
      "Total number of parameters: 785602\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "rbf-seed100.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 200 lr 5.000e-04 [train_loss] tar_ll -0.7070 loss 0.7070 (14.815 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 400 lr 5.000e-04 [train_loss] tar_ll -0.6813 loss 0.6813 (14.636 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 600 lr 5.000e-04 [train_loss] tar_ll -0.5525 loss 0.5525 (14.918 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 800 lr 4.999e-04 [train_loss] tar_ll -0.4524 loss 0.4524 (14.135 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 1000 lr 4.999e-04 [train_loss] tar_ll -0.3307 loss 0.3307 (14.285 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 1200 lr 4.998e-04 [train_loss] tar_ll -0.3036 loss 0.3036 (14.642 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 1400 lr 4.998e-04 [train_loss] tar_ll -0.2296 loss 0.2296 (14.608 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 1600 lr 4.997e-04 [train_loss] tar_ll -0.1394 loss 0.1394 (14.387 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 1800 lr 4.996e-04 [train_loss] tar_ll -0.0666 loss 0.0666 (14.437 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 2000 lr 4.995e-04 [train_loss] tar_ll -0.0345 loss 0.0345 (14.324 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 2200 lr 4.994e-04 [train_loss] tar_ll 0.0139 loss -0.0139 (14.655 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 2400 lr 4.993e-04 [train_loss] tar_ll 0.1213 loss -0.1213 (14.363 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1519 loss -0.1519 (14.046 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 2800 lr 4.990e-04 [train_loss] tar_ll 0.1715 loss -0.1715 (13.875 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 3000 lr 4.989e-04 [train_loss] tar_ll 0.2608 loss -0.2608 (14.928 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 3200 lr 4.987e-04 [train_loss] tar_ll 0.1723 loss -0.1723 (15.082 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 3400 lr 4.986e-04 [train_loss] tar_ll 0.2377 loss -0.2377 (15.007 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2794 loss -0.2794 (15.123 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 3800 lr 4.982e-04 [train_loss] tar_ll 0.2968 loss -0.2968 (16.476 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 4000 lr 4.980e-04 [train_loss] tar_ll 0.3193 loss -0.3193 (15.737 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 4200 lr 4.978e-04 [train_loss] tar_ll 0.2938 loss -0.2938 (15.100 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 4400 lr 4.976e-04 [train_loss] tar_ll 0.2479 loss -0.2479 (14.793 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 4600 lr 4.974e-04 [train_loss] tar_ll 0.3606 loss -0.3606 (15.029 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 4800 lr 4.972e-04 [train_loss] tar_ll 0.3980 loss -0.3980 (14.889 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 5000 lr 4.969e-04 [train_loss] tar_ll 0.4260 loss -0.4260 (14.774 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:12<00:00, 41.64it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 0.4239 loss -0.4239 (72.057 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 5200 lr 4.967e-04 [train_loss] tar_ll 0.4094 loss -0.4094 (14.408 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 5400 lr 4.964e-04 [train_loss] tar_ll 0.4572 loss -0.4572 (14.460 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4262 loss -0.4262 (14.631 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 5800 lr 4.959e-04 [train_loss] tar_ll 0.4008 loss -0.4008 (14.349 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4768 loss -0.4768 (14.617 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 6200 lr 4.953e-04 [train_loss] tar_ll 0.4409 loss -0.4409 (14.376 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 6400 lr 4.950e-04 [train_loss] tar_ll 0.4265 loss -0.4265 (14.564 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5106 loss -0.5106 (14.456 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 6800 lr 4.943e-04 [train_loss] tar_ll 0.4886 loss -0.4886 (14.418 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 7000 lr 4.940e-04 [train_loss] tar_ll 0.4353 loss -0.4353 (14.989 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 7200 lr 4.936e-04 [train_loss] tar_ll 0.4634 loss -0.4634 (14.961 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5022 loss -0.5022 (14.174 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 7600 lr 4.929e-04 [train_loss] tar_ll 0.5776 loss -0.5776 (14.709 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 7800 lr 4.925e-04 [train_loss] tar_ll 0.5790 loss -0.5790 (14.212 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 8000 lr 4.921e-04 [train_loss] tar_ll 0.5048 loss -0.5048 (14.337 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 8200 lr 4.918e-04 [train_loss] tar_ll 0.5092 loss -0.5092 (14.116 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 8400 lr 4.913e-04 [train_loss] tar_ll 0.5701 loss -0.5701 (14.505 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 8600 lr 4.909e-04 [train_loss] tar_ll 0.5413 loss -0.5413 (14.682 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 8800 lr 4.905e-04 [train_loss] tar_ll 0.5827 loss -0.5827 (14.875 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 9000 lr 4.901e-04 [train_loss] tar_ll 0.5043 loss -0.5043 (14.993 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 9200 lr 4.896e-04 [train_loss] tar_ll 0.5576 loss -0.5576 (14.914 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 9400 lr 4.892e-04 [train_loss] tar_ll 0.5911 loss -0.5911 (14.604 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 9600 lr 4.887e-04 [train_loss] tar_ll 0.5446 loss -0.5446 (14.799 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 9800 lr 4.882e-04 [train_loss] tar_ll 0.4952 loss -0.4952 (14.533 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 10000 lr 4.878e-04 [train_loss] tar_ll 0.5148 loss -0.5148 (14.665 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:11<00:00, 41.79it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 0.6132 loss -0.6132 (71.793 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 10200 lr 4.873e-04 [train_loss] tar_ll 0.6194 loss -0.6194 (14.297 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 10400 lr 4.868e-04 [train_loss] tar_ll 0.5558 loss -0.5558 (14.386 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 10600 lr 4.863e-04 [train_loss] tar_ll 0.5144 loss -0.5144 (14.740 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 10800 lr 4.857e-04 [train_loss] tar_ll 0.4866 loss -0.4866 (14.521 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 11000 lr 4.852e-04 [train_loss] tar_ll 0.6604 loss -0.6604 (14.847 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 11200 lr 4.847e-04 [train_loss] tar_ll 0.6174 loss -0.6174 (14.765 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 11400 lr 4.841e-04 [train_loss] tar_ll 0.6372 loss -0.6372 (14.720 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 11600 lr 4.836e-04 [train_loss] tar_ll 0.7285 loss -0.7285 (14.476 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 11800 lr 4.830e-04 [train_loss] tar_ll 0.5331 loss -0.5331 (14.826 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 12000 lr 4.824e-04 [train_loss] tar_ll 0.7252 loss -0.7252 (14.688 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 12200 lr 4.819e-04 [train_loss] tar_ll 0.7401 loss -0.7401 (14.711 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 12400 lr 4.813e-04 [train_loss] tar_ll 0.6387 loss -0.6387 (14.607 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 12600 lr 4.807e-04 [train_loss] tar_ll 0.6059 loss -0.6059 (14.366 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 12800 lr 4.801e-04 [train_loss] tar_ll 0.7141 loss -0.7141 (14.362 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 13000 lr 4.794e-04 [train_loss] tar_ll 0.6618 loss -0.6618 (14.390 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 13200 lr 4.788e-04 [train_loss] tar_ll 0.5299 loss -0.5299 (14.428 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 13400 lr 4.782e-04 [train_loss] tar_ll 0.6423 loss -0.6423 (14.147 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 13600 lr 4.775e-04 [train_loss] tar_ll 0.7080 loss -0.7080 (14.327 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 13800 lr 4.769e-04 [train_loss] tar_ll 0.6675 loss -0.6675 (14.102 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 14000 lr 4.762e-04 [train_loss] tar_ll 0.7189 loss -0.7189 (13.966 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 14200 lr 4.755e-04 [train_loss] tar_ll 0.6688 loss -0.6688 (15.328 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 14400 lr 4.749e-04 [train_loss] tar_ll 0.5671 loss -0.5671 (14.994 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 14600 lr 4.742e-04 [train_loss] tar_ll 0.6088 loss -0.6088 (14.973 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 14800 lr 4.735e-04 [train_loss] tar_ll 0.5917 loss -0.5917 (15.108 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 15000 lr 4.728e-04 [train_loss] tar_ll 0.6267 loss -0.6267 (14.870 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:11<00:00, 41.80it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 0.7427 loss -0.7427 (71.775 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 15200 lr 4.720e-04 [train_loss] tar_ll 0.6224 loss -0.6224 (15.035 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 15400 lr 4.713e-04 [train_loss] tar_ll 0.6685 loss -0.6685 (14.415 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 15600 lr 4.706e-04 [train_loss] tar_ll 0.6408 loss -0.6408 (14.584 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 15800 lr 4.698e-04 [train_loss] tar_ll 0.7093 loss -0.7093 (14.352 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 16000 lr 4.691e-04 [train_loss] tar_ll 0.7493 loss -0.7493 (14.437 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 16200 lr 4.683e-04 [train_loss] tar_ll 0.6902 loss -0.6902 (14.690 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 16400 lr 4.675e-04 [train_loss] tar_ll 0.6978 loss -0.6978 (14.660 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 16600 lr 4.668e-04 [train_loss] tar_ll 0.6481 loss -0.6481 (14.588 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 16800 lr 4.660e-04 [train_loss] tar_ll 0.7461 loss -0.7461 (14.569 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 17000 lr 4.652e-04 [train_loss] tar_ll 0.7740 loss -0.7740 (14.658 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 17200 lr 4.644e-04 [train_loss] tar_ll 0.7130 loss -0.7130 (14.704 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 17400 lr 4.636e-04 [train_loss] tar_ll 0.6895 loss -0.6895 (14.346 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 17600 lr 4.627e-04 [train_loss] tar_ll 0.7522 loss -0.7522 (14.674 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 17800 lr 4.619e-04 [train_loss] tar_ll 0.7123 loss -0.7123 (14.395 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 18000 lr 4.611e-04 [train_loss] tar_ll 0.7396 loss -0.7396 (14.517 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 18200 lr 4.602e-04 [train_loss] tar_ll 0.6160 loss -0.6160 (14.145 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 18400 lr 4.594e-04 [train_loss] tar_ll 0.6982 loss -0.6982 (14.467 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 18600 lr 4.585e-04 [train_loss] tar_ll 0.5972 loss -0.5972 (14.191 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 18800 lr 4.576e-04 [train_loss] tar_ll 0.6380 loss -0.6380 (14.283 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 19000 lr 4.568e-04 [train_loss] tar_ll 0.7056 loss -0.7056 (14.218 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 19200 lr 4.559e-04 [train_loss] tar_ll 0.5883 loss -0.5883 (14.368 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 19400 lr 4.550e-04 [train_loss] tar_ll 0.6810 loss -0.6810 (15.654 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 19600 lr 4.541e-04 [train_loss] tar_ll 0.7680 loss -0.7680 (14.964 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 19800 lr 4.532e-04 [train_loss] tar_ll 0.6950 loss -0.6950 (16.905 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 20000 lr 4.523e-04 [train_loss] tar_ll 0.7462 loss -0.7462 (15.825 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:16<00:00, 39.27it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 0.7417 loss -0.7417 (76.392 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 20200 lr 4.513e-04 [train_loss] tar_ll 0.5391 loss -0.5391 (15.379 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 20400 lr 4.504e-04 [train_loss] tar_ll 0.6847 loss -0.6847 (15.444 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 20600 lr 4.494e-04 [train_loss] tar_ll 0.6684 loss -0.6684 (14.984 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 20800 lr 4.485e-04 [train_loss] tar_ll 0.7075 loss -0.7075 (15.195 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 21000 lr 4.475e-04 [train_loss] tar_ll 0.7430 loss -0.7430 (14.693 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 21200 lr 4.466e-04 [train_loss] tar_ll 0.7889 loss -0.7889 (15.244 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 21400 lr 4.456e-04 [train_loss] tar_ll 0.7788 loss -0.7788 (14.894 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 21600 lr 4.446e-04 [train_loss] tar_ll 0.6890 loss -0.6890 (15.155 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 21800 lr 4.436e-04 [train_loss] tar_ll 0.7867 loss -0.7867 (14.840 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 22000 lr 4.426e-04 [train_loss] tar_ll 0.7871 loss -0.7871 (15.626 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 22200 lr 4.416e-04 [train_loss] tar_ll 0.6853 loss -0.6853 (15.385 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 22400 lr 4.406e-04 [train_loss] tar_ll 0.6511 loss -0.6511 (15.296 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 22600 lr 4.396e-04 [train_loss] tar_ll 0.7817 loss -0.7817 (14.905 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 22800 lr 4.386e-04 [train_loss] tar_ll 0.7972 loss -0.7972 (14.902 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 23000 lr 4.375e-04 [train_loss] tar_ll 0.8085 loss -0.8085 (14.662 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 23200 lr 4.365e-04 [train_loss] tar_ll 0.8047 loss -0.8047 (15.369 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 23400 lr 4.354e-04 [train_loss] tar_ll 0.8566 loss -0.8566 (14.793 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 23600 lr 4.344e-04 [train_loss] tar_ll 0.7754 loss -0.7754 (15.043 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 23800 lr 4.333e-04 [train_loss] tar_ll 0.7166 loss -0.7166 (14.556 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 24000 lr 4.322e-04 [train_loss] tar_ll 0.7560 loss -0.7560 (15.034 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 24200 lr 4.312e-04 [train_loss] tar_ll 0.7341 loss -0.7341 (14.909 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 24400 lr 4.301e-04 [train_loss] tar_ll 0.8351 loss -0.8351 (14.897 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 24600 lr 4.290e-04 [train_loss] tar_ll 0.8396 loss -0.8396 (14.686 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 24800 lr 4.279e-04 [train_loss] tar_ll 0.6516 loss -0.6516 (14.556 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 25000 lr 4.268e-04 [train_loss] tar_ll 0.8522 loss -0.8522 (15.170 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:17<00:00, 38.91it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 0.8562 loss -0.8562 (77.106 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 25200 lr 4.257e-04 [train_loss] tar_ll 0.7941 loss -0.7941 (15.547 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 25400 lr 4.245e-04 [train_loss] tar_ll 0.7373 loss -0.7373 (15.524 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 25600 lr 4.234e-04 [train_loss] tar_ll 0.7432 loss -0.7432 (14.783 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 25800 lr 4.223e-04 [train_loss] tar_ll 0.8689 loss -0.8689 (16.122 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 26000 lr 4.211e-04 [train_loss] tar_ll 0.7665 loss -0.7665 (14.905 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 26200 lr 4.200e-04 [train_loss] tar_ll 0.8485 loss -0.8485 (15.494 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 26400 lr 4.188e-04 [train_loss] tar_ll 0.8402 loss -0.8402 (14.961 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 26600 lr 4.177e-04 [train_loss] tar_ll 0.7959 loss -0.7959 (15.369 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 26800 lr 4.165e-04 [train_loss] tar_ll 0.7347 loss -0.7347 (14.894 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 27000 lr 4.153e-04 [train_loss] tar_ll 0.8192 loss -0.8192 (15.283 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 27200 lr 4.141e-04 [train_loss] tar_ll 0.8203 loss -0.8203 (15.246 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 27400 lr 4.130e-04 [train_loss] tar_ll 0.8419 loss -0.8419 (15.399 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 27600 lr 4.118e-04 [train_loss] tar_ll 0.8505 loss -0.8505 (15.053 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 27800 lr 4.106e-04 [train_loss] tar_ll 0.8550 loss -0.8550 (15.636 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 28000 lr 4.094e-04 [train_loss] tar_ll 0.8573 loss -0.8573 (15.329 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 28200 lr 4.081e-04 [train_loss] tar_ll 0.8719 loss -0.8719 (15.639 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 28400 lr 4.069e-04 [train_loss] tar_ll 0.7701 loss -0.7701 (15.224 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 28600 lr 4.057e-04 [train_loss] tar_ll 0.7038 loss -0.7038 (15.197 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 28800 lr 4.045e-04 [train_loss] tar_ll 0.7757 loss -0.7757 (14.618 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 29000 lr 4.032e-04 [train_loss] tar_ll 0.8346 loss -0.8346 (15.155 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 29200 lr 4.020e-04 [train_loss] tar_ll 0.7812 loss -0.7812 (14.984 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 29400 lr 4.007e-04 [train_loss] tar_ll 0.7980 loss -0.7980 (14.641 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 29600 lr 3.995e-04 [train_loss] tar_ll 0.8569 loss -0.8569 (15.035 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 29800 lr 3.982e-04 [train_loss] tar_ll 0.8599 loss -0.8599 (14.588 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 30000 lr 3.969e-04 [train_loss] tar_ll 0.8659 loss -0.8659 (14.575 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:14<00:00, 40.27it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 0.9543 loss -0.9543 (74.499 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 30200 lr 3.957e-04 [train_loss] tar_ll 0.8734 loss -0.8734 (15.974 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 30400 lr 3.944e-04 [train_loss] tar_ll 0.8139 loss -0.8139 (15.485 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 30600 lr 3.931e-04 [train_loss] tar_ll 0.8908 loss -0.8908 (16.019 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 30800 lr 3.918e-04 [train_loss] tar_ll 0.9063 loss -0.9063 (15.612 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 31000 lr 3.905e-04 [train_loss] tar_ll 0.8567 loss -0.8567 (15.318 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 31200 lr 3.892e-04 [train_loss] tar_ll 0.8461 loss -0.8461 (15.702 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 31400 lr 3.879e-04 [train_loss] tar_ll 0.8340 loss -0.8340 (15.155 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 31600 lr 3.866e-04 [train_loss] tar_ll 0.8933 loss -0.8933 (15.221 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 31800 lr 3.853e-04 [train_loss] tar_ll 0.7681 loss -0.7681 (14.812 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 32000 lr 3.840e-04 [train_loss] tar_ll 0.8672 loss -0.8672 (15.260 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 32200 lr 3.826e-04 [train_loss] tar_ll 0.7855 loss -0.7855 (14.870 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 32400 lr 3.813e-04 [train_loss] tar_ll 0.8569 loss -0.8569 (15.107 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 32600 lr 3.800e-04 [train_loss] tar_ll 0.8742 loss -0.8742 (15.491 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 32800 lr 3.786e-04 [train_loss] tar_ll 0.8195 loss -0.8195 (15.544 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 33000 lr 3.773e-04 [train_loss] tar_ll 0.8636 loss -0.8636 (15.235 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 33200 lr 3.759e-04 [train_loss] tar_ll 0.7728 loss -0.7728 (15.718 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 33400 lr 3.745e-04 [train_loss] tar_ll 0.9166 loss -0.9166 (15.882 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 33600 lr 3.732e-04 [train_loss] tar_ll 0.9309 loss -0.9309 (16.138 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 33800 lr 3.718e-04 [train_loss] tar_ll 0.8797 loss -0.8797 (15.058 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 34000 lr 3.704e-04 [train_loss] tar_ll 0.8129 loss -0.8129 (14.969 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 34200 lr 3.691e-04 [train_loss] tar_ll 0.8211 loss -0.8211 (15.770 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 34400 lr 3.677e-04 [train_loss] tar_ll 0.8186 loss -0.8186 (14.794 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 34600 lr 3.663e-04 [train_loss] tar_ll 0.8094 loss -0.8094 (14.814 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 34800 lr 3.649e-04 [train_loss] tar_ll 0.9065 loss -0.9065 (14.864 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 35000 lr 3.635e-04 [train_loss] tar_ll 0.8275 loss -0.8275 (14.829 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:13<00:00, 40.60it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 0.8341 loss -0.8341 (73.902 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 35200 lr 3.621e-04 [train_loss] tar_ll 0.8348 loss -0.8348 (15.763 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 35400 lr 3.607e-04 [train_loss] tar_ll 0.9246 loss -0.9246 (15.929 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 35600 lr 3.593e-04 [train_loss] tar_ll 0.9092 loss -0.9092 (15.484 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 35800 lr 3.579e-04 [train_loss] tar_ll 0.9643 loss -0.9643 (15.777 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 36000 lr 3.564e-04 [train_loss] tar_ll 0.8633 loss -0.8633 (15.708 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 36200 lr 3.550e-04 [train_loss] tar_ll 0.9603 loss -0.9603 (15.380 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 36400 lr 3.536e-04 [train_loss] tar_ll 0.7966 loss -0.7966 (15.562 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 36600 lr 3.522e-04 [train_loss] tar_ll 0.9192 loss -0.9192 (15.122 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 36800 lr 3.507e-04 [train_loss] tar_ll 0.9243 loss -0.9243 (15.421 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 37000 lr 3.493e-04 [train_loss] tar_ll 0.8776 loss -0.8776 (14.908 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 37200 lr 3.478e-04 [train_loss] tar_ll 0.8490 loss -0.8490 (15.269 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 37400 lr 3.464e-04 [train_loss] tar_ll 0.9131 loss -0.9131 (14.854 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 37600 lr 3.449e-04 [train_loss] tar_ll 0.9736 loss -0.9736 (15.035 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 37800 lr 3.435e-04 [train_loss] tar_ll 0.8451 loss -0.8451 (15.393 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 38000 lr 3.420e-04 [train_loss] tar_ll 0.9524 loss -0.9524 (15.316 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 38200 lr 3.406e-04 [train_loss] tar_ll 0.8154 loss -0.8154 (15.285 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 38400 lr 3.391e-04 [train_loss] tar_ll 0.8844 loss -0.8844 (15.287 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 38600 lr 3.376e-04 [train_loss] tar_ll 0.8631 loss -0.8631 (15.238 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 38800 lr 3.362e-04 [train_loss] tar_ll 0.9259 loss -0.9259 (15.733 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 39000 lr 3.347e-04 [train_loss] tar_ll 0.8927 loss -0.8927 (15.306 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 39200 lr 3.332e-04 [train_loss] tar_ll 0.9161 loss -0.9161 (15.441 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 39400 lr 3.317e-04 [train_loss] tar_ll 0.8811 loss -0.8811 (14.824 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 39600 lr 3.302e-04 [train_loss] tar_ll 0.9152 loss -0.9152 (14.974 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 39800 lr 3.287e-04 [train_loss] tar_ll 0.9356 loss -0.9356 (14.905 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 40000 lr 3.273e-04 [train_loss] tar_ll 0.8475 loss -0.8475 (14.982 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:13<00:00, 40.63it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 0.9296 loss -0.9296 (73.841 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 40200 lr 3.258e-04 [train_loss] tar_ll 0.9352 loss -0.9352 (15.920 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 40400 lr 3.243e-04 [train_loss] tar_ll 0.9044 loss -0.9044 (15.687 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 40600 lr 3.228e-04 [train_loss] tar_ll 0.9050 loss -0.9050 (15.583 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 40800 lr 3.213e-04 [train_loss] tar_ll 0.8773 loss -0.8773 (15.702 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 41000 lr 3.197e-04 [train_loss] tar_ll 0.8709 loss -0.8709 (15.910 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 41200 lr 3.182e-04 [train_loss] tar_ll 0.9118 loss -0.9118 (15.634 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 41400 lr 3.167e-04 [train_loss] tar_ll 0.9565 loss -0.9565 (15.645 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 41600 lr 3.152e-04 [train_loss] tar_ll 0.9411 loss -0.9411 (14.874 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 41800 lr 3.137e-04 [train_loss] tar_ll 0.8615 loss -0.8615 (15.691 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 42000 lr 3.122e-04 [train_loss] tar_ll 0.9244 loss -0.9244 (14.708 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 42200 lr 3.106e-04 [train_loss] tar_ll 0.9776 loss -0.9776 (16.289 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 42400 lr 3.091e-04 [train_loss] tar_ll 0.9315 loss -0.9315 (14.929 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 42600 lr 3.076e-04 [train_loss] tar_ll 0.9548 loss -0.9548 (15.457 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 42800 lr 3.061e-04 [train_loss] tar_ll 0.8469 loss -0.8469 (14.640 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 43000 lr 3.045e-04 [train_loss] tar_ll 1.0026 loss -1.0026 (15.556 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 43200 lr 3.030e-04 [train_loss] tar_ll 0.8961 loss -0.8961 (15.185 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 43400 lr 3.015e-04 [train_loss] tar_ll 1.0522 loss -1.0522 (15.632 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 43600 lr 2.999e-04 [train_loss] tar_ll 0.8902 loss -0.8902 (15.446 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 43800 lr 2.984e-04 [train_loss] tar_ll 0.9339 loss -0.9339 (15.856 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 44000 lr 2.968e-04 [train_loss] tar_ll 0.9904 loss -0.9904 (15.221 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 44200 lr 2.953e-04 [train_loss] tar_ll 0.9768 loss -0.9768 (15.831 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 44400 lr 2.938e-04 [train_loss] tar_ll 0.9381 loss -0.9381 (15.370 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 44600 lr 2.922e-04 [train_loss] tar_ll 0.9501 loss -0.9501 (15.519 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 44800 lr 2.907e-04 [train_loss] tar_ll 0.9999 loss -0.9999 (15.077 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 45000 lr 2.891e-04 [train_loss] tar_ll 1.0194 loss -1.0194 (14.809 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:13<00:00, 40.82it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 0.9834 loss -0.9834 (73.501 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 45200 lr 2.876e-04 [train_loss] tar_ll 0.9120 loss -0.9120 (15.894 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 45400 lr 2.860e-04 [train_loss] tar_ll 0.9005 loss -0.9005 (15.235 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 45600 lr 2.844e-04 [train_loss] tar_ll 0.9387 loss -0.9387 (16.036 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 45800 lr 2.829e-04 [train_loss] tar_ll 0.9511 loss -0.9511 (15.891 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 46000 lr 2.813e-04 [train_loss] tar_ll 0.9700 loss -0.9700 (15.816 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 46200 lr 2.798e-04 [train_loss] tar_ll 1.0377 loss -1.0377 (15.837 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 46400 lr 2.782e-04 [train_loss] tar_ll 0.9521 loss -0.9521 (15.554 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 46600 lr 2.767e-04 [train_loss] tar_ll 1.0516 loss -1.0516 (15.515 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 46800 lr 2.751e-04 [train_loss] tar_ll 0.9726 loss -0.9726 (15.143 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 47000 lr 2.735e-04 [train_loss] tar_ll 0.9232 loss -0.9232 (14.980 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 47200 lr 2.720e-04 [train_loss] tar_ll 0.9885 loss -0.9885 (15.499 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 47400 lr 2.704e-04 [train_loss] tar_ll 0.8730 loss -0.8730 (15.186 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 47600 lr 2.688e-04 [train_loss] tar_ll 1.0172 loss -1.0172 (15.340 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 47800 lr 2.673e-04 [train_loss] tar_ll 1.0688 loss -1.0688 (15.375 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 48000 lr 2.657e-04 [train_loss] tar_ll 0.9919 loss -0.9919 (14.847 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 48200 lr 2.641e-04 [train_loss] tar_ll 1.0156 loss -1.0156 (15.330 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 48400 lr 2.626e-04 [train_loss] tar_ll 0.9827 loss -0.9827 (15.412 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 48600 lr 2.610e-04 [train_loss] tar_ll 1.0082 loss -1.0082 (15.225 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 48800 lr 2.594e-04 [train_loss] tar_ll 1.0166 loss -1.0166 (15.290 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 49000 lr 2.579e-04 [train_loss] tar_ll 1.0956 loss -1.0956 (15.162 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 49200 lr 2.563e-04 [train_loss] tar_ll 0.9750 loss -0.9750 (15.533 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 49400 lr 2.547e-04 [train_loss] tar_ll 1.0076 loss -1.0076 (15.300 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 49600 lr 2.531e-04 [train_loss] tar_ll 0.9299 loss -0.9299 (15.137 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 49800 lr 2.516e-04 [train_loss] tar_ll 1.0058 loss -1.0058 (15.058 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 50000 lr 2.500e-04 [train_loss] tar_ll 0.9939 loss -0.9939 (14.724 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:14<00:00, 40.49it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 1.0021 loss -1.0021 (74.094 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 50200 lr 2.484e-04 [train_loss] tar_ll 0.9764 loss -0.9764 (14.217 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 50400 lr 2.469e-04 [train_loss] tar_ll 1.0012 loss -1.0012 (14.387 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 50600 lr 2.453e-04 [train_loss] tar_ll 0.9464 loss -0.9464 (15.581 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 50800 lr 2.437e-04 [train_loss] tar_ll 0.9663 loss -0.9663 (15.128 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 51000 lr 2.421e-04 [train_loss] tar_ll 1.0042 loss -1.0042 (15.577 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 51200 lr 2.406e-04 [train_loss] tar_ll 1.1077 loss -1.1077 (15.072 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 51400 lr 2.390e-04 [train_loss] tar_ll 0.9600 loss -0.9600 (15.462 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 51600 lr 2.374e-04 [train_loss] tar_ll 1.0723 loss -1.0723 (14.712 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 51800 lr 2.359e-04 [train_loss] tar_ll 1.0869 loss -1.0869 (15.845 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 52000 lr 2.343e-04 [train_loss] tar_ll 1.0903 loss -1.0903 (15.171 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 52200 lr 2.327e-04 [train_loss] tar_ll 1.0785 loss -1.0785 (15.474 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 52400 lr 2.312e-04 [train_loss] tar_ll 1.0278 loss -1.0278 (15.218 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 52600 lr 2.296e-04 [train_loss] tar_ll 1.0071 loss -1.0071 (15.281 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 52800 lr 2.280e-04 [train_loss] tar_ll 1.0390 loss -1.0390 (14.663 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 53000 lr 2.265e-04 [train_loss] tar_ll 1.0440 loss -1.0440 (15.323 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 53200 lr 2.249e-04 [train_loss] tar_ll 1.0034 loss -1.0034 (14.907 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 53400 lr 2.233e-04 [train_loss] tar_ll 1.0525 loss -1.0525 (15.482 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 53600 lr 2.218e-04 [train_loss] tar_ll 0.9414 loss -0.9414 (14.792 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 53800 lr 2.202e-04 [train_loss] tar_ll 0.9763 loss -0.9763 (15.276 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 54000 lr 2.187e-04 [train_loss] tar_ll 1.0252 loss -1.0252 (15.247 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 54200 lr 2.171e-04 [train_loss] tar_ll 1.0583 loss -1.0583 (15.991 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 54400 lr 2.156e-04 [train_loss] tar_ll 1.0052 loss -1.0052 (15.050 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 54600 lr 2.140e-04 [train_loss] tar_ll 1.0117 loss -1.0117 (15.210 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 54800 lr 2.124e-04 [train_loss] tar_ll 1.0042 loss -1.0042 (14.969 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 55000 lr 2.109e-04 [train_loss] tar_ll 1.0243 loss -1.0243 (15.303 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:14<00:00, 40.40it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 1.0112 loss -1.0112 (74.264 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 55200 lr 2.093e-04 [train_loss] tar_ll 1.0362 loss -1.0362 (15.139 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 55400 lr 2.078e-04 [train_loss] tar_ll 1.0604 loss -1.0604 (14.893 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 55600 lr 2.062e-04 [train_loss] tar_ll 0.9889 loss -0.9889 (14.315 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 55800 lr 2.047e-04 [train_loss] tar_ll 1.0657 loss -1.0657 (15.683 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 56000 lr 2.032e-04 [train_loss] tar_ll 1.0182 loss -1.0182 (15.078 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 56200 lr 2.016e-04 [train_loss] tar_ll 1.1014 loss -1.1014 (16.097 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 56400 lr 2.001e-04 [train_loss] tar_ll 1.0913 loss -1.0913 (15.726 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 56600 lr 1.985e-04 [train_loss] tar_ll 1.0629 loss -1.0629 (15.830 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0447 loss -1.0447 (15.949 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 57000 lr 1.955e-04 [train_loss] tar_ll 1.1337 loss -1.1337 (15.415 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 57200 lr 1.939e-04 [train_loss] tar_ll 1.0774 loss -1.0774 (15.745 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 57400 lr 1.924e-04 [train_loss] tar_ll 1.0128 loss -1.0128 (15.014 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 57600 lr 1.909e-04 [train_loss] tar_ll 1.0688 loss -1.0688 (15.101 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 57800 lr 1.894e-04 [train_loss] tar_ll 1.1061 loss -1.1061 (15.128 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 58000 lr 1.878e-04 [train_loss] tar_ll 1.1210 loss -1.1210 (15.128 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 58200 lr 1.863e-04 [train_loss] tar_ll 1.1296 loss -1.1296 (15.088 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 58400 lr 1.848e-04 [train_loss] tar_ll 1.0464 loss -1.0464 (14.970 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 58600 lr 1.833e-04 [train_loss] tar_ll 1.0462 loss -1.0462 (14.877 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 58800 lr 1.818e-04 [train_loss] tar_ll 1.0938 loss -1.0938 (15.287 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 59000 lr 1.803e-04 [train_loss] tar_ll 1.0823 loss -1.0823 (15.241 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 59200 lr 1.787e-04 [train_loss] tar_ll 1.1216 loss -1.1216 (15.642 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 59400 lr 1.772e-04 [train_loss] tar_ll 1.1336 loss -1.1336 (15.069 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 59600 lr 1.757e-04 [train_loss] tar_ll 1.0214 loss -1.0214 (15.292 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 59800 lr 1.742e-04 [train_loss] tar_ll 1.1091 loss -1.1091 (15.215 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 60000 lr 1.727e-04 [train_loss] tar_ll 1.0978 loss -1.0978 (15.358 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:16<00:00, 39.46it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 1.1223 loss -1.1223 (76.036 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 60200 lr 1.713e-04 [train_loss] tar_ll 1.1231 loss -1.1231 (14.614 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 60400 lr 1.698e-04 [train_loss] tar_ll 1.0772 loss -1.0772 (15.268 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 60600 lr 1.683e-04 [train_loss] tar_ll 1.1081 loss -1.1081 (14.995 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 60800 lr 1.668e-04 [train_loss] tar_ll 1.1198 loss -1.1198 (15.179 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 61000 lr 1.653e-04 [train_loss] tar_ll 0.9747 loss -0.9747 (15.027 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 61200 lr 1.638e-04 [train_loss] tar_ll 1.1288 loss -1.1288 (15.705 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 61400 lr 1.624e-04 [train_loss] tar_ll 1.0967 loss -1.0967 (15.552 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 61600 lr 1.609e-04 [train_loss] tar_ll 1.0953 loss -1.0953 (15.902 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 61800 lr 1.594e-04 [train_loss] tar_ll 1.0778 loss -1.0778 (15.601 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 62000 lr 1.580e-04 [train_loss] tar_ll 0.9996 loss -0.9996 (15.314 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 62200 lr 1.565e-04 [train_loss] tar_ll 1.0413 loss -1.0413 (16.024 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 62400 lr 1.551e-04 [train_loss] tar_ll 1.0691 loss -1.0691 (15.463 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 62600 lr 1.536e-04 [train_loss] tar_ll 1.1251 loss -1.1251 (15.778 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 62800 lr 1.522e-04 [train_loss] tar_ll 1.1126 loss -1.1126 (14.968 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 63000 lr 1.507e-04 [train_loss] tar_ll 1.1294 loss -1.1294 (15.223 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 63200 lr 1.493e-04 [train_loss] tar_ll 1.1461 loss -1.1461 (15.057 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 63400 lr 1.478e-04 [train_loss] tar_ll 1.1889 loss -1.1889 (15.323 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 63600 lr 1.464e-04 [train_loss] tar_ll 1.0484 loss -1.0484 (14.987 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 63800 lr 1.450e-04 [train_loss] tar_ll 1.2138 loss -1.2138 (15.347 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 64000 lr 1.436e-04 [train_loss] tar_ll 1.1138 loss -1.1138 (14.980 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 64200 lr 1.421e-04 [train_loss] tar_ll 1.1571 loss -1.1571 (15.628 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 64400 lr 1.407e-04 [train_loss] tar_ll 1.1940 loss -1.1940 (15.233 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 64600 lr 1.393e-04 [train_loss] tar_ll 1.1315 loss -1.1315 (15.522 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 64800 lr 1.379e-04 [train_loss] tar_ll 1.1603 loss -1.1603 (15.130 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 65000 lr 1.365e-04 [train_loss] tar_ll 1.1212 loss -1.1212 (15.191 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:14<00:00, 40.33it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 1.1574 loss -1.1574 (74.388 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 65200 lr 1.351e-04 [train_loss] tar_ll 1.0952 loss -1.0952 (15.096 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 65400 lr 1.337e-04 [train_loss] tar_ll 1.1591 loss -1.1591 (14.859 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 65600 lr 1.323e-04 [train_loss] tar_ll 1.1447 loss -1.1447 (15.538 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 65800 lr 1.309e-04 [train_loss] tar_ll 1.0857 loss -1.0857 (14.939 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 66000 lr 1.296e-04 [train_loss] tar_ll 1.0837 loss -1.0837 (14.704 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 66200 lr 1.282e-04 [train_loss] tar_ll 1.0727 loss -1.0727 (15.119 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 66400 lr 1.268e-04 [train_loss] tar_ll 1.1499 loss -1.1499 (15.756 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 66600 lr 1.255e-04 [train_loss] tar_ll 1.1801 loss -1.1801 (15.711 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 66800 lr 1.241e-04 [train_loss] tar_ll 1.1462 loss -1.1462 (16.359 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 67000 lr 1.227e-04 [train_loss] tar_ll 1.2192 loss -1.2192 (15.599 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 67200 lr 1.214e-04 [train_loss] tar_ll 1.2288 loss -1.2288 (16.194 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 67400 lr 1.200e-04 [train_loss] tar_ll 1.1907 loss -1.1907 (15.410 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 67600 lr 1.187e-04 [train_loss] tar_ll 1.0893 loss -1.0893 (15.622 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 67800 lr 1.174e-04 [train_loss] tar_ll 1.0909 loss -1.0909 (15.094 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 68000 lr 1.160e-04 [train_loss] tar_ll 1.1379 loss -1.1379 (15.695 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 68200 lr 1.147e-04 [train_loss] tar_ll 1.1978 loss -1.1978 (15.012 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 68400 lr 1.134e-04 [train_loss] tar_ll 1.1596 loss -1.1596 (15.497 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 68600 lr 1.121e-04 [train_loss] tar_ll 1.1415 loss -1.1415 (14.922 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 68800 lr 1.108e-04 [train_loss] tar_ll 1.1612 loss -1.1612 (15.429 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 69000 lr 1.095e-04 [train_loss] tar_ll 1.1007 loss -1.1007 (14.674 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 69200 lr 1.082e-04 [train_loss] tar_ll 1.2359 loss -1.2359 (15.220 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 69400 lr 1.069e-04 [train_loss] tar_ll 1.1264 loss -1.1264 (14.673 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 69600 lr 1.056e-04 [train_loss] tar_ll 1.2259 loss -1.2259 (14.968 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 69800 lr 1.043e-04 [train_loss] tar_ll 1.1455 loss -1.1455 (14.394 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 70000 lr 1.031e-04 [train_loss] tar_ll 1.2293 loss -1.2293 (14.610 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:14<00:00, 40.51it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 1.1935 loss -1.1935 (74.051 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 70200 lr 1.018e-04 [train_loss] tar_ll 1.1147 loss -1.1147 (13.941 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 70400 lr 1.005e-04 [train_loss] tar_ll 1.1881 loss -1.1881 (13.815 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 70600 lr 9.927e-05 [train_loss] tar_ll 1.1649 loss -1.1649 (14.005 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 70800 lr 9.802e-05 [train_loss] tar_ll 1.1368 loss -1.1368 (14.908 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 71000 lr 9.677e-05 [train_loss] tar_ll 1.1968 loss -1.1968 (14.592 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 71200 lr 9.554e-05 [train_loss] tar_ll 1.1572 loss -1.1572 (15.046 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 71400 lr 9.430e-05 [train_loss] tar_ll 1.1242 loss -1.1242 (14.857 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 71600 lr 9.308e-05 [train_loss] tar_ll 1.1712 loss -1.1712 (15.309 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 71800 lr 9.186e-05 [train_loss] tar_ll 1.1756 loss -1.1756 (16.028 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 72000 lr 9.064e-05 [train_loss] tar_ll 1.1756 loss -1.1756 (15.708 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 72200 lr 8.944e-05 [train_loss] tar_ll 1.2119 loss -1.2119 (15.812 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 72400 lr 8.824e-05 [train_loss] tar_ll 1.2056 loss -1.2056 (15.417 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 72600 lr 8.704e-05 [train_loss] tar_ll 1.1878 loss -1.1878 (15.838 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 72800 lr 8.585e-05 [train_loss] tar_ll 1.1298 loss -1.1298 (15.605 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 73000 lr 8.467e-05 [train_loss] tar_ll 1.1901 loss -1.1901 (15.761 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 73200 lr 8.350e-05 [train_loss] tar_ll 1.2076 loss -1.2076 (15.318 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 73400 lr 8.233e-05 [train_loss] tar_ll 1.1818 loss -1.1818 (15.198 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 73600 lr 8.117e-05 [train_loss] tar_ll 1.2620 loss -1.2620 (15.339 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 73800 lr 8.001e-05 [train_loss] tar_ll 1.1763 loss -1.1763 (15.251 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 74000 lr 7.886e-05 [train_loss] tar_ll 1.2365 loss -1.2365 (15.129 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 74200 lr 7.772e-05 [train_loss] tar_ll 1.1773 loss -1.1773 (15.246 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 74400 lr 7.659e-05 [train_loss] tar_ll 1.1287 loss -1.1287 (15.138 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 74600 lr 7.546e-05 [train_loss] tar_ll 1.2598 loss -1.2598 (15.190 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 74800 lr 7.434e-05 [train_loss] tar_ll 1.2753 loss -1.2753 (15.339 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1530 loss -1.1530 (15.187 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:15<00:00, 39.69it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 1.2004 loss -1.2004 (75.593 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1992 loss -1.1992 (14.588 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 75400 lr 7.102e-05 [train_loss] tar_ll 1.1552 loss -1.1552 (13.984 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 75600 lr 6.992e-05 [train_loss] tar_ll 1.1770 loss -1.1770 (14.071 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 75800 lr 6.884e-05 [train_loss] tar_ll 1.1384 loss -1.1384 (14.591 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 76000 lr 6.776e-05 [train_loss] tar_ll 1.2706 loss -1.2706 (14.085 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 76200 lr 6.669e-05 [train_loss] tar_ll 1.2233 loss -1.2233 (13.925 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 76400 lr 6.562e-05 [train_loss] tar_ll 1.2021 loss -1.2021 (14.025 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 76600 lr 6.456e-05 [train_loss] tar_ll 1.2224 loss -1.2224 (13.297 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 76800 lr 6.351e-05 [train_loss] tar_ll 1.2900 loss -1.2900 (13.381 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 77000 lr 6.247e-05 [train_loss] tar_ll 1.2832 loss -1.2832 (13.477 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 77200 lr 6.144e-05 [train_loss] tar_ll 1.1934 loss -1.1934 (14.297 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 77400 lr 6.041e-05 [train_loss] tar_ll 1.1460 loss -1.1460 (14.839 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 77600 lr 5.939e-05 [train_loss] tar_ll 1.1777 loss -1.1777 (15.317 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2080 loss -1.2080 (15.356 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 78000 lr 5.737e-05 [train_loss] tar_ll 1.1759 loss -1.1759 (14.504 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 78200 lr 5.637e-05 [train_loss] tar_ll 1.2240 loss -1.2240 (14.285 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 78400 lr 5.538e-05 [train_loss] tar_ll 1.2739 loss -1.2739 (14.287 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 78600 lr 5.440e-05 [train_loss] tar_ll 1.1632 loss -1.1632 (14.497 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 78800 lr 5.343e-05 [train_loss] tar_ll 1.1777 loss -1.1777 (15.431 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 79000 lr 5.246e-05 [train_loss] tar_ll 1.1707 loss -1.1707 (14.979 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 79200 lr 5.150e-05 [train_loss] tar_ll 1.1903 loss -1.1903 (15.700 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 79400 lr 5.055e-05 [train_loss] tar_ll 1.2301 loss -1.2301 (14.562 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 79600 lr 4.961e-05 [train_loss] tar_ll 1.1823 loss -1.1823 (14.952 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 79800 lr 4.867e-05 [train_loss] tar_ll 1.1697 loss -1.1697 (15.031 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 80000 lr 4.775e-05 [train_loss] tar_ll 1.2368 loss -1.2368 (15.389 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:16<00:00, 39.47it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 1.2444 loss -1.2444 (76.014 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 80200 lr 4.683e-05 [train_loss] tar_ll 1.2682 loss -1.2682 (15.410 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 80400 lr 4.592e-05 [train_loss] tar_ll 1.2046 loss -1.2046 (14.985 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 80600 lr 4.501e-05 [train_loss] tar_ll 1.2549 loss -1.2549 (15.333 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2211 loss -1.2211 (14.582 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 81000 lr 4.323e-05 [train_loss] tar_ll 1.2307 loss -1.2307 (15.361 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 81200 lr 4.235e-05 [train_loss] tar_ll 1.2558 loss -1.2558 (15.000 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 81400 lr 4.148e-05 [train_loss] tar_ll 1.2159 loss -1.2159 (14.965 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 81600 lr 4.062e-05 [train_loss] tar_ll 1.2386 loss -1.2386 (14.897 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2222 loss -1.2222 (15.316 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 82000 lr 3.892e-05 [train_loss] tar_ll 1.3256 loss -1.3256 (14.578 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 82200 lr 3.808e-05 [train_loss] tar_ll 1.2667 loss -1.2667 (14.867 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 82400 lr 3.725e-05 [train_loss] tar_ll 1.2023 loss -1.2023 (14.680 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 82600 lr 3.643e-05 [train_loss] tar_ll 1.2242 loss -1.2242 (15.665 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 82800 lr 3.562e-05 [train_loss] tar_ll 1.2063 loss -1.2063 (15.306 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 83000 lr 3.481e-05 [train_loss] tar_ll 1.1748 loss -1.1748 (15.796 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 83200 lr 3.402e-05 [train_loss] tar_ll 1.1635 loss -1.1635 (15.949 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 83400 lr 3.323e-05 [train_loss] tar_ll 1.1971 loss -1.1971 (14.819 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 83600 lr 3.245e-05 [train_loss] tar_ll 1.2588 loss -1.2588 (15.339 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 83800 lr 3.168e-05 [train_loss] tar_ll 1.2258 loss -1.2258 (15.532 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 84000 lr 3.092e-05 [train_loss] tar_ll 1.2817 loss -1.2817 (15.330 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2723 loss -1.2723 (15.403 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 84400 lr 2.943e-05 [train_loss] tar_ll 1.2899 loss -1.2899 (15.149 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 84600 lr 2.869e-05 [train_loss] tar_ll 1.2946 loss -1.2946 (15.152 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 84800 lr 2.797e-05 [train_loss] tar_ll 1.2915 loss -1.2915 (15.008 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 85000 lr 2.725e-05 [train_loss] tar_ll 1.2865 loss -1.2865 (15.227 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:14<00:00, 40.07it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 1.2663 loss -1.2663 (74.868 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 85200 lr 2.654e-05 [train_loss] tar_ll 1.2714 loss -1.2714 (15.299 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 85400 lr 2.584e-05 [train_loss] tar_ll 1.2370 loss -1.2370 (13.968 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 85600 lr 2.515e-05 [train_loss] tar_ll 1.3225 loss -1.3225 (14.034 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 85800 lr 2.447e-05 [train_loss] tar_ll 1.2420 loss -1.2420 (14.135 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 86000 lr 2.379e-05 [train_loss] tar_ll 1.1670 loss -1.1670 (16.262 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 86200 lr 2.313e-05 [train_loss] tar_ll 1.2120 loss -1.2120 (15.648 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 86400 lr 2.247e-05 [train_loss] tar_ll 1.2558 loss -1.2558 (14.841 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 86600 lr 2.183e-05 [train_loss] tar_ll 1.2130 loss -1.2130 (14.563 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 86800 lr 2.119e-05 [train_loss] tar_ll 1.2897 loss -1.2897 (14.686 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 87000 lr 2.056e-05 [train_loss] tar_ll 1.2818 loss -1.2818 (14.951 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 87200 lr 1.994e-05 [train_loss] tar_ll 1.2207 loss -1.2207 (15.075 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 87400 lr 1.933e-05 [train_loss] tar_ll 1.3239 loss -1.3239 (14.317 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 87600 lr 1.873e-05 [train_loss] tar_ll 1.2380 loss -1.2380 (14.795 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 87800 lr 1.814e-05 [train_loss] tar_ll 1.1523 loss -1.1523 (15.294 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 88000 lr 1.756e-05 [train_loss] tar_ll 1.2409 loss -1.2409 (15.223 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 88200 lr 1.698e-05 [train_loss] tar_ll 1.2364 loss -1.2364 (15.283 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 88400 lr 1.642e-05 [train_loss] tar_ll 1.3205 loss -1.3205 (15.515 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 88600 lr 1.586e-05 [train_loss] tar_ll 1.2672 loss -1.2672 (15.557 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 88800 lr 1.532e-05 [train_loss] tar_ll 1.3094 loss -1.3094 (15.009 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 89000 lr 1.478e-05 [train_loss] tar_ll 1.1809 loss -1.1809 (15.214 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 89200 lr 1.425e-05 [train_loss] tar_ll 1.2514 loss -1.2514 (15.306 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 89400 lr 1.373e-05 [train_loss] tar_ll 1.2213 loss -1.2213 (15.067 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 89600 lr 1.323e-05 [train_loss] tar_ll 1.2829 loss -1.2829 (14.955 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2502 loss -1.2502 (14.932 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 90000 lr 1.224e-05 [train_loss] tar_ll 1.3006 loss -1.3006 (14.773 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:12<00:00, 41.23it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 1.2833 loss -1.2833 (72.762 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 90200 lr 1.176e-05 [train_loss] tar_ll 1.2803 loss -1.2803 (15.218 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 90400 lr 1.128e-05 [train_loss] tar_ll 1.2956 loss -1.2956 (15.008 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 90600 lr 1.082e-05 [train_loss] tar_ll 1.2497 loss -1.2497 (15.260 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 90800 lr 1.037e-05 [train_loss] tar_ll 1.2775 loss -1.2775 (15.229 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 91000 lr 9.927e-06 [train_loss] tar_ll 1.2807 loss -1.2807 (15.194 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2571 loss -1.2571 (14.937 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2458 loss -1.2458 (14.943 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 91600 lr 8.655e-06 [train_loss] tar_ll 1.2585 loss -1.2585 (14.943 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2722 loss -1.2722 (14.744 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2956 loss -1.2956 (14.490 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 92200 lr 7.468e-06 [train_loss] tar_ll 1.2487 loss -1.2487 (14.667 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 92400 lr 7.092e-06 [train_loss] tar_ll 1.2723 loss -1.2723 (15.026 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 92600 lr 6.725e-06 [train_loss] tar_ll 1.1949 loss -1.1949 (14.807 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 92800 lr 6.368e-06 [train_loss] tar_ll 1.2094 loss -1.2094 (14.488 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 93000 lr 6.021e-06 [train_loss] tar_ll 1.2232 loss -1.2232 (14.573 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 93200 lr 5.683e-06 [train_loss] tar_ll 1.2844 loss -1.2844 (15.279 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 93400 lr 5.355e-06 [train_loss] tar_ll 1.3242 loss -1.3242 (15.434 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 93600 lr 5.036e-06 [train_loss] tar_ll 1.1912 loss -1.1912 (15.392 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 93800 lr 4.727e-06 [train_loss] tar_ll 1.3052 loss -1.3052 (15.699 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 94000 lr 4.428e-06 [train_loss] tar_ll 1.2035 loss -1.2035 (15.525 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 94200 lr 4.139e-06 [train_loss] tar_ll 1.2084 loss -1.2084 (16.019 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 94400 lr 3.859e-06 [train_loss] tar_ll 1.2796 loss -1.2796 (15.989 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 94600 lr 3.589e-06 [train_loss] tar_ll 1.2474 loss -1.2474 (15.451 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 94800 lr 3.329e-06 [train_loss] tar_ll 1.3308 loss -1.3308 (15.219 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 95000 lr 3.078e-06 [train_loss] tar_ll 1.3277 loss -1.3277 (15.345 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:13<00:00, 40.80it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 1.2890 loss -1.2890 (73.529 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 95200 lr 2.837e-06 [train_loss] tar_ll 1.2079 loss -1.2079 (15.239 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 95400 lr 2.606e-06 [train_loss] tar_ll 1.3014 loss -1.3014 (15.189 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 95600 lr 2.385e-06 [train_loss] tar_ll 1.1970 loss -1.1970 (14.492 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 95800 lr 2.173e-06 [train_loss] tar_ll 1.2558 loss -1.2558 (14.606 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 96000 lr 1.971e-06 [train_loss] tar_ll 1.2787 loss -1.2787 (14.628 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 96200 lr 1.779e-06 [train_loss] tar_ll 1.3149 loss -1.3149 (14.641 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 96400 lr 1.597e-06 [train_loss] tar_ll 1.3029 loss -1.3029 (14.785 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 96600 lr 1.425e-06 [train_loss] tar_ll 1.2697 loss -1.2697 (14.537 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 96800 lr 1.262e-06 [train_loss] tar_ll 1.2154 loss -1.2154 (14.120 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 97000 lr 1.110e-06 [train_loss] tar_ll 1.3002 loss -1.3002 (14.260 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 97200 lr 9.666e-07 [train_loss] tar_ll 1.2468 loss -1.2468 (13.879 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 97400 lr 8.335e-07 [train_loss] tar_ll 1.2314 loss -1.2314 (13.818 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 97600 lr 7.103e-07 [train_loss] tar_ll 1.2526 loss -1.2526 (14.054 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 97800 lr 5.969e-07 [train_loss] tar_ll 1.2759 loss -1.2759 (13.992 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 98000 lr 4.933e-07 [train_loss] tar_ll 1.2437 loss -1.2437 (13.848 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 98200 lr 3.996e-07 [train_loss] tar_ll 1.2347 loss -1.2347 (14.126 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 98400 lr 3.158e-07 [train_loss] tar_ll 1.2438 loss -1.2438 (13.504 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 98600 lr 2.418e-07 [train_loss] tar_ll 1.2513 loss -1.2513 (13.366 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2880 loss -1.2880 (14.873 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 99000 lr 1.234e-07 [train_loss] tar_ll 1.2828 loss -1.2828 (14.805 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 99200 lr 7.895e-08 [train_loss] tar_ll 1.2242 loss -1.2242 (15.200 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2511 loss -1.2511 (14.913 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 99600 lr 1.974e-08 [train_loss] tar_ll 1.2896 loss -1.2896 (14.689 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 99800 lr 4.935e-09 [train_loss] tar_ll 1.2763 loss -1.2763 (14.561 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 100000 lr 0.000e+00 [train_loss] tar_ll 1.3096 loss -1.3096 (14.671 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:12<00:00, 41.33it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 1.2904 loss -1.2904 (72.593 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:12<00:00, 41.43it/s]\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 1.2904 loss -1.2904 (72.422 secs)\n",
      "isanp:isanp-num_latents-8_rbf rbf tar_ll 1.2904 loss -1.2904 (72.422 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "Execution time: 9114785.0 miliseconds\n",
      "Execution time: 9114.785 seconds\n",
      "Initial Memory Usage: 0.0 MB\n",
      "Final Memory Usage: 55.09033203125 MB\n",
      "Memory Usage Change: 55.09033203125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='isanp-num_latents-8_rbf',val_seed=100, val_l=8,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b6a1222-bfc0-4405-ae6f-5815b9aaacb6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "generating evaluation sets...\n",
      "Generating Evaluation Sets with rbf kernel\n",
      "Generating Evaluation sets with context and target points:   None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:07<00:00, 423.84it/s]\n",
      "Experiment: isanp-isanp-num_latents-8_rbf\n",
      "Total number of parameters: 785602\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "rbf-seed500.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_rbf step 200 lr 5.000e-04 [train_loss] tar_ll -0.7006 loss 0.7006 (14.421 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 400 lr 5.000e-04 [train_loss] tar_ll -0.5496 loss 0.5496 (14.663 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 600 lr 5.000e-04 [train_loss] tar_ll -0.5627 loss 0.5627 (14.556 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 800 lr 4.999e-04 [train_loss] tar_ll -0.4717 loss 0.4717 (14.453 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 1000 lr 4.999e-04 [train_loss] tar_ll -0.3733 loss 0.3733 (14.374 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 1200 lr 4.998e-04 [train_loss] tar_ll -0.3264 loss 0.3264 (14.775 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 1400 lr 4.998e-04 [train_loss] tar_ll -0.2220 loss 0.2220 (14.455 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 1600 lr 4.997e-04 [train_loss] tar_ll -0.1158 loss 0.1158 (14.565 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 1800 lr 4.996e-04 [train_loss] tar_ll -0.0548 loss 0.0548 (14.701 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 2000 lr 4.995e-04 [train_loss] tar_ll 0.0149 loss -0.0149 (14.860 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 2200 lr 4.994e-04 [train_loss] tar_ll 0.0723 loss -0.0723 (14.587 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 2400 lr 4.993e-04 [train_loss] tar_ll 0.0461 loss -0.0461 (15.022 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1322 loss -0.1322 (14.584 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 2800 lr 4.990e-04 [train_loss] tar_ll 0.1024 loss -0.1024 (14.333 secs)\n",
      "isanp:isanp-num_latents-8_rbf step 3000 lr 4.989e-04 [train_loss] tar_ll 0.1893 loss -0.1893 (14.212 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 239929.6875 miliseconds\n",
      "Execution time: 239.9296875 seconds\n",
      "Initial Memory Usage: 17.10888671875 MB\n",
      "Final Memory Usage: 49.52099609375 MB\n",
      "Memory Usage Change: 32.412109375 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='isanp-num_latents-8_rbf',val_seed=500, val_l=8,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e8e91-8853-49ea-b78f-f95debdf7981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3baa2375-5b6f-4324-9f9a-03adf78511ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: cnp-cnp\n",
      "Total number of parameters: 215682\n",
      "\n",
      "cnp:cnp step 200 lr 5.000e-04 [train_loss] loss 0.6599 (1.921 secs)\n",
      "cnp:cnp step 400 lr 5.000e-04 [train_loss] loss 0.5867 (1.569 secs)\n",
      "cnp:cnp step 600 lr 5.000e-04 [train_loss] loss 0.5299 (1.635 secs)\n",
      "cnp:cnp step 800 lr 4.999e-04 [train_loss] loss 0.5386 (1.673 secs)\n",
      "cnp:cnp step 1000 lr 4.999e-04 [train_loss] loss 0.5276 (1.756 secs)\n",
      "cnp:cnp step 1200 lr 4.998e-04 [train_loss] loss 0.4881 (1.816 secs)\n",
      "cnp:cnp step 1400 lr 4.998e-04 [train_loss] loss 0.4225 (1.562 secs)\n",
      "cnp:cnp step 1600 lr 4.997e-04 [train_loss] loss 0.4530 (1.336 secs)\n",
      "cnp:cnp step 1800 lr 4.996e-04 [train_loss] loss 0.4580 (1.344 secs)\n",
      "cnp:cnp step 2000 lr 4.995e-04 [train_loss] loss 0.4585 (1.328 secs)\n",
      "cnp:cnp step 2200 lr 4.994e-04 [train_loss] loss 0.4080 (1.355 secs)\n",
      "cnp:cnp step 2400 lr 4.993e-04 [train_loss] loss 0.3984 (1.324 secs)\n",
      "cnp:cnp step 2600 lr 4.992e-04 [train_loss] loss 0.4339 (1.362 secs)\n",
      "cnp:cnp step 2800 lr 4.990e-04 [train_loss] loss 0.4006 (1.413 secs)\n",
      "cnp:cnp step 3000 lr 4.989e-04 [train_loss] loss 0.3942 (1.346 secs)\n",
      "cnp:cnp step 3200 lr 4.987e-04 [train_loss] loss 0.4425 (1.382 secs)\n",
      "cnp:cnp step 3400 lr 4.986e-04 [train_loss] loss 0.3597 (1.377 secs)\n",
      "cnp:cnp step 3600 lr 4.984e-04 [train_loss] loss 0.3145 (1.603 secs)\n",
      "cnp:cnp step 3800 lr 4.982e-04 [train_loss] loss 0.3506 (1.385 secs)\n",
      "cnp:cnp step 4000 lr 4.980e-04 [train_loss] loss 0.3360 (1.427 secs)\n",
      "cnp:cnp step 4200 lr 4.978e-04 [train_loss] loss 0.3349 (1.378 secs)\n",
      "cnp:cnp step 4400 lr 4.976e-04 [train_loss] loss 0.3161 (1.372 secs)\n",
      "cnp:cnp step 4600 lr 4.974e-04 [train_loss] loss 0.3020 (1.370 secs)\n",
      "cnp:cnp step 4800 lr 4.972e-04 [train_loss] loss 0.3024 (1.410 secs)\n",
      "cnp:cnp step 5000 lr 4.969e-04 [train_loss] loss 0.2929 (1.367 secs)\n",
      "100%|##########| 3000/3000 [00:04<00:00, 669.48it/s]\n",
      "cnp:cnp rbf ctx_ll -0.1197 tar_ll -0.3757 (4.483 secs)\n",
      "\n",
      "cnp:cnp step 5200 lr 4.967e-04 [train_loss] loss 0.2709 (1.589 secs)\n",
      "cnp:cnp step 5400 lr 4.964e-04 [train_loss] loss 0.3198 (1.489 secs)\n",
      "cnp:cnp step 5600 lr 4.961e-04 [train_loss] loss 0.2817 (1.442 secs)\n",
      "cnp:cnp step 5800 lr 4.959e-04 [train_loss] loss 0.2706 (1.425 secs)\n",
      "cnp:cnp step 6000 lr 4.956e-04 [train_loss] loss 0.2708 (1.375 secs)\n",
      "cnp:cnp step 6200 lr 4.953e-04 [train_loss] loss 0.3235 (1.389 secs)\n",
      "cnp:cnp step 6400 lr 4.950e-04 [train_loss] loss 0.2147 (1.363 secs)\n",
      "cnp:cnp step 6600 lr 4.946e-04 [train_loss] loss 0.2454 (1.346 secs)\n",
      "cnp:cnp step 6800 lr 4.943e-04 [train_loss] loss 0.2101 (1.425 secs)\n",
      "cnp:cnp step 7000 lr 4.940e-04 [train_loss] loss 0.2229 (1.387 secs)\n",
      "cnp:cnp step 7200 lr 4.936e-04 [train_loss] loss 0.2044 (1.421 secs)\n",
      "cnp:cnp step 7400 lr 4.933e-04 [train_loss] loss 0.2005 (1.538 secs)\n",
      "cnp:cnp step 7600 lr 4.929e-04 [train_loss] loss 0.2170 (1.352 secs)\n",
      "cnp:cnp step 7800 lr 4.925e-04 [train_loss] loss 0.1807 (1.416 secs)\n",
      "cnp:cnp step 8000 lr 4.921e-04 [train_loss] loss 0.1996 (1.444 secs)\n",
      "cnp:cnp step 8200 lr 4.918e-04 [train_loss] loss 0.2183 (1.389 secs)\n",
      "cnp:cnp step 8400 lr 4.913e-04 [train_loss] loss 0.1998 (1.358 secs)\n",
      "cnp:cnp step 8600 lr 4.909e-04 [train_loss] loss 0.1857 (1.453 secs)\n",
      "cnp:cnp step 8800 lr 4.905e-04 [train_loss] loss 0.2086 (1.479 secs)\n",
      "cnp:cnp step 9000 lr 4.901e-04 [train_loss] loss 0.1538 (1.366 secs)\n",
      "cnp:cnp step 9200 lr 4.896e-04 [train_loss] loss 0.1563 (1.459 secs)\n",
      "cnp:cnp step 9400 lr 4.892e-04 [train_loss] loss 0.1659 (1.374 secs)\n",
      "cnp:cnp step 9600 lr 4.887e-04 [train_loss] loss 0.1910 (1.732 secs)\n",
      "cnp:cnp step 9800 lr 4.882e-04 [train_loss] loss 0.1388 (1.437 secs)\n",
      "cnp:cnp step 10000 lr 4.878e-04 [train_loss] loss 0.1672 (1.350 secs)\n",
      "100%|##########| 3000/3000 [00:04<00:00, 655.27it/s]\n",
      "cnp:cnp rbf ctx_ll 0.0415 tar_ll -0.2512 (4.579 secs)\n",
      "\n",
      "cnp:cnp step 10200 lr 4.873e-04 [train_loss] loss 0.0952 (1.480 secs)\n",
      "cnp:cnp step 10400 lr 4.868e-04 [train_loss] loss 0.1169 (1.627 secs)\n",
      "cnp:cnp step 10600 lr 4.863e-04 [train_loss] loss 0.1806 (1.586 secs)\n",
      "cnp:cnp step 10800 lr 4.857e-04 [train_loss] loss 0.1065 (1.626 secs)\n",
      "cnp:cnp step 11000 lr 4.852e-04 [train_loss] loss 0.0838 (1.885 secs)\n",
      "cnp:cnp step 11200 lr 4.847e-04 [train_loss] loss 0.0977 (1.393 secs)\n",
      "cnp:cnp step 11400 lr 4.841e-04 [train_loss] loss 0.0904 (1.390 secs)\n",
      "cnp:cnp step 11600 lr 4.836e-04 [train_loss] loss 0.1329 (1.574 secs)\n",
      "cnp:cnp step 11800 lr 4.830e-04 [train_loss] loss 0.1068 (1.597 secs)\n",
      "cnp:cnp step 12000 lr 4.824e-04 [train_loss] loss 0.1405 (1.463 secs)\n",
      "cnp:cnp step 12200 lr 4.819e-04 [train_loss] loss 0.0655 (1.389 secs)\n",
      "cnp:cnp step 12400 lr 4.813e-04 [train_loss] loss 0.0385 (1.435 secs)\n",
      "cnp:cnp step 12600 lr 4.807e-04 [train_loss] loss 0.0630 (1.468 secs)\n",
      "cnp:cnp step 12800 lr 4.801e-04 [train_loss] loss 0.1140 (1.535 secs)\n",
      "cnp:cnp step 13000 lr 4.794e-04 [train_loss] loss 0.0810 (1.451 secs)\n",
      "cnp:cnp step 13200 lr 4.788e-04 [train_loss] loss 0.0418 (1.538 secs)\n",
      "cnp:cnp step 13400 lr 4.782e-04 [train_loss] loss 0.0270 (1.351 secs)\n",
      "cnp:cnp step 13600 lr 4.775e-04 [train_loss] loss 0.1011 (1.355 secs)\n",
      "cnp:cnp step 13800 lr 4.769e-04 [train_loss] loss 0.0503 (1.408 secs)\n",
      "cnp:cnp step 14000 lr 4.762e-04 [train_loss] loss 0.0500 (1.369 secs)\n",
      "cnp:cnp step 14200 lr 4.755e-04 [train_loss] loss 0.0176 (1.441 secs)\n",
      "cnp:cnp step 14400 lr 4.749e-04 [train_loss] loss -0.0057 (1.379 secs)\n",
      "cnp:cnp step 14600 lr 4.742e-04 [train_loss] loss 0.0129 (1.419 secs)\n",
      "cnp:cnp step 14800 lr 4.735e-04 [train_loss] loss 0.0130 (1.374 secs)\n",
      "cnp:cnp step 15000 lr 4.728e-04 [train_loss] loss -0.0139 (1.416 secs)\n",
      "100%|##########| 3000/3000 [00:04<00:00, 641.79it/s]\n",
      "cnp:cnp rbf ctx_ll 0.1669 tar_ll -0.1322 (4.677 secs)\n",
      "\n",
      "cnp:cnp step 15200 lr 4.720e-04 [train_loss] loss -0.0067 (1.410 secs)\n",
      "cnp:cnp step 15400 lr 4.713e-04 [train_loss] loss -0.0575 (1.390 secs)\n",
      "cnp:cnp step 15600 lr 4.706e-04 [train_loss] loss -0.0055 (1.420 secs)\n",
      "cnp:cnp step 15800 lr 4.698e-04 [train_loss] loss -0.0682 (1.392 secs)\n",
      "cnp:cnp step 16000 lr 4.691e-04 [train_loss] loss -0.0128 (1.456 secs)\n",
      "cnp:cnp step 16200 lr 4.683e-04 [train_loss] loss -0.0038 (1.422 secs)\n",
      "cnp:cnp step 16400 lr 4.675e-04 [train_loss] loss -0.0183 (1.445 secs)\n",
      "cnp:cnp step 16600 lr 4.668e-04 [train_loss] loss -0.0595 (1.602 secs)\n",
      "cnp:cnp step 16800 lr 4.660e-04 [train_loss] loss -0.0188 (1.534 secs)\n",
      "cnp:cnp step 17000 lr 4.652e-04 [train_loss] loss -0.0340 (1.570 secs)\n",
      "cnp:cnp step 17200 lr 4.644e-04 [train_loss] loss -0.0494 (1.362 secs)\n",
      "cnp:cnp step 17400 lr 4.636e-04 [train_loss] loss -0.0220 (1.601 secs)\n",
      "cnp:cnp step 17600 lr 4.627e-04 [train_loss] loss -0.0306 (1.558 secs)\n",
      "cnp:cnp step 17800 lr 4.619e-04 [train_loss] loss -0.0610 (1.404 secs)\n",
      "cnp:cnp step 18000 lr 4.611e-04 [train_loss] loss 0.0109 (1.341 secs)\n",
      "cnp:cnp step 18200 lr 4.602e-04 [train_loss] loss -0.0439 (1.358 secs)\n",
      "cnp:cnp step 18400 lr 4.594e-04 [train_loss] loss -0.0517 (1.357 secs)\n",
      "cnp:cnp step 18600 lr 4.585e-04 [train_loss] loss -0.0472 (1.410 secs)\n",
      "cnp:cnp step 18800 lr 4.576e-04 [train_loss] loss -0.0496 (1.425 secs)\n",
      "cnp:cnp step 19000 lr 4.568e-04 [train_loss] loss -0.0351 (1.460 secs)\n",
      "cnp:cnp step 19200 lr 4.559e-04 [train_loss] loss -0.0936 (1.512 secs)\n",
      "cnp:cnp step 19400 lr 4.550e-04 [train_loss] loss -0.0662 (1.312 secs)\n",
      "cnp:cnp step 19600 lr 4.541e-04 [train_loss] loss -0.0752 (1.360 secs)\n",
      "cnp:cnp step 19800 lr 4.532e-04 [train_loss] loss -0.0734 (1.422 secs)\n",
      "cnp:cnp step 20000 lr 4.523e-04 [train_loss] loss -0.0513 (1.354 secs)\n",
      "100%|##########| 3000/3000 [00:04<00:00, 664.49it/s]\n",
      "cnp:cnp rbf ctx_ll 0.1626 tar_ll -0.1206 (4.517 secs)\n",
      "\n",
      "cnp:cnp step 20200 lr 4.513e-04 [train_loss] loss -0.0806 (1.506 secs)\n",
      "cnp:cnp step 20400 lr 4.504e-04 [train_loss] loss -0.0823 (1.426 secs)\n",
      "cnp:cnp step 20600 lr 4.494e-04 [train_loss] loss -0.0682 (1.543 secs)\n",
      "cnp:cnp step 20800 lr 4.485e-04 [train_loss] loss -0.0865 (1.408 secs)\n",
      "cnp:cnp step 21000 lr 4.475e-04 [train_loss] loss -0.0653 (1.365 secs)\n",
      "cnp:cnp step 21200 lr 4.466e-04 [train_loss] loss -0.1198 (1.430 secs)\n",
      "cnp:cnp step 21400 lr 4.456e-04 [train_loss] loss -0.0660 (1.357 secs)\n",
      "cnp:cnp step 21600 lr 4.446e-04 [train_loss] loss -0.1017 (1.863 secs)\n",
      "cnp:cnp step 21800 lr 4.436e-04 [train_loss] loss -0.1075 (1.671 secs)\n",
      "cnp:cnp step 22000 lr 4.426e-04 [train_loss] loss -0.0959 (1.541 secs)\n",
      "cnp:cnp step 22200 lr 4.416e-04 [train_loss] loss -0.0609 (1.943 secs)\n",
      "cnp:cnp step 22400 lr 4.406e-04 [train_loss] loss -0.0898 (1.513 secs)\n",
      "cnp:cnp step 22600 lr 4.396e-04 [train_loss] loss -0.1139 (1.760 secs)\n",
      "cnp:cnp step 22800 lr 4.386e-04 [train_loss] loss -0.1219 (1.503 secs)\n",
      "cnp:cnp step 23000 lr 4.375e-04 [train_loss] loss -0.1258 (1.383 secs)\n",
      "cnp:cnp step 23200 lr 4.365e-04 [train_loss] loss -0.0752 (1.365 secs)\n",
      "cnp:cnp step 23400 lr 4.354e-04 [train_loss] loss -0.1047 (1.548 secs)\n",
      "cnp:cnp step 23600 lr 4.344e-04 [train_loss] loss -0.1039 (1.482 secs)\n",
      "cnp:cnp step 23800 lr 4.333e-04 [train_loss] loss -0.1324 (1.470 secs)\n",
      "cnp:cnp step 24000 lr 4.322e-04 [train_loss] loss -0.1386 (1.485 secs)\n",
      "cnp:cnp step 24200 lr 4.312e-04 [train_loss] loss -0.1541 (1.736 secs)\n",
      "cnp:cnp step 24400 lr 4.301e-04 [train_loss] loss -0.1628 (1.643 secs)\n",
      "cnp:cnp step 24600 lr 4.290e-04 [train_loss] loss -0.1336 (1.607 secs)\n",
      "cnp:cnp step 24800 lr 4.279e-04 [train_loss] loss -0.1452 (1.950 secs)\n",
      "cnp:cnp step 25000 lr 4.268e-04 [train_loss] loss -0.0924 (1.921 secs)\n",
      "100%|##########| 3000/3000 [00:05<00:00, 521.85it/s]\n",
      "cnp:cnp rbf ctx_ll 0.3034 tar_ll -0.0112 (5.750 secs)\n",
      "\n",
      "cnp:cnp step 25200 lr 4.257e-04 [train_loss] loss -0.1518 (1.597 secs)\n",
      "cnp:cnp step 25400 lr 4.245e-04 [train_loss] loss -0.1270 (1.388 secs)\n",
      "cnp:cnp step 25600 lr 4.234e-04 [train_loss] loss -0.1428 (1.471 secs)\n",
      "cnp:cnp step 25800 lr 4.223e-04 [train_loss] loss -0.1064 (1.602 secs)\n",
      "cnp:cnp step 26000 lr 4.211e-04 [train_loss] loss -0.1570 (1.803 secs)\n",
      "cnp:cnp step 26200 lr 4.200e-04 [train_loss] loss -0.1511 (1.917 secs)\n",
      "cnp:cnp step 26400 lr 4.188e-04 [train_loss] loss -0.1880 (1.634 secs)\n",
      "cnp:cnp step 26600 lr 4.177e-04 [train_loss] loss -0.1688 (1.729 secs)\n",
      "cnp:cnp step 26800 lr 4.165e-04 [train_loss] loss -0.1122 (1.384 secs)\n",
      "cnp:cnp step 27000 lr 4.153e-04 [train_loss] loss -0.1952 (1.452 secs)\n",
      "cnp:cnp step 27200 lr 4.141e-04 [train_loss] loss -0.1962 (1.433 secs)\n",
      "cnp:cnp step 27400 lr 4.130e-04 [train_loss] loss -0.1645 (1.478 secs)\n",
      "cnp:cnp step 27600 lr 4.118e-04 [train_loss] loss -0.1860 (1.385 secs)\n",
      "cnp:cnp step 27800 lr 4.106e-04 [train_loss] loss -0.1881 (1.569 secs)\n",
      "cnp:cnp step 28000 lr 4.094e-04 [train_loss] loss -0.1833 (1.647 secs)\n",
      "cnp:cnp step 28200 lr 4.081e-04 [train_loss] loss -0.1701 (1.561 secs)\n",
      "cnp:cnp step 28400 lr 4.069e-04 [train_loss] loss -0.2238 (2.066 secs)\n",
      "cnp:cnp step 28600 lr 4.057e-04 [train_loss] loss -0.1807 (1.702 secs)\n",
      "cnp:cnp step 28800 lr 4.045e-04 [train_loss] loss -0.1747 (1.766 secs)\n",
      "cnp:cnp step 29000 lr 4.032e-04 [train_loss] loss -0.1867 (1.512 secs)\n",
      "cnp:cnp step 29200 lr 4.020e-04 [train_loss] loss -0.1834 (1.609 secs)\n",
      "cnp:cnp step 29400 lr 4.007e-04 [train_loss] loss -0.2032 (1.498 secs)\n",
      "cnp:cnp step 29600 lr 3.995e-04 [train_loss] loss -0.1831 (1.545 secs)\n",
      "cnp:cnp step 29800 lr 3.982e-04 [train_loss] loss -0.2388 (1.687 secs)\n",
      "cnp:cnp step 30000 lr 3.969e-04 [train_loss] loss -0.2104 (1.745 secs)\n",
      "100%|##########| 3000/3000 [00:05<00:00, 594.87it/s]\n",
      "cnp:cnp rbf ctx_ll 0.3946 tar_ll 0.0408 (5.045 secs)\n",
      "\n",
      "cnp:cnp step 30200 lr 3.957e-04 [train_loss] loss -0.1985 (1.762 secs)\n",
      "cnp:cnp step 30400 lr 3.944e-04 [train_loss] loss -0.2190 (1.855 secs)\n",
      "cnp:cnp step 30600 lr 3.931e-04 [train_loss] loss -0.2117 (1.544 secs)\n",
      "cnp:cnp step 30800 lr 3.918e-04 [train_loss] loss -0.2106 (1.651 secs)\n",
      "cnp:cnp step 31000 lr 3.905e-04 [train_loss] loss -0.2434 (1.689 secs)\n",
      "cnp:cnp step 31200 lr 3.892e-04 [train_loss] loss -0.1928 (1.537 secs)\n",
      "cnp:cnp step 31400 lr 3.879e-04 [train_loss] loss -0.2228 (1.664 secs)\n",
      "cnp:cnp step 31600 lr 3.866e-04 [train_loss] loss -0.1351 (1.619 secs)\n",
      "cnp:cnp step 31800 lr 3.853e-04 [train_loss] loss -0.2267 (1.714 secs)\n",
      "cnp:cnp step 32000 lr 3.840e-04 [train_loss] loss -0.2619 (1.732 secs)\n",
      "cnp:cnp step 32200 lr 3.826e-04 [train_loss] loss -0.2211 (1.722 secs)\n",
      "cnp:cnp step 32400 lr 3.813e-04 [train_loss] loss -0.1939 (1.439 secs)\n",
      "cnp:cnp step 32600 lr 3.800e-04 [train_loss] loss -0.2219 (1.458 secs)\n",
      "cnp:cnp step 32800 lr 3.786e-04 [train_loss] loss -0.2514 (1.476 secs)\n",
      "cnp:cnp step 33000 lr 3.773e-04 [train_loss] loss -0.2201 (1.933 secs)\n",
      "cnp:cnp step 33200 lr 3.759e-04 [train_loss] loss -0.1558 (1.537 secs)\n",
      "cnp:cnp step 33400 lr 3.745e-04 [train_loss] loss -0.2339 (1.740 secs)\n",
      "cnp:cnp step 33600 lr 3.732e-04 [train_loss] loss -0.2472 (1.839 secs)\n",
      "cnp:cnp step 33800 lr 3.718e-04 [train_loss] loss -0.2318 (1.483 secs)\n",
      "cnp:cnp step 34000 lr 3.704e-04 [train_loss] loss -0.2618 (1.552 secs)\n",
      "cnp:cnp step 34200 lr 3.691e-04 [train_loss] loss -0.2174 (1.422 secs)\n",
      "cnp:cnp step 34400 lr 3.677e-04 [train_loss] loss -0.2309 (1.537 secs)\n",
      "cnp:cnp step 34600 lr 3.663e-04 [train_loss] loss -0.2660 (1.420 secs)\n",
      "cnp:cnp step 34800 lr 3.649e-04 [train_loss] loss -0.2706 (1.524 secs)\n",
      "cnp:cnp step 35000 lr 3.635e-04 [train_loss] loss -0.2320 (1.733 secs)\n",
      "100%|##########| 3000/3000 [00:04<00:00, 653.60it/s]\n",
      "cnp:cnp rbf ctx_ll 0.4878 tar_ll 0.0468 (4.591 secs)\n",
      "\n",
      "cnp:cnp step 35200 lr 3.621e-04 [train_loss] loss -0.2907 (1.520 secs)\n",
      "cnp:cnp step 35400 lr 3.607e-04 [train_loss] loss -0.2474 (1.572 secs)\n",
      "cnp:cnp step 35600 lr 3.593e-04 [train_loss] loss -0.2393 (1.532 secs)\n",
      "cnp:cnp step 35800 lr 3.579e-04 [train_loss] loss -0.2454 (1.430 secs)\n",
      "cnp:cnp step 36000 lr 3.564e-04 [train_loss] loss -0.2433 (1.421 secs)\n",
      "cnp:cnp step 36200 lr 3.550e-04 [train_loss] loss -0.2203 (1.554 secs)\n",
      "cnp:cnp step 36400 lr 3.536e-04 [train_loss] loss -0.3046 (1.800 secs)\n",
      "cnp:cnp step 36600 lr 3.522e-04 [train_loss] loss -0.2490 (1.708 secs)\n",
      "cnp:cnp step 36800 lr 3.507e-04 [train_loss] loss -0.2329 (1.427 secs)\n",
      "cnp:cnp step 37000 lr 3.493e-04 [train_loss] loss -0.2763 (1.445 secs)\n",
      "cnp:cnp step 37200 lr 3.478e-04 [train_loss] loss -0.2935 (1.409 secs)\n",
      "cnp:cnp step 37400 lr 3.464e-04 [train_loss] loss -0.2784 (1.520 secs)\n",
      "cnp:cnp step 37600 lr 3.449e-04 [train_loss] loss -0.2596 (2.067 secs)\n",
      "cnp:cnp step 37800 lr 3.435e-04 [train_loss] loss -0.2656 (1.473 secs)\n",
      "cnp:cnp step 38000 lr 3.420e-04 [train_loss] loss -0.2435 (1.518 secs)\n",
      "cnp:cnp step 38200 lr 3.406e-04 [train_loss] loss -0.2652 (1.628 secs)\n",
      "cnp:cnp step 38400 lr 3.391e-04 [train_loss] loss -0.2959 (1.621 secs)\n",
      "cnp:cnp step 38600 lr 3.376e-04 [train_loss] loss -0.2545 (1.481 secs)\n",
      "cnp:cnp step 38800 lr 3.362e-04 [train_loss] loss -0.2831 (1.587 secs)\n",
      "cnp:cnp step 39000 lr 3.347e-04 [train_loss] loss -0.3138 (1.533 secs)\n",
      "cnp:cnp step 39200 lr 3.332e-04 [train_loss] loss -0.2821 (1.434 secs)\n",
      "cnp:cnp step 39400 lr 3.317e-04 [train_loss] loss -0.2994 (1.613 secs)\n",
      "cnp:cnp step 39600 lr 3.302e-04 [train_loss] loss -0.3035 (1.501 secs)\n",
      "cnp:cnp step 39800 lr 3.287e-04 [train_loss] loss -0.2867 (1.424 secs)\n",
      "cnp:cnp step 40000 lr 3.273e-04 [train_loss] loss -0.2846 (1.565 secs)\n",
      "100%|##########| 3000/3000 [00:05<00:00, 584.43it/s]\n",
      "cnp:cnp rbf ctx_ll 0.3865 tar_ll 0.0742 (5.135 secs)\n",
      "\n",
      "cnp:cnp step 40200 lr 3.258e-04 [train_loss] loss -0.2481 (1.406 secs)\n",
      "cnp:cnp step 40400 lr 3.243e-04 [train_loss] loss -0.2702 (1.526 secs)\n",
      "cnp:cnp step 40600 lr 3.228e-04 [train_loss] loss -0.2804 (1.638 secs)\n",
      "cnp:cnp step 40800 lr 3.213e-04 [train_loss] loss -0.2682 (1.463 secs)\n",
      "cnp:cnp step 41000 lr 3.197e-04 [train_loss] loss -0.2516 (1.465 secs)\n",
      "cnp:cnp step 41200 lr 3.182e-04 [train_loss] loss -0.2527 (1.425 secs)\n",
      "cnp:cnp step 41400 lr 3.167e-04 [train_loss] loss -0.2908 (1.457 secs)\n",
      "cnp:cnp step 41600 lr 3.152e-04 [train_loss] loss -0.2975 (1.456 secs)\n",
      "cnp:cnp step 41800 lr 3.137e-04 [train_loss] loss -0.2960 (1.885 secs)\n",
      "cnp:cnp step 42000 lr 3.122e-04 [train_loss] loss -0.2777 (1.634 secs)\n",
      "cnp:cnp step 42200 lr 3.106e-04 [train_loss] loss -0.3470 (1.455 secs)\n",
      "cnp:cnp step 42400 lr 3.091e-04 [train_loss] loss -0.3298 (1.673 secs)\n",
      "cnp:cnp step 42600 lr 3.076e-04 [train_loss] loss -0.2759 (1.456 secs)\n",
      "cnp:cnp step 42800 lr 3.061e-04 [train_loss] loss -0.2938 (1.542 secs)\n",
      "cnp:cnp step 43000 lr 3.045e-04 [train_loss] loss -0.3374 (1.684 secs)\n",
      "cnp:cnp step 43200 lr 3.030e-04 [train_loss] loss -0.3297 (1.612 secs)\n",
      "cnp:cnp step 43400 lr 3.015e-04 [train_loss] loss -0.3292 (1.642 secs)\n",
      "cnp:cnp step 43600 lr 2.999e-04 [train_loss] loss -0.2998 (1.498 secs)\n",
      "cnp:cnp step 43800 lr 2.984e-04 [train_loss] loss -0.3151 (1.968 secs)\n",
      "cnp:cnp step 44000 lr 2.968e-04 [train_loss] loss -0.2814 (1.457 secs)\n",
      "cnp:cnp step 44200 lr 2.953e-04 [train_loss] loss -0.3476 (1.727 secs)\n",
      "cnp:cnp step 44400 lr 2.938e-04 [train_loss] loss -0.3210 (1.523 secs)\n",
      "cnp:cnp step 44600 lr 2.922e-04 [train_loss] loss -0.3157 (1.670 secs)\n",
      "cnp:cnp step 44800 lr 2.907e-04 [train_loss] loss -0.3555 (1.609 secs)\n",
      "cnp:cnp step 45000 lr 2.891e-04 [train_loss] loss -0.2936 (1.527 secs)\n",
      "100%|##########| 3000/3000 [00:05<00:00, 585.98it/s]\n",
      "cnp:cnp rbf ctx_ll 0.5611 tar_ll 0.1290 (5.122 secs)\n",
      "\n",
      "cnp:cnp step 45200 lr 2.876e-04 [train_loss] loss -0.3423 (1.698 secs)\n",
      "cnp:cnp step 45400 lr 2.860e-04 [train_loss] loss -0.3126 (1.409 secs)\n",
      "cnp:cnp step 45600 lr 2.844e-04 [train_loss] loss -0.3302 (1.519 secs)\n",
      "cnp:cnp step 45800 lr 2.829e-04 [train_loss] loss -0.2608 (1.397 secs)\n",
      "cnp:cnp step 46000 lr 2.813e-04 [train_loss] loss -0.3631 (1.528 secs)\n",
      "cnp:cnp step 46200 lr 2.798e-04 [train_loss] loss -0.3405 (1.530 secs)\n",
      "cnp:cnp step 46400 lr 2.782e-04 [train_loss] loss -0.2861 (1.745 secs)\n",
      "cnp:cnp step 46600 lr 2.767e-04 [train_loss] loss -0.3320 (1.488 secs)\n",
      "cnp:cnp step 46800 lr 2.751e-04 [train_loss] loss -0.3876 (1.471 secs)\n",
      "cnp:cnp step 47000 lr 2.735e-04 [train_loss] loss -0.3372 (1.613 secs)\n",
      "cnp:cnp step 47200 lr 2.720e-04 [train_loss] loss -0.2954 (1.680 secs)\n",
      "cnp:cnp step 47400 lr 2.704e-04 [train_loss] loss -0.3097 (1.395 secs)\n",
      "cnp:cnp step 47600 lr 2.688e-04 [train_loss] loss -0.3346 (1.498 secs)\n",
      "cnp:cnp step 47800 lr 2.673e-04 [train_loss] loss -0.3207 (1.536 secs)\n",
      "cnp:cnp step 48000 lr 2.657e-04 [train_loss] loss -0.3556 (1.407 secs)\n",
      "cnp:cnp step 48200 lr 2.641e-04 [train_loss] loss -0.2947 (1.920 secs)\n",
      "cnp:cnp step 48400 lr 2.626e-04 [train_loss] loss -0.2952 (1.498 secs)\n",
      "cnp:cnp step 48600 lr 2.610e-04 [train_loss] loss -0.3819 (1.479 secs)\n",
      "cnp:cnp step 48800 lr 2.594e-04 [train_loss] loss -0.3154 (1.448 secs)\n",
      "cnp:cnp step 49000 lr 2.579e-04 [train_loss] loss -0.3322 (1.569 secs)\n",
      "cnp:cnp step 49200 lr 2.563e-04 [train_loss] loss -0.3456 (1.717 secs)\n",
      "cnp:cnp step 49400 lr 2.547e-04 [train_loss] loss -0.3652 (1.494 secs)\n",
      "cnp:cnp step 49600 lr 2.531e-04 [train_loss] loss -0.3593 (1.601 secs)\n",
      "cnp:cnp step 49800 lr 2.516e-04 [train_loss] loss -0.3446 (1.440 secs)\n",
      "cnp:cnp step 50000 lr 2.500e-04 [train_loss] loss -0.3617 (1.449 secs)\n",
      "100%|##########| 3000/3000 [00:04<00:00, 621.19it/s]\n",
      "cnp:cnp rbf ctx_ll 0.5662 tar_ll 0.1655 (4.831 secs)\n",
      "\n",
      "cnp:cnp step 50200 lr 2.484e-04 [train_loss] loss -0.3301 (1.544 secs)\n",
      "cnp:cnp step 50400 lr 2.469e-04 [train_loss] loss -0.3662 (1.836 secs)\n",
      "cnp:cnp step 50600 lr 2.453e-04 [train_loss] loss -0.3051 (1.659 secs)\n",
      "cnp:cnp step 50800 lr 2.437e-04 [train_loss] loss -0.3287 (1.680 secs)\n",
      "cnp:cnp step 51000 lr 2.421e-04 [train_loss] loss -0.3286 (1.686 secs)\n",
      "cnp:cnp step 51200 lr 2.406e-04 [train_loss] loss -0.3763 (1.461 secs)\n",
      "cnp:cnp step 51400 lr 2.390e-04 [train_loss] loss -0.3695 (1.574 secs)\n",
      "cnp:cnp step 51600 lr 2.374e-04 [train_loss] loss -0.3289 (1.587 secs)\n",
      "cnp:cnp step 51800 lr 2.359e-04 [train_loss] loss -0.3677 (1.705 secs)\n",
      "cnp:cnp step 52000 lr 2.343e-04 [train_loss] loss -0.3835 (1.454 secs)\n",
      "cnp:cnp step 52200 lr 2.327e-04 [train_loss] loss -0.3914 (1.400 secs)\n",
      "cnp:cnp step 52400 lr 2.312e-04 [train_loss] loss -0.3761 (1.624 secs)\n",
      "cnp:cnp step 52600 lr 2.296e-04 [train_loss] loss -0.3429 (1.630 secs)\n",
      "cnp:cnp step 52800 lr 2.280e-04 [train_loss] loss -0.3363 (1.668 secs)\n",
      "cnp:cnp step 53000 lr 2.265e-04 [train_loss] loss -0.3747 (1.616 secs)\n",
      "cnp:cnp step 53200 lr 2.249e-04 [train_loss] loss -0.3930 (1.608 secs)\n",
      "cnp:cnp step 53400 lr 2.233e-04 [train_loss] loss -0.3998 (1.604 secs)\n",
      "cnp:cnp step 53600 lr 2.218e-04 [train_loss] loss -0.3724 (1.561 secs)\n",
      "cnp:cnp step 53800 lr 2.202e-04 [train_loss] loss -0.3487 (1.640 secs)\n",
      "cnp:cnp step 54000 lr 2.187e-04 [train_loss] loss -0.3592 (1.602 secs)\n",
      "cnp:cnp step 54200 lr 2.171e-04 [train_loss] loss -0.3219 (1.520 secs)\n",
      "cnp:cnp step 54400 lr 2.156e-04 [train_loss] loss -0.3618 (1.700 secs)\n",
      "cnp:cnp step 54600 lr 2.140e-04 [train_loss] loss -0.3714 (1.771 secs)\n",
      "cnp:cnp step 54800 lr 2.124e-04 [train_loss] loss -0.3608 (1.661 secs)\n",
      "cnp:cnp step 55000 lr 2.109e-04 [train_loss] loss -0.3858 (1.628 secs)\n",
      "100%|##########| 3000/3000 [00:05<00:00, 592.63it/s]\n",
      "cnp:cnp rbf ctx_ll 0.6282 tar_ll 0.1603 (5.063 secs)\n",
      "\n",
      "cnp:cnp step 55200 lr 2.093e-04 [train_loss] loss -0.3491 (1.932 secs)\n",
      "cnp:cnp step 55400 lr 2.078e-04 [train_loss] loss -0.3637 (1.704 secs)\n",
      "cnp:cnp step 55600 lr 2.062e-04 [train_loss] loss -0.3665 (1.851 secs)\n",
      "cnp:cnp step 55800 lr 2.047e-04 [train_loss] loss -0.3872 (1.922 secs)\n",
      "cnp:cnp step 56000 lr 2.032e-04 [train_loss] loss -0.3765 (1.851 secs)\n",
      "cnp:cnp step 56200 lr 2.016e-04 [train_loss] loss -0.3944 (1.598 secs)\n",
      "cnp:cnp step 56400 lr 2.001e-04 [train_loss] loss -0.3855 (1.359 secs)\n",
      "cnp:cnp step 56600 lr 1.985e-04 [train_loss] loss -0.4001 (1.430 secs)\n",
      "cnp:cnp step 56800 lr 1.970e-04 [train_loss] loss -0.3428 (1.539 secs)\n",
      "cnp:cnp step 57000 lr 1.955e-04 [train_loss] loss -0.3769 (1.493 secs)\n",
      "cnp:cnp step 57200 lr 1.939e-04 [train_loss] loss -0.3746 (1.593 secs)\n",
      "cnp:cnp step 57400 lr 1.924e-04 [train_loss] loss -0.3948 (1.656 secs)\n",
      "cnp:cnp step 57600 lr 1.909e-04 [train_loss] loss -0.3750 (1.896 secs)\n",
      "cnp:cnp step 57800 lr 1.894e-04 [train_loss] loss -0.3803 (1.770 secs)\n",
      "cnp:cnp step 58000 lr 1.878e-04 [train_loss] loss -0.3968 (1.695 secs)\n",
      "cnp:cnp step 58200 lr 1.863e-04 [train_loss] loss -0.3879 (1.542 secs)\n",
      "cnp:cnp step 58400 lr 1.848e-04 [train_loss] loss -0.3998 (1.646 secs)\n",
      "cnp:cnp step 58600 lr 1.833e-04 [train_loss] loss -0.3954 (1.556 secs)\n",
      "cnp:cnp step 58800 lr 1.818e-04 [train_loss] loss -0.3733 (1.601 secs)\n",
      "cnp:cnp step 59000 lr 1.803e-04 [train_loss] loss -0.3912 (1.676 secs)\n",
      "cnp:cnp step 59200 lr 1.787e-04 [train_loss] loss -0.4226 (1.591 secs)\n",
      "cnp:cnp step 59400 lr 1.772e-04 [train_loss] loss -0.3932 (1.637 secs)\n",
      "cnp:cnp step 59600 lr 1.757e-04 [train_loss] loss -0.3889 (1.618 secs)\n",
      "cnp:cnp step 59800 lr 1.742e-04 [train_loss] loss -0.3955 (1.456 secs)\n",
      "cnp:cnp step 60000 lr 1.727e-04 [train_loss] loss -0.4261 (1.626 secs)\n",
      "100%|##########| 3000/3000 [00:04<00:00, 627.55it/s]\n",
      "cnp:cnp rbf ctx_ll 0.6521 tar_ll 0.1915 (4.782 secs)\n",
      "\n",
      "cnp:cnp step 60200 lr 1.713e-04 [train_loss] loss -0.3896 (1.751 secs)\n",
      "cnp:cnp step 60400 lr 1.698e-04 [train_loss] loss -0.4004 (1.543 secs)\n",
      "cnp:cnp step 60600 lr 1.683e-04 [train_loss] loss -0.4217 (1.444 secs)\n",
      "cnp:cnp step 60800 lr 1.668e-04 [train_loss] loss -0.3771 (1.784 secs)\n",
      "cnp:cnp step 61000 lr 1.653e-04 [train_loss] loss -0.4142 (1.835 secs)\n",
      "cnp:cnp step 61200 lr 1.638e-04 [train_loss] loss -0.3768 (1.526 secs)\n",
      "cnp:cnp step 61400 lr 1.624e-04 [train_loss] loss -0.3692 (1.803 secs)\n",
      "cnp:cnp step 61600 lr 1.609e-04 [train_loss] loss -0.4068 (1.504 secs)\n",
      "cnp:cnp step 61800 lr 1.594e-04 [train_loss] loss -0.4053 (1.513 secs)\n",
      "cnp:cnp step 62000 lr 1.580e-04 [train_loss] loss -0.3791 (1.463 secs)\n",
      "cnp:cnp step 62200 lr 1.565e-04 [train_loss] loss -0.3624 (1.651 secs)\n",
      "cnp:cnp step 62400 lr 1.551e-04 [train_loss] loss -0.4333 (1.590 secs)\n",
      "cnp:cnp step 62600 lr 1.536e-04 [train_loss] loss -0.3785 (1.593 secs)\n",
      "cnp:cnp step 62800 lr 1.522e-04 [train_loss] loss -0.4199 (2.050 secs)\n",
      "cnp:cnp step 63000 lr 1.507e-04 [train_loss] loss -0.3983 (1.514 secs)\n",
      "cnp:cnp step 63200 lr 1.493e-04 [train_loss] loss -0.3533 (1.493 secs)\n",
      "cnp:cnp step 63400 lr 1.478e-04 [train_loss] loss -0.3929 (1.483 secs)\n",
      "cnp:cnp step 63600 lr 1.464e-04 [train_loss] loss -0.3682 (1.617 secs)\n",
      "cnp:cnp step 63800 lr 1.450e-04 [train_loss] loss -0.4253 (1.642 secs)\n",
      "cnp:cnp step 64000 lr 1.436e-04 [train_loss] loss -0.4145 (1.743 secs)\n",
      "cnp:cnp step 64200 lr 1.421e-04 [train_loss] loss -0.4128 (1.868 secs)\n",
      "cnp:cnp step 64400 lr 1.407e-04 [train_loss] loss -0.4138 (1.516 secs)\n",
      "cnp:cnp step 64600 lr 1.393e-04 [train_loss] loss -0.3935 (1.504 secs)\n",
      "cnp:cnp step 64800 lr 1.379e-04 [train_loss] loss -0.3892 (1.774 secs)\n",
      "cnp:cnp step 65000 lr 1.365e-04 [train_loss] loss -0.3775 (1.615 secs)\n",
      "100%|##########| 3000/3000 [00:04<00:00, 633.37it/s]\n",
      "cnp:cnp rbf ctx_ll 0.6638 tar_ll 0.2080 (4.738 secs)\n",
      "\n",
      "cnp:cnp step 65200 lr 1.351e-04 [train_loss] loss -0.3641 (1.574 secs)\n",
      "cnp:cnp step 65400 lr 1.337e-04 [train_loss] loss -0.3486 (1.476 secs)\n",
      "cnp:cnp step 65600 lr 1.323e-04 [train_loss] loss -0.3924 (1.411 secs)\n",
      "cnp:cnp step 65800 lr 1.309e-04 [train_loss] loss -0.4089 (1.485 secs)\n",
      "cnp:cnp step 66000 lr 1.296e-04 [train_loss] loss -0.4170 (1.625 secs)\n",
      "cnp:cnp step 66200 lr 1.282e-04 [train_loss] loss -0.3995 (1.625 secs)\n",
      "cnp:cnp step 66400 lr 1.268e-04 [train_loss] loss -0.4514 (1.566 secs)\n",
      "cnp:cnp step 66600 lr 1.255e-04 [train_loss] loss -0.4390 (1.532 secs)\n",
      "cnp:cnp step 66800 lr 1.241e-04 [train_loss] loss -0.4044 (1.510 secs)\n",
      "cnp:cnp step 67000 lr 1.227e-04 [train_loss] loss -0.4218 (1.449 secs)\n",
      "cnp:cnp step 67200 lr 1.214e-04 [train_loss] loss -0.4359 (1.528 secs)\n",
      "cnp:cnp step 67400 lr 1.200e-04 [train_loss] loss -0.3839 (1.514 secs)\n",
      "cnp:cnp step 67600 lr 1.187e-04 [train_loss] loss -0.4089 (1.414 secs)\n",
      "cnp:cnp step 67800 lr 1.174e-04 [train_loss] loss -0.4089 (1.679 secs)\n",
      "cnp:cnp step 68000 lr 1.160e-04 [train_loss] loss -0.4079 (1.507 secs)\n",
      "cnp:cnp step 68200 lr 1.147e-04 [train_loss] loss -0.4344 (1.758 secs)\n",
      "cnp:cnp step 68400 lr 1.134e-04 [train_loss] loss -0.4124 (1.530 secs)\n",
      "cnp:cnp step 68600 lr 1.121e-04 [train_loss] loss -0.4536 (1.414 secs)\n",
      "cnp:cnp step 68800 lr 1.108e-04 [train_loss] loss -0.4206 (1.513 secs)\n",
      "cnp:cnp step 69000 lr 1.095e-04 [train_loss] loss -0.4250 (1.687 secs)\n",
      "cnp:cnp step 69200 lr 1.082e-04 [train_loss] loss -0.4197 (1.434 secs)\n",
      "cnp:cnp step 69400 lr 1.069e-04 [train_loss] loss -0.3965 (1.597 secs)\n",
      "cnp:cnp step 69600 lr 1.056e-04 [train_loss] loss -0.4640 (1.503 secs)\n",
      "cnp:cnp step 69800 lr 1.043e-04 [train_loss] loss -0.4165 (1.544 secs)\n",
      "cnp:cnp step 70000 lr 1.031e-04 [train_loss] loss -0.4430 (1.548 secs)\n",
      "100%|##########| 3000/3000 [00:04<00:00, 620.12it/s]\n",
      "cnp:cnp rbf ctx_ll 0.6710 tar_ll 0.2190 (4.840 secs)\n",
      "\n",
      "cnp:cnp step 70200 lr 1.018e-04 [train_loss] loss -0.4207 (1.525 secs)\n",
      "cnp:cnp step 70400 lr 1.005e-04 [train_loss] loss -0.4346 (1.404 secs)\n",
      "cnp:cnp step 70600 lr 9.927e-05 [train_loss] loss -0.4611 (1.404 secs)\n",
      "cnp:cnp step 70800 lr 9.802e-05 [train_loss] loss -0.4114 (1.483 secs)\n",
      "cnp:cnp step 71000 lr 9.677e-05 [train_loss] loss -0.4108 (1.654 secs)\n",
      "cnp:cnp step 71200 lr 9.554e-05 [train_loss] loss -0.4293 (1.560 secs)\n",
      "cnp:cnp step 71400 lr 9.430e-05 [train_loss] loss -0.3917 (1.411 secs)\n",
      "cnp:cnp step 71600 lr 9.308e-05 [train_loss] loss -0.4485 (1.780 secs)\n",
      "cnp:cnp step 71800 lr 9.186e-05 [train_loss] loss -0.4069 (1.612 secs)\n",
      "cnp:cnp step 72000 lr 9.064e-05 [train_loss] loss -0.4082 (1.591 secs)\n",
      "cnp:cnp step 72200 lr 8.944e-05 [train_loss] loss -0.4499 (1.499 secs)\n",
      "cnp:cnp step 72400 lr 8.824e-05 [train_loss] loss -0.3983 (1.588 secs)\n",
      "cnp:cnp step 72600 lr 8.704e-05 [train_loss] loss -0.4738 (1.501 secs)\n",
      "cnp:cnp step 72800 lr 8.585e-05 [train_loss] loss -0.4523 (1.588 secs)\n",
      "cnp:cnp step 73000 lr 8.467e-05 [train_loss] loss -0.4394 (1.570 secs)\n",
      "cnp:cnp step 73200 lr 8.350e-05 [train_loss] loss -0.4347 (1.536 secs)\n",
      "cnp:cnp step 73400 lr 8.233e-05 [train_loss] loss -0.4618 (1.655 secs)\n",
      "cnp:cnp step 73600 lr 8.117e-05 [train_loss] loss -0.4227 (1.797 secs)\n",
      "cnp:cnp step 73800 lr 8.001e-05 [train_loss] loss -0.4298 (1.677 secs)\n",
      "cnp:cnp step 74000 lr 7.886e-05 [train_loss] loss -0.4442 (1.680 secs)\n",
      "cnp:cnp step 74200 lr 7.772e-05 [train_loss] loss -0.4961 (1.516 secs)\n",
      "cnp:cnp step 74400 lr 7.659e-05 [train_loss] loss -0.4026 (1.461 secs)\n",
      "cnp:cnp step 74600 lr 7.546e-05 [train_loss] loss -0.4154 (1.585 secs)\n",
      "cnp:cnp step 74800 lr 7.434e-05 [train_loss] loss -0.4293 (1.656 secs)\n",
      "cnp:cnp step 75000 lr 7.322e-05 [train_loss] loss -0.4420 (1.473 secs)\n",
      "100%|##########| 3000/3000 [00:05<00:00, 569.98it/s]\n",
      "cnp:cnp rbf ctx_ll 0.6841 tar_ll 0.2333 (5.265 secs)\n",
      "\n",
      "cnp:cnp step 75200 lr 7.212e-05 [train_loss] loss -0.4818 (1.743 secs)\n",
      "cnp:cnp step 75400 lr 7.102e-05 [train_loss] loss -0.4546 (1.437 secs)\n",
      "cnp:cnp step 75600 lr 6.992e-05 [train_loss] loss -0.4308 (1.446 secs)\n",
      "cnp:cnp step 75800 lr 6.884e-05 [train_loss] loss -0.4588 (1.534 secs)\n",
      "cnp:cnp step 76000 lr 6.776e-05 [train_loss] loss -0.4681 (1.715 secs)\n",
      "cnp:cnp step 76200 lr 6.669e-05 [train_loss] loss -0.4476 (1.507 secs)\n",
      "cnp:cnp step 76400 lr 6.562e-05 [train_loss] loss -0.4333 (1.588 secs)\n",
      "cnp:cnp step 76600 lr 6.456e-05 [train_loss] loss -0.4437 (1.539 secs)\n",
      "cnp:cnp step 76800 lr 6.351e-05 [train_loss] loss -0.4815 (1.615 secs)\n",
      "cnp:cnp step 77000 lr 6.247e-05 [train_loss] loss -0.4408 (1.698 secs)\n",
      "cnp:cnp step 77200 lr 6.144e-05 [train_loss] loss -0.4345 (1.468 secs)\n",
      "cnp:cnp step 77400 lr 6.041e-05 [train_loss] loss -0.4065 (1.619 secs)\n",
      "cnp:cnp step 77600 lr 5.939e-05 [train_loss] loss -0.4475 (1.491 secs)\n",
      "cnp:cnp step 77800 lr 5.838e-05 [train_loss] loss -0.4077 (1.620 secs)\n",
      "cnp:cnp step 78000 lr 5.737e-05 [train_loss] loss -0.4455 (1.367 secs)\n",
      "cnp:cnp step 78200 lr 5.637e-05 [train_loss] loss -0.4758 (1.389 secs)\n",
      "cnp:cnp step 78400 lr 5.538e-05 [train_loss] loss -0.4473 (1.386 secs)\n",
      "cnp:cnp step 78600 lr 5.440e-05 [train_loss] loss -0.4489 (1.469 secs)\n",
      "cnp:cnp step 78800 lr 5.343e-05 [train_loss] loss -0.4175 (1.480 secs)\n",
      "cnp:cnp step 79000 lr 5.246e-05 [train_loss] loss -0.4785 (1.791 secs)\n",
      "cnp:cnp step 79200 lr 5.150e-05 [train_loss] loss -0.4385 (1.501 secs)\n",
      "cnp:cnp step 79400 lr 5.055e-05 [train_loss] loss -0.3956 (1.405 secs)\n",
      "cnp:cnp step 79600 lr 4.961e-05 [train_loss] loss -0.4997 (1.519 secs)\n",
      "cnp:cnp step 79800 lr 4.867e-05 [train_loss] loss -0.4600 (1.594 secs)\n",
      "cnp:cnp step 80000 lr 4.775e-05 [train_loss] loss -0.4672 (1.470 secs)\n",
      "100%|##########| 3000/3000 [00:04<00:00, 618.36it/s]\n",
      "cnp:cnp rbf ctx_ll 0.6990 tar_ll 0.2482 (4.854 secs)\n",
      "\n",
      "cnp:cnp step 80200 lr 4.683e-05 [train_loss] loss -0.4270 (1.586 secs)\n",
      "cnp:cnp step 80400 lr 4.592e-05 [train_loss] loss -0.4217 (2.067 secs)\n",
      "cnp:cnp step 80600 lr 4.501e-05 [train_loss] loss -0.4274 (1.525 secs)\n",
      "cnp:cnp step 80800 lr 4.412e-05 [train_loss] loss -0.4353 (1.545 secs)\n",
      "cnp:cnp step 81000 lr 4.323e-05 [train_loss] loss -0.3937 (1.436 secs)\n",
      "cnp:cnp step 81200 lr 4.235e-05 [train_loss] loss -0.3835 (1.450 secs)\n",
      "cnp:cnp step 81400 lr 4.148e-05 [train_loss] loss -0.4913 (1.502 secs)\n",
      "cnp:cnp step 81600 lr 4.062e-05 [train_loss] loss -0.4820 (1.424 secs)\n",
      "cnp:cnp step 81800 lr 3.976e-05 [train_loss] loss -0.4406 (1.656 secs)\n",
      "cnp:cnp step 82000 lr 3.892e-05 [train_loss] loss -0.4611 (1.629 secs)\n",
      "cnp:cnp step 82200 lr 3.808e-05 [train_loss] loss -0.4207 (1.884 secs)\n",
      "cnp:cnp step 82400 lr 3.725e-05 [train_loss] loss -0.4742 (1.812 secs)\n",
      "cnp:cnp step 82600 lr 3.643e-05 [train_loss] loss -0.4362 (1.485 secs)\n",
      "cnp:cnp step 82800 lr 3.562e-05 [train_loss] loss -0.4620 (1.531 secs)\n",
      "cnp:cnp step 83000 lr 3.481e-05 [train_loss] loss -0.4674 (1.622 secs)\n",
      "cnp:cnp step 83200 lr 3.402e-05 [train_loss] loss -0.3979 (1.572 secs)\n",
      "cnp:cnp step 83400 lr 3.323e-05 [train_loss] loss -0.4476 (1.669 secs)\n",
      "cnp:cnp step 83600 lr 3.245e-05 [train_loss] loss -0.4732 (1.591 secs)\n",
      "cnp:cnp step 83800 lr 3.168e-05 [train_loss] loss -0.4659 (1.459 secs)\n",
      "cnp:cnp step 84000 lr 3.092e-05 [train_loss] loss -0.4284 (1.411 secs)\n",
      "cnp:cnp step 84200 lr 3.017e-05 [train_loss] loss -0.4489 (1.403 secs)\n",
      "cnp:cnp step 84400 lr 2.943e-05 [train_loss] loss -0.4539 (1.846 secs)\n",
      "cnp:cnp step 84600 lr 2.869e-05 [train_loss] loss -0.4261 (1.481 secs)\n",
      "cnp:cnp step 84800 lr 2.797e-05 [train_loss] loss -0.4340 (1.885 secs)\n",
      "cnp:cnp step 85000 lr 2.725e-05 [train_loss] loss -0.4387 (1.503 secs)\n",
      "100%|##########| 3000/3000 [00:04<00:00, 627.93it/s]\n",
      "cnp:cnp rbf ctx_ll 0.7025 tar_ll 0.2568 (4.780 secs)\n",
      "\n",
      "cnp:cnp step 85200 lr 2.654e-05 [train_loss] loss -0.4441 (1.481 secs)\n",
      "cnp:cnp step 85400 lr 2.584e-05 [train_loss] loss -0.4769 (1.447 secs)\n",
      "cnp:cnp step 85600 lr 2.515e-05 [train_loss] loss -0.4745 (1.538 secs)\n",
      "cnp:cnp step 85800 lr 2.447e-05 [train_loss] loss -0.4776 (1.547 secs)\n",
      "cnp:cnp step 86000 lr 2.379e-05 [train_loss] loss -0.5189 (1.447 secs)\n",
      "cnp:cnp step 86200 lr 2.313e-05 [train_loss] loss -0.4084 (1.475 secs)\n",
      "cnp:cnp step 86400 lr 2.247e-05 [train_loss] loss -0.4998 (1.530 secs)\n",
      "cnp:cnp step 86600 lr 2.183e-05 [train_loss] loss -0.4738 (1.613 secs)\n",
      "cnp:cnp step 86800 lr 2.119e-05 [train_loss] loss -0.4568 (1.530 secs)\n",
      "cnp:cnp step 87000 lr 2.056e-05 [train_loss] loss -0.4681 (1.492 secs)\n",
      "cnp:cnp step 87200 lr 1.994e-05 [train_loss] loss -0.4974 (1.462 secs)\n",
      "cnp:cnp step 87400 lr 1.933e-05 [train_loss] loss -0.4532 (1.708 secs)\n",
      "cnp:cnp step 87600 lr 1.873e-05 [train_loss] loss -0.4775 (1.527 secs)\n",
      "cnp:cnp step 87800 lr 1.814e-05 [train_loss] loss -0.4835 (1.576 secs)\n",
      "cnp:cnp step 88000 lr 1.756e-05 [train_loss] loss -0.4671 (1.682 secs)\n",
      "cnp:cnp step 88200 lr 1.698e-05 [train_loss] loss -0.4806 (1.415 secs)\n",
      "cnp:cnp step 88400 lr 1.642e-05 [train_loss] loss -0.4914 (1.443 secs)\n",
      "cnp:cnp step 88600 lr 1.586e-05 [train_loss] loss -0.4811 (1.415 secs)\n",
      "cnp:cnp step 88800 lr 1.532e-05 [train_loss] loss -0.4511 (1.499 secs)\n",
      "cnp:cnp step 89000 lr 1.478e-05 [train_loss] loss -0.5245 (1.378 secs)\n",
      "cnp:cnp step 89200 lr 1.425e-05 [train_loss] loss -0.4538 (1.431 secs)\n",
      "cnp:cnp step 89400 lr 1.373e-05 [train_loss] loss -0.4749 (1.429 secs)\n",
      "cnp:cnp step 89600 lr 1.323e-05 [train_loss] loss -0.4452 (1.416 secs)\n",
      "cnp:cnp step 89800 lr 1.273e-05 [train_loss] loss -0.4805 (1.533 secs)\n",
      "cnp:cnp step 90000 lr 1.224e-05 [train_loss] loss -0.4686 (1.672 secs)\n",
      "100%|##########| 3000/3000 [00:04<00:00, 636.64it/s]\n",
      "cnp:cnp rbf ctx_ll 0.7147 tar_ll 0.2561 (4.714 secs)\n",
      "\n",
      "cnp:cnp step 90200 lr 1.176e-05 [train_loss] loss -0.4486 (1.605 secs)\n",
      "cnp:cnp step 90400 lr 1.128e-05 [train_loss] loss -0.4851 (1.426 secs)\n",
      "cnp:cnp step 90600 lr 1.082e-05 [train_loss] loss -0.4923 (1.417 secs)\n",
      "cnp:cnp step 90800 lr 1.037e-05 [train_loss] loss -0.4253 (1.640 secs)\n",
      "cnp:cnp step 91000 lr 9.927e-06 [train_loss] loss -0.4605 (1.685 secs)\n",
      "cnp:cnp step 91200 lr 9.493e-06 [train_loss] loss -0.4617 (1.766 secs)\n",
      "cnp:cnp step 91400 lr 9.069e-06 [train_loss] loss -0.4838 (1.612 secs)\n",
      "cnp:cnp step 91600 lr 8.655e-06 [train_loss] loss -0.4702 (1.417 secs)\n",
      "cnp:cnp step 91800 lr 8.250e-06 [train_loss] loss -0.4435 (1.444 secs)\n",
      "cnp:cnp step 92000 lr 7.854e-06 [train_loss] loss -0.4434 (1.378 secs)\n",
      "cnp:cnp step 92200 lr 7.468e-06 [train_loss] loss -0.4620 (1.393 secs)\n",
      "cnp:cnp step 92400 lr 7.092e-06 [train_loss] loss -0.4409 (1.768 secs)\n",
      "cnp:cnp step 92600 lr 6.725e-06 [train_loss] loss -0.4393 (1.486 secs)\n",
      "cnp:cnp step 92800 lr 6.368e-06 [train_loss] loss -0.4216 (1.374 secs)\n",
      "cnp:cnp step 93000 lr 6.021e-06 [train_loss] loss -0.4922 (1.387 secs)\n",
      "cnp:cnp step 93200 lr 5.683e-06 [train_loss] loss -0.4456 (1.551 secs)\n",
      "cnp:cnp step 93400 lr 5.355e-06 [train_loss] loss -0.4850 (1.610 secs)\n",
      "cnp:cnp step 93600 lr 5.036e-06 [train_loss] loss -0.4520 (1.610 secs)\n",
      "cnp:cnp step 93800 lr 4.727e-06 [train_loss] loss -0.4721 (1.410 secs)\n",
      "cnp:cnp step 94000 lr 4.428e-06 [train_loss] loss -0.4783 (1.566 secs)\n",
      "cnp:cnp step 94200 lr 4.139e-06 [train_loss] loss -0.4797 (1.525 secs)\n",
      "cnp:cnp step 94400 lr 3.859e-06 [train_loss] loss -0.4957 (1.545 secs)\n",
      "cnp:cnp step 94600 lr 3.589e-06 [train_loss] loss -0.4999 (1.667 secs)\n",
      "cnp:cnp step 94800 lr 3.329e-06 [train_loss] loss -0.4796 (1.589 secs)\n",
      "cnp:cnp step 95000 lr 3.078e-06 [train_loss] loss -0.4596 (1.648 secs)\n",
      "100%|##########| 3000/3000 [00:04<00:00, 611.22it/s]\n",
      "cnp:cnp rbf ctx_ll 0.7120 tar_ll 0.2636 (4.910 secs)\n",
      "\n",
      "cnp:cnp step 95200 lr 2.837e-06 [train_loss] loss -0.4668 (1.481 secs)\n",
      "cnp:cnp step 95400 lr 2.606e-06 [train_loss] loss -0.4165 (1.368 secs)\n",
      "cnp:cnp step 95600 lr 2.385e-06 [train_loss] loss -0.4496 (1.429 secs)\n",
      "cnp:cnp step 95800 lr 2.173e-06 [train_loss] loss -0.4589 (1.448 secs)\n",
      "cnp:cnp step 96000 lr 1.971e-06 [train_loss] loss -0.4440 (1.515 secs)\n",
      "cnp:cnp step 96200 lr 1.779e-06 [train_loss] loss -0.4661 (1.520 secs)\n",
      "cnp:cnp step 96400 lr 1.597e-06 [train_loss] loss -0.4573 (1.557 secs)\n",
      "cnp:cnp step 96600 lr 1.425e-06 [train_loss] loss -0.4692 (1.487 secs)\n",
      "cnp:cnp step 96800 lr 1.262e-06 [train_loss] loss -0.5077 (1.562 secs)\n",
      "cnp:cnp step 97000 lr 1.110e-06 [train_loss] loss -0.4507 (1.610 secs)\n",
      "cnp:cnp step 97200 lr 9.666e-07 [train_loss] loss -0.4963 (1.344 secs)\n",
      "cnp:cnp step 97400 lr 8.335e-07 [train_loss] loss -0.4501 (1.344 secs)\n",
      "cnp:cnp step 97600 lr 7.103e-07 [train_loss] loss -0.4688 (1.356 secs)\n",
      "cnp:cnp step 97800 lr 5.969e-07 [train_loss] loss -0.4692 (1.489 secs)\n",
      "cnp:cnp step 98000 lr 4.933e-07 [train_loss] loss -0.5173 (1.503 secs)\n",
      "cnp:cnp step 98200 lr 3.996e-07 [train_loss] loss -0.4715 (1.449 secs)\n",
      "cnp:cnp step 98400 lr 3.158e-07 [train_loss] loss -0.4689 (1.597 secs)\n",
      "cnp:cnp step 98600 lr 2.418e-07 [train_loss] loss -0.4444 (1.522 secs)\n",
      "cnp:cnp step 98800 lr 1.776e-07 [train_loss] loss -0.4681 (1.459 secs)\n",
      "cnp:cnp step 99000 lr 1.234e-07 [train_loss] loss -0.4839 (1.656 secs)\n",
      "cnp:cnp step 99200 lr 7.895e-08 [train_loss] loss -0.4682 (1.508 secs)\n",
      "cnp:cnp step 99400 lr 4.441e-08 [train_loss] loss -0.4783 (1.365 secs)\n",
      "cnp:cnp step 99600 lr 1.974e-08 [train_loss] loss -0.4632 (1.350 secs)\n",
      "cnp:cnp step 99800 lr 4.935e-09 [train_loss] loss -0.4906 (1.338 secs)\n",
      "cnp:cnp step 100000 lr 0.000e+00 [train_loss] loss -0.4787 (1.397 secs)\n",
      "100%|##########| 3000/3000 [00:04<00:00, 612.12it/s]\n",
      "cnp:cnp rbf ctx_ll 0.7169 tar_ll 0.2602 (4.903 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:04<00:00, 602.11it/s]\n",
      "cnp:cnp rbf ctx_ll 0.7169 tar_ll 0.2602 (4.983 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 893686.0 miliseconds\n",
      "Execution time: 893.686 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 39.58251953125 MB\n",
      "Memory Usage Change: 23.33251953125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='cnp', name='cnp',val_seed=0, val_l=None,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a61e0b-68db-47d5-a34a-034ba6b3fc31",
   "metadata": {},
   "source": [
    "# MATERN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e103f9a3-0675-45e0-a8af-ce652ab19237",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "093d1235-598d-4da1-a3b9-9d8e784bab82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating evaluation sets...\n",
      "Generating Evaluation Sets with matern kernel\n",
      "Generating Evaluation sets with context and target points:   None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:05<00:00, 565.56it/s]\n",
      "Experiment: cnp-cnp_matern\n",
      "Experiment: cnp-cnp_matern\n",
      "Total number of parameters: 215682\n",
      "\n",
      "Total number of parameters: 215682\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "matern-seed100.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 200 lr 5.000e-04 [train_loss] loss 0.6762 (1.542 secs)\n",
      "cnp:cnp_matern step 200 lr 5.000e-04 [train_loss] loss 0.6762 (1.542 secs)\n",
      "cnp:cnp_matern step 400 lr 5.000e-04 [train_loss] loss 0.6138 (1.454 secs)\n",
      "cnp:cnp_matern step 400 lr 5.000e-04 [train_loss] loss 0.6138 (1.454 secs)\n",
      "cnp:cnp_matern step 600 lr 5.000e-04 [train_loss] loss 0.5356 (1.577 secs)\n",
      "cnp:cnp_matern step 600 lr 5.000e-04 [train_loss] loss 0.5356 (1.577 secs)\n",
      "cnp:cnp_matern step 800 lr 4.999e-04 [train_loss] loss 0.5164 (1.244 secs)\n",
      "cnp:cnp_matern step 800 lr 4.999e-04 [train_loss] loss 0.5164 (1.244 secs)\n",
      "cnp:cnp_matern step 1000 lr 4.999e-04 [train_loss] loss 0.5377 (1.219 secs)\n",
      "cnp:cnp_matern step 1000 lr 4.999e-04 [train_loss] loss 0.5377 (1.219 secs)\n",
      "cnp:cnp_matern step 1200 lr 4.998e-04 [train_loss] loss 0.5476 (1.195 secs)\n",
      "cnp:cnp_matern step 1200 lr 4.998e-04 [train_loss] loss 0.5476 (1.195 secs)\n",
      "cnp:cnp_matern step 1400 lr 4.998e-04 [train_loss] loss 0.5144 (1.195 secs)\n",
      "cnp:cnp_matern step 1400 lr 4.998e-04 [train_loss] loss 0.5144 (1.195 secs)\n",
      "cnp:cnp_matern step 1600 lr 4.997e-04 [train_loss] loss 0.4690 (1.198 secs)\n",
      "cnp:cnp_matern step 1600 lr 4.997e-04 [train_loss] loss 0.4690 (1.198 secs)\n",
      "cnp:cnp_matern step 1800 lr 4.996e-04 [train_loss] loss 0.4445 (1.209 secs)\n",
      "cnp:cnp_matern step 1800 lr 4.996e-04 [train_loss] loss 0.4445 (1.209 secs)\n",
      "cnp:cnp_matern step 2000 lr 4.995e-04 [train_loss] loss 0.4778 (1.178 secs)\n",
      "cnp:cnp_matern step 2000 lr 4.995e-04 [train_loss] loss 0.4778 (1.178 secs)\n",
      "cnp:cnp_matern step 2200 lr 4.994e-04 [train_loss] loss 0.4440 (1.212 secs)\n",
      "cnp:cnp_matern step 2200 lr 4.994e-04 [train_loss] loss 0.4440 (1.212 secs)\n",
      "cnp:cnp_matern step 2400 lr 4.993e-04 [train_loss] loss 0.4202 (1.195 secs)\n",
      "cnp:cnp_matern step 2400 lr 4.993e-04 [train_loss] loss 0.4202 (1.195 secs)\n",
      "cnp:cnp_matern step 2600 lr 4.992e-04 [train_loss] loss 0.4512 (1.195 secs)\n",
      "cnp:cnp_matern step 2600 lr 4.992e-04 [train_loss] loss 0.4512 (1.195 secs)\n",
      "cnp:cnp_matern step 2800 lr 4.990e-04 [train_loss] loss 0.4229 (1.195 secs)\n",
      "cnp:cnp_matern step 2800 lr 4.990e-04 [train_loss] loss 0.4229 (1.195 secs)\n",
      "cnp:cnp_matern step 3000 lr 4.989e-04 [train_loss] loss 0.3979 (1.321 secs)\n",
      "cnp:cnp_matern step 3000 lr 4.989e-04 [train_loss] loss 0.3979 (1.321 secs)\n",
      "cnp:cnp_matern step 3200 lr 4.987e-04 [train_loss] loss 0.4407 (1.384 secs)\n",
      "cnp:cnp_matern step 3200 lr 4.987e-04 [train_loss] loss 0.4407 (1.384 secs)\n",
      "cnp:cnp_matern step 3400 lr 4.986e-04 [train_loss] loss 0.4065 (1.289 secs)\n",
      "cnp:cnp_matern step 3400 lr 4.986e-04 [train_loss] loss 0.4065 (1.289 secs)\n",
      "cnp:cnp_matern step 3600 lr 4.984e-04 [train_loss] loss 0.3908 (1.343 secs)\n",
      "cnp:cnp_matern step 3600 lr 4.984e-04 [train_loss] loss 0.3908 (1.343 secs)\n",
      "cnp:cnp_matern step 3800 lr 4.982e-04 [train_loss] loss 0.3773 (1.362 secs)\n",
      "cnp:cnp_matern step 3800 lr 4.982e-04 [train_loss] loss 0.3773 (1.362 secs)\n",
      "cnp:cnp_matern step 4000 lr 4.980e-04 [train_loss] loss 0.3567 (1.369 secs)\n",
      "cnp:cnp_matern step 4000 lr 4.980e-04 [train_loss] loss 0.3567 (1.369 secs)\n",
      "cnp:cnp_matern step 4200 lr 4.978e-04 [train_loss] loss 0.3620 (1.447 secs)\n",
      "cnp:cnp_matern step 4200 lr 4.978e-04 [train_loss] loss 0.3620 (1.447 secs)\n",
      "cnp:cnp_matern step 4400 lr 4.976e-04 [train_loss] loss 0.3446 (1.336 secs)\n",
      "cnp:cnp_matern step 4400 lr 4.976e-04 [train_loss] loss 0.3446 (1.336 secs)\n",
      "cnp:cnp_matern step 4600 lr 4.974e-04 [train_loss] loss 0.3408 (1.367 secs)\n",
      "cnp:cnp_matern step 4600 lr 4.974e-04 [train_loss] loss 0.3408 (1.367 secs)\n",
      "cnp:cnp_matern step 4800 lr 4.972e-04 [train_loss] loss 0.3240 (1.339 secs)\n",
      "cnp:cnp_matern step 4800 lr 4.972e-04 [train_loss] loss 0.3240 (1.339 secs)\n",
      "cnp:cnp_matern step 5000 lr 4.969e-04 [train_loss] loss 0.3245 (1.337 secs)\n",
      "cnp:cnp_matern step 5000 lr 4.969e-04 [train_loss] loss 0.3245 (1.337 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:05<00:00, 584.69it/s]\n",
      "cnp:cnp_matern matern ctx_ll -0.2247 tar_ll -0.4642 (5.131 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll -0.2247 tar_ll -0.4642 (5.131 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 5200 lr 4.967e-04 [train_loss] loss 0.3169 (1.328 secs)\n",
      "cnp:cnp_matern step 5200 lr 4.967e-04 [train_loss] loss 0.3169 (1.328 secs)\n",
      "cnp:cnp_matern step 5400 lr 4.964e-04 [train_loss] loss 0.2853 (1.291 secs)\n",
      "cnp:cnp_matern step 5400 lr 4.964e-04 [train_loss] loss 0.2853 (1.291 secs)\n",
      "cnp:cnp_matern step 5600 lr 4.961e-04 [train_loss] loss 0.2903 (1.330 secs)\n",
      "cnp:cnp_matern step 5600 lr 4.961e-04 [train_loss] loss 0.2903 (1.330 secs)\n",
      "cnp:cnp_matern step 5800 lr 4.959e-04 [train_loss] loss 0.3016 (1.286 secs)\n",
      "cnp:cnp_matern step 5800 lr 4.959e-04 [train_loss] loss 0.3016 (1.286 secs)\n",
      "cnp:cnp_matern step 6000 lr 4.956e-04 [train_loss] loss 0.2694 (1.286 secs)\n",
      "cnp:cnp_matern step 6000 lr 4.956e-04 [train_loss] loss 0.2694 (1.286 secs)\n",
      "cnp:cnp_matern step 6200 lr 4.953e-04 [train_loss] loss 0.2651 (1.416 secs)\n",
      "cnp:cnp_matern step 6200 lr 4.953e-04 [train_loss] loss 0.2651 (1.416 secs)\n",
      "cnp:cnp_matern step 6400 lr 4.950e-04 [train_loss] loss 0.2584 (1.380 secs)\n",
      "cnp:cnp_matern step 6400 lr 4.950e-04 [train_loss] loss 0.2584 (1.380 secs)\n",
      "cnp:cnp_matern step 6600 lr 4.946e-04 [train_loss] loss 0.2309 (1.302 secs)\n",
      "cnp:cnp_matern step 6600 lr 4.946e-04 [train_loss] loss 0.2309 (1.302 secs)\n",
      "cnp:cnp_matern step 6800 lr 4.943e-04 [train_loss] loss 0.2138 (1.334 secs)\n",
      "cnp:cnp_matern step 6800 lr 4.943e-04 [train_loss] loss 0.2138 (1.334 secs)\n",
      "cnp:cnp_matern step 7000 lr 4.940e-04 [train_loss] loss 0.2283 (1.633 secs)\n",
      "cnp:cnp_matern step 7000 lr 4.940e-04 [train_loss] loss 0.2283 (1.633 secs)\n",
      "cnp:cnp_matern step 7200 lr 4.936e-04 [train_loss] loss 0.1958 (1.412 secs)\n",
      "cnp:cnp_matern step 7200 lr 4.936e-04 [train_loss] loss 0.1958 (1.412 secs)\n",
      "cnp:cnp_matern step 7400 lr 4.933e-04 [train_loss] loss 0.2220 (1.318 secs)\n",
      "cnp:cnp_matern step 7400 lr 4.933e-04 [train_loss] loss 0.2220 (1.318 secs)\n",
      "cnp:cnp_matern step 7600 lr 4.929e-04 [train_loss] loss 0.1621 (1.318 secs)\n",
      "cnp:cnp_matern step 7600 lr 4.929e-04 [train_loss] loss 0.1621 (1.318 secs)\n",
      "cnp:cnp_matern step 7800 lr 4.925e-04 [train_loss] loss 0.2191 (1.349 secs)\n",
      "cnp:cnp_matern step 7800 lr 4.925e-04 [train_loss] loss 0.2191 (1.349 secs)\n",
      "cnp:cnp_matern step 8000 lr 4.921e-04 [train_loss] loss 0.1803 (1.319 secs)\n",
      "cnp:cnp_matern step 8000 lr 4.921e-04 [train_loss] loss 0.1803 (1.319 secs)\n",
      "cnp:cnp_matern step 8200 lr 4.918e-04 [train_loss] loss 0.1776 (1.365 secs)\n",
      "cnp:cnp_matern step 8200 lr 4.918e-04 [train_loss] loss 0.1776 (1.365 secs)\n",
      "cnp:cnp_matern step 8400 lr 4.913e-04 [train_loss] loss 0.1460 (1.333 secs)\n",
      "cnp:cnp_matern step 8400 lr 4.913e-04 [train_loss] loss 0.1460 (1.333 secs)\n",
      "cnp:cnp_matern step 8600 lr 4.909e-04 [train_loss] loss 0.1576 (1.319 secs)\n",
      "cnp:cnp_matern step 8600 lr 4.909e-04 [train_loss] loss 0.1576 (1.319 secs)\n",
      "cnp:cnp_matern step 8800 lr 4.905e-04 [train_loss] loss 0.1629 (1.333 secs)\n",
      "cnp:cnp_matern step 8800 lr 4.905e-04 [train_loss] loss 0.1629 (1.333 secs)\n",
      "cnp:cnp_matern step 9000 lr 4.901e-04 [train_loss] loss 0.0884 (1.349 secs)\n",
      "cnp:cnp_matern step 9000 lr 4.901e-04 [train_loss] loss 0.0884 (1.349 secs)\n",
      "cnp:cnp_matern step 9200 lr 4.896e-04 [train_loss] loss 0.1434 (1.461 secs)\n",
      "cnp:cnp_matern step 9200 lr 4.896e-04 [train_loss] loss 0.1434 (1.461 secs)\n",
      "cnp:cnp_matern step 9400 lr 4.892e-04 [train_loss] loss 0.1639 (1.553 secs)\n",
      "cnp:cnp_matern step 9400 lr 4.892e-04 [train_loss] loss 0.1639 (1.553 secs)\n",
      "cnp:cnp_matern step 9600 lr 4.887e-04 [train_loss] loss 0.1099 (1.526 secs)\n",
      "cnp:cnp_matern step 9600 lr 4.887e-04 [train_loss] loss 0.1099 (1.526 secs)\n",
      "cnp:cnp_matern step 9800 lr 4.882e-04 [train_loss] loss 0.1730 (1.562 secs)\n",
      "cnp:cnp_matern step 9800 lr 4.882e-04 [train_loss] loss 0.1730 (1.562 secs)\n",
      "cnp:cnp_matern step 10000 lr 4.878e-04 [train_loss] loss 0.1490 (1.343 secs)\n",
      "cnp:cnp_matern step 10000 lr 4.878e-04 [train_loss] loss 0.1490 (1.343 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:04<00:00, 669.10it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.0085 tar_ll -0.3110 (4.484 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.0085 tar_ll -0.3110 (4.484 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 10200 lr 4.873e-04 [train_loss] loss 0.1348 (1.533 secs)\n",
      "cnp:cnp_matern step 10200 lr 4.873e-04 [train_loss] loss 0.1348 (1.533 secs)\n",
      "cnp:cnp_matern step 10400 lr 4.868e-04 [train_loss] loss 0.1460 (1.600 secs)\n",
      "cnp:cnp_matern step 10400 lr 4.868e-04 [train_loss] loss 0.1460 (1.600 secs)\n",
      "cnp:cnp_matern step 10600 lr 4.863e-04 [train_loss] loss 0.1108 (1.602 secs)\n",
      "cnp:cnp_matern step 10600 lr 4.863e-04 [train_loss] loss 0.1108 (1.602 secs)\n",
      "cnp:cnp_matern step 10800 lr 4.857e-04 [train_loss] loss 0.1038 (1.859 secs)\n",
      "cnp:cnp_matern step 10800 lr 4.857e-04 [train_loss] loss 0.1038 (1.859 secs)\n",
      "cnp:cnp_matern step 11000 lr 4.852e-04 [train_loss] loss 0.0818 (1.543 secs)\n",
      "cnp:cnp_matern step 11000 lr 4.852e-04 [train_loss] loss 0.0818 (1.543 secs)\n",
      "cnp:cnp_matern step 11200 lr 4.847e-04 [train_loss] loss 0.0828 (1.582 secs)\n",
      "cnp:cnp_matern step 11200 lr 4.847e-04 [train_loss] loss 0.0828 (1.582 secs)\n",
      "cnp:cnp_matern step 11400 lr 4.841e-04 [train_loss] loss 0.0777 (1.695 secs)\n",
      "cnp:cnp_matern step 11400 lr 4.841e-04 [train_loss] loss 0.0777 (1.695 secs)\n",
      "cnp:cnp_matern step 11600 lr 4.836e-04 [train_loss] loss 0.1399 (1.414 secs)\n",
      "cnp:cnp_matern step 11600 lr 4.836e-04 [train_loss] loss 0.1399 (1.414 secs)\n",
      "cnp:cnp_matern step 11800 lr 4.830e-04 [train_loss] loss 0.0360 (1.448 secs)\n",
      "cnp:cnp_matern step 11800 lr 4.830e-04 [train_loss] loss 0.0360 (1.448 secs)\n",
      "cnp:cnp_matern step 12000 lr 4.824e-04 [train_loss] loss 0.0702 (1.481 secs)\n",
      "cnp:cnp_matern step 12000 lr 4.824e-04 [train_loss] loss 0.0702 (1.481 secs)\n",
      "cnp:cnp_matern step 12200 lr 4.819e-04 [train_loss] loss 0.0319 (1.432 secs)\n",
      "cnp:cnp_matern step 12200 lr 4.819e-04 [train_loss] loss 0.0319 (1.432 secs)\n",
      "cnp:cnp_matern step 12400 lr 4.813e-04 [train_loss] loss 0.0038 (1.384 secs)\n",
      "cnp:cnp_matern step 12400 lr 4.813e-04 [train_loss] loss 0.0038 (1.384 secs)\n",
      "cnp:cnp_matern step 12600 lr 4.807e-04 [train_loss] loss 0.0675 (1.593 secs)\n",
      "cnp:cnp_matern step 12600 lr 4.807e-04 [train_loss] loss 0.0675 (1.593 secs)\n",
      "cnp:cnp_matern step 12800 lr 4.801e-04 [train_loss] loss 0.0166 (1.987 secs)\n",
      "cnp:cnp_matern step 12800 lr 4.801e-04 [train_loss] loss 0.0166 (1.987 secs)\n",
      "cnp:cnp_matern step 13000 lr 4.794e-04 [train_loss] loss 0.0229 (1.681 secs)\n",
      "cnp:cnp_matern step 13000 lr 4.794e-04 [train_loss] loss 0.0229 (1.681 secs)\n",
      "cnp:cnp_matern step 13200 lr 4.788e-04 [train_loss] loss 0.0325 (1.961 secs)\n",
      "cnp:cnp_matern step 13200 lr 4.788e-04 [train_loss] loss 0.0325 (1.961 secs)\n",
      "cnp:cnp_matern step 13400 lr 4.782e-04 [train_loss] loss 0.0397 (1.582 secs)\n",
      "cnp:cnp_matern step 13400 lr 4.782e-04 [train_loss] loss 0.0397 (1.582 secs)\n",
      "cnp:cnp_matern step 13600 lr 4.775e-04 [train_loss] loss 0.0391 (1.726 secs)\n",
      "cnp:cnp_matern step 13600 lr 4.775e-04 [train_loss] loss 0.0391 (1.726 secs)\n",
      "cnp:cnp_matern step 13800 lr 4.769e-04 [train_loss] loss -0.0237 (1.777 secs)\n",
      "cnp:cnp_matern step 13800 lr 4.769e-04 [train_loss] loss -0.0237 (1.777 secs)\n",
      "cnp:cnp_matern step 14000 lr 4.762e-04 [train_loss] loss 0.0257 (1.568 secs)\n",
      "cnp:cnp_matern step 14000 lr 4.762e-04 [train_loss] loss 0.0257 (1.568 secs)\n",
      "cnp:cnp_matern step 14200 lr 4.755e-04 [train_loss] loss 0.0004 (1.665 secs)\n",
      "cnp:cnp_matern step 14200 lr 4.755e-04 [train_loss] loss 0.0004 (1.665 secs)\n",
      "cnp:cnp_matern step 14400 lr 4.749e-04 [train_loss] loss 0.0172 (1.659 secs)\n",
      "cnp:cnp_matern step 14400 lr 4.749e-04 [train_loss] loss 0.0172 (1.659 secs)\n",
      "cnp:cnp_matern step 14600 lr 4.742e-04 [train_loss] loss -0.0120 (1.668 secs)\n",
      "cnp:cnp_matern step 14600 lr 4.742e-04 [train_loss] loss -0.0120 (1.668 secs)\n",
      "cnp:cnp_matern step 14800 lr 4.735e-04 [train_loss] loss 0.0449 (1.473 secs)\n",
      "cnp:cnp_matern step 14800 lr 4.735e-04 [train_loss] loss 0.0449 (1.473 secs)\n",
      "cnp:cnp_matern step 15000 lr 4.728e-04 [train_loss] loss -0.0342 (1.338 secs)\n",
      "cnp:cnp_matern step 15000 lr 4.728e-04 [train_loss] loss -0.0342 (1.338 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:04<00:00, 661.43it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.1339 tar_ll -0.2635 (4.536 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.1339 tar_ll -0.2635 (4.536 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 15200 lr 4.720e-04 [train_loss] loss -0.0042 (1.394 secs)\n",
      "cnp:cnp_matern step 15200 lr 4.720e-04 [train_loss] loss -0.0042 (1.394 secs)\n",
      "cnp:cnp_matern step 15400 lr 4.713e-04 [train_loss] loss -0.0081 (1.725 secs)\n",
      "cnp:cnp_matern step 15400 lr 4.713e-04 [train_loss] loss -0.0081 (1.725 secs)\n",
      "cnp:cnp_matern step 15600 lr 4.706e-04 [train_loss] loss -0.0258 (1.621 secs)\n",
      "cnp:cnp_matern step 15600 lr 4.706e-04 [train_loss] loss -0.0258 (1.621 secs)\n",
      "cnp:cnp_matern step 15800 lr 4.698e-04 [train_loss] loss -0.0085 (1.709 secs)\n",
      "cnp:cnp_matern step 15800 lr 4.698e-04 [train_loss] loss -0.0085 (1.709 secs)\n",
      "cnp:cnp_matern step 16000 lr 4.691e-04 [train_loss] loss -0.0476 (1.921 secs)\n",
      "cnp:cnp_matern step 16000 lr 4.691e-04 [train_loss] loss -0.0476 (1.921 secs)\n",
      "cnp:cnp_matern step 16200 lr 4.683e-04 [train_loss] loss -0.0136 (1.584 secs)\n",
      "cnp:cnp_matern step 16200 lr 4.683e-04 [train_loss] loss -0.0136 (1.584 secs)\n",
      "cnp:cnp_matern step 16400 lr 4.675e-04 [train_loss] loss 0.0241 (1.368 secs)\n",
      "cnp:cnp_matern step 16400 lr 4.675e-04 [train_loss] loss 0.0241 (1.368 secs)\n",
      "cnp:cnp_matern step 16600 lr 4.668e-04 [train_loss] loss -0.0355 (1.553 secs)\n",
      "cnp:cnp_matern step 16600 lr 4.668e-04 [train_loss] loss -0.0355 (1.553 secs)\n",
      "cnp:cnp_matern step 16800 lr 4.660e-04 [train_loss] loss -0.0342 (1.414 secs)\n",
      "cnp:cnp_matern step 16800 lr 4.660e-04 [train_loss] loss -0.0342 (1.414 secs)\n",
      "cnp:cnp_matern step 17000 lr 4.652e-04 [train_loss] loss 0.0070 (1.513 secs)\n",
      "cnp:cnp_matern step 17000 lr 4.652e-04 [train_loss] loss 0.0070 (1.513 secs)\n",
      "cnp:cnp_matern step 17200 lr 4.644e-04 [train_loss] loss 0.0009 (1.614 secs)\n",
      "cnp:cnp_matern step 17200 lr 4.644e-04 [train_loss] loss 0.0009 (1.614 secs)\n",
      "cnp:cnp_matern step 17400 lr 4.636e-04 [train_loss] loss -0.0855 (1.915 secs)\n",
      "cnp:cnp_matern step 17400 lr 4.636e-04 [train_loss] loss -0.0855 (1.915 secs)\n",
      "cnp:cnp_matern step 17600 lr 4.627e-04 [train_loss] loss -0.0389 (1.536 secs)\n",
      "cnp:cnp_matern step 17600 lr 4.627e-04 [train_loss] loss -0.0389 (1.536 secs)\n",
      "cnp:cnp_matern step 17800 lr 4.619e-04 [train_loss] loss -0.0272 (1.508 secs)\n",
      "cnp:cnp_matern step 17800 lr 4.619e-04 [train_loss] loss -0.0272 (1.508 secs)\n",
      "cnp:cnp_matern step 18000 lr 4.611e-04 [train_loss] loss -0.0436 (1.724 secs)\n",
      "cnp:cnp_matern step 18000 lr 4.611e-04 [train_loss] loss -0.0436 (1.724 secs)\n",
      "cnp:cnp_matern step 18200 lr 4.602e-04 [train_loss] loss -0.0785 (1.516 secs)\n",
      "cnp:cnp_matern step 18200 lr 4.602e-04 [train_loss] loss -0.0785 (1.516 secs)\n",
      "cnp:cnp_matern step 18400 lr 4.594e-04 [train_loss] loss -0.0993 (1.414 secs)\n",
      "cnp:cnp_matern step 18400 lr 4.594e-04 [train_loss] loss -0.0993 (1.414 secs)\n",
      "cnp:cnp_matern step 18600 lr 4.585e-04 [train_loss] loss -0.0573 (1.354 secs)\n",
      "cnp:cnp_matern step 18600 lr 4.585e-04 [train_loss] loss -0.0573 (1.354 secs)\n",
      "cnp:cnp_matern step 18800 lr 4.576e-04 [train_loss] loss -0.0476 (1.385 secs)\n",
      "cnp:cnp_matern step 18800 lr 4.576e-04 [train_loss] loss -0.0476 (1.385 secs)\n",
      "cnp:cnp_matern step 19000 lr 4.568e-04 [train_loss] loss -0.0813 (1.370 secs)\n",
      "cnp:cnp_matern step 19000 lr 4.568e-04 [train_loss] loss -0.0813 (1.370 secs)\n",
      "cnp:cnp_matern step 19200 lr 4.559e-04 [train_loss] loss -0.0868 (1.385 secs)\n",
      "cnp:cnp_matern step 19200 lr 4.559e-04 [train_loss] loss -0.0868 (1.385 secs)\n",
      "cnp:cnp_matern step 19400 lr 4.550e-04 [train_loss] loss -0.0408 (1.368 secs)\n",
      "cnp:cnp_matern step 19400 lr 4.550e-04 [train_loss] loss -0.0408 (1.368 secs)\n",
      "cnp:cnp_matern step 19600 lr 4.541e-04 [train_loss] loss -0.0660 (1.341 secs)\n",
      "cnp:cnp_matern step 19600 lr 4.541e-04 [train_loss] loss -0.0660 (1.341 secs)\n",
      "cnp:cnp_matern step 19800 lr 4.532e-04 [train_loss] loss -0.0901 (1.403 secs)\n",
      "cnp:cnp_matern step 19800 lr 4.532e-04 [train_loss] loss -0.0901 (1.403 secs)\n",
      "cnp:cnp_matern step 20000 lr 4.523e-04 [train_loss] loss -0.0403 (1.353 secs)\n",
      "cnp:cnp_matern step 20000 lr 4.523e-04 [train_loss] loss -0.0403 (1.353 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:05<00:00, 591.80it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.1416 tar_ll -0.1943 (5.069 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.1416 tar_ll -0.1943 (5.069 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 20200 lr 4.513e-04 [train_loss] loss -0.1228 (1.555 secs)\n",
      "cnp:cnp_matern step 20200 lr 4.513e-04 [train_loss] loss -0.1228 (1.555 secs)\n",
      "cnp:cnp_matern step 20400 lr 4.504e-04 [train_loss] loss -0.1588 (1.523 secs)\n",
      "cnp:cnp_matern step 20400 lr 4.504e-04 [train_loss] loss -0.1588 (1.523 secs)\n",
      "cnp:cnp_matern step 20600 lr 4.494e-04 [train_loss] loss -0.1216 (1.518 secs)\n",
      "cnp:cnp_matern step 20600 lr 4.494e-04 [train_loss] loss -0.1216 (1.518 secs)\n",
      "cnp:cnp_matern step 20800 lr 4.485e-04 [train_loss] loss -0.0885 (1.513 secs)\n",
      "cnp:cnp_matern step 20800 lr 4.485e-04 [train_loss] loss -0.0885 (1.513 secs)\n",
      "cnp:cnp_matern step 21000 lr 4.475e-04 [train_loss] loss -0.0772 (1.504 secs)\n",
      "cnp:cnp_matern step 21000 lr 4.475e-04 [train_loss] loss -0.0772 (1.504 secs)\n",
      "cnp:cnp_matern step 21200 lr 4.466e-04 [train_loss] loss -0.0907 (1.533 secs)\n",
      "cnp:cnp_matern step 21200 lr 4.466e-04 [train_loss] loss -0.0907 (1.533 secs)\n",
      "cnp:cnp_matern step 21400 lr 4.456e-04 [train_loss] loss -0.1196 (1.569 secs)\n",
      "cnp:cnp_matern step 21400 lr 4.456e-04 [train_loss] loss -0.1196 (1.569 secs)\n",
      "cnp:cnp_matern step 21600 lr 4.446e-04 [train_loss] loss -0.1297 (1.745 secs)\n",
      "cnp:cnp_matern step 21600 lr 4.446e-04 [train_loss] loss -0.1297 (1.745 secs)\n",
      "cnp:cnp_matern step 21800 lr 4.436e-04 [train_loss] loss -0.1359 (1.543 secs)\n",
      "cnp:cnp_matern step 21800 lr 4.436e-04 [train_loss] loss -0.1359 (1.543 secs)\n",
      "cnp:cnp_matern step 22000 lr 4.426e-04 [train_loss] loss -0.0514 (1.518 secs)\n",
      "cnp:cnp_matern step 22000 lr 4.426e-04 [train_loss] loss -0.0514 (1.518 secs)\n",
      "cnp:cnp_matern step 22200 lr 4.416e-04 [train_loss] loss -0.1040 (1.490 secs)\n",
      "cnp:cnp_matern step 22200 lr 4.416e-04 [train_loss] loss -0.1040 (1.490 secs)\n",
      "cnp:cnp_matern step 22400 lr 4.406e-04 [train_loss] loss -0.1109 (1.542 secs)\n",
      "cnp:cnp_matern step 22400 lr 4.406e-04 [train_loss] loss -0.1109 (1.542 secs)\n",
      "cnp:cnp_matern step 22600 lr 4.396e-04 [train_loss] loss -0.0852 (1.712 secs)\n",
      "cnp:cnp_matern step 22600 lr 4.396e-04 [train_loss] loss -0.0852 (1.712 secs)\n",
      "cnp:cnp_matern step 22800 lr 4.386e-04 [train_loss] loss -0.0750 (2.153 secs)\n",
      "cnp:cnp_matern step 22800 lr 4.386e-04 [train_loss] loss -0.0750 (2.153 secs)\n",
      "cnp:cnp_matern step 23000 lr 4.375e-04 [train_loss] loss -0.1192 (3.136 secs)\n",
      "cnp:cnp_matern step 23000 lr 4.375e-04 [train_loss] loss -0.1192 (3.136 secs)\n",
      "cnp:cnp_matern step 23200 lr 4.365e-04 [train_loss] loss -0.1147 (2.778 secs)\n",
      "cnp:cnp_matern step 23200 lr 4.365e-04 [train_loss] loss -0.1147 (2.778 secs)\n",
      "cnp:cnp_matern step 23400 lr 4.354e-04 [train_loss] loss -0.1052 (3.388 secs)\n",
      "cnp:cnp_matern step 23400 lr 4.354e-04 [train_loss] loss -0.1052 (3.388 secs)\n",
      "cnp:cnp_matern step 23600 lr 4.344e-04 [train_loss] loss -0.1067 (2.808 secs)\n",
      "cnp:cnp_matern step 23600 lr 4.344e-04 [train_loss] loss -0.1067 (2.808 secs)\n",
      "cnp:cnp_matern step 23800 lr 4.333e-04 [train_loss] loss -0.1340 (2.782 secs)\n",
      "cnp:cnp_matern step 23800 lr 4.333e-04 [train_loss] loss -0.1340 (2.782 secs)\n",
      "cnp:cnp_matern step 24000 lr 4.322e-04 [train_loss] loss -0.1284 (2.288 secs)\n",
      "cnp:cnp_matern step 24000 lr 4.322e-04 [train_loss] loss -0.1284 (2.288 secs)\n",
      "cnp:cnp_matern step 24200 lr 4.312e-04 [train_loss] loss -0.1073 (3.056 secs)\n",
      "cnp:cnp_matern step 24200 lr 4.312e-04 [train_loss] loss -0.1073 (3.056 secs)\n",
      "cnp:cnp_matern step 24400 lr 4.301e-04 [train_loss] loss -0.0975 (2.550 secs)\n",
      "cnp:cnp_matern step 24400 lr 4.301e-04 [train_loss] loss -0.0975 (2.550 secs)\n",
      "cnp:cnp_matern step 24600 lr 4.290e-04 [train_loss] loss -0.1415 (2.288 secs)\n",
      "cnp:cnp_matern step 24600 lr 4.290e-04 [train_loss] loss -0.1415 (2.288 secs)\n",
      "cnp:cnp_matern step 24800 lr 4.279e-04 [train_loss] loss -0.1303 (1.820 secs)\n",
      "cnp:cnp_matern step 24800 lr 4.279e-04 [train_loss] loss -0.1303 (1.820 secs)\n",
      "cnp:cnp_matern step 25000 lr 4.268e-04 [train_loss] loss -0.1299 (1.620 secs)\n",
      "cnp:cnp_matern step 25000 lr 4.268e-04 [train_loss] loss -0.1299 (1.620 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:05<00:00, 592.59it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.2199 tar_ll -0.1762 (5.063 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.2199 tar_ll -0.1762 (5.063 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 25200 lr 4.257e-04 [train_loss] loss -0.0921 (1.522 secs)\n",
      "cnp:cnp_matern step 25200 lr 4.257e-04 [train_loss] loss -0.0921 (1.522 secs)\n",
      "cnp:cnp_matern step 25400 lr 4.245e-04 [train_loss] loss -0.1695 (1.901 secs)\n",
      "cnp:cnp_matern step 25400 lr 4.245e-04 [train_loss] loss -0.1695 (1.901 secs)\n",
      "cnp:cnp_matern step 25600 lr 4.234e-04 [train_loss] loss -0.1358 (1.697 secs)\n",
      "cnp:cnp_matern step 25600 lr 4.234e-04 [train_loss] loss -0.1358 (1.697 secs)\n",
      "cnp:cnp_matern step 25800 lr 4.223e-04 [train_loss] loss -0.1652 (1.494 secs)\n",
      "cnp:cnp_matern step 25800 lr 4.223e-04 [train_loss] loss -0.1652 (1.494 secs)\n",
      "cnp:cnp_matern step 26000 lr 4.211e-04 [train_loss] loss -0.2059 (1.525 secs)\n",
      "cnp:cnp_matern step 26000 lr 4.211e-04 [train_loss] loss -0.2059 (1.525 secs)\n",
      "cnp:cnp_matern step 26200 lr 4.200e-04 [train_loss] loss -0.1255 (1.510 secs)\n",
      "cnp:cnp_matern step 26200 lr 4.200e-04 [train_loss] loss -0.1255 (1.510 secs)\n",
      "cnp:cnp_matern step 26400 lr 4.188e-04 [train_loss] loss -0.1508 (1.462 secs)\n",
      "cnp:cnp_matern step 26400 lr 4.188e-04 [train_loss] loss -0.1508 (1.462 secs)\n",
      "cnp:cnp_matern step 26600 lr 4.177e-04 [train_loss] loss -0.1388 (1.494 secs)\n",
      "cnp:cnp_matern step 26600 lr 4.177e-04 [train_loss] loss -0.1388 (1.494 secs)\n",
      "cnp:cnp_matern step 26800 lr 4.165e-04 [train_loss] loss -0.1270 (1.730 secs)\n",
      "cnp:cnp_matern step 26800 lr 4.165e-04 [train_loss] loss -0.1270 (1.730 secs)\n",
      "cnp:cnp_matern step 27000 lr 4.153e-04 [train_loss] loss -0.1378 (1.587 secs)\n",
      "cnp:cnp_matern step 27000 lr 4.153e-04 [train_loss] loss -0.1378 (1.587 secs)\n",
      "cnp:cnp_matern step 27200 lr 4.141e-04 [train_loss] loss -0.1704 (1.556 secs)\n",
      "cnp:cnp_matern step 27200 lr 4.141e-04 [train_loss] loss -0.1704 (1.556 secs)\n",
      "cnp:cnp_matern step 27400 lr 4.130e-04 [train_loss] loss -0.1459 (1.808 secs)\n",
      "cnp:cnp_matern step 27400 lr 4.130e-04 [train_loss] loss -0.1459 (1.808 secs)\n",
      "cnp:cnp_matern step 27600 lr 4.118e-04 [train_loss] loss -0.1193 (1.591 secs)\n",
      "cnp:cnp_matern step 27600 lr 4.118e-04 [train_loss] loss -0.1193 (1.591 secs)\n",
      "cnp:cnp_matern step 27800 lr 4.106e-04 [train_loss] loss -0.1231 (1.496 secs)\n",
      "cnp:cnp_matern step 27800 lr 4.106e-04 [train_loss] loss -0.1231 (1.496 secs)\n",
      "cnp:cnp_matern step 28000 lr 4.094e-04 [train_loss] loss -0.1399 (1.508 secs)\n",
      "cnp:cnp_matern step 28000 lr 4.094e-04 [train_loss] loss -0.1399 (1.508 secs)\n",
      "cnp:cnp_matern step 28200 lr 4.081e-04 [train_loss] loss -0.1480 (1.510 secs)\n",
      "cnp:cnp_matern step 28200 lr 4.081e-04 [train_loss] loss -0.1480 (1.510 secs)\n",
      "cnp:cnp_matern step 28400 lr 4.069e-04 [train_loss] loss -0.1707 (1.478 secs)\n",
      "cnp:cnp_matern step 28400 lr 4.069e-04 [train_loss] loss -0.1707 (1.478 secs)\n",
      "cnp:cnp_matern step 28600 lr 4.057e-04 [train_loss] loss -0.1583 (1.500 secs)\n",
      "cnp:cnp_matern step 28600 lr 4.057e-04 [train_loss] loss -0.1583 (1.500 secs)\n",
      "cnp:cnp_matern step 28800 lr 4.045e-04 [train_loss] loss -0.1631 (1.494 secs)\n",
      "cnp:cnp_matern step 28800 lr 4.045e-04 [train_loss] loss -0.1631 (1.494 secs)\n",
      "cnp:cnp_matern step 29000 lr 4.032e-04 [train_loss] loss -0.1544 (1.509 secs)\n",
      "cnp:cnp_matern step 29000 lr 4.032e-04 [train_loss] loss -0.1544 (1.509 secs)\n",
      "cnp:cnp_matern step 29200 lr 4.020e-04 [train_loss] loss -0.1425 (1.558 secs)\n",
      "cnp:cnp_matern step 29200 lr 4.020e-04 [train_loss] loss -0.1425 (1.558 secs)\n",
      "cnp:cnp_matern step 29400 lr 4.007e-04 [train_loss] loss -0.1886 (1.856 secs)\n",
      "cnp:cnp_matern step 29400 lr 4.007e-04 [train_loss] loss -0.1886 (1.856 secs)\n",
      "cnp:cnp_matern step 29600 lr 3.995e-04 [train_loss] loss -0.1836 (1.778 secs)\n",
      "cnp:cnp_matern step 29600 lr 3.995e-04 [train_loss] loss -0.1836 (1.778 secs)\n",
      "cnp:cnp_matern step 29800 lr 3.982e-04 [train_loss] loss -0.1433 (1.699 secs)\n",
      "cnp:cnp_matern step 29800 lr 3.982e-04 [train_loss] loss -0.1433 (1.699 secs)\n",
      "cnp:cnp_matern step 30000 lr 3.969e-04 [train_loss] loss -0.1820 (1.683 secs)\n",
      "cnp:cnp_matern step 30000 lr 3.969e-04 [train_loss] loss -0.1820 (1.683 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:05<00:00, 570.90it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.2755 tar_ll -0.1032 (5.255 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.2755 tar_ll -0.1032 (5.255 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 30200 lr 3.957e-04 [train_loss] loss -0.1966 (1.572 secs)\n",
      "cnp:cnp_matern step 30200 lr 3.957e-04 [train_loss] loss -0.1966 (1.572 secs)\n",
      "cnp:cnp_matern step 30400 lr 3.944e-04 [train_loss] loss -0.1878 (1.640 secs)\n",
      "cnp:cnp_matern step 30400 lr 3.944e-04 [train_loss] loss -0.1878 (1.640 secs)\n",
      "cnp:cnp_matern step 30600 lr 3.931e-04 [train_loss] loss -0.2037 (1.962 secs)\n",
      "cnp:cnp_matern step 30600 lr 3.931e-04 [train_loss] loss -0.2037 (1.962 secs)\n",
      "cnp:cnp_matern step 30800 lr 3.918e-04 [train_loss] loss -0.1800 (1.715 secs)\n",
      "cnp:cnp_matern step 30800 lr 3.918e-04 [train_loss] loss -0.1800 (1.715 secs)\n",
      "cnp:cnp_matern step 31000 lr 3.905e-04 [train_loss] loss -0.1800 (1.632 secs)\n",
      "cnp:cnp_matern step 31000 lr 3.905e-04 [train_loss] loss -0.1800 (1.632 secs)\n",
      "cnp:cnp_matern step 31200 lr 3.892e-04 [train_loss] loss -0.2255 (1.684 secs)\n",
      "cnp:cnp_matern step 31200 lr 3.892e-04 [train_loss] loss -0.2255 (1.684 secs)\n",
      "cnp:cnp_matern step 31400 lr 3.879e-04 [train_loss] loss -0.2286 (1.684 secs)\n",
      "cnp:cnp_matern step 31400 lr 3.879e-04 [train_loss] loss -0.2286 (1.684 secs)\n",
      "cnp:cnp_matern step 31600 lr 3.866e-04 [train_loss] loss -0.2283 (1.746 secs)\n",
      "cnp:cnp_matern step 31600 lr 3.866e-04 [train_loss] loss -0.2283 (1.746 secs)\n",
      "cnp:cnp_matern step 31800 lr 3.853e-04 [train_loss] loss -0.2032 (1.700 secs)\n",
      "cnp:cnp_matern step 31800 lr 3.853e-04 [train_loss] loss -0.2032 (1.700 secs)\n",
      "cnp:cnp_matern step 32000 lr 3.840e-04 [train_loss] loss -0.1930 (1.651 secs)\n",
      "cnp:cnp_matern step 32000 lr 3.840e-04 [train_loss] loss -0.1930 (1.651 secs)\n",
      "cnp:cnp_matern step 32200 lr 3.826e-04 [train_loss] loss -0.2230 (1.698 secs)\n",
      "cnp:cnp_matern step 32200 lr 3.826e-04 [train_loss] loss -0.2230 (1.698 secs)\n",
      "cnp:cnp_matern step 32400 lr 3.813e-04 [train_loss] loss -0.1899 (1.889 secs)\n",
      "cnp:cnp_matern step 32400 lr 3.813e-04 [train_loss] loss -0.1899 (1.889 secs)\n",
      "cnp:cnp_matern step 32600 lr 3.800e-04 [train_loss] loss -0.2793 (1.810 secs)\n",
      "cnp:cnp_matern step 32600 lr 3.800e-04 [train_loss] loss -0.2793 (1.810 secs)\n",
      "cnp:cnp_matern step 32800 lr 3.786e-04 [train_loss] loss -0.2304 (1.684 secs)\n",
      "cnp:cnp_matern step 32800 lr 3.786e-04 [train_loss] loss -0.2304 (1.684 secs)\n",
      "cnp:cnp_matern step 33000 lr 3.773e-04 [train_loss] loss -0.1724 (1.730 secs)\n",
      "cnp:cnp_matern step 33000 lr 3.773e-04 [train_loss] loss -0.1724 (1.730 secs)\n",
      "cnp:cnp_matern step 33200 lr 3.759e-04 [train_loss] loss -0.2102 (1.669 secs)\n",
      "cnp:cnp_matern step 33200 lr 3.759e-04 [train_loss] loss -0.2102 (1.669 secs)\n",
      "cnp:cnp_matern step 33400 lr 3.745e-04 [train_loss] loss -0.1877 (1.731 secs)\n",
      "cnp:cnp_matern step 33400 lr 3.745e-04 [train_loss] loss -0.1877 (1.731 secs)\n",
      "cnp:cnp_matern step 33600 lr 3.732e-04 [train_loss] loss -0.2068 (1.729 secs)\n",
      "cnp:cnp_matern step 33600 lr 3.732e-04 [train_loss] loss -0.2068 (1.729 secs)\n",
      "cnp:cnp_matern step 33800 lr 3.718e-04 [train_loss] loss -0.2017 (1.652 secs)\n",
      "cnp:cnp_matern step 33800 lr 3.718e-04 [train_loss] loss -0.2017 (1.652 secs)\n",
      "cnp:cnp_matern step 34000 lr 3.704e-04 [train_loss] loss -0.2009 (1.730 secs)\n",
      "cnp:cnp_matern step 34000 lr 3.704e-04 [train_loss] loss -0.2009 (1.730 secs)\n",
      "cnp:cnp_matern step 34200 lr 3.691e-04 [train_loss] loss -0.2486 (1.685 secs)\n",
      "cnp:cnp_matern step 34200 lr 3.691e-04 [train_loss] loss -0.2486 (1.685 secs)\n",
      "cnp:cnp_matern step 34400 lr 3.677e-04 [train_loss] loss -0.2353 (1.792 secs)\n",
      "cnp:cnp_matern step 34400 lr 3.677e-04 [train_loss] loss -0.2353 (1.792 secs)\n",
      "cnp:cnp_matern step 34600 lr 3.663e-04 [train_loss] loss -0.2576 (1.762 secs)\n",
      "cnp:cnp_matern step 34600 lr 3.663e-04 [train_loss] loss -0.2576 (1.762 secs)\n",
      "cnp:cnp_matern step 34800 lr 3.649e-04 [train_loss] loss -0.2269 (1.667 secs)\n",
      "cnp:cnp_matern step 34800 lr 3.649e-04 [train_loss] loss -0.2269 (1.667 secs)\n",
      "cnp:cnp_matern step 35000 lr 3.635e-04 [train_loss] loss -0.2312 (1.560 secs)\n",
      "cnp:cnp_matern step 35000 lr 3.635e-04 [train_loss] loss -0.2312 (1.560 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:04<00:00, 657.82it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.3077 tar_ll -0.1000 (4.576 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.3077 tar_ll -0.1000 (4.576 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 35200 lr 3.621e-04 [train_loss] loss -0.2782 (1.509 secs)\n",
      "cnp:cnp_matern step 35200 lr 3.621e-04 [train_loss] loss -0.2782 (1.509 secs)\n",
      "cnp:cnp_matern step 35400 lr 3.607e-04 [train_loss] loss -0.2385 (1.542 secs)\n",
      "cnp:cnp_matern step 35400 lr 3.607e-04 [train_loss] loss -0.2385 (1.542 secs)\n",
      "cnp:cnp_matern step 35600 lr 3.593e-04 [train_loss] loss -0.2265 (1.795 secs)\n",
      "cnp:cnp_matern step 35600 lr 3.593e-04 [train_loss] loss -0.2265 (1.795 secs)\n",
      "cnp:cnp_matern step 35800 lr 3.579e-04 [train_loss] loss -0.2339 (1.669 secs)\n",
      "cnp:cnp_matern step 35800 lr 3.579e-04 [train_loss] loss -0.2339 (1.669 secs)\n",
      "cnp:cnp_matern step 36000 lr 3.564e-04 [train_loss] loss -0.2321 (1.544 secs)\n",
      "cnp:cnp_matern step 36000 lr 3.564e-04 [train_loss] loss -0.2321 (1.544 secs)\n",
      "cnp:cnp_matern step 36200 lr 3.550e-04 [train_loss] loss -0.2305 (1.575 secs)\n",
      "cnp:cnp_matern step 36200 lr 3.550e-04 [train_loss] loss -0.2305 (1.575 secs)\n",
      "cnp:cnp_matern step 36400 lr 3.536e-04 [train_loss] loss -0.2814 (1.478 secs)\n",
      "cnp:cnp_matern step 36400 lr 3.536e-04 [train_loss] loss -0.2814 (1.478 secs)\n",
      "cnp:cnp_matern step 36600 lr 3.522e-04 [train_loss] loss -0.2776 (1.509 secs)\n",
      "cnp:cnp_matern step 36600 lr 3.522e-04 [train_loss] loss -0.2776 (1.509 secs)\n",
      "cnp:cnp_matern step 36800 lr 3.507e-04 [train_loss] loss -0.2835 (1.557 secs)\n",
      "cnp:cnp_matern step 36800 lr 3.507e-04 [train_loss] loss -0.2835 (1.557 secs)\n",
      "cnp:cnp_matern step 37000 lr 3.493e-04 [train_loss] loss -0.2294 (1.509 secs)\n",
      "cnp:cnp_matern step 37000 lr 3.493e-04 [train_loss] loss -0.2294 (1.509 secs)\n",
      "cnp:cnp_matern step 37200 lr 3.478e-04 [train_loss] loss -0.2135 (1.525 secs)\n",
      "cnp:cnp_matern step 37200 lr 3.478e-04 [train_loss] loss -0.2135 (1.525 secs)\n",
      "cnp:cnp_matern step 37400 lr 3.464e-04 [train_loss] loss -0.2562 (1.610 secs)\n",
      "cnp:cnp_matern step 37400 lr 3.464e-04 [train_loss] loss -0.2562 (1.610 secs)\n",
      "cnp:cnp_matern step 37600 lr 3.449e-04 [train_loss] loss -0.2898 (1.809 secs)\n",
      "cnp:cnp_matern step 37600 lr 3.449e-04 [train_loss] loss -0.2898 (1.809 secs)\n",
      "cnp:cnp_matern step 37800 lr 3.435e-04 [train_loss] loss -0.2509 (1.628 secs)\n",
      "cnp:cnp_matern step 37800 lr 3.435e-04 [train_loss] loss -0.2509 (1.628 secs)\n",
      "cnp:cnp_matern step 38000 lr 3.420e-04 [train_loss] loss -0.2349 (1.481 secs)\n",
      "cnp:cnp_matern step 38000 lr 3.420e-04 [train_loss] loss -0.2349 (1.481 secs)\n",
      "cnp:cnp_matern step 38200 lr 3.406e-04 [train_loss] loss -0.3007 (1.494 secs)\n",
      "cnp:cnp_matern step 38200 lr 3.406e-04 [train_loss] loss -0.3007 (1.494 secs)\n",
      "cnp:cnp_matern step 38400 lr 3.391e-04 [train_loss] loss -0.2468 (1.527 secs)\n",
      "cnp:cnp_matern step 38400 lr 3.391e-04 [train_loss] loss -0.2468 (1.527 secs)\n",
      "cnp:cnp_matern step 38600 lr 3.376e-04 [train_loss] loss -0.2642 (1.497 secs)\n",
      "cnp:cnp_matern step 38600 lr 3.376e-04 [train_loss] loss -0.2642 (1.497 secs)\n",
      "cnp:cnp_matern step 38800 lr 3.362e-04 [train_loss] loss -0.2604 (1.525 secs)\n",
      "cnp:cnp_matern step 38800 lr 3.362e-04 [train_loss] loss -0.2604 (1.525 secs)\n",
      "cnp:cnp_matern step 39000 lr 3.347e-04 [train_loss] loss -0.2515 (1.609 secs)\n",
      "cnp:cnp_matern step 39000 lr 3.347e-04 [train_loss] loss -0.2515 (1.609 secs)\n",
      "cnp:cnp_matern step 39200 lr 3.332e-04 [train_loss] loss -0.2788 (1.542 secs)\n",
      "cnp:cnp_matern step 39200 lr 3.332e-04 [train_loss] loss -0.2788 (1.542 secs)\n",
      "cnp:cnp_matern step 39400 lr 3.317e-04 [train_loss] loss -0.2386 (1.630 secs)\n",
      "cnp:cnp_matern step 39400 lr 3.317e-04 [train_loss] loss -0.2386 (1.630 secs)\n",
      "cnp:cnp_matern step 39600 lr 3.302e-04 [train_loss] loss -0.2590 (1.849 secs)\n",
      "cnp:cnp_matern step 39600 lr 3.302e-04 [train_loss] loss -0.2590 (1.849 secs)\n",
      "cnp:cnp_matern step 39800 lr 3.287e-04 [train_loss] loss -0.2819 (1.639 secs)\n",
      "cnp:cnp_matern step 39800 lr 3.287e-04 [train_loss] loss -0.2819 (1.639 secs)\n",
      "cnp:cnp_matern step 40000 lr 3.273e-04 [train_loss] loss -0.2772 (1.511 secs)\n",
      "cnp:cnp_matern step 40000 lr 3.273e-04 [train_loss] loss -0.2772 (1.511 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:04<00:00, 641.93it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.3541 tar_ll -0.1465 (4.673 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.3541 tar_ll -0.1465 (4.673 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 40200 lr 3.258e-04 [train_loss] loss -0.2615 (1.478 secs)\n",
      "cnp:cnp_matern step 40200 lr 3.258e-04 [train_loss] loss -0.2615 (1.478 secs)\n",
      "cnp:cnp_matern step 40400 lr 3.243e-04 [train_loss] loss -0.2630 (1.533 secs)\n",
      "cnp:cnp_matern step 40400 lr 3.243e-04 [train_loss] loss -0.2630 (1.533 secs)\n",
      "cnp:cnp_matern step 40600 lr 3.228e-04 [train_loss] loss -0.2627 (1.474 secs)\n",
      "cnp:cnp_matern step 40600 lr 3.228e-04 [train_loss] loss -0.2627 (1.474 secs)\n",
      "cnp:cnp_matern step 40800 lr 3.213e-04 [train_loss] loss -0.2833 (1.613 secs)\n",
      "cnp:cnp_matern step 40800 lr 3.213e-04 [train_loss] loss -0.2833 (1.613 secs)\n",
      "cnp:cnp_matern step 41000 lr 3.197e-04 [train_loss] loss -0.2822 (1.839 secs)\n",
      "cnp:cnp_matern step 41000 lr 3.197e-04 [train_loss] loss -0.2822 (1.839 secs)\n",
      "cnp:cnp_matern step 41200 lr 3.182e-04 [train_loss] loss -0.2720 (1.511 secs)\n",
      "cnp:cnp_matern step 41200 lr 3.182e-04 [train_loss] loss -0.2720 (1.511 secs)\n",
      "cnp:cnp_matern step 41400 lr 3.167e-04 [train_loss] loss -0.3072 (1.590 secs)\n",
      "cnp:cnp_matern step 41400 lr 3.167e-04 [train_loss] loss -0.3072 (1.590 secs)\n",
      "cnp:cnp_matern step 41600 lr 3.152e-04 [train_loss] loss -0.2652 (1.462 secs)\n",
      "cnp:cnp_matern step 41600 lr 3.152e-04 [train_loss] loss -0.2652 (1.462 secs)\n",
      "cnp:cnp_matern step 41800 lr 3.137e-04 [train_loss] loss -0.2566 (1.541 secs)\n",
      "cnp:cnp_matern step 41800 lr 3.137e-04 [train_loss] loss -0.2566 (1.541 secs)\n",
      "cnp:cnp_matern step 42000 lr 3.122e-04 [train_loss] loss -0.2970 (1.464 secs)\n",
      "cnp:cnp_matern step 42000 lr 3.122e-04 [train_loss] loss -0.2970 (1.464 secs)\n",
      "cnp:cnp_matern step 42200 lr 3.106e-04 [train_loss] loss -0.3101 (1.623 secs)\n",
      "cnp:cnp_matern step 42200 lr 3.106e-04 [train_loss] loss -0.3101 (1.623 secs)\n",
      "cnp:cnp_matern step 42400 lr 3.091e-04 [train_loss] loss -0.2911 (1.766 secs)\n",
      "cnp:cnp_matern step 42400 lr 3.091e-04 [train_loss] loss -0.2911 (1.766 secs)\n",
      "cnp:cnp_matern step 42600 lr 3.076e-04 [train_loss] loss -0.2904 (1.687 secs)\n",
      "cnp:cnp_matern step 42600 lr 3.076e-04 [train_loss] loss -0.2904 (1.687 secs)\n",
      "cnp:cnp_matern step 42800 lr 3.061e-04 [train_loss] loss -0.2973 (1.587 secs)\n",
      "cnp:cnp_matern step 42800 lr 3.061e-04 [train_loss] loss -0.2973 (1.587 secs)\n",
      "cnp:cnp_matern step 43000 lr 3.045e-04 [train_loss] loss -0.3065 (1.684 secs)\n",
      "cnp:cnp_matern step 43000 lr 3.045e-04 [train_loss] loss -0.3065 (1.684 secs)\n",
      "cnp:cnp_matern step 43200 lr 3.030e-04 [train_loss] loss -0.2971 (1.462 secs)\n",
      "cnp:cnp_matern step 43200 lr 3.030e-04 [train_loss] loss -0.2971 (1.462 secs)\n",
      "cnp:cnp_matern step 43400 lr 3.015e-04 [train_loss] loss -0.3325 (1.464 secs)\n",
      "cnp:cnp_matern step 43400 lr 3.015e-04 [train_loss] loss -0.3325 (1.464 secs)\n",
      "cnp:cnp_matern step 43600 lr 2.999e-04 [train_loss] loss -0.2740 (1.510 secs)\n",
      "cnp:cnp_matern step 43600 lr 2.999e-04 [train_loss] loss -0.2740 (1.510 secs)\n",
      "cnp:cnp_matern step 43800 lr 2.984e-04 [train_loss] loss -0.3090 (1.463 secs)\n",
      "cnp:cnp_matern step 43800 lr 2.984e-04 [train_loss] loss -0.3090 (1.463 secs)\n",
      "cnp:cnp_matern step 44000 lr 2.968e-04 [train_loss] loss -0.3171 (1.464 secs)\n",
      "cnp:cnp_matern step 44000 lr 2.968e-04 [train_loss] loss -0.3171 (1.464 secs)\n",
      "cnp:cnp_matern step 44200 lr 2.953e-04 [train_loss] loss -0.2643 (1.510 secs)\n",
      "cnp:cnp_matern step 44200 lr 2.953e-04 [train_loss] loss -0.2643 (1.510 secs)\n",
      "cnp:cnp_matern step 44400 lr 2.938e-04 [train_loss] loss -0.3583 (1.509 secs)\n",
      "cnp:cnp_matern step 44400 lr 2.938e-04 [train_loss] loss -0.3583 (1.509 secs)\n",
      "cnp:cnp_matern step 44600 lr 2.922e-04 [train_loss] loss -0.2994 (1.542 secs)\n",
      "cnp:cnp_matern step 44600 lr 2.922e-04 [train_loss] loss -0.2994 (1.542 secs)\n",
      "cnp:cnp_matern step 44800 lr 2.907e-04 [train_loss] loss -0.2857 (1.478 secs)\n",
      "cnp:cnp_matern step 44800 lr 2.907e-04 [train_loss] loss -0.2857 (1.478 secs)\n",
      "cnp:cnp_matern step 45000 lr 2.891e-04 [train_loss] loss -0.3046 (1.863 secs)\n",
      "cnp:cnp_matern step 45000 lr 2.891e-04 [train_loss] loss -0.3046 (1.863 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:04<00:00, 630.97it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.4345 tar_ll -0.0768 (4.755 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.4345 tar_ll -0.0768 (4.755 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 45200 lr 2.876e-04 [train_loss] loss -0.3618 (1.527 secs)\n",
      "cnp:cnp_matern step 45200 lr 2.876e-04 [train_loss] loss -0.3618 (1.527 secs)\n",
      "cnp:cnp_matern step 45400 lr 2.860e-04 [train_loss] loss -0.3242 (1.478 secs)\n",
      "cnp:cnp_matern step 45400 lr 2.860e-04 [train_loss] loss -0.3242 (1.478 secs)\n",
      "cnp:cnp_matern step 45600 lr 2.844e-04 [train_loss] loss -0.2994 (1.447 secs)\n",
      "cnp:cnp_matern step 45600 lr 2.844e-04 [train_loss] loss -0.2994 (1.447 secs)\n",
      "cnp:cnp_matern step 45800 lr 2.829e-04 [train_loss] loss -0.3300 (1.478 secs)\n",
      "cnp:cnp_matern step 45800 lr 2.829e-04 [train_loss] loss -0.3300 (1.478 secs)\n",
      "cnp:cnp_matern step 46000 lr 2.813e-04 [train_loss] loss -0.3040 (1.511 secs)\n",
      "cnp:cnp_matern step 46000 lr 2.813e-04 [train_loss] loss -0.3040 (1.511 secs)\n",
      "cnp:cnp_matern step 46200 lr 2.798e-04 [train_loss] loss -0.3188 (1.592 secs)\n",
      "cnp:cnp_matern step 46200 lr 2.798e-04 [train_loss] loss -0.3188 (1.592 secs)\n",
      "cnp:cnp_matern step 46400 lr 2.782e-04 [train_loss] loss -0.3185 (1.855 secs)\n",
      "cnp:cnp_matern step 46400 lr 2.782e-04 [train_loss] loss -0.3185 (1.855 secs)\n",
      "cnp:cnp_matern step 46600 lr 2.767e-04 [train_loss] loss -0.3041 (1.512 secs)\n",
      "cnp:cnp_matern step 46600 lr 2.767e-04 [train_loss] loss -0.3041 (1.512 secs)\n",
      "cnp:cnp_matern step 46800 lr 2.751e-04 [train_loss] loss -0.3236 (1.495 secs)\n",
      "cnp:cnp_matern step 46800 lr 2.751e-04 [train_loss] loss -0.3236 (1.495 secs)\n",
      "cnp:cnp_matern step 47000 lr 2.735e-04 [train_loss] loss -0.2962 (1.495 secs)\n",
      "cnp:cnp_matern step 47000 lr 2.735e-04 [train_loss] loss -0.2962 (1.495 secs)\n",
      "cnp:cnp_matern step 47200 lr 2.720e-04 [train_loss] loss -0.3152 (1.560 secs)\n",
      "cnp:cnp_matern step 47200 lr 2.720e-04 [train_loss] loss -0.3152 (1.560 secs)\n",
      "cnp:cnp_matern step 47400 lr 2.704e-04 [train_loss] loss -0.2984 (1.460 secs)\n",
      "cnp:cnp_matern step 47400 lr 2.704e-04 [train_loss] loss -0.2984 (1.460 secs)\n",
      "cnp:cnp_matern step 47600 lr 2.688e-04 [train_loss] loss -0.3036 (1.462 secs)\n",
      "cnp:cnp_matern step 47600 lr 2.688e-04 [train_loss] loss -0.3036 (1.462 secs)\n",
      "cnp:cnp_matern step 47800 lr 2.673e-04 [train_loss] loss -0.3153 (1.494 secs)\n",
      "cnp:cnp_matern step 47800 lr 2.673e-04 [train_loss] loss -0.3153 (1.494 secs)\n",
      "cnp:cnp_matern step 48000 lr 2.657e-04 [train_loss] loss -0.3226 (1.496 secs)\n",
      "cnp:cnp_matern step 48000 lr 2.657e-04 [train_loss] loss -0.3226 (1.496 secs)\n",
      "cnp:cnp_matern step 48200 lr 2.641e-04 [train_loss] loss -0.3716 (1.555 secs)\n",
      "cnp:cnp_matern step 48200 lr 2.641e-04 [train_loss] loss -0.3716 (1.555 secs)\n",
      "cnp:cnp_matern step 48400 lr 2.626e-04 [train_loss] loss -0.3260 (1.825 secs)\n",
      "cnp:cnp_matern step 48400 lr 2.626e-04 [train_loss] loss -0.3260 (1.825 secs)\n",
      "cnp:cnp_matern step 48600 lr 2.610e-04 [train_loss] loss -0.3376 (1.673 secs)\n",
      "cnp:cnp_matern step 48600 lr 2.610e-04 [train_loss] loss -0.3376 (1.673 secs)\n",
      "cnp:cnp_matern step 48800 lr 2.594e-04 [train_loss] loss -0.3126 (1.642 secs)\n",
      "cnp:cnp_matern step 48800 lr 2.594e-04 [train_loss] loss -0.3126 (1.642 secs)\n",
      "cnp:cnp_matern step 49000 lr 2.579e-04 [train_loss] loss -0.3504 (1.543 secs)\n",
      "cnp:cnp_matern step 49000 lr 2.579e-04 [train_loss] loss -0.3504 (1.543 secs)\n",
      "cnp:cnp_matern step 49200 lr 2.563e-04 [train_loss] loss -0.3016 (1.610 secs)\n",
      "cnp:cnp_matern step 49200 lr 2.563e-04 [train_loss] loss -0.3016 (1.610 secs)\n",
      "cnp:cnp_matern step 49400 lr 2.547e-04 [train_loss] loss -0.3209 (1.563 secs)\n",
      "cnp:cnp_matern step 49400 lr 2.547e-04 [train_loss] loss -0.3209 (1.563 secs)\n",
      "cnp:cnp_matern step 49600 lr 2.531e-04 [train_loss] loss -0.3085 (1.646 secs)\n",
      "cnp:cnp_matern step 49600 lr 2.531e-04 [train_loss] loss -0.3085 (1.646 secs)\n",
      "cnp:cnp_matern step 49800 lr 2.516e-04 [train_loss] loss -0.3468 (1.532 secs)\n",
      "cnp:cnp_matern step 49800 lr 2.516e-04 [train_loss] loss -0.3468 (1.532 secs)\n",
      "cnp:cnp_matern step 50000 lr 2.500e-04 [train_loss] loss -0.2713 (1.512 secs)\n",
      "cnp:cnp_matern step 50000 lr 2.500e-04 [train_loss] loss -0.2713 (1.512 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:05<00:00, 559.23it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.4581 tar_ll -0.0566 (5.365 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.4581 tar_ll -0.0566 (5.365 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 50200 lr 2.484e-04 [train_loss] loss -0.3419 (1.596 secs)\n",
      "cnp:cnp_matern step 50200 lr 2.484e-04 [train_loss] loss -0.3419 (1.596 secs)\n",
      "cnp:cnp_matern step 50400 lr 2.469e-04 [train_loss] loss -0.3322 (1.528 secs)\n",
      "cnp:cnp_matern step 50400 lr 2.469e-04 [train_loss] loss -0.3322 (1.528 secs)\n",
      "cnp:cnp_matern step 50600 lr 2.453e-04 [train_loss] loss -0.3660 (1.546 secs)\n",
      "cnp:cnp_matern step 50600 lr 2.453e-04 [train_loss] loss -0.3660 (1.546 secs)\n",
      "cnp:cnp_matern step 50800 lr 2.437e-04 [train_loss] loss -0.3526 (1.578 secs)\n",
      "cnp:cnp_matern step 50800 lr 2.437e-04 [train_loss] loss -0.3526 (1.578 secs)\n",
      "cnp:cnp_matern step 51000 lr 2.421e-04 [train_loss] loss -0.3490 (1.546 secs)\n",
      "cnp:cnp_matern step 51000 lr 2.421e-04 [train_loss] loss -0.3490 (1.546 secs)\n",
      "cnp:cnp_matern step 51200 lr 2.406e-04 [train_loss] loss -0.3431 (1.532 secs)\n",
      "cnp:cnp_matern step 51200 lr 2.406e-04 [train_loss] loss -0.3431 (1.532 secs)\n",
      "cnp:cnp_matern step 51400 lr 2.390e-04 [train_loss] loss -0.3321 (1.513 secs)\n",
      "cnp:cnp_matern step 51400 lr 2.390e-04 [train_loss] loss -0.3321 (1.513 secs)\n",
      "cnp:cnp_matern step 51600 lr 2.374e-04 [train_loss] loss -0.3156 (2.219 secs)\n",
      "cnp:cnp_matern step 51600 lr 2.374e-04 [train_loss] loss -0.3156 (2.219 secs)\n",
      "cnp:cnp_matern step 51800 lr 2.359e-04 [train_loss] loss -0.3464 (1.750 secs)\n",
      "cnp:cnp_matern step 51800 lr 2.359e-04 [train_loss] loss -0.3464 (1.750 secs)\n",
      "cnp:cnp_matern step 52000 lr 2.343e-04 [train_loss] loss -0.3534 (1.668 secs)\n",
      "cnp:cnp_matern step 52000 lr 2.343e-04 [train_loss] loss -0.3534 (1.668 secs)\n",
      "cnp:cnp_matern step 52200 lr 2.327e-04 [train_loss] loss -0.3912 (1.722 secs)\n",
      "cnp:cnp_matern step 52200 lr 2.327e-04 [train_loss] loss -0.3912 (1.722 secs)\n",
      "cnp:cnp_matern step 52400 lr 2.312e-04 [train_loss] loss -0.2963 (1.507 secs)\n",
      "cnp:cnp_matern step 52400 lr 2.312e-04 [train_loss] loss -0.2963 (1.507 secs)\n",
      "cnp:cnp_matern step 52600 lr 2.296e-04 [train_loss] loss -0.3399 (1.542 secs)\n",
      "cnp:cnp_matern step 52600 lr 2.296e-04 [train_loss] loss -0.3399 (1.542 secs)\n",
      "cnp:cnp_matern step 52800 lr 2.280e-04 [train_loss] loss -0.3326 (1.654 secs)\n",
      "cnp:cnp_matern step 52800 lr 2.280e-04 [train_loss] loss -0.3326 (1.654 secs)\n",
      "cnp:cnp_matern step 53000 lr 2.265e-04 [train_loss] loss -0.3368 (1.541 secs)\n",
      "cnp:cnp_matern step 53000 lr 2.265e-04 [train_loss] loss -0.3368 (1.541 secs)\n",
      "cnp:cnp_matern step 53200 lr 2.249e-04 [train_loss] loss -0.2906 (1.777 secs)\n",
      "cnp:cnp_matern step 53200 lr 2.249e-04 [train_loss] loss -0.2906 (1.777 secs)\n",
      "cnp:cnp_matern step 53400 lr 2.233e-04 [train_loss] loss -0.3203 (1.782 secs)\n",
      "cnp:cnp_matern step 53400 lr 2.233e-04 [train_loss] loss -0.3203 (1.782 secs)\n",
      "cnp:cnp_matern step 53600 lr 2.218e-04 [train_loss] loss -0.3422 (1.855 secs)\n",
      "cnp:cnp_matern step 53600 lr 2.218e-04 [train_loss] loss -0.3422 (1.855 secs)\n",
      "cnp:cnp_matern step 53800 lr 2.202e-04 [train_loss] loss -0.3732 (1.703 secs)\n",
      "cnp:cnp_matern step 53800 lr 2.202e-04 [train_loss] loss -0.3732 (1.703 secs)\n",
      "cnp:cnp_matern step 54000 lr 2.187e-04 [train_loss] loss -0.3649 (1.926 secs)\n",
      "cnp:cnp_matern step 54000 lr 2.187e-04 [train_loss] loss -0.3649 (1.926 secs)\n",
      "cnp:cnp_matern step 54200 lr 2.171e-04 [train_loss] loss -0.3891 (1.710 secs)\n",
      "cnp:cnp_matern step 54200 lr 2.171e-04 [train_loss] loss -0.3891 (1.710 secs)\n",
      "cnp:cnp_matern step 54400 lr 2.156e-04 [train_loss] loss -0.3888 (1.685 secs)\n",
      "cnp:cnp_matern step 54400 lr 2.156e-04 [train_loss] loss -0.3888 (1.685 secs)\n",
      "cnp:cnp_matern step 54600 lr 2.140e-04 [train_loss] loss -0.3598 (1.867 secs)\n",
      "cnp:cnp_matern step 54600 lr 2.140e-04 [train_loss] loss -0.3598 (1.867 secs)\n",
      "cnp:cnp_matern step 54800 lr 2.124e-04 [train_loss] loss -0.3425 (1.715 secs)\n",
      "cnp:cnp_matern step 54800 lr 2.124e-04 [train_loss] loss -0.3425 (1.715 secs)\n",
      "cnp:cnp_matern step 55000 lr 2.109e-04 [train_loss] loss -0.3612 (1.717 secs)\n",
      "cnp:cnp_matern step 55000 lr 2.109e-04 [train_loss] loss -0.3612 (1.717 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:05<00:00, 517.29it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.4583 tar_ll -0.0020 (5.799 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.4583 tar_ll -0.0020 (5.799 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 55200 lr 2.093e-04 [train_loss] loss -0.3553 (1.840 secs)\n",
      "cnp:cnp_matern step 55200 lr 2.093e-04 [train_loss] loss -0.3553 (1.840 secs)\n",
      "cnp:cnp_matern step 55400 lr 2.078e-04 [train_loss] loss -0.3549 (1.634 secs)\n",
      "cnp:cnp_matern step 55400 lr 2.078e-04 [train_loss] loss -0.3549 (1.634 secs)\n",
      "cnp:cnp_matern step 55600 lr 2.062e-04 [train_loss] loss -0.3176 (1.632 secs)\n",
      "cnp:cnp_matern step 55600 lr 2.062e-04 [train_loss] loss -0.3176 (1.632 secs)\n",
      "cnp:cnp_matern step 55800 lr 2.047e-04 [train_loss] loss -0.3626 (1.591 secs)\n",
      "cnp:cnp_matern step 55800 lr 2.047e-04 [train_loss] loss -0.3626 (1.591 secs)\n",
      "cnp:cnp_matern step 56000 lr 2.032e-04 [train_loss] loss -0.4003 (1.633 secs)\n",
      "cnp:cnp_matern step 56000 lr 2.032e-04 [train_loss] loss -0.4003 (1.633 secs)\n",
      "cnp:cnp_matern step 56200 lr 2.016e-04 [train_loss] loss -0.3798 (1.753 secs)\n",
      "cnp:cnp_matern step 56200 lr 2.016e-04 [train_loss] loss -0.3798 (1.753 secs)\n",
      "cnp:cnp_matern step 56400 lr 2.001e-04 [train_loss] loss -0.3404 (1.897 secs)\n",
      "cnp:cnp_matern step 56400 lr 2.001e-04 [train_loss] loss -0.3404 (1.897 secs)\n",
      "cnp:cnp_matern step 56600 lr 1.985e-04 [train_loss] loss -0.3635 (1.767 secs)\n",
      "cnp:cnp_matern step 56600 lr 1.985e-04 [train_loss] loss -0.3635 (1.767 secs)\n",
      "cnp:cnp_matern step 56800 lr 1.970e-04 [train_loss] loss -0.3778 (1.618 secs)\n",
      "cnp:cnp_matern step 56800 lr 1.970e-04 [train_loss] loss -0.3778 (1.618 secs)\n",
      "cnp:cnp_matern step 57000 lr 1.955e-04 [train_loss] loss -0.4013 (1.680 secs)\n",
      "cnp:cnp_matern step 57000 lr 1.955e-04 [train_loss] loss -0.4013 (1.680 secs)\n",
      "cnp:cnp_matern step 57200 lr 1.939e-04 [train_loss] loss -0.3291 (1.619 secs)\n",
      "cnp:cnp_matern step 57200 lr 1.939e-04 [train_loss] loss -0.3291 (1.619 secs)\n",
      "cnp:cnp_matern step 57400 lr 1.924e-04 [train_loss] loss -0.4231 (1.719 secs)\n",
      "cnp:cnp_matern step 57400 lr 1.924e-04 [train_loss] loss -0.4231 (1.719 secs)\n",
      "cnp:cnp_matern step 57600 lr 1.909e-04 [train_loss] loss -0.3711 (1.698 secs)\n",
      "cnp:cnp_matern step 57600 lr 1.909e-04 [train_loss] loss -0.3711 (1.698 secs)\n",
      "cnp:cnp_matern step 57800 lr 1.894e-04 [train_loss] loss -0.3908 (1.671 secs)\n",
      "cnp:cnp_matern step 57800 lr 1.894e-04 [train_loss] loss -0.3908 (1.671 secs)\n",
      "cnp:cnp_matern step 58000 lr 1.878e-04 [train_loss] loss -0.3676 (1.716 secs)\n",
      "cnp:cnp_matern step 58000 lr 1.878e-04 [train_loss] loss -0.3676 (1.716 secs)\n",
      "cnp:cnp_matern step 58200 lr 1.863e-04 [train_loss] loss -0.3931 (1.766 secs)\n",
      "cnp:cnp_matern step 58200 lr 1.863e-04 [train_loss] loss -0.3931 (1.766 secs)\n",
      "cnp:cnp_matern step 58400 lr 1.848e-04 [train_loss] loss -0.4543 (1.826 secs)\n",
      "cnp:cnp_matern step 58400 lr 1.848e-04 [train_loss] loss -0.4543 (1.826 secs)\n",
      "cnp:cnp_matern step 58600 lr 1.833e-04 [train_loss] loss -0.3432 (1.633 secs)\n",
      "cnp:cnp_matern step 58600 lr 1.833e-04 [train_loss] loss -0.3432 (1.633 secs)\n",
      "cnp:cnp_matern step 58800 lr 1.818e-04 [train_loss] loss -0.3839 (1.797 secs)\n",
      "cnp:cnp_matern step 58800 lr 1.818e-04 [train_loss] loss -0.3839 (1.797 secs)\n",
      "cnp:cnp_matern step 59000 lr 1.803e-04 [train_loss] loss -0.3986 (1.644 secs)\n",
      "cnp:cnp_matern step 59000 lr 1.803e-04 [train_loss] loss -0.3986 (1.644 secs)\n",
      "cnp:cnp_matern step 59200 lr 1.787e-04 [train_loss] loss -0.4029 (1.693 secs)\n",
      "cnp:cnp_matern step 59200 lr 1.787e-04 [train_loss] loss -0.4029 (1.693 secs)\n",
      "cnp:cnp_matern step 59400 lr 1.772e-04 [train_loss] loss -0.3623 (1.669 secs)\n",
      "cnp:cnp_matern step 59400 lr 1.772e-04 [train_loss] loss -0.3623 (1.669 secs)\n",
      "cnp:cnp_matern step 59600 lr 1.757e-04 [train_loss] loss -0.3567 (1.740 secs)\n",
      "cnp:cnp_matern step 59600 lr 1.757e-04 [train_loss] loss -0.3567 (1.740 secs)\n",
      "cnp:cnp_matern step 59800 lr 1.742e-04 [train_loss] loss -0.3918 (1.615 secs)\n",
      "cnp:cnp_matern step 59800 lr 1.742e-04 [train_loss] loss -0.3918 (1.615 secs)\n",
      "cnp:cnp_matern step 60000 lr 1.727e-04 [train_loss] loss -0.3832 (1.731 secs)\n",
      "cnp:cnp_matern step 60000 lr 1.727e-04 [train_loss] loss -0.3832 (1.731 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:05<00:00, 573.78it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.4977 tar_ll -0.0363 (5.229 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.4977 tar_ll -0.0363 (5.229 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 60200 lr 1.713e-04 [train_loss] loss -0.4174 (1.507 secs)\n",
      "cnp:cnp_matern step 60200 lr 1.713e-04 [train_loss] loss -0.4174 (1.507 secs)\n",
      "cnp:cnp_matern step 60400 lr 1.698e-04 [train_loss] loss -0.3723 (1.490 secs)\n",
      "cnp:cnp_matern step 60400 lr 1.698e-04 [train_loss] loss -0.3723 (1.490 secs)\n",
      "cnp:cnp_matern step 60600 lr 1.683e-04 [train_loss] loss -0.3727 (1.489 secs)\n",
      "cnp:cnp_matern step 60600 lr 1.683e-04 [train_loss] loss -0.3727 (1.489 secs)\n",
      "cnp:cnp_matern step 60800 lr 1.668e-04 [train_loss] loss -0.3928 (1.553 secs)\n",
      "cnp:cnp_matern step 60800 lr 1.668e-04 [train_loss] loss -0.3928 (1.553 secs)\n",
      "cnp:cnp_matern step 61000 lr 1.653e-04 [train_loss] loss -0.3721 (1.506 secs)\n",
      "cnp:cnp_matern step 61000 lr 1.653e-04 [train_loss] loss -0.3721 (1.506 secs)\n",
      "cnp:cnp_matern step 61200 lr 1.638e-04 [train_loss] loss -0.3780 (1.492 secs)\n",
      "cnp:cnp_matern step 61200 lr 1.638e-04 [train_loss] loss -0.3780 (1.492 secs)\n",
      "cnp:cnp_matern step 61400 lr 1.624e-04 [train_loss] loss -0.3869 (1.789 secs)\n",
      "cnp:cnp_matern step 61400 lr 1.624e-04 [train_loss] loss -0.3869 (1.789 secs)\n",
      "cnp:cnp_matern step 61600 lr 1.609e-04 [train_loss] loss -0.3721 (1.631 secs)\n",
      "cnp:cnp_matern step 61600 lr 1.609e-04 [train_loss] loss -0.3721 (1.631 secs)\n",
      "cnp:cnp_matern step 61800 lr 1.594e-04 [train_loss] loss -0.3607 (1.491 secs)\n",
      "cnp:cnp_matern step 61800 lr 1.594e-04 [train_loss] loss -0.3607 (1.491 secs)\n",
      "cnp:cnp_matern step 62000 lr 1.580e-04 [train_loss] loss -0.3880 (1.505 secs)\n",
      "cnp:cnp_matern step 62000 lr 1.580e-04 [train_loss] loss -0.3880 (1.505 secs)\n",
      "cnp:cnp_matern step 62200 lr 1.565e-04 [train_loss] loss -0.3931 (1.523 secs)\n",
      "cnp:cnp_matern step 62200 lr 1.565e-04 [train_loss] loss -0.3931 (1.523 secs)\n",
      "cnp:cnp_matern step 62400 lr 1.551e-04 [train_loss] loss -0.3921 (1.538 secs)\n",
      "cnp:cnp_matern step 62400 lr 1.551e-04 [train_loss] loss -0.3921 (1.538 secs)\n",
      "cnp:cnp_matern step 62600 lr 1.536e-04 [train_loss] loss -0.3662 (1.569 secs)\n",
      "cnp:cnp_matern step 62600 lr 1.536e-04 [train_loss] loss -0.3662 (1.569 secs)\n",
      "cnp:cnp_matern step 62800 lr 1.522e-04 [train_loss] loss -0.4394 (1.523 secs)\n",
      "cnp:cnp_matern step 62800 lr 1.522e-04 [train_loss] loss -0.4394 (1.523 secs)\n",
      "cnp:cnp_matern step 63000 lr 1.507e-04 [train_loss] loss -0.3767 (1.506 secs)\n",
      "cnp:cnp_matern step 63000 lr 1.507e-04 [train_loss] loss -0.3767 (1.506 secs)\n",
      "cnp:cnp_matern step 63200 lr 1.493e-04 [train_loss] loss -0.4226 (1.663 secs)\n",
      "cnp:cnp_matern step 63200 lr 1.493e-04 [train_loss] loss -0.4226 (1.663 secs)\n",
      "cnp:cnp_matern step 63400 lr 1.478e-04 [train_loss] loss -0.4095 (2.040 secs)\n",
      "cnp:cnp_matern step 63400 lr 1.478e-04 [train_loss] loss -0.4095 (2.040 secs)\n",
      "cnp:cnp_matern step 63600 lr 1.464e-04 [train_loss] loss -0.3968 (1.756 secs)\n",
      "cnp:cnp_matern step 63600 lr 1.464e-04 [train_loss] loss -0.3968 (1.756 secs)\n",
      "cnp:cnp_matern step 63800 lr 1.450e-04 [train_loss] loss -0.3944 (1.664 secs)\n",
      "cnp:cnp_matern step 63800 lr 1.450e-04 [train_loss] loss -0.3944 (1.664 secs)\n",
      "cnp:cnp_matern step 64000 lr 1.436e-04 [train_loss] loss -0.3894 (1.616 secs)\n",
      "cnp:cnp_matern step 64000 lr 1.436e-04 [train_loss] loss -0.3894 (1.616 secs)\n",
      "cnp:cnp_matern step 64200 lr 1.421e-04 [train_loss] loss -0.3947 (1.632 secs)\n",
      "cnp:cnp_matern step 64200 lr 1.421e-04 [train_loss] loss -0.3947 (1.632 secs)\n",
      "cnp:cnp_matern step 64400 lr 1.407e-04 [train_loss] loss -0.3723 (1.447 secs)\n",
      "cnp:cnp_matern step 64400 lr 1.407e-04 [train_loss] loss -0.3723 (1.447 secs)\n",
      "cnp:cnp_matern step 64600 lr 1.393e-04 [train_loss] loss -0.3845 (1.519 secs)\n",
      "cnp:cnp_matern step 64600 lr 1.393e-04 [train_loss] loss -0.3845 (1.519 secs)\n",
      "cnp:cnp_matern step 64800 lr 1.379e-04 [train_loss] loss -0.4374 (1.569 secs)\n",
      "cnp:cnp_matern step 64800 lr 1.379e-04 [train_loss] loss -0.4374 (1.569 secs)\n",
      "cnp:cnp_matern step 65000 lr 1.365e-04 [train_loss] loss -0.4255 (1.791 secs)\n",
      "cnp:cnp_matern step 65000 lr 1.365e-04 [train_loss] loss -0.4255 (1.791 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:05<00:00, 509.03it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.4736 tar_ll 0.0230 (5.894 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.4736 tar_ll 0.0230 (5.894 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 65200 lr 1.351e-04 [train_loss] loss -0.3962 (1.493 secs)\n",
      "cnp:cnp_matern step 65200 lr 1.351e-04 [train_loss] loss -0.3962 (1.493 secs)\n",
      "cnp:cnp_matern step 65400 lr 1.337e-04 [train_loss] loss -0.3997 (1.447 secs)\n",
      "cnp:cnp_matern step 65400 lr 1.337e-04 [train_loss] loss -0.3997 (1.447 secs)\n",
      "cnp:cnp_matern step 65600 lr 1.323e-04 [train_loss] loss -0.4298 (1.479 secs)\n",
      "cnp:cnp_matern step 65600 lr 1.323e-04 [train_loss] loss -0.4298 (1.479 secs)\n",
      "cnp:cnp_matern step 65800 lr 1.309e-04 [train_loss] loss -0.4233 (1.556 secs)\n",
      "cnp:cnp_matern step 65800 lr 1.309e-04 [train_loss] loss -0.4233 (1.556 secs)\n",
      "cnp:cnp_matern step 66000 lr 1.296e-04 [train_loss] loss -0.4063 (1.479 secs)\n",
      "cnp:cnp_matern step 66000 lr 1.296e-04 [train_loss] loss -0.4063 (1.479 secs)\n",
      "cnp:cnp_matern step 66200 lr 1.282e-04 [train_loss] loss -0.3989 (1.526 secs)\n",
      "cnp:cnp_matern step 66200 lr 1.282e-04 [train_loss] loss -0.3989 (1.526 secs)\n",
      "cnp:cnp_matern step 66400 lr 1.268e-04 [train_loss] loss -0.3816 (1.509 secs)\n",
      "cnp:cnp_matern step 66400 lr 1.268e-04 [train_loss] loss -0.3816 (1.509 secs)\n",
      "cnp:cnp_matern step 66600 lr 1.255e-04 [train_loss] loss -0.4030 (1.804 secs)\n",
      "cnp:cnp_matern step 66600 lr 1.255e-04 [train_loss] loss -0.4030 (1.804 secs)\n",
      "cnp:cnp_matern step 66800 lr 1.241e-04 [train_loss] loss -0.4372 (1.603 secs)\n",
      "cnp:cnp_matern step 66800 lr 1.241e-04 [train_loss] loss -0.4372 (1.603 secs)\n",
      "cnp:cnp_matern step 67000 lr 1.227e-04 [train_loss] loss -0.3673 (1.540 secs)\n",
      "cnp:cnp_matern step 67000 lr 1.227e-04 [train_loss] loss -0.3673 (1.540 secs)\n",
      "cnp:cnp_matern step 67200 lr 1.214e-04 [train_loss] loss -0.4112 (1.526 secs)\n",
      "cnp:cnp_matern step 67200 lr 1.214e-04 [train_loss] loss -0.4112 (1.526 secs)\n",
      "cnp:cnp_matern step 67400 lr 1.200e-04 [train_loss] loss -0.3696 (1.541 secs)\n",
      "cnp:cnp_matern step 67400 lr 1.200e-04 [train_loss] loss -0.3696 (1.541 secs)\n",
      "cnp:cnp_matern step 67600 lr 1.187e-04 [train_loss] loss -0.3872 (1.557 secs)\n",
      "cnp:cnp_matern step 67600 lr 1.187e-04 [train_loss] loss -0.3872 (1.557 secs)\n",
      "cnp:cnp_matern step 67800 lr 1.174e-04 [train_loss] loss -0.3881 (1.605 secs)\n",
      "cnp:cnp_matern step 67800 lr 1.174e-04 [train_loss] loss -0.3881 (1.605 secs)\n",
      "cnp:cnp_matern step 68000 lr 1.160e-04 [train_loss] loss -0.4037 (1.621 secs)\n",
      "cnp:cnp_matern step 68000 lr 1.160e-04 [train_loss] loss -0.4037 (1.621 secs)\n",
      "cnp:cnp_matern step 68200 lr 1.147e-04 [train_loss] loss -0.4062 (1.842 secs)\n",
      "cnp:cnp_matern step 68200 lr 1.147e-04 [train_loss] loss -0.4062 (1.842 secs)\n",
      "cnp:cnp_matern step 68400 lr 1.134e-04 [train_loss] loss -0.4055 (1.556 secs)\n",
      "cnp:cnp_matern step 68400 lr 1.134e-04 [train_loss] loss -0.4055 (1.556 secs)\n",
      "cnp:cnp_matern step 68600 lr 1.121e-04 [train_loss] loss -0.4223 (2.283 secs)\n",
      "cnp:cnp_matern step 68600 lr 1.121e-04 [train_loss] loss -0.4223 (2.283 secs)\n",
      "cnp:cnp_matern step 68800 lr 1.108e-04 [train_loss] loss -0.3952 (1.919 secs)\n",
      "cnp:cnp_matern step 68800 lr 1.108e-04 [train_loss] loss -0.3952 (1.919 secs)\n",
      "cnp:cnp_matern step 69000 lr 1.095e-04 [train_loss] loss -0.3612 (1.919 secs)\n",
      "cnp:cnp_matern step 69000 lr 1.095e-04 [train_loss] loss -0.3612 (1.919 secs)\n",
      "cnp:cnp_matern step 69200 lr 1.082e-04 [train_loss] loss -0.4017 (1.855 secs)\n",
      "cnp:cnp_matern step 69200 lr 1.082e-04 [train_loss] loss -0.4017 (1.855 secs)\n",
      "cnp:cnp_matern step 69400 lr 1.069e-04 [train_loss] loss -0.4827 (1.671 secs)\n",
      "cnp:cnp_matern step 69400 lr 1.069e-04 [train_loss] loss -0.4827 (1.671 secs)\n",
      "cnp:cnp_matern step 69600 lr 1.056e-04 [train_loss] loss -0.4002 (1.679 secs)\n",
      "cnp:cnp_matern step 69600 lr 1.056e-04 [train_loss] loss -0.4002 (1.679 secs)\n",
      "cnp:cnp_matern step 69800 lr 1.043e-04 [train_loss] loss -0.4172 (1.747 secs)\n",
      "cnp:cnp_matern step 69800 lr 1.043e-04 [train_loss] loss -0.4172 (1.747 secs)\n",
      "cnp:cnp_matern step 70000 lr 1.031e-04 [train_loss] loss -0.4272 (1.792 secs)\n",
      "cnp:cnp_matern step 70000 lr 1.031e-04 [train_loss] loss -0.4272 (1.792 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:05<00:00, 526.56it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.4998 tar_ll 0.0280 (5.697 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.4998 tar_ll 0.0280 (5.697 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 70200 lr 1.018e-04 [train_loss] loss -0.4245 (2.049 secs)\n",
      "cnp:cnp_matern step 70200 lr 1.018e-04 [train_loss] loss -0.4245 (2.049 secs)\n",
      "cnp:cnp_matern step 70400 lr 1.005e-04 [train_loss] loss -0.4069 (2.176 secs)\n",
      "cnp:cnp_matern step 70400 lr 1.005e-04 [train_loss] loss -0.4069 (2.176 secs)\n",
      "cnp:cnp_matern step 70600 lr 9.927e-05 [train_loss] loss -0.4193 (2.083 secs)\n",
      "cnp:cnp_matern step 70600 lr 9.927e-05 [train_loss] loss -0.4193 (2.083 secs)\n",
      "cnp:cnp_matern step 70800 lr 9.802e-05 [train_loss] loss -0.4150 (2.400 secs)\n",
      "cnp:cnp_matern step 70800 lr 9.802e-05 [train_loss] loss -0.4150 (2.400 secs)\n",
      "cnp:cnp_matern step 71000 lr 9.677e-05 [train_loss] loss -0.4266 (2.293 secs)\n",
      "cnp:cnp_matern step 71000 lr 9.677e-05 [train_loss] loss -0.4266 (2.293 secs)\n",
      "cnp:cnp_matern step 71200 lr 9.554e-05 [train_loss] loss -0.4015 (1.955 secs)\n",
      "cnp:cnp_matern step 71200 lr 9.554e-05 [train_loss] loss -0.4015 (1.955 secs)\n",
      "cnp:cnp_matern step 71400 lr 9.430e-05 [train_loss] loss -0.3948 (1.618 secs)\n",
      "cnp:cnp_matern step 71400 lr 9.430e-05 [train_loss] loss -0.3948 (1.618 secs)\n",
      "cnp:cnp_matern step 71600 lr 9.308e-05 [train_loss] loss -0.4682 (1.512 secs)\n",
      "cnp:cnp_matern step 71600 lr 9.308e-05 [train_loss] loss -0.4682 (1.512 secs)\n",
      "cnp:cnp_matern step 71800 lr 9.186e-05 [train_loss] loss -0.4092 (1.524 secs)\n",
      "cnp:cnp_matern step 71800 lr 9.186e-05 [train_loss] loss -0.4092 (1.524 secs)\n",
      "cnp:cnp_matern step 72000 lr 9.064e-05 [train_loss] loss -0.4545 (1.580 secs)\n",
      "cnp:cnp_matern step 72000 lr 9.064e-05 [train_loss] loss -0.4545 (1.580 secs)\n",
      "cnp:cnp_matern step 72200 lr 8.944e-05 [train_loss] loss -0.4413 (1.479 secs)\n",
      "cnp:cnp_matern step 72200 lr 8.944e-05 [train_loss] loss -0.4413 (1.479 secs)\n",
      "cnp:cnp_matern step 72400 lr 8.824e-05 [train_loss] loss -0.4371 (1.478 secs)\n",
      "cnp:cnp_matern step 72400 lr 8.824e-05 [train_loss] loss -0.4371 (1.478 secs)\n",
      "cnp:cnp_matern step 72600 lr 8.704e-05 [train_loss] loss -0.4263 (1.541 secs)\n",
      "cnp:cnp_matern step 72600 lr 8.704e-05 [train_loss] loss -0.4263 (1.541 secs)\n",
      "cnp:cnp_matern step 72800 lr 8.585e-05 [train_loss] loss -0.4228 (1.731 secs)\n",
      "cnp:cnp_matern step 72800 lr 8.585e-05 [train_loss] loss -0.4228 (1.731 secs)\n",
      "cnp:cnp_matern step 73000 lr 8.467e-05 [train_loss] loss -0.4218 (1.974 secs)\n",
      "cnp:cnp_matern step 73000 lr 8.467e-05 [train_loss] loss -0.4218 (1.974 secs)\n",
      "cnp:cnp_matern step 73200 lr 8.350e-05 [train_loss] loss -0.4177 (1.906 secs)\n",
      "cnp:cnp_matern step 73200 lr 8.350e-05 [train_loss] loss -0.4177 (1.906 secs)\n",
      "cnp:cnp_matern step 73400 lr 8.233e-05 [train_loss] loss -0.4185 (1.620 secs)\n",
      "cnp:cnp_matern step 73400 lr 8.233e-05 [train_loss] loss -0.4185 (1.620 secs)\n",
      "cnp:cnp_matern step 73600 lr 8.117e-05 [train_loss] loss -0.3828 (1.845 secs)\n",
      "cnp:cnp_matern step 73600 lr 8.117e-05 [train_loss] loss -0.3828 (1.845 secs)\n",
      "cnp:cnp_matern step 73800 lr 8.001e-05 [train_loss] loss -0.4614 (1.682 secs)\n",
      "cnp:cnp_matern step 73800 lr 8.001e-05 [train_loss] loss -0.4614 (1.682 secs)\n",
      "cnp:cnp_matern step 74000 lr 7.886e-05 [train_loss] loss -0.4106 (1.877 secs)\n",
      "cnp:cnp_matern step 74000 lr 7.886e-05 [train_loss] loss -0.4106 (1.877 secs)\n",
      "cnp:cnp_matern step 74200 lr 7.772e-05 [train_loss] loss -0.4037 (1.635 secs)\n",
      "cnp:cnp_matern step 74200 lr 7.772e-05 [train_loss] loss -0.4037 (1.635 secs)\n",
      "cnp:cnp_matern step 74400 lr 7.659e-05 [train_loss] loss -0.4544 (1.573 secs)\n",
      "cnp:cnp_matern step 74400 lr 7.659e-05 [train_loss] loss -0.4544 (1.573 secs)\n",
      "cnp:cnp_matern step 74600 lr 7.546e-05 [train_loss] loss -0.4458 (1.525 secs)\n",
      "cnp:cnp_matern step 74600 lr 7.546e-05 [train_loss] loss -0.4458 (1.525 secs)\n",
      "cnp:cnp_matern step 74800 lr 7.434e-05 [train_loss] loss -0.3937 (1.667 secs)\n",
      "cnp:cnp_matern step 74800 lr 7.434e-05 [train_loss] loss -0.3937 (1.667 secs)\n",
      "cnp:cnp_matern step 75000 lr 7.322e-05 [train_loss] loss -0.4650 (1.764 secs)\n",
      "cnp:cnp_matern step 75000 lr 7.322e-05 [train_loss] loss -0.4650 (1.764 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:04<00:00, 638.02it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.5257 tar_ll 0.0280 (4.702 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.5257 tar_ll 0.0280 (4.702 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 75200 lr 7.212e-05 [train_loss] loss -0.4578 (1.559 secs)\n",
      "cnp:cnp_matern step 75200 lr 7.212e-05 [train_loss] loss -0.4578 (1.559 secs)\n",
      "cnp:cnp_matern step 75400 lr 7.102e-05 [train_loss] loss -0.4240 (1.549 secs)\n",
      "cnp:cnp_matern step 75400 lr 7.102e-05 [train_loss] loss -0.4240 (1.549 secs)\n",
      "cnp:cnp_matern step 75600 lr 6.992e-05 [train_loss] loss -0.4164 (1.549 secs)\n",
      "cnp:cnp_matern step 75600 lr 6.992e-05 [train_loss] loss -0.4164 (1.549 secs)\n",
      "cnp:cnp_matern step 75800 lr 6.884e-05 [train_loss] loss -0.4109 (1.559 secs)\n",
      "cnp:cnp_matern step 75800 lr 6.884e-05 [train_loss] loss -0.4109 (1.559 secs)\n",
      "cnp:cnp_matern step 76000 lr 6.776e-05 [train_loss] loss -0.4124 (1.509 secs)\n",
      "cnp:cnp_matern step 76000 lr 6.776e-05 [train_loss] loss -0.4124 (1.509 secs)\n",
      "cnp:cnp_matern step 76200 lr 6.669e-05 [train_loss] loss -0.4111 (1.891 secs)\n",
      "cnp:cnp_matern step 76200 lr 6.669e-05 [train_loss] loss -0.4111 (1.891 secs)\n",
      "cnp:cnp_matern step 76400 lr 6.562e-05 [train_loss] loss -0.4105 (1.758 secs)\n",
      "cnp:cnp_matern step 76400 lr 6.562e-05 [train_loss] loss -0.4105 (1.758 secs)\n",
      "cnp:cnp_matern step 76600 lr 6.456e-05 [train_loss] loss -0.4209 (1.604 secs)\n",
      "cnp:cnp_matern step 76600 lr 6.456e-05 [train_loss] loss -0.4209 (1.604 secs)\n",
      "cnp:cnp_matern step 76800 lr 6.351e-05 [train_loss] loss -0.4471 (1.558 secs)\n",
      "cnp:cnp_matern step 76800 lr 6.351e-05 [train_loss] loss -0.4471 (1.558 secs)\n",
      "cnp:cnp_matern step 77000 lr 6.247e-05 [train_loss] loss -0.4759 (1.591 secs)\n",
      "cnp:cnp_matern step 77000 lr 6.247e-05 [train_loss] loss -0.4759 (1.591 secs)\n",
      "cnp:cnp_matern step 77200 lr 6.144e-05 [train_loss] loss -0.4690 (1.525 secs)\n",
      "cnp:cnp_matern step 77200 lr 6.144e-05 [train_loss] loss -0.4690 (1.525 secs)\n",
      "cnp:cnp_matern step 77400 lr 6.041e-05 [train_loss] loss -0.4571 (1.795 secs)\n",
      "cnp:cnp_matern step 77400 lr 6.041e-05 [train_loss] loss -0.4571 (1.795 secs)\n",
      "cnp:cnp_matern step 77600 lr 5.939e-05 [train_loss] loss -0.3931 (1.159 secs)\n",
      "cnp:cnp_matern step 77600 lr 5.939e-05 [train_loss] loss -0.3931 (1.159 secs)\n",
      "cnp:cnp_matern step 77800 lr 5.838e-05 [train_loss] loss -0.4323 (1.125 secs)\n",
      "cnp:cnp_matern step 77800 lr 5.838e-05 [train_loss] loss -0.4323 (1.125 secs)\n",
      "cnp:cnp_matern step 78000 lr 5.737e-05 [train_loss] loss -0.4770 (1.088 secs)\n",
      "cnp:cnp_matern step 78000 lr 5.737e-05 [train_loss] loss -0.4770 (1.088 secs)\n",
      "cnp:cnp_matern step 78200 lr 5.637e-05 [train_loss] loss -0.4441 (1.219 secs)\n",
      "cnp:cnp_matern step 78200 lr 5.637e-05 [train_loss] loss -0.4441 (1.219 secs)\n",
      "cnp:cnp_matern step 78400 lr 5.538e-05 [train_loss] loss -0.4225 (1.189 secs)\n",
      "cnp:cnp_matern step 78400 lr 5.538e-05 [train_loss] loss -0.4225 (1.189 secs)\n",
      "cnp:cnp_matern step 78600 lr 5.440e-05 [train_loss] loss -0.4504 (1.154 secs)\n",
      "cnp:cnp_matern step 78600 lr 5.440e-05 [train_loss] loss -0.4504 (1.154 secs)\n",
      "cnp:cnp_matern step 78800 lr 5.343e-05 [train_loss] loss -0.5058 (1.161 secs)\n",
      "cnp:cnp_matern step 78800 lr 5.343e-05 [train_loss] loss -0.5058 (1.161 secs)\n",
      "cnp:cnp_matern step 79000 lr 5.246e-05 [train_loss] loss -0.4610 (1.230 secs)\n",
      "cnp:cnp_matern step 79000 lr 5.246e-05 [train_loss] loss -0.4610 (1.230 secs)\n",
      "cnp:cnp_matern step 79200 lr 5.150e-05 [train_loss] loss -0.4824 (1.087 secs)\n",
      "cnp:cnp_matern step 79200 lr 5.150e-05 [train_loss] loss -0.4824 (1.087 secs)\n",
      "cnp:cnp_matern step 79400 lr 5.055e-05 [train_loss] loss -0.4165 (1.061 secs)\n",
      "cnp:cnp_matern step 79400 lr 5.055e-05 [train_loss] loss -0.4165 (1.061 secs)\n",
      "cnp:cnp_matern step 79600 lr 4.961e-05 [train_loss] loss -0.4414 (1.083 secs)\n",
      "cnp:cnp_matern step 79600 lr 4.961e-05 [train_loss] loss -0.4414 (1.083 secs)\n",
      "cnp:cnp_matern step 79800 lr 4.867e-05 [train_loss] loss -0.4349 (1.040 secs)\n",
      "cnp:cnp_matern step 79800 lr 4.867e-05 [train_loss] loss -0.4349 (1.040 secs)\n",
      "cnp:cnp_matern step 80000 lr 4.775e-05 [train_loss] loss -0.4306 (1.050 secs)\n",
      "cnp:cnp_matern step 80000 lr 4.775e-05 [train_loss] loss -0.4306 (1.050 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:03<00:00, 930.23it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.5390 tar_ll 0.0342 (3.227 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.5390 tar_ll 0.0342 (3.227 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 80200 lr 4.683e-05 [train_loss] loss -0.4170 (1.027 secs)\n",
      "cnp:cnp_matern step 80200 lr 4.683e-05 [train_loss] loss -0.4170 (1.027 secs)\n",
      "cnp:cnp_matern step 80400 lr 4.592e-05 [train_loss] loss -0.4597 (1.049 secs)\n",
      "cnp:cnp_matern step 80400 lr 4.592e-05 [train_loss] loss -0.4597 (1.049 secs)\n",
      "cnp:cnp_matern step 80600 lr 4.501e-05 [train_loss] loss -0.4081 (1.110 secs)\n",
      "cnp:cnp_matern step 80600 lr 4.501e-05 [train_loss] loss -0.4081 (1.110 secs)\n",
      "cnp:cnp_matern step 80800 lr 4.412e-05 [train_loss] loss -0.4335 (1.069 secs)\n",
      "cnp:cnp_matern step 80800 lr 4.412e-05 [train_loss] loss -0.4335 (1.069 secs)\n",
      "cnp:cnp_matern step 81000 lr 4.323e-05 [train_loss] loss -0.4880 (1.043 secs)\n",
      "cnp:cnp_matern step 81000 lr 4.323e-05 [train_loss] loss -0.4880 (1.043 secs)\n",
      "cnp:cnp_matern step 81200 lr 4.235e-05 [train_loss] loss -0.4430 (1.048 secs)\n",
      "cnp:cnp_matern step 81200 lr 4.235e-05 [train_loss] loss -0.4430 (1.048 secs)\n",
      "cnp:cnp_matern step 81400 lr 4.148e-05 [train_loss] loss -0.4432 (1.093 secs)\n",
      "cnp:cnp_matern step 81400 lr 4.148e-05 [train_loss] loss -0.4432 (1.093 secs)\n",
      "cnp:cnp_matern step 81600 lr 4.062e-05 [train_loss] loss -0.4555 (1.059 secs)\n",
      "cnp:cnp_matern step 81600 lr 4.062e-05 [train_loss] loss -0.4555 (1.059 secs)\n",
      "cnp:cnp_matern step 81800 lr 3.976e-05 [train_loss] loss -0.4548 (1.075 secs)\n",
      "cnp:cnp_matern step 81800 lr 3.976e-05 [train_loss] loss -0.4548 (1.075 secs)\n",
      "cnp:cnp_matern step 82000 lr 3.892e-05 [train_loss] loss -0.4488 (1.028 secs)\n",
      "cnp:cnp_matern step 82000 lr 3.892e-05 [train_loss] loss -0.4488 (1.028 secs)\n",
      "cnp:cnp_matern step 82200 lr 3.808e-05 [train_loss] loss -0.4706 (1.032 secs)\n",
      "cnp:cnp_matern step 82200 lr 3.808e-05 [train_loss] loss -0.4706 (1.032 secs)\n",
      "cnp:cnp_matern step 82400 lr 3.725e-05 [train_loss] loss -0.4461 (0.998 secs)\n",
      "cnp:cnp_matern step 82400 lr 3.725e-05 [train_loss] loss -0.4461 (0.998 secs)\n",
      "cnp:cnp_matern step 82600 lr 3.643e-05 [train_loss] loss -0.4315 (0.989 secs)\n",
      "cnp:cnp_matern step 82600 lr 3.643e-05 [train_loss] loss -0.4315 (0.989 secs)\n",
      "cnp:cnp_matern step 82800 lr 3.562e-05 [train_loss] loss -0.4383 (1.031 secs)\n",
      "cnp:cnp_matern step 82800 lr 3.562e-05 [train_loss] loss -0.4383 (1.031 secs)\n",
      "cnp:cnp_matern step 83000 lr 3.481e-05 [train_loss] loss -0.4488 (1.006 secs)\n",
      "cnp:cnp_matern step 83000 lr 3.481e-05 [train_loss] loss -0.4488 (1.006 secs)\n",
      "cnp:cnp_matern step 83200 lr 3.402e-05 [train_loss] loss -0.4523 (1.041 secs)\n",
      "cnp:cnp_matern step 83200 lr 3.402e-05 [train_loss] loss -0.4523 (1.041 secs)\n",
      "cnp:cnp_matern step 83400 lr 3.323e-05 [train_loss] loss -0.4375 (1.191 secs)\n",
      "cnp:cnp_matern step 83400 lr 3.323e-05 [train_loss] loss -0.4375 (1.191 secs)\n",
      "cnp:cnp_matern step 83600 lr 3.245e-05 [train_loss] loss -0.4461 (1.207 secs)\n",
      "cnp:cnp_matern step 83600 lr 3.245e-05 [train_loss] loss -0.4461 (1.207 secs)\n",
      "cnp:cnp_matern step 83800 lr 3.168e-05 [train_loss] loss -0.4690 (1.225 secs)\n",
      "cnp:cnp_matern step 83800 lr 3.168e-05 [train_loss] loss -0.4690 (1.225 secs)\n",
      "cnp:cnp_matern step 84000 lr 3.092e-05 [train_loss] loss -0.4154 (1.130 secs)\n",
      "cnp:cnp_matern step 84000 lr 3.092e-05 [train_loss] loss -0.4154 (1.130 secs)\n",
      "cnp:cnp_matern step 84200 lr 3.017e-05 [train_loss] loss -0.4276 (1.064 secs)\n",
      "cnp:cnp_matern step 84200 lr 3.017e-05 [train_loss] loss -0.4276 (1.064 secs)\n",
      "cnp:cnp_matern step 84400 lr 2.943e-05 [train_loss] loss -0.4617 (1.047 secs)\n",
      "cnp:cnp_matern step 84400 lr 2.943e-05 [train_loss] loss -0.4617 (1.047 secs)\n",
      "cnp:cnp_matern step 84600 lr 2.869e-05 [train_loss] loss -0.4859 (1.032 secs)\n",
      "cnp:cnp_matern step 84600 lr 2.869e-05 [train_loss] loss -0.4859 (1.032 secs)\n",
      "cnp:cnp_matern step 84800 lr 2.797e-05 [train_loss] loss -0.4609 (1.058 secs)\n",
      "cnp:cnp_matern step 84800 lr 2.797e-05 [train_loss] loss -0.4609 (1.058 secs)\n",
      "cnp:cnp_matern step 85000 lr 2.725e-05 [train_loss] loss -0.5026 (1.031 secs)\n",
      "cnp:cnp_matern step 85000 lr 2.725e-05 [train_loss] loss -0.5026 (1.031 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:03<00:00, 925.46it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.5500 tar_ll 0.0254 (3.244 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.5500 tar_ll 0.0254 (3.244 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 85200 lr 2.654e-05 [train_loss] loss -0.4607 (1.095 secs)\n",
      "cnp:cnp_matern step 85200 lr 2.654e-05 [train_loss] loss -0.4607 (1.095 secs)\n",
      "cnp:cnp_matern step 85400 lr 2.584e-05 [train_loss] loss -0.3904 (1.085 secs)\n",
      "cnp:cnp_matern step 85400 lr 2.584e-05 [train_loss] loss -0.3904 (1.085 secs)\n",
      "cnp:cnp_matern step 85600 lr 2.515e-05 [train_loss] loss -0.4043 (1.128 secs)\n",
      "cnp:cnp_matern step 85600 lr 2.515e-05 [train_loss] loss -0.4043 (1.128 secs)\n",
      "cnp:cnp_matern step 85800 lr 2.447e-05 [train_loss] loss -0.4843 (1.149 secs)\n",
      "cnp:cnp_matern step 85800 lr 2.447e-05 [train_loss] loss -0.4843 (1.149 secs)\n",
      "cnp:cnp_matern step 86000 lr 2.379e-05 [train_loss] loss -0.4461 (1.128 secs)\n",
      "cnp:cnp_matern step 86000 lr 2.379e-05 [train_loss] loss -0.4461 (1.128 secs)\n",
      "cnp:cnp_matern step 86200 lr 2.313e-05 [train_loss] loss -0.4441 (1.071 secs)\n",
      "cnp:cnp_matern step 86200 lr 2.313e-05 [train_loss] loss -0.4441 (1.071 secs)\n",
      "cnp:cnp_matern step 86400 lr 2.247e-05 [train_loss] loss -0.4617 (1.057 secs)\n",
      "cnp:cnp_matern step 86400 lr 2.247e-05 [train_loss] loss -0.4617 (1.057 secs)\n",
      "cnp:cnp_matern step 86600 lr 2.183e-05 [train_loss] loss -0.4598 (1.041 secs)\n",
      "cnp:cnp_matern step 86600 lr 2.183e-05 [train_loss] loss -0.4598 (1.041 secs)\n",
      "cnp:cnp_matern step 86800 lr 2.119e-05 [train_loss] loss -0.4367 (1.043 secs)\n",
      "cnp:cnp_matern step 86800 lr 2.119e-05 [train_loss] loss -0.4367 (1.043 secs)\n",
      "cnp:cnp_matern step 87000 lr 2.056e-05 [train_loss] loss -0.4144 (1.093 secs)\n",
      "cnp:cnp_matern step 87000 lr 2.056e-05 [train_loss] loss -0.4144 (1.093 secs)\n",
      "cnp:cnp_matern step 87200 lr 1.994e-05 [train_loss] loss -0.4360 (1.094 secs)\n",
      "cnp:cnp_matern step 87200 lr 1.994e-05 [train_loss] loss -0.4360 (1.094 secs)\n",
      "cnp:cnp_matern step 87400 lr 1.933e-05 [train_loss] loss -0.4433 (1.064 secs)\n",
      "cnp:cnp_matern step 87400 lr 1.933e-05 [train_loss] loss -0.4433 (1.064 secs)\n",
      "cnp:cnp_matern step 87600 lr 1.873e-05 [train_loss] loss -0.4326 (1.034 secs)\n",
      "cnp:cnp_matern step 87600 lr 1.873e-05 [train_loss] loss -0.4326 (1.034 secs)\n",
      "cnp:cnp_matern step 87800 lr 1.814e-05 [train_loss] loss -0.4684 (1.041 secs)\n",
      "cnp:cnp_matern step 87800 lr 1.814e-05 [train_loss] loss -0.4684 (1.041 secs)\n",
      "cnp:cnp_matern step 88000 lr 1.756e-05 [train_loss] loss -0.4139 (1.093 secs)\n",
      "cnp:cnp_matern step 88000 lr 1.756e-05 [train_loss] loss -0.4139 (1.093 secs)\n",
      "cnp:cnp_matern step 88200 lr 1.698e-05 [train_loss] loss -0.4045 (1.108 secs)\n",
      "cnp:cnp_matern step 88200 lr 1.698e-05 [train_loss] loss -0.4045 (1.108 secs)\n",
      "cnp:cnp_matern step 88400 lr 1.642e-05 [train_loss] loss -0.4595 (1.062 secs)\n",
      "cnp:cnp_matern step 88400 lr 1.642e-05 [train_loss] loss -0.4595 (1.062 secs)\n",
      "cnp:cnp_matern step 88600 lr 1.586e-05 [train_loss] loss -0.4583 (1.047 secs)\n",
      "cnp:cnp_matern step 88600 lr 1.586e-05 [train_loss] loss -0.4583 (1.047 secs)\n",
      "cnp:cnp_matern step 88800 lr 1.532e-05 [train_loss] loss -0.4769 (1.164 secs)\n",
      "cnp:cnp_matern step 88800 lr 1.532e-05 [train_loss] loss -0.4769 (1.164 secs)\n",
      "cnp:cnp_matern step 89000 lr 1.478e-05 [train_loss] loss -0.4366 (1.095 secs)\n",
      "cnp:cnp_matern step 89000 lr 1.478e-05 [train_loss] loss -0.4366 (1.095 secs)\n",
      "cnp:cnp_matern step 89200 lr 1.425e-05 [train_loss] loss -0.4637 (1.109 secs)\n",
      "cnp:cnp_matern step 89200 lr 1.425e-05 [train_loss] loss -0.4637 (1.109 secs)\n",
      "cnp:cnp_matern step 89400 lr 1.373e-05 [train_loss] loss -0.5141 (1.045 secs)\n",
      "cnp:cnp_matern step 89400 lr 1.373e-05 [train_loss] loss -0.5141 (1.045 secs)\n",
      "cnp:cnp_matern step 89600 lr 1.323e-05 [train_loss] loss -0.3747 (1.034 secs)\n",
      "cnp:cnp_matern step 89600 lr 1.323e-05 [train_loss] loss -0.3747 (1.034 secs)\n",
      "cnp:cnp_matern step 89800 lr 1.273e-05 [train_loss] loss -0.4664 (1.021 secs)\n",
      "cnp:cnp_matern step 89800 lr 1.273e-05 [train_loss] loss -0.4664 (1.021 secs)\n",
      "cnp:cnp_matern step 90000 lr 1.224e-05 [train_loss] loss -0.4700 (1.066 secs)\n",
      "cnp:cnp_matern step 90000 lr 1.224e-05 [train_loss] loss -0.4700 (1.066 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:03<00:00, 909.49it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.5558 tar_ll 0.0186 (3.301 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.5558 tar_ll 0.0186 (3.301 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 90200 lr 1.176e-05 [train_loss] loss -0.4977 (1.079 secs)\n",
      "cnp:cnp_matern step 90200 lr 1.176e-05 [train_loss] loss -0.4977 (1.079 secs)\n",
      "cnp:cnp_matern step 90400 lr 1.128e-05 [train_loss] loss -0.4114 (1.027 secs)\n",
      "cnp:cnp_matern step 90400 lr 1.128e-05 [train_loss] loss -0.4114 (1.027 secs)\n",
      "cnp:cnp_matern step 90600 lr 1.082e-05 [train_loss] loss -0.4792 (1.075 secs)\n",
      "cnp:cnp_matern step 90600 lr 1.082e-05 [train_loss] loss -0.4792 (1.075 secs)\n",
      "cnp:cnp_matern step 90800 lr 1.037e-05 [train_loss] loss -0.4870 (1.051 secs)\n",
      "cnp:cnp_matern step 90800 lr 1.037e-05 [train_loss] loss -0.4870 (1.051 secs)\n",
      "cnp:cnp_matern step 91000 lr 9.927e-06 [train_loss] loss -0.4754 (1.155 secs)\n",
      "cnp:cnp_matern step 91000 lr 9.927e-06 [train_loss] loss -0.4754 (1.155 secs)\n",
      "cnp:cnp_matern step 91200 lr 9.493e-06 [train_loss] loss -0.4262 (1.205 secs)\n",
      "cnp:cnp_matern step 91200 lr 9.493e-06 [train_loss] loss -0.4262 (1.205 secs)\n",
      "cnp:cnp_matern step 91400 lr 9.069e-06 [train_loss] loss -0.4791 (1.065 secs)\n",
      "cnp:cnp_matern step 91400 lr 9.069e-06 [train_loss] loss -0.4791 (1.065 secs)\n",
      "cnp:cnp_matern step 91600 lr 8.655e-06 [train_loss] loss -0.4436 (1.087 secs)\n",
      "cnp:cnp_matern step 91600 lr 8.655e-06 [train_loss] loss -0.4436 (1.087 secs)\n",
      "cnp:cnp_matern step 91800 lr 8.250e-06 [train_loss] loss -0.4748 (1.063 secs)\n",
      "cnp:cnp_matern step 91800 lr 8.250e-06 [train_loss] loss -0.4748 (1.063 secs)\n",
      "cnp:cnp_matern step 92000 lr 7.854e-06 [train_loss] loss -0.4682 (1.059 secs)\n",
      "cnp:cnp_matern step 92000 lr 7.854e-06 [train_loss] loss -0.4682 (1.059 secs)\n",
      "cnp:cnp_matern step 92200 lr 7.468e-06 [train_loss] loss -0.4561 (1.098 secs)\n",
      "cnp:cnp_matern step 92200 lr 7.468e-06 [train_loss] loss -0.4561 (1.098 secs)\n",
      "cnp:cnp_matern step 92400 lr 7.092e-06 [train_loss] loss -0.4690 (1.075 secs)\n",
      "cnp:cnp_matern step 92400 lr 7.092e-06 [train_loss] loss -0.4690 (1.075 secs)\n",
      "cnp:cnp_matern step 92600 lr 6.725e-06 [train_loss] loss -0.4539 (1.082 secs)\n",
      "cnp:cnp_matern step 92600 lr 6.725e-06 [train_loss] loss -0.4539 (1.082 secs)\n",
      "cnp:cnp_matern step 92800 lr 6.368e-06 [train_loss] loss -0.4432 (1.079 secs)\n",
      "cnp:cnp_matern step 92800 lr 6.368e-06 [train_loss] loss -0.4432 (1.079 secs)\n",
      "cnp:cnp_matern step 93000 lr 6.021e-06 [train_loss] loss -0.4491 (1.055 secs)\n",
      "cnp:cnp_matern step 93000 lr 6.021e-06 [train_loss] loss -0.4491 (1.055 secs)\n",
      "cnp:cnp_matern step 93200 lr 5.683e-06 [train_loss] loss -0.4610 (1.069 secs)\n",
      "cnp:cnp_matern step 93200 lr 5.683e-06 [train_loss] loss -0.4610 (1.069 secs)\n",
      "cnp:cnp_matern step 93400 lr 5.355e-06 [train_loss] loss -0.4502 (1.033 secs)\n",
      "cnp:cnp_matern step 93400 lr 5.355e-06 [train_loss] loss -0.4502 (1.033 secs)\n",
      "cnp:cnp_matern step 93600 lr 5.036e-06 [train_loss] loss -0.4604 (1.019 secs)\n",
      "cnp:cnp_matern step 93600 lr 5.036e-06 [train_loss] loss -0.4604 (1.019 secs)\n",
      "cnp:cnp_matern step 93800 lr 4.727e-06 [train_loss] loss -0.4406 (1.067 secs)\n",
      "cnp:cnp_matern step 93800 lr 4.727e-06 [train_loss] loss -0.4406 (1.067 secs)\n",
      "cnp:cnp_matern step 94000 lr 4.428e-06 [train_loss] loss -0.4434 (1.159 secs)\n",
      "cnp:cnp_matern step 94000 lr 4.428e-06 [train_loss] loss -0.4434 (1.159 secs)\n",
      "cnp:cnp_matern step 94200 lr 4.139e-06 [train_loss] loss -0.4849 (1.138 secs)\n",
      "cnp:cnp_matern step 94200 lr 4.139e-06 [train_loss] loss -0.4849 (1.138 secs)\n",
      "cnp:cnp_matern step 94400 lr 3.859e-06 [train_loss] loss -0.4382 (1.055 secs)\n",
      "cnp:cnp_matern step 94400 lr 3.859e-06 [train_loss] loss -0.4382 (1.055 secs)\n",
      "cnp:cnp_matern step 94600 lr 3.589e-06 [train_loss] loss -0.4299 (1.058 secs)\n",
      "cnp:cnp_matern step 94600 lr 3.589e-06 [train_loss] loss -0.4299 (1.058 secs)\n",
      "cnp:cnp_matern step 94800 lr 3.329e-06 [train_loss] loss -0.4841 (1.076 secs)\n",
      "cnp:cnp_matern step 94800 lr 3.329e-06 [train_loss] loss -0.4841 (1.076 secs)\n",
      "cnp:cnp_matern step 95000 lr 3.078e-06 [train_loss] loss -0.4471 (1.134 secs)\n",
      "cnp:cnp_matern step 95000 lr 3.078e-06 [train_loss] loss -0.4471 (1.134 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:03<00:00, 877.42it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.5584 tar_ll 0.0213 (3.420 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.5584 tar_ll 0.0213 (3.420 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnp:cnp_matern step 95200 lr 2.837e-06 [train_loss] loss -0.4661 (1.117 secs)\n",
      "cnp:cnp_matern step 95200 lr 2.837e-06 [train_loss] loss -0.4661 (1.117 secs)\n",
      "cnp:cnp_matern step 95400 lr 2.606e-06 [train_loss] loss -0.4659 (1.093 secs)\n",
      "cnp:cnp_matern step 95400 lr 2.606e-06 [train_loss] loss -0.4659 (1.093 secs)\n",
      "cnp:cnp_matern step 95600 lr 2.385e-06 [train_loss] loss -0.4439 (1.083 secs)\n",
      "cnp:cnp_matern step 95600 lr 2.385e-06 [train_loss] loss -0.4439 (1.083 secs)\n",
      "cnp:cnp_matern step 95800 lr 2.173e-06 [train_loss] loss -0.4465 (1.065 secs)\n",
      "cnp:cnp_matern step 95800 lr 2.173e-06 [train_loss] loss -0.4465 (1.065 secs)\n",
      "cnp:cnp_matern step 96000 lr 1.971e-06 [train_loss] loss -0.4457 (1.089 secs)\n",
      "cnp:cnp_matern step 96000 lr 1.971e-06 [train_loss] loss -0.4457 (1.089 secs)\n",
      "cnp:cnp_matern step 96200 lr 1.779e-06 [train_loss] loss -0.4508 (1.183 secs)\n",
      "cnp:cnp_matern step 96200 lr 1.779e-06 [train_loss] loss -0.4508 (1.183 secs)\n",
      "cnp:cnp_matern step 96400 lr 1.597e-06 [train_loss] loss -0.5014 (1.135 secs)\n",
      "cnp:cnp_matern step 96400 lr 1.597e-06 [train_loss] loss -0.5014 (1.135 secs)\n",
      "cnp:cnp_matern step 96600 lr 1.425e-06 [train_loss] loss -0.4847 (1.062 secs)\n",
      "cnp:cnp_matern step 96600 lr 1.425e-06 [train_loss] loss -0.4847 (1.062 secs)\n",
      "cnp:cnp_matern step 96800 lr 1.262e-06 [train_loss] loss -0.4370 (1.082 secs)\n",
      "cnp:cnp_matern step 96800 lr 1.262e-06 [train_loss] loss -0.4370 (1.082 secs)\n",
      "cnp:cnp_matern step 97000 lr 1.110e-06 [train_loss] loss -0.4428 (1.107 secs)\n",
      "cnp:cnp_matern step 97000 lr 1.110e-06 [train_loss] loss -0.4428 (1.107 secs)\n",
      "cnp:cnp_matern step 97200 lr 9.666e-07 [train_loss] loss -0.4509 (1.107 secs)\n",
      "cnp:cnp_matern step 97200 lr 9.666e-07 [train_loss] loss -0.4509 (1.107 secs)\n",
      "cnp:cnp_matern step 97400 lr 8.335e-07 [train_loss] loss -0.4436 (1.096 secs)\n",
      "cnp:cnp_matern step 97400 lr 8.335e-07 [train_loss] loss -0.4436 (1.096 secs)\n",
      "cnp:cnp_matern step 97600 lr 7.103e-07 [train_loss] loss -0.4991 (1.091 secs)\n",
      "cnp:cnp_matern step 97600 lr 7.103e-07 [train_loss] loss -0.4991 (1.091 secs)\n",
      "cnp:cnp_matern step 97800 lr 5.969e-07 [train_loss] loss -0.4794 (1.088 secs)\n",
      "cnp:cnp_matern step 97800 lr 5.969e-07 [train_loss] loss -0.4794 (1.088 secs)\n",
      "cnp:cnp_matern step 98000 lr 4.933e-07 [train_loss] loss -0.4650 (1.088 secs)\n",
      "cnp:cnp_matern step 98000 lr 4.933e-07 [train_loss] loss -0.4650 (1.088 secs)\n",
      "cnp:cnp_matern step 98200 lr 3.996e-07 [train_loss] loss -0.4599 (1.103 secs)\n",
      "cnp:cnp_matern step 98200 lr 3.996e-07 [train_loss] loss -0.4599 (1.103 secs)\n",
      "cnp:cnp_matern step 98400 lr 3.158e-07 [train_loss] loss -0.4483 (1.089 secs)\n",
      "cnp:cnp_matern step 98400 lr 3.158e-07 [train_loss] loss -0.4483 (1.089 secs)\n",
      "cnp:cnp_matern step 98600 lr 2.418e-07 [train_loss] loss -0.4153 (1.068 secs)\n",
      "cnp:cnp_matern step 98600 lr 2.418e-07 [train_loss] loss -0.4153 (1.068 secs)\n",
      "cnp:cnp_matern step 98800 lr 1.776e-07 [train_loss] loss -0.4799 (1.123 secs)\n",
      "cnp:cnp_matern step 98800 lr 1.776e-07 [train_loss] loss -0.4799 (1.123 secs)\n",
      "cnp:cnp_matern step 99000 lr 1.234e-07 [train_loss] loss -0.4259 (1.151 secs)\n",
      "cnp:cnp_matern step 99000 lr 1.234e-07 [train_loss] loss -0.4259 (1.151 secs)\n",
      "cnp:cnp_matern step 99200 lr 7.895e-08 [train_loss] loss -0.4358 (1.178 secs)\n",
      "cnp:cnp_matern step 99200 lr 7.895e-08 [train_loss] loss -0.4358 (1.178 secs)\n",
      "cnp:cnp_matern step 99400 lr 4.441e-08 [train_loss] loss -0.4908 (1.075 secs)\n",
      "cnp:cnp_matern step 99400 lr 4.441e-08 [train_loss] loss -0.4908 (1.075 secs)\n",
      "cnp:cnp_matern step 99600 lr 1.974e-08 [train_loss] loss -0.4383 (1.093 secs)\n",
      "cnp:cnp_matern step 99600 lr 1.974e-08 [train_loss] loss -0.4383 (1.093 secs)\n",
      "cnp:cnp_matern step 99800 lr 4.935e-09 [train_loss] loss -0.4904 (1.099 secs)\n",
      "cnp:cnp_matern step 99800 lr 4.935e-09 [train_loss] loss -0.4904 (1.099 secs)\n",
      "cnp:cnp_matern step 100000 lr 0.000e+00 [train_loss] loss -0.4466 (1.064 secs)\n",
      "cnp:cnp_matern step 100000 lr 0.000e+00 [train_loss] loss -0.4466 (1.064 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:03<00:00, 875.56it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.5580 tar_ll 0.0269 (3.429 secs)\n",
      "\n",
      "cnp:cnp_matern matern ctx_ll 0.5580 tar_ll 0.0269 (3.429 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:03<00:00, 875.48it/s]\n",
      "cnp:cnp_matern matern ctx_ll 0.5580 tar_ll 0.0269 (3.429 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "Execution time: 870869.5 miliseconds\n",
      "Execution time: 870.8695 seconds\n",
      "Initial Memory Usage: 19.80224609375 MB\n",
      "Final Memory Usage: 43.087890625 MB\n",
      "Memory Usage Change: 23.28564453125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='cnp', name='cnp_matern',val_seed=100, val_l=None,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca6a17-5a23-4907-b69f-89c53ed4edd2",
   "metadata": {},
   "source": [
    "## CANP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcaaf2fd-ba8b-4c3a-b2af-ef6e9378e198",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: canp-canp_matern\n",
      "Total number of parameters: 331906\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "matern-seed100.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 200 lr 5.000e-04 [train_loss] loss 0.5031 (1.771 secs)\n",
      "canp:canp_matern step 400 lr 5.000e-04 [train_loss] loss 0.3016 (1.770 secs)\n",
      "canp:canp_matern step 600 lr 5.000e-04 [train_loss] loss -0.1500 (1.690 secs)\n",
      "canp:canp_matern step 800 lr 4.999e-04 [train_loss] loss -0.2428 (1.697 secs)\n",
      "canp:canp_matern step 1000 lr 4.999e-04 [train_loss] loss -0.3733 (1.716 secs)\n",
      "canp:canp_matern step 1200 lr 4.998e-04 [train_loss] loss -0.3201 (1.707 secs)\n",
      "canp:canp_matern step 1400 lr 4.998e-04 [train_loss] loss -0.4249 (1.715 secs)\n",
      "canp:canp_matern step 1600 lr 4.997e-04 [train_loss] loss -0.5067 (1.684 secs)\n",
      "canp:canp_matern step 1800 lr 4.996e-04 [train_loss] loss -0.5390 (1.706 secs)\n",
      "canp:canp_matern step 2000 lr 4.995e-04 [train_loss] loss -0.5924 (1.699 secs)\n",
      "canp:canp_matern step 2200 lr 4.994e-04 [train_loss] loss -0.6900 (1.813 secs)\n",
      "canp:canp_matern step 2400 lr 4.993e-04 [train_loss] loss -0.7062 (1.731 secs)\n",
      "canp:canp_matern step 2600 lr 4.992e-04 [train_loss] loss -0.7792 (1.668 secs)\n",
      "canp:canp_matern step 2800 lr 4.990e-04 [train_loss] loss -0.5505 (1.701 secs)\n",
      "canp:canp_matern step 3000 lr 4.989e-04 [train_loss] loss -0.8125 (1.732 secs)\n",
      "canp:canp_matern step 3200 lr 4.987e-04 [train_loss] loss -0.8551 (1.732 secs)\n",
      "canp:canp_matern step 3400 lr 4.986e-04 [train_loss] loss -0.8302 (1.681 secs)\n",
      "canp:canp_matern step 3600 lr 4.984e-04 [train_loss] loss -0.7234 (1.739 secs)\n",
      "canp:canp_matern step 3800 lr 4.982e-04 [train_loss] loss -0.8314 (1.675 secs)\n",
      "canp:canp_matern step 4000 lr 4.980e-04 [train_loss] loss -0.8229 (1.763 secs)\n",
      "canp:canp_matern step 4200 lr 4.978e-04 [train_loss] loss -0.8929 (1.796 secs)\n",
      "canp:canp_matern step 4400 lr 4.976e-04 [train_loss] loss -0.8313 (1.724 secs)\n",
      "canp:canp_matern step 4600 lr 4.974e-04 [train_loss] loss -0.8560 (1.693 secs)\n",
      "canp:canp_matern step 4800 lr 4.972e-04 [train_loss] loss -0.9341 (1.723 secs)\n",
      "canp:canp_matern step 5000 lr 4.969e-04 [train_loss] loss -0.9005 (1.722 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 442.47it/s]\n",
      "canp:canp_matern matern ctx_ll 1.1963 tar_ll 0.3080 (6.782 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 5200 lr 4.967e-04 [train_loss] loss -0.8773 (1.688 secs)\n",
      "canp:canp_matern step 5400 lr 4.964e-04 [train_loss] loss -0.9180 (1.612 secs)\n",
      "canp:canp_matern step 5600 lr 4.961e-04 [train_loss] loss -0.9159 (1.591 secs)\n",
      "canp:canp_matern step 5800 lr 4.959e-04 [train_loss] loss -0.8575 (1.627 secs)\n",
      "canp:canp_matern step 6000 lr 4.956e-04 [train_loss] loss -0.8546 (1.652 secs)\n",
      "canp:canp_matern step 6200 lr 4.953e-04 [train_loss] loss -0.8237 (1.727 secs)\n",
      "canp:canp_matern step 6400 lr 4.950e-04 [train_loss] loss -0.7332 (1.688 secs)\n",
      "canp:canp_matern step 6600 lr 4.946e-04 [train_loss] loss -0.9098 (1.603 secs)\n",
      "canp:canp_matern step 6800 lr 4.943e-04 [train_loss] loss -0.8828 (1.587 secs)\n",
      "canp:canp_matern step 7000 lr 4.940e-04 [train_loss] loss -0.8514 (1.779 secs)\n",
      "canp:canp_matern step 7200 lr 4.936e-04 [train_loss] loss -0.8603 (1.641 secs)\n",
      "canp:canp_matern step 7400 lr 4.933e-04 [train_loss] loss -0.8581 (1.604 secs)\n",
      "canp:canp_matern step 7600 lr 4.929e-04 [train_loss] loss -0.5202 (1.622 secs)\n",
      "canp:canp_matern step 7800 lr 4.925e-04 [train_loss] loss -0.7978 (1.631 secs)\n",
      "canp:canp_matern step 8000 lr 4.921e-04 [train_loss] loss -0.8337 (1.650 secs)\n",
      "canp:canp_matern step 8200 lr 4.918e-04 [train_loss] loss -0.8732 (1.662 secs)\n",
      "canp:canp_matern step 8400 lr 4.913e-04 [train_loss] loss -0.9474 (1.602 secs)\n",
      "canp:canp_matern step 8600 lr 4.909e-04 [train_loss] loss -0.9524 (1.607 secs)\n",
      "canp:canp_matern step 8800 lr 4.905e-04 [train_loss] loss -0.9621 (1.599 secs)\n",
      "canp:canp_matern step 9000 lr 4.901e-04 [train_loss] loss -0.9798 (1.746 secs)\n",
      "canp:canp_matern step 9200 lr 4.896e-04 [train_loss] loss -0.9437 (1.626 secs)\n",
      "canp:canp_matern step 9400 lr 4.892e-04 [train_loss] loss -0.9427 (1.601 secs)\n",
      "canp:canp_matern step 9600 lr 4.887e-04 [train_loss] loss -0.9506 (1.639 secs)\n",
      "canp:canp_matern step 9800 lr 4.882e-04 [train_loss] loss -0.9415 (1.580 secs)\n",
      "canp:canp_matern step 10000 lr 4.878e-04 [train_loss] loss -0.9465 (1.588 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 473.64it/s]\n",
      "canp:canp_matern matern ctx_ll 1.2439 tar_ll 0.4069 (6.335 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 10200 lr 4.873e-04 [train_loss] loss -0.8745 (1.696 secs)\n",
      "canp:canp_matern step 10400 lr 4.868e-04 [train_loss] loss -0.9058 (1.633 secs)\n",
      "canp:canp_matern step 10600 lr 4.863e-04 [train_loss] loss -0.8029 (1.589 secs)\n",
      "canp:canp_matern step 10800 lr 4.857e-04 [train_loss] loss -0.8522 (1.615 secs)\n",
      "canp:canp_matern step 11000 lr 4.852e-04 [train_loss] loss -0.8074 (1.593 secs)\n",
      "canp:canp_matern step 11200 lr 4.847e-04 [train_loss] loss -0.9295 (1.629 secs)\n",
      "canp:canp_matern step 11400 lr 4.841e-04 [train_loss] loss -0.9470 (1.637 secs)\n",
      "canp:canp_matern step 11600 lr 4.836e-04 [train_loss] loss -0.9475 (1.609 secs)\n",
      "canp:canp_matern step 11800 lr 4.830e-04 [train_loss] loss -1.0078 (1.642 secs)\n",
      "canp:canp_matern step 12000 lr 4.824e-04 [train_loss] loss -0.9486 (1.713 secs)\n",
      "canp:canp_matern step 12200 lr 4.819e-04 [train_loss] loss -0.9579 (1.660 secs)\n",
      "canp:canp_matern step 12400 lr 4.813e-04 [train_loss] loss -0.8883 (1.634 secs)\n",
      "canp:canp_matern step 12600 lr 4.807e-04 [train_loss] loss -0.8779 (1.641 secs)\n",
      "canp:canp_matern step 12800 lr 4.801e-04 [train_loss] loss -0.7500 (1.616 secs)\n",
      "canp:canp_matern step 13000 lr 4.794e-04 [train_loss] loss -0.9766 (1.612 secs)\n",
      "canp:canp_matern step 13200 lr 4.788e-04 [train_loss] loss -0.8407 (1.642 secs)\n",
      "canp:canp_matern step 13400 lr 4.782e-04 [train_loss] loss -0.9728 (1.588 secs)\n",
      "canp:canp_matern step 13600 lr 4.775e-04 [train_loss] loss -0.9815 (1.624 secs)\n",
      "canp:canp_matern step 13800 lr 4.769e-04 [train_loss] loss -0.9634 (1.622 secs)\n",
      "canp:canp_matern step 14000 lr 4.762e-04 [train_loss] loss -0.9103 (1.735 secs)\n",
      "canp:canp_matern step 14200 lr 4.755e-04 [train_loss] loss -0.9681 (1.629 secs)\n",
      "canp:canp_matern step 14400 lr 4.749e-04 [train_loss] loss -0.9405 (1.695 secs)\n",
      "canp:canp_matern step 14600 lr 4.742e-04 [train_loss] loss -0.9265 (1.649 secs)\n",
      "canp:canp_matern step 14800 lr 4.735e-04 [train_loss] loss -0.9428 (1.660 secs)\n",
      "canp:canp_matern step 15000 lr 4.728e-04 [train_loss] loss -0.9151 (1.616 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:07<00:00, 426.16it/s]\n",
      "canp:canp_matern matern ctx_ll 1.0840 tar_ll 0.3387 (7.041 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 15200 lr 4.720e-04 [train_loss] loss -0.8785 (1.903 secs)\n",
      "canp:canp_matern step 15400 lr 4.713e-04 [train_loss] loss -0.9552 (1.608 secs)\n",
      "canp:canp_matern step 15600 lr 4.706e-04 [train_loss] loss -0.9494 (1.624 secs)\n",
      "canp:canp_matern step 15800 lr 4.698e-04 [train_loss] loss -0.9921 (1.677 secs)\n",
      "canp:canp_matern step 16000 lr 4.691e-04 [train_loss] loss -0.9953 (1.596 secs)\n",
      "canp:canp_matern step 16200 lr 4.683e-04 [train_loss] loss -1.0003 (1.624 secs)\n",
      "canp:canp_matern step 16400 lr 4.675e-04 [train_loss] loss -0.9286 (1.627 secs)\n",
      "canp:canp_matern step 16600 lr 4.668e-04 [train_loss] loss -0.9500 (1.613 secs)\n",
      "canp:canp_matern step 16800 lr 4.660e-04 [train_loss] loss -0.8436 (1.611 secs)\n",
      "canp:canp_matern step 17000 lr 4.652e-04 [train_loss] loss -0.9125 (1.950 secs)\n",
      "canp:canp_matern step 17200 lr 4.644e-04 [train_loss] loss -0.9933 (1.634 secs)\n",
      "canp:canp_matern step 17400 lr 4.636e-04 [train_loss] loss -0.9869 (1.611 secs)\n",
      "canp:canp_matern step 17600 lr 4.627e-04 [train_loss] loss -0.9640 (1.660 secs)\n",
      "canp:canp_matern step 17800 lr 4.619e-04 [train_loss] loss -0.9660 (1.627 secs)\n",
      "canp:canp_matern step 18000 lr 4.611e-04 [train_loss] loss -0.9319 (1.653 secs)\n",
      "canp:canp_matern step 18200 lr 4.602e-04 [train_loss] loss -0.9205 (1.647 secs)\n",
      "canp:canp_matern step 18400 lr 4.594e-04 [train_loss] loss -0.9004 (1.639 secs)\n",
      "canp:canp_matern step 18600 lr 4.585e-04 [train_loss] loss -0.8507 (1.659 secs)\n",
      "canp:canp_matern step 18800 lr 4.576e-04 [train_loss] loss -0.9449 (2.051 secs)\n",
      "canp:canp_matern step 19000 lr 4.568e-04 [train_loss] loss -0.9382 (1.974 secs)\n",
      "canp:canp_matern step 19200 lr 4.559e-04 [train_loss] loss -0.9494 (1.687 secs)\n",
      "canp:canp_matern step 19400 lr 4.550e-04 [train_loss] loss -0.9290 (1.632 secs)\n",
      "canp:canp_matern step 19600 lr 4.541e-04 [train_loss] loss -0.9997 (1.625 secs)\n",
      "canp:canp_matern step 19800 lr 4.532e-04 [train_loss] loss -1.0279 (1.659 secs)\n",
      "canp:canp_matern step 20000 lr 4.523e-04 [train_loss] loss -1.0069 (1.627 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:07<00:00, 423.90it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3175 tar_ll 0.4845 (7.079 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 20200 lr 4.513e-04 [train_loss] loss -0.9438 (1.648 secs)\n",
      "canp:canp_matern step 20400 lr 4.504e-04 [train_loss] loss -0.8831 (1.631 secs)\n",
      "canp:canp_matern step 20600 lr 4.494e-04 [train_loss] loss -1.0041 (1.630 secs)\n",
      "canp:canp_matern step 20800 lr 4.485e-04 [train_loss] loss -0.9813 (1.649 secs)\n",
      "canp:canp_matern step 21000 lr 4.475e-04 [train_loss] loss -0.8951 (1.649 secs)\n",
      "canp:canp_matern step 21200 lr 4.466e-04 [train_loss] loss -0.9134 (1.645 secs)\n",
      "canp:canp_matern step 21400 lr 4.456e-04 [train_loss] loss -0.9299 (1.706 secs)\n",
      "canp:canp_matern step 21600 lr 4.446e-04 [train_loss] loss -1.0085 (1.788 secs)\n",
      "canp:canp_matern step 21800 lr 4.436e-04 [train_loss] loss -1.0109 (2.034 secs)\n",
      "canp:canp_matern step 22000 lr 4.426e-04 [train_loss] loss -0.9368 (1.651 secs)\n",
      "canp:canp_matern step 22200 lr 4.416e-04 [train_loss] loss -0.9762 (1.655 secs)\n",
      "canp:canp_matern step 22400 lr 4.406e-04 [train_loss] loss -0.9003 (1.613 secs)\n",
      "canp:canp_matern step 22600 lr 4.396e-04 [train_loss] loss -0.9456 (1.651 secs)\n",
      "canp:canp_matern step 22800 lr 4.386e-04 [train_loss] loss -0.9847 (1.586 secs)\n",
      "canp:canp_matern step 23000 lr 4.375e-04 [train_loss] loss -0.9864 (1.607 secs)\n",
      "canp:canp_matern step 23200 lr 4.365e-04 [train_loss] loss -0.9329 (1.712 secs)\n",
      "canp:canp_matern step 23400 lr 4.354e-04 [train_loss] loss -0.9982 (1.690 secs)\n",
      "canp:canp_matern step 23600 lr 4.344e-04 [train_loss] loss -0.9751 (2.106 secs)\n",
      "canp:canp_matern step 23800 lr 4.333e-04 [train_loss] loss -0.9999 (1.792 secs)\n",
      "canp:canp_matern step 24000 lr 4.322e-04 [train_loss] loss -0.9283 (1.656 secs)\n",
      "canp:canp_matern step 24200 lr 4.312e-04 [train_loss] loss -0.9454 (1.652 secs)\n",
      "canp:canp_matern step 24400 lr 4.301e-04 [train_loss] loss -0.9721 (1.623 secs)\n",
      "canp:canp_matern step 24600 lr 4.290e-04 [train_loss] loss -0.9382 (1.622 secs)\n",
      "canp:canp_matern step 24800 lr 4.279e-04 [train_loss] loss -0.9229 (1.669 secs)\n",
      "canp:canp_matern step 25000 lr 4.268e-04 [train_loss] loss -1.0005 (1.698 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 429.47it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3328 tar_ll 0.4957 (6.988 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 25200 lr 4.257e-04 [train_loss] loss -0.9041 (1.637 secs)\n",
      "canp:canp_matern step 25400 lr 4.245e-04 [train_loss] loss -0.9496 (1.636 secs)\n",
      "canp:canp_matern step 25600 lr 4.234e-04 [train_loss] loss -1.0061 (1.611 secs)\n",
      "canp:canp_matern step 25800 lr 4.223e-04 [train_loss] loss -0.9794 (1.644 secs)\n",
      "canp:canp_matern step 26000 lr 4.211e-04 [train_loss] loss -0.9104 (1.617 secs)\n",
      "canp:canp_matern step 26200 lr 4.200e-04 [train_loss] loss -1.0204 (1.627 secs)\n",
      "canp:canp_matern step 26400 lr 4.188e-04 [train_loss] loss -0.9074 (1.804 secs)\n",
      "canp:canp_matern step 26600 lr 4.177e-04 [train_loss] loss -0.9836 (1.922 secs)\n",
      "canp:canp_matern step 26800 lr 4.165e-04 [train_loss] loss -0.9639 (1.626 secs)\n",
      "canp:canp_matern step 27000 lr 4.153e-04 [train_loss] loss -1.0283 (1.651 secs)\n",
      "canp:canp_matern step 27200 lr 4.141e-04 [train_loss] loss -0.9862 (1.656 secs)\n",
      "canp:canp_matern step 27400 lr 4.130e-04 [train_loss] loss -0.9861 (1.660 secs)\n",
      "canp:canp_matern step 27600 lr 4.118e-04 [train_loss] loss -0.8658 (1.646 secs)\n",
      "canp:canp_matern step 27800 lr 4.106e-04 [train_loss] loss -1.0354 (1.680 secs)\n",
      "canp:canp_matern step 28000 lr 4.094e-04 [train_loss] loss -0.9359 (1.730 secs)\n",
      "canp:canp_matern step 28200 lr 4.081e-04 [train_loss] loss -0.7910 (1.786 secs)\n",
      "canp:canp_matern step 28400 lr 4.069e-04 [train_loss] loss -0.7337 (2.090 secs)\n",
      "canp:canp_matern step 28600 lr 4.057e-04 [train_loss] loss -0.9017 (1.613 secs)\n",
      "canp:canp_matern step 28800 lr 4.045e-04 [train_loss] loss -0.9916 (1.648 secs)\n",
      "canp:canp_matern step 29000 lr 4.032e-04 [train_loss] loss -1.0160 (1.621 secs)\n",
      "canp:canp_matern step 29200 lr 4.020e-04 [train_loss] loss -0.9344 (1.613 secs)\n",
      "canp:canp_matern step 29400 lr 4.007e-04 [train_loss] loss -0.9916 (1.640 secs)\n",
      "canp:canp_matern step 29600 lr 3.995e-04 [train_loss] loss -1.0075 (1.743 secs)\n",
      "canp:canp_matern step 29800 lr 3.982e-04 [train_loss] loss -1.0310 (1.738 secs)\n",
      "canp:canp_matern step 30000 lr 3.969e-04 [train_loss] loss -0.8769 (1.709 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 438.95it/s]\n",
      "canp:canp_matern matern ctx_ll 0.8772 tar_ll 0.1851 (6.836 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 30200 lr 3.957e-04 [train_loss] loss -0.8211 (1.676 secs)\n",
      "canp:canp_matern step 30400 lr 3.944e-04 [train_loss] loss -0.9263 (1.853 secs)\n",
      "canp:canp_matern step 30600 lr 3.931e-04 [train_loss] loss -0.9679 (1.782 secs)\n",
      "canp:canp_matern step 30800 lr 3.918e-04 [train_loss] loss -0.9586 (1.726 secs)\n",
      "canp:canp_matern step 31000 lr 3.905e-04 [train_loss] loss -0.9809 (1.695 secs)\n",
      "canp:canp_matern step 31200 lr 3.892e-04 [train_loss] loss -0.9133 (2.035 secs)\n",
      "canp:canp_matern step 31400 lr 3.879e-04 [train_loss] loss -0.9949 (1.694 secs)\n",
      "canp:canp_matern step 31600 lr 3.866e-04 [train_loss] loss -0.9744 (1.646 secs)\n",
      "canp:canp_matern step 31800 lr 3.853e-04 [train_loss] loss -0.9820 (1.635 secs)\n",
      "canp:canp_matern step 32000 lr 3.840e-04 [train_loss] loss -0.9721 (1.663 secs)\n",
      "canp:canp_matern step 32200 lr 3.826e-04 [train_loss] loss -0.9997 (1.683 secs)\n",
      "canp:canp_matern step 32400 lr 3.813e-04 [train_loss] loss -0.9851 (1.745 secs)\n",
      "canp:canp_matern step 32600 lr 3.800e-04 [train_loss] loss -0.9699 (1.713 secs)\n",
      "canp:canp_matern step 32800 lr 3.786e-04 [train_loss] loss -0.9776 (1.745 secs)\n",
      "canp:canp_matern step 33000 lr 3.773e-04 [train_loss] loss -0.9633 (2.094 secs)\n",
      "canp:canp_matern step 33200 lr 3.759e-04 [train_loss] loss -0.9351 (1.860 secs)\n",
      "canp:canp_matern step 33400 lr 3.745e-04 [train_loss] loss -0.9465 (1.711 secs)\n",
      "canp:canp_matern step 33600 lr 3.732e-04 [train_loss] loss -1.0135 (1.713 secs)\n",
      "canp:canp_matern step 33800 lr 3.718e-04 [train_loss] loss -0.9975 (1.695 secs)\n",
      "canp:canp_matern step 34000 lr 3.704e-04 [train_loss] loss -1.0052 (1.742 secs)\n",
      "canp:canp_matern step 34200 lr 3.691e-04 [train_loss] loss -1.0017 (1.795 secs)\n",
      "canp:canp_matern step 34400 lr 3.677e-04 [train_loss] loss -1.0093 (1.723 secs)\n",
      "canp:canp_matern step 34600 lr 3.663e-04 [train_loss] loss -1.0121 (1.828 secs)\n",
      "canp:canp_matern step 34800 lr 3.649e-04 [train_loss] loss -1.0049 (2.014 secs)\n",
      "canp:canp_matern step 35000 lr 3.635e-04 [train_loss] loss -1.0047 (1.740 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 439.10it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3471 tar_ll 0.5013 (6.833 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 35200 lr 3.621e-04 [train_loss] loss -0.9838 (1.835 secs)\n",
      "canp:canp_matern step 35400 lr 3.607e-04 [train_loss] loss -1.0184 (1.759 secs)\n",
      "canp:canp_matern step 35600 lr 3.593e-04 [train_loss] loss -0.9771 (1.823 secs)\n",
      "canp:canp_matern step 35800 lr 3.579e-04 [train_loss] loss -0.8926 (2.007 secs)\n",
      "canp:canp_matern step 36000 lr 3.564e-04 [train_loss] loss -1.0035 (1.717 secs)\n",
      "canp:canp_matern step 36200 lr 3.550e-04 [train_loss] loss -0.9009 (1.686 secs)\n",
      "canp:canp_matern step 36400 lr 3.536e-04 [train_loss] loss -0.9741 (1.755 secs)\n",
      "canp:canp_matern step 36600 lr 3.522e-04 [train_loss] loss -0.9434 (1.737 secs)\n",
      "canp:canp_matern step 36800 lr 3.507e-04 [train_loss] loss -1.0130 (1.729 secs)\n",
      "canp:canp_matern step 37000 lr 3.493e-04 [train_loss] loss -0.9392 (1.754 secs)\n",
      "canp:canp_matern step 37200 lr 3.478e-04 [train_loss] loss -0.9572 (1.720 secs)\n",
      "canp:canp_matern step 37400 lr 3.464e-04 [train_loss] loss -0.9984 (1.843 secs)\n",
      "canp:canp_matern step 37600 lr 3.449e-04 [train_loss] loss -0.9977 (2.036 secs)\n",
      "canp:canp_matern step 37800 lr 3.435e-04 [train_loss] loss -0.9560 (1.731 secs)\n",
      "canp:canp_matern step 38000 lr 3.420e-04 [train_loss] loss -0.9497 (1.757 secs)\n",
      "canp:canp_matern step 38200 lr 3.406e-04 [train_loss] loss -1.0133 (1.752 secs)\n",
      "canp:canp_matern step 38400 lr 3.391e-04 [train_loss] loss -0.8908 (1.677 secs)\n",
      "canp:canp_matern step 38600 lr 3.376e-04 [train_loss] loss -1.0045 (1.744 secs)\n",
      "canp:canp_matern step 38800 lr 3.362e-04 [train_loss] loss -1.0156 (1.744 secs)\n",
      "canp:canp_matern step 39000 lr 3.347e-04 [train_loss] loss -1.0154 (1.770 secs)\n",
      "canp:canp_matern step 39200 lr 3.332e-04 [train_loss] loss -0.9929 (1.990 secs)\n",
      "canp:canp_matern step 39400 lr 3.317e-04 [train_loss] loss -0.9901 (1.984 secs)\n",
      "canp:canp_matern step 39600 lr 3.302e-04 [train_loss] loss -1.0338 (1.715 secs)\n",
      "canp:canp_matern step 39800 lr 3.287e-04 [train_loss] loss -0.9924 (1.711 secs)\n",
      "canp:canp_matern step 40000 lr 3.273e-04 [train_loss] loss -1.0493 (1.692 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 428.64it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3424 tar_ll 0.5437 (7.000 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 40200 lr 3.258e-04 [train_loss] loss -1.0353 (1.871 secs)\n",
      "canp:canp_matern step 40400 lr 3.243e-04 [train_loss] loss -1.0183 (1.905 secs)\n",
      "canp:canp_matern step 40600 lr 3.228e-04 [train_loss] loss -1.0057 (1.732 secs)\n",
      "canp:canp_matern step 40800 lr 3.213e-04 [train_loss] loss -1.0170 (1.752 secs)\n",
      "canp:canp_matern step 41000 lr 3.197e-04 [train_loss] loss -1.0117 (1.618 secs)\n",
      "canp:canp_matern step 41200 lr 3.182e-04 [train_loss] loss -1.0612 (1.670 secs)\n",
      "canp:canp_matern step 41400 lr 3.167e-04 [train_loss] loss -1.0022 (1.692 secs)\n",
      "canp:canp_matern step 41600 lr 3.152e-04 [train_loss] loss -1.0484 (1.686 secs)\n",
      "canp:canp_matern step 41800 lr 3.137e-04 [train_loss] loss -1.0294 (1.706 secs)\n",
      "canp:canp_matern step 42000 lr 3.122e-04 [train_loss] loss -1.0533 (1.994 secs)\n",
      "canp:canp_matern step 42200 lr 3.106e-04 [train_loss] loss -0.9825 (1.958 secs)\n",
      "canp:canp_matern step 42400 lr 3.091e-04 [train_loss] loss -0.9915 (1.657 secs)\n",
      "canp:canp_matern step 42600 lr 3.076e-04 [train_loss] loss -0.9104 (1.634 secs)\n",
      "canp:canp_matern step 42800 lr 3.061e-04 [train_loss] loss -0.9975 (1.628 secs)\n",
      "canp:canp_matern step 43000 lr 3.045e-04 [train_loss] loss -1.0359 (1.646 secs)\n",
      "canp:canp_matern step 43200 lr 3.030e-04 [train_loss] loss -1.0739 (1.707 secs)\n",
      "canp:canp_matern step 43400 lr 3.015e-04 [train_loss] loss -1.0038 (1.668 secs)\n",
      "canp:canp_matern step 43600 lr 2.999e-04 [train_loss] loss -1.0030 (1.843 secs)\n",
      "canp:canp_matern step 43800 lr 2.984e-04 [train_loss] loss -0.9802 (1.720 secs)\n",
      "canp:canp_matern step 44000 lr 2.968e-04 [train_loss] loss -1.0400 (1.930 secs)\n",
      "canp:canp_matern step 44200 lr 2.953e-04 [train_loss] loss -0.9651 (1.624 secs)\n",
      "canp:canp_matern step 44400 lr 2.938e-04 [train_loss] loss -1.0873 (1.641 secs)\n",
      "canp:canp_matern step 44600 lr 2.922e-04 [train_loss] loss -1.0228 (1.601 secs)\n",
      "canp:canp_matern step 44800 lr 2.907e-04 [train_loss] loss -1.0082 (1.597 secs)\n",
      "canp:canp_matern step 45000 lr 2.891e-04 [train_loss] loss -1.0350 (1.582 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 438.28it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3422 tar_ll 0.5436 (6.846 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 45200 lr 2.876e-04 [train_loss] loss -0.9875 (1.769 secs)\n",
      "canp:canp_matern step 45400 lr 2.860e-04 [train_loss] loss -1.0201 (1.631 secs)\n",
      "canp:canp_matern step 45600 lr 2.844e-04 [train_loss] loss -0.9588 (1.642 secs)\n",
      "canp:canp_matern step 45800 lr 2.829e-04 [train_loss] loss -0.9601 (1.622 secs)\n",
      "canp:canp_matern step 46000 lr 2.813e-04 [train_loss] loss -1.0121 (1.625 secs)\n",
      "canp:canp_matern step 46200 lr 2.798e-04 [train_loss] loss -1.0243 (1.659 secs)\n",
      "canp:canp_matern step 46400 lr 2.782e-04 [train_loss] loss -1.0061 (1.711 secs)\n",
      "canp:canp_matern step 46600 lr 2.767e-04 [train_loss] loss -0.9856 (1.669 secs)\n",
      "canp:canp_matern step 46800 lr 2.751e-04 [train_loss] loss -1.0415 (1.869 secs)\n",
      "canp:canp_matern step 47000 lr 2.735e-04 [train_loss] loss -1.0347 (1.905 secs)\n",
      "canp:canp_matern step 47200 lr 2.720e-04 [train_loss] loss -0.9662 (1.677 secs)\n",
      "canp:canp_matern step 47400 lr 2.704e-04 [train_loss] loss -1.0198 (1.626 secs)\n",
      "canp:canp_matern step 47600 lr 2.688e-04 [train_loss] loss -1.0306 (1.626 secs)\n",
      "canp:canp_matern step 47800 lr 2.673e-04 [train_loss] loss -0.9970 (1.597 secs)\n",
      "canp:canp_matern step 48000 lr 2.657e-04 [train_loss] loss -0.9678 (1.588 secs)\n",
      "canp:canp_matern step 48200 lr 2.641e-04 [train_loss] loss -1.0125 (1.669 secs)\n",
      "canp:canp_matern step 48400 lr 2.626e-04 [train_loss] loss -0.9939 (1.756 secs)\n",
      "canp:canp_matern step 48600 lr 2.610e-04 [train_loss] loss -1.0556 (1.675 secs)\n",
      "canp:canp_matern step 48800 lr 2.594e-04 [train_loss] loss -1.0003 (2.043 secs)\n",
      "canp:canp_matern step 49000 lr 2.579e-04 [train_loss] loss -0.9683 (1.697 secs)\n",
      "canp:canp_matern step 49200 lr 2.563e-04 [train_loss] loss -0.9278 (1.717 secs)\n",
      "canp:canp_matern step 49400 lr 2.547e-04 [train_loss] loss -0.9719 (1.711 secs)\n",
      "canp:canp_matern step 49600 lr 2.531e-04 [train_loss] loss -1.0147 (1.727 secs)\n",
      "canp:canp_matern step 49800 lr 2.516e-04 [train_loss] loss -1.0339 (1.732 secs)\n",
      "canp:canp_matern step 50000 lr 2.500e-04 [train_loss] loss -0.9894 (1.777 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:07<00:00, 424.64it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3402 tar_ll 0.5634 (7.066 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 50200 lr 2.484e-04 [train_loss] loss -1.0257 (1.915 secs)\n",
      "canp:canp_matern step 50400 lr 2.469e-04 [train_loss] loss -1.0351 (1.700 secs)\n",
      "canp:canp_matern step 50600 lr 2.453e-04 [train_loss] loss -1.0346 (1.684 secs)\n",
      "canp:canp_matern step 50800 lr 2.437e-04 [train_loss] loss -0.9990 (1.665 secs)\n",
      "canp:canp_matern step 51000 lr 2.421e-04 [train_loss] loss -1.0460 (1.650 secs)\n",
      "canp:canp_matern step 51200 lr 2.406e-04 [train_loss] loss -1.0008 (1.647 secs)\n",
      "canp:canp_matern step 51400 lr 2.390e-04 [train_loss] loss -1.0625 (1.723 secs)\n",
      "canp:canp_matern step 51600 lr 2.374e-04 [train_loss] loss -1.0473 (1.961 secs)\n",
      "canp:canp_matern step 51800 lr 2.359e-04 [train_loss] loss -0.9989 (1.694 secs)\n",
      "canp:canp_matern step 52000 lr 2.343e-04 [train_loss] loss -1.0539 (1.657 secs)\n",
      "canp:canp_matern step 52200 lr 2.327e-04 [train_loss] loss -1.0276 (1.648 secs)\n",
      "canp:canp_matern step 52400 lr 2.312e-04 [train_loss] loss -1.0502 (1.618 secs)\n",
      "canp:canp_matern step 52600 lr 2.296e-04 [train_loss] loss -0.9947 (1.631 secs)\n",
      "canp:canp_matern step 52800 lr 2.280e-04 [train_loss] loss -1.0630 (1.658 secs)\n",
      "canp:canp_matern step 53000 lr 2.265e-04 [train_loss] loss -1.0316 (1.770 secs)\n",
      "canp:canp_matern step 53200 lr 2.249e-04 [train_loss] loss -0.9639 (1.649 secs)\n",
      "canp:canp_matern step 53400 lr 2.233e-04 [train_loss] loss -1.0651 (2.000 secs)\n",
      "canp:canp_matern step 53600 lr 2.218e-04 [train_loss] loss -1.0131 (1.809 secs)\n",
      "canp:canp_matern step 53800 lr 2.202e-04 [train_loss] loss -1.0500 (1.643 secs)\n",
      "canp:canp_matern step 54000 lr 2.187e-04 [train_loss] loss -1.0434 (1.634 secs)\n",
      "canp:canp_matern step 54200 lr 2.171e-04 [train_loss] loss -0.9975 (1.662 secs)\n",
      "canp:canp_matern step 54400 lr 2.156e-04 [train_loss] loss -1.0133 (1.842 secs)\n",
      "canp:canp_matern step 54600 lr 2.140e-04 [train_loss] loss -1.0194 (1.729 secs)\n",
      "canp:canp_matern step 54800 lr 2.124e-04 [train_loss] loss -1.0701 (1.740 secs)\n",
      "canp:canp_matern step 55000 lr 2.109e-04 [train_loss] loss -1.0454 (1.713 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 436.90it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3540 tar_ll 0.5616 (6.870 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 55200 lr 2.093e-04 [train_loss] loss -0.9878 (1.680 secs)\n",
      "canp:canp_matern step 55400 lr 2.078e-04 [train_loss] loss -1.0366 (1.667 secs)\n",
      "canp:canp_matern step 55600 lr 2.062e-04 [train_loss] loss -1.0034 (1.661 secs)\n",
      "canp:canp_matern step 55800 lr 2.047e-04 [train_loss] loss -0.9930 (1.670 secs)\n",
      "canp:canp_matern step 56000 lr 2.032e-04 [train_loss] loss -1.0469 (1.645 secs)\n",
      "canp:canp_matern step 56200 lr 2.016e-04 [train_loss] loss -1.0117 (1.828 secs)\n",
      "canp:canp_matern step 56400 lr 2.001e-04 [train_loss] loss -0.9787 (1.852 secs)\n",
      "canp:canp_matern step 56600 lr 1.985e-04 [train_loss] loss -1.0970 (1.676 secs)\n",
      "canp:canp_matern step 56800 lr 1.970e-04 [train_loss] loss -1.0640 (1.652 secs)\n",
      "canp:canp_matern step 57000 lr 1.955e-04 [train_loss] loss -1.0566 (1.650 secs)\n",
      "canp:canp_matern step 57200 lr 1.939e-04 [train_loss] loss -1.0393 (1.643 secs)\n",
      "canp:canp_matern step 57400 lr 1.924e-04 [train_loss] loss -1.0139 (1.650 secs)\n",
      "canp:canp_matern step 57600 lr 1.909e-04 [train_loss] loss -1.0440 (1.618 secs)\n",
      "canp:canp_matern step 57800 lr 1.894e-04 [train_loss] loss -1.0224 (1.738 secs)\n",
      "canp:canp_matern step 58000 lr 1.878e-04 [train_loss] loss -1.0151 (1.791 secs)\n",
      "canp:canp_matern step 58200 lr 1.863e-04 [train_loss] loss -1.0379 (1.994 secs)\n",
      "canp:canp_matern step 58400 lr 1.848e-04 [train_loss] loss -1.0179 (1.652 secs)\n",
      "canp:canp_matern step 58600 lr 1.833e-04 [train_loss] loss -1.0355 (1.640 secs)\n",
      "canp:canp_matern step 58800 lr 1.818e-04 [train_loss] loss -1.0164 (1.663 secs)\n",
      "canp:canp_matern step 59000 lr 1.803e-04 [train_loss] loss -1.0457 (1.619 secs)\n",
      "canp:canp_matern step 59200 lr 1.787e-04 [train_loss] loss -1.0598 (1.664 secs)\n",
      "canp:canp_matern step 59400 lr 1.772e-04 [train_loss] loss -1.0216 (1.643 secs)\n",
      "canp:canp_matern step 59600 lr 1.757e-04 [train_loss] loss -1.0642 (1.651 secs)\n",
      "canp:canp_matern step 59800 lr 1.742e-04 [train_loss] loss -1.0493 (1.641 secs)\n",
      "canp:canp_matern step 60000 lr 1.727e-04 [train_loss] loss -1.0643 (1.965 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 465.63it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3579 tar_ll 0.5798 (6.445 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 60200 lr 1.713e-04 [train_loss] loss -1.0248 (1.654 secs)\n",
      "canp:canp_matern step 60400 lr 1.698e-04 [train_loss] loss -1.0697 (1.647 secs)\n",
      "canp:canp_matern step 60600 lr 1.683e-04 [train_loss] loss -1.0389 (1.628 secs)\n",
      "canp:canp_matern step 60800 lr 1.668e-04 [train_loss] loss -1.0478 (1.661 secs)\n",
      "canp:canp_matern step 61000 lr 1.653e-04 [train_loss] loss -1.0116 (1.864 secs)\n",
      "canp:canp_matern step 61200 lr 1.638e-04 [train_loss] loss -0.9446 (1.829 secs)\n",
      "canp:canp_matern step 61400 lr 1.624e-04 [train_loss] loss -1.0554 (1.635 secs)\n",
      "canp:canp_matern step 61600 lr 1.609e-04 [train_loss] loss -0.9929 (1.650 secs)\n",
      "canp:canp_matern step 61800 lr 1.594e-04 [train_loss] loss -1.0856 (1.645 secs)\n",
      "canp:canp_matern step 62000 lr 1.580e-04 [train_loss] loss -1.0107 (1.658 secs)\n",
      "canp:canp_matern step 62200 lr 1.565e-04 [train_loss] loss -1.0280 (1.629 secs)\n",
      "canp:canp_matern step 62400 lr 1.551e-04 [train_loss] loss -1.0354 (1.803 secs)\n",
      "canp:canp_matern step 62600 lr 1.536e-04 [train_loss] loss -1.0526 (1.627 secs)\n",
      "canp:canp_matern step 62800 lr 1.522e-04 [train_loss] loss -1.0296 (1.695 secs)\n",
      "canp:canp_matern step 63000 lr 1.507e-04 [train_loss] loss -0.9728 (1.915 secs)\n",
      "canp:canp_matern step 63200 lr 1.493e-04 [train_loss] loss -1.0271 (1.684 secs)\n",
      "canp:canp_matern step 63400 lr 1.478e-04 [train_loss] loss -0.9895 (1.641 secs)\n",
      "canp:canp_matern step 63600 lr 1.464e-04 [train_loss] loss -1.0497 (1.635 secs)\n",
      "canp:canp_matern step 63800 lr 1.450e-04 [train_loss] loss -1.0154 (1.615 secs)\n",
      "canp:canp_matern step 64000 lr 1.436e-04 [train_loss] loss -1.0219 (1.615 secs)\n",
      "canp:canp_matern step 64200 lr 1.421e-04 [train_loss] loss -1.0329 (1.689 secs)\n",
      "canp:canp_matern step 64400 lr 1.407e-04 [train_loss] loss -1.0913 (1.749 secs)\n",
      "canp:canp_matern step 64600 lr 1.393e-04 [train_loss] loss -1.0475 (1.831 secs)\n",
      "canp:canp_matern step 64800 lr 1.379e-04 [train_loss] loss -1.0452 (2.044 secs)\n",
      "canp:canp_matern step 65000 lr 1.365e-04 [train_loss] loss -1.0611 (1.826 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 444.53it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3526 tar_ll 0.5845 (6.750 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 65200 lr 1.351e-04 [train_loss] loss -1.0021 (1.668 secs)\n",
      "canp:canp_matern step 65400 lr 1.337e-04 [train_loss] loss -1.0934 (1.712 secs)\n",
      "canp:canp_matern step 65600 lr 1.323e-04 [train_loss] loss -1.0672 (1.840 secs)\n",
      "canp:canp_matern step 65800 lr 1.309e-04 [train_loss] loss -1.0846 (2.192 secs)\n",
      "canp:canp_matern step 66000 lr 1.296e-04 [train_loss] loss -1.0562 (1.747 secs)\n",
      "canp:canp_matern step 66200 lr 1.282e-04 [train_loss] loss -1.0527 (1.726 secs)\n",
      "canp:canp_matern step 66400 lr 1.268e-04 [train_loss] loss -1.0385 (1.640 secs)\n",
      "canp:canp_matern step 66600 lr 1.255e-04 [train_loss] loss -1.0410 (1.657 secs)\n",
      "canp:canp_matern step 66800 lr 1.241e-04 [train_loss] loss -1.0067 (1.708 secs)\n",
      "canp:canp_matern step 67000 lr 1.227e-04 [train_loss] loss -1.0071 (1.811 secs)\n",
      "canp:canp_matern step 67200 lr 1.214e-04 [train_loss] loss -1.0564 (1.667 secs)\n",
      "canp:canp_matern step 67400 lr 1.200e-04 [train_loss] loss -1.0265 (1.681 secs)\n",
      "canp:canp_matern step 67600 lr 1.187e-04 [train_loss] loss -1.0731 (2.089 secs)\n",
      "canp:canp_matern step 67800 lr 1.174e-04 [train_loss] loss -1.0631 (1.675 secs)\n",
      "canp:canp_matern step 68000 lr 1.160e-04 [train_loss] loss -1.0519 (1.684 secs)\n",
      "canp:canp_matern step 68200 lr 1.147e-04 [train_loss] loss -1.0873 (1.697 secs)\n",
      "canp:canp_matern step 68400 lr 1.134e-04 [train_loss] loss -1.0418 (1.683 secs)\n",
      "canp:canp_matern step 68600 lr 1.121e-04 [train_loss] loss -1.0390 (1.718 secs)\n",
      "canp:canp_matern step 68800 lr 1.108e-04 [train_loss] loss -1.1043 (1.754 secs)\n",
      "canp:canp_matern step 69000 lr 1.095e-04 [train_loss] loss -1.0807 (1.770 secs)\n",
      "canp:canp_matern step 69200 lr 1.082e-04 [train_loss] loss -1.0589 (1.805 secs)\n",
      "canp:canp_matern step 69400 lr 1.069e-04 [train_loss] loss -1.0158 (2.081 secs)\n",
      "canp:canp_matern step 69600 lr 1.056e-04 [train_loss] loss -1.0587 (1.773 secs)\n",
      "canp:canp_matern step 69800 lr 1.043e-04 [train_loss] loss -1.0660 (1.729 secs)\n",
      "canp:canp_matern step 70000 lr 1.031e-04 [train_loss] loss -1.0868 (1.737 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:07<00:00, 423.00it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3664 tar_ll 0.6017 (7.094 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 70200 lr 1.018e-04 [train_loss] loss -1.0353 (1.859 secs)\n",
      "canp:canp_matern step 70400 lr 1.005e-04 [train_loss] loss -1.0225 (1.934 secs)\n",
      "canp:canp_matern step 70600 lr 9.927e-05 [train_loss] loss -1.0505 (1.717 secs)\n",
      "canp:canp_matern step 70800 lr 9.802e-05 [train_loss] loss -1.0456 (1.718 secs)\n",
      "canp:canp_matern step 71000 lr 9.677e-05 [train_loss] loss -1.0770 (1.768 secs)\n",
      "canp:canp_matern step 71200 lr 9.554e-05 [train_loss] loss -1.0487 (1.708 secs)\n",
      "canp:canp_matern step 71400 lr 9.430e-05 [train_loss] loss -1.0562 (1.754 secs)\n",
      "canp:canp_matern step 71600 lr 9.308e-05 [train_loss] loss -1.0210 (1.803 secs)\n",
      "canp:canp_matern step 71800 lr 9.186e-05 [train_loss] loss -1.0695 (1.718 secs)\n",
      "canp:canp_matern step 72000 lr 9.064e-05 [train_loss] loss -1.0229 (1.881 secs)\n",
      "canp:canp_matern step 72200 lr 8.944e-05 [train_loss] loss -1.0633 (2.055 secs)\n",
      "canp:canp_matern step 72400 lr 8.824e-05 [train_loss] loss -1.0338 (1.722 secs)\n",
      "canp:canp_matern step 72600 lr 8.704e-05 [train_loss] loss -0.9993 (1.709 secs)\n",
      "canp:canp_matern step 72800 lr 8.585e-05 [train_loss] loss -1.0609 (1.718 secs)\n",
      "canp:canp_matern step 73000 lr 8.467e-05 [train_loss] loss -1.0372 (1.679 secs)\n",
      "canp:canp_matern step 73200 lr 8.350e-05 [train_loss] loss -1.0699 (1.724 secs)\n",
      "canp:canp_matern step 73400 lr 8.233e-05 [train_loss] loss -1.0006 (1.711 secs)\n",
      "canp:canp_matern step 73600 lr 8.117e-05 [train_loss] loss -1.0567 (1.707 secs)\n",
      "canp:canp_matern step 73800 lr 8.001e-05 [train_loss] loss -1.0485 (2.103 secs)\n",
      "canp:canp_matern step 74000 lr 7.886e-05 [train_loss] loss -1.0202 (2.179 secs)\n",
      "canp:canp_matern step 74200 lr 7.772e-05 [train_loss] loss -1.0154 (1.769 secs)\n",
      "canp:canp_matern step 74400 lr 7.659e-05 [train_loss] loss -0.9965 (1.725 secs)\n",
      "canp:canp_matern step 74600 lr 7.546e-05 [train_loss] loss -1.0135 (1.717 secs)\n",
      "canp:canp_matern step 74800 lr 7.434e-05 [train_loss] loss -1.0513 (1.746 secs)\n",
      "canp:canp_matern step 75000 lr 7.322e-05 [train_loss] loss -1.0751 (1.724 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:07<00:00, 408.08it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3674 tar_ll 0.6064 (7.353 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 75200 lr 7.212e-05 [train_loss] loss -1.0607 (1.751 secs)\n",
      "canp:canp_matern step 75400 lr 7.102e-05 [train_loss] loss -1.0107 (1.741 secs)\n",
      "canp:canp_matern step 75600 lr 6.992e-05 [train_loss] loss -1.0601 (1.756 secs)\n",
      "canp:canp_matern step 75800 lr 6.884e-05 [train_loss] loss -1.0500 (1.763 secs)\n",
      "canp:canp_matern step 76000 lr 6.776e-05 [train_loss] loss -1.0789 (2.145 secs)\n",
      "canp:canp_matern step 76200 lr 6.669e-05 [train_loss] loss -1.0598 (2.129 secs)\n",
      "canp:canp_matern step 76400 lr 6.562e-05 [train_loss] loss -1.0711 (1.973 secs)\n",
      "canp:canp_matern step 76600 lr 6.456e-05 [train_loss] loss -1.1033 (1.824 secs)\n",
      "canp:canp_matern step 76800 lr 6.351e-05 [train_loss] loss -1.0424 (1.606 secs)\n",
      "canp:canp_matern step 77000 lr 6.247e-05 [train_loss] loss -1.1341 (1.647 secs)\n",
      "canp:canp_matern step 77200 lr 6.144e-05 [train_loss] loss -1.0397 (1.639 secs)\n",
      "canp:canp_matern step 77400 lr 6.041e-05 [train_loss] loss -1.0570 (1.679 secs)\n",
      "canp:canp_matern step 77600 lr 5.939e-05 [train_loss] loss -1.0565 (1.822 secs)\n",
      "canp:canp_matern step 77800 lr 5.838e-05 [train_loss] loss -1.0908 (1.808 secs)\n",
      "canp:canp_matern step 78000 lr 5.737e-05 [train_loss] loss -1.0565 (1.716 secs)\n",
      "canp:canp_matern step 78200 lr 5.637e-05 [train_loss] loss -1.0880 (1.852 secs)\n",
      "canp:canp_matern step 78400 lr 5.538e-05 [train_loss] loss -1.0477 (1.895 secs)\n",
      "canp:canp_matern step 78600 lr 5.440e-05 [train_loss] loss -1.0525 (1.636 secs)\n",
      "canp:canp_matern step 78800 lr 5.343e-05 [train_loss] loss -1.0683 (1.638 secs)\n",
      "canp:canp_matern step 79000 lr 5.246e-05 [train_loss] loss -1.0290 (1.618 secs)\n",
      "canp:canp_matern step 79200 lr 5.150e-05 [train_loss] loss -1.0319 (1.656 secs)\n",
      "canp:canp_matern step 79400 lr 5.055e-05 [train_loss] loss -1.0164 (1.612 secs)\n",
      "canp:canp_matern step 79600 lr 4.961e-05 [train_loss] loss -1.0091 (1.646 secs)\n",
      "canp:canp_matern step 79800 lr 4.867e-05 [train_loss] loss -1.0535 (1.607 secs)\n",
      "canp:canp_matern step 80000 lr 4.775e-05 [train_loss] loss -1.0634 (1.603 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 438.74it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3681 tar_ll 0.6108 (6.840 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 80200 lr 4.683e-05 [train_loss] loss -1.0490 (1.674 secs)\n",
      "canp:canp_matern step 80400 lr 4.592e-05 [train_loss] loss -1.0254 (1.637 secs)\n",
      "canp:canp_matern step 80600 lr 4.501e-05 [train_loss] loss -1.0488 (1.710 secs)\n",
      "canp:canp_matern step 80800 lr 4.412e-05 [train_loss] loss -1.0658 (1.628 secs)\n",
      "canp:canp_matern step 81000 lr 4.323e-05 [train_loss] loss -1.0901 (1.689 secs)\n",
      "canp:canp_matern step 81200 lr 4.235e-05 [train_loss] loss -1.1017 (1.899 secs)\n",
      "canp:canp_matern step 81400 lr 4.148e-05 [train_loss] loss -1.0411 (1.820 secs)\n",
      "canp:canp_matern step 81600 lr 4.062e-05 [train_loss] loss -1.1106 (1.674 secs)\n",
      "canp:canp_matern step 81800 lr 3.976e-05 [train_loss] loss -1.1010 (1.824 secs)\n",
      "canp:canp_matern step 82000 lr 3.892e-05 [train_loss] loss -1.0430 (1.655 secs)\n",
      "canp:canp_matern step 82200 lr 3.808e-05 [train_loss] loss -1.0944 (1.708 secs)\n",
      "canp:canp_matern step 82400 lr 3.725e-05 [train_loss] loss -1.0508 (1.730 secs)\n",
      "canp:canp_matern step 82600 lr 3.643e-05 [train_loss] loss -1.0071 (1.823 secs)\n",
      "canp:canp_matern step 82800 lr 3.562e-05 [train_loss] loss -1.0607 (1.658 secs)\n",
      "canp:canp_matern step 83000 lr 3.481e-05 [train_loss] loss -1.0403 (1.859 secs)\n",
      "canp:canp_matern step 83200 lr 3.402e-05 [train_loss] loss -1.0644 (1.842 secs)\n",
      "canp:canp_matern step 83400 lr 3.323e-05 [train_loss] loss -1.0185 (1.632 secs)\n",
      "canp:canp_matern step 83600 lr 3.245e-05 [train_loss] loss -1.0488 (1.658 secs)\n",
      "canp:canp_matern step 83800 lr 3.168e-05 [train_loss] loss -1.0599 (1.630 secs)\n",
      "canp:canp_matern step 84000 lr 3.092e-05 [train_loss] loss -1.0710 (1.715 secs)\n",
      "canp:canp_matern step 84200 lr 3.017e-05 [train_loss] loss -1.0857 (1.731 secs)\n",
      "canp:canp_matern step 84400 lr 2.943e-05 [train_loss] loss -1.0849 (1.669 secs)\n",
      "canp:canp_matern step 84600 lr 2.869e-05 [train_loss] loss -1.0817 (1.689 secs)\n",
      "canp:canp_matern step 84800 lr 2.797e-05 [train_loss] loss -1.0330 (1.870 secs)\n",
      "canp:canp_matern step 85000 lr 2.725e-05 [train_loss] loss -1.0814 (2.098 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 437.68it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3622 tar_ll 0.6101 (6.856 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 85200 lr 2.654e-05 [train_loss] loss -1.0955 (1.676 secs)\n",
      "canp:canp_matern step 85400 lr 2.584e-05 [train_loss] loss -1.0242 (1.654 secs)\n",
      "canp:canp_matern step 85600 lr 2.515e-05 [train_loss] loss -1.0868 (1.624 secs)\n",
      "canp:canp_matern step 85800 lr 2.447e-05 [train_loss] loss -1.0299 (1.682 secs)\n",
      "canp:canp_matern step 86000 lr 2.379e-05 [train_loss] loss -1.0569 (1.667 secs)\n",
      "canp:canp_matern step 86200 lr 2.313e-05 [train_loss] loss -1.0521 (1.635 secs)\n",
      "canp:canp_matern step 86400 lr 2.247e-05 [train_loss] loss -1.0242 (1.630 secs)\n",
      "canp:canp_matern step 86600 lr 2.183e-05 [train_loss] loss -1.0202 (1.633 secs)\n",
      "canp:canp_matern step 86800 lr 2.119e-05 [train_loss] loss -1.0657 (1.604 secs)\n",
      "canp:canp_matern step 87000 lr 2.056e-05 [train_loss] loss -1.0729 (1.659 secs)\n",
      "canp:canp_matern step 87200 lr 1.994e-05 [train_loss] loss -1.0594 (1.698 secs)\n",
      "canp:canp_matern step 87400 lr 1.933e-05 [train_loss] loss -1.1182 (1.622 secs)\n",
      "canp:canp_matern step 87600 lr 1.873e-05 [train_loss] loss -1.0678 (1.624 secs)\n",
      "canp:canp_matern step 87800 lr 1.814e-05 [train_loss] loss -1.0551 (1.782 secs)\n",
      "canp:canp_matern step 88000 lr 1.756e-05 [train_loss] loss -0.9718 (1.627 secs)\n",
      "canp:canp_matern step 88200 lr 1.698e-05 [train_loss] loss -1.0320 (1.618 secs)\n",
      "canp:canp_matern step 88400 lr 1.642e-05 [train_loss] loss -1.0586 (1.629 secs)\n",
      "canp:canp_matern step 88600 lr 1.586e-05 [train_loss] loss -0.9752 (1.622 secs)\n",
      "canp:canp_matern step 88800 lr 1.532e-05 [train_loss] loss -1.0395 (1.639 secs)\n",
      "canp:canp_matern step 89000 lr 1.478e-05 [train_loss] loss -1.0495 (1.645 secs)\n",
      "canp:canp_matern step 89200 lr 1.425e-05 [train_loss] loss -1.1002 (1.663 secs)\n",
      "canp:canp_matern step 89400 lr 1.373e-05 [train_loss] loss -1.0810 (1.628 secs)\n",
      "canp:canp_matern step 89600 lr 1.323e-05 [train_loss] loss -1.0637 (1.662 secs)\n",
      "canp:canp_matern step 89800 lr 1.273e-05 [train_loss] loss -1.0575 (1.807 secs)\n",
      "canp:canp_matern step 90000 lr 1.224e-05 [train_loss] loss -1.0960 (1.727 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 470.11it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3711 tar_ll 0.6169 (6.384 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 90200 lr 1.176e-05 [train_loss] loss -1.0187 (1.756 secs)\n",
      "canp:canp_matern step 90400 lr 1.128e-05 [train_loss] loss -1.0994 (1.722 secs)\n",
      "canp:canp_matern step 90600 lr 1.082e-05 [train_loss] loss -1.0633 (1.727 secs)\n",
      "canp:canp_matern step 90800 lr 1.037e-05 [train_loss] loss -0.9953 (1.761 secs)\n",
      "canp:canp_matern step 91000 lr 9.927e-06 [train_loss] loss -1.0571 (1.642 secs)\n",
      "canp:canp_matern step 91200 lr 9.493e-06 [train_loss] loss -1.0427 (1.652 secs)\n",
      "canp:canp_matern step 91400 lr 9.069e-06 [train_loss] loss -1.0902 (1.640 secs)\n",
      "canp:canp_matern step 91600 lr 8.655e-06 [train_loss] loss -1.0470 (1.610 secs)\n",
      "canp:canp_matern step 91800 lr 8.250e-06 [train_loss] loss -1.0330 (1.612 secs)\n",
      "canp:canp_matern step 92000 lr 7.854e-06 [train_loss] loss -1.0330 (1.660 secs)\n",
      "canp:canp_matern step 92200 lr 7.468e-06 [train_loss] loss -1.0507 (1.660 secs)\n",
      "canp:canp_matern step 92400 lr 7.092e-06 [train_loss] loss -1.0468 (1.651 secs)\n",
      "canp:canp_matern step 92600 lr 6.725e-06 [train_loss] loss -1.1043 (1.679 secs)\n",
      "canp:canp_matern step 92800 lr 6.368e-06 [train_loss] loss -1.0745 (1.752 secs)\n",
      "canp:canp_matern step 93000 lr 6.021e-06 [train_loss] loss -1.0503 (1.638 secs)\n",
      "canp:canp_matern step 93200 lr 5.683e-06 [train_loss] loss -1.0693 (1.632 secs)\n",
      "canp:canp_matern step 93400 lr 5.355e-06 [train_loss] loss -1.0011 (1.623 secs)\n",
      "canp:canp_matern step 93600 lr 5.036e-06 [train_loss] loss -1.0340 (1.623 secs)\n",
      "canp:canp_matern step 93800 lr 4.727e-06 [train_loss] loss -1.0130 (1.631 secs)\n",
      "canp:canp_matern step 94000 lr 4.428e-06 [train_loss] loss -1.0707 (1.612 secs)\n",
      "canp:canp_matern step 94200 lr 4.139e-06 [train_loss] loss -1.0907 (1.668 secs)\n",
      "canp:canp_matern step 94400 lr 3.859e-06 [train_loss] loss -1.1152 (1.626 secs)\n",
      "canp:canp_matern step 94600 lr 3.589e-06 [train_loss] loss -0.9846 (1.694 secs)\n",
      "canp:canp_matern step 94800 lr 3.329e-06 [train_loss] loss -1.0233 (1.689 secs)\n",
      "canp:canp_matern step 95000 lr 3.078e-06 [train_loss] loss -1.1037 (1.644 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 440.88it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3715 tar_ll 0.6162 (6.806 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "canp:canp_matern step 95200 lr 2.837e-06 [train_loss] loss -1.0752 (1.774 secs)\n",
      "canp:canp_matern step 95400 lr 2.606e-06 [train_loss] loss -1.0779 (1.774 secs)\n",
      "canp:canp_matern step 95600 lr 2.385e-06 [train_loss] loss -1.0424 (1.770 secs)\n",
      "canp:canp_matern step 95800 lr 2.173e-06 [train_loss] loss -1.0025 (1.615 secs)\n",
      "canp:canp_matern step 96000 lr 1.971e-06 [train_loss] loss -1.0777 (1.659 secs)\n",
      "canp:canp_matern step 96200 lr 1.779e-06 [train_loss] loss -1.0690 (1.615 secs)\n",
      "canp:canp_matern step 96400 lr 1.597e-06 [train_loss] loss -1.0212 (1.632 secs)\n",
      "canp:canp_matern step 96600 lr 1.425e-06 [train_loss] loss -1.0653 (1.624 secs)\n",
      "canp:canp_matern step 96800 lr 1.262e-06 [train_loss] loss -1.0241 (1.594 secs)\n",
      "canp:canp_matern step 97000 lr 1.110e-06 [train_loss] loss -1.0978 (1.621 secs)\n",
      "canp:canp_matern step 97200 lr 9.666e-07 [train_loss] loss -1.0799 (1.625 secs)\n",
      "canp:canp_matern step 97400 lr 8.335e-07 [train_loss] loss -1.0737 (1.622 secs)\n",
      "canp:canp_matern step 97600 lr 7.103e-07 [train_loss] loss -1.0878 (1.766 secs)\n",
      "canp:canp_matern step 97800 lr 5.969e-07 [train_loss] loss -1.0501 (1.629 secs)\n",
      "canp:canp_matern step 98000 lr 4.933e-07 [train_loss] loss -1.0596 (1.597 secs)\n",
      "canp:canp_matern step 98200 lr 3.996e-07 [train_loss] loss -1.0582 (1.652 secs)\n",
      "canp:canp_matern step 98400 lr 3.158e-07 [train_loss] loss -1.0337 (1.628 secs)\n",
      "canp:canp_matern step 98600 lr 2.418e-07 [train_loss] loss -1.0359 (1.614 secs)\n",
      "canp:canp_matern step 98800 lr 1.776e-07 [train_loss] loss -0.9532 (1.630 secs)\n",
      "canp:canp_matern step 99000 lr 1.234e-07 [train_loss] loss -1.0516 (1.600 secs)\n",
      "canp:canp_matern step 99200 lr 7.895e-08 [train_loss] loss -1.0235 (1.612 secs)\n",
      "canp:canp_matern step 99400 lr 4.441e-08 [train_loss] loss -1.0419 (1.641 secs)\n",
      "canp:canp_matern step 99600 lr 1.974e-08 [train_loss] loss -0.9883 (1.784 secs)\n",
      "canp:canp_matern step 99800 lr 4.935e-09 [train_loss] loss -1.0909 (1.642 secs)\n",
      "canp:canp_matern step 100000 lr 0.000e+00 [train_loss] loss -1.0295 (1.665 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 470.42it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3717 tar_ll 0.6158 (6.379 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 440.58it/s]\n",
      "canp:canp_matern matern ctx_ll 1.3717 tar_ll 0.6158 (6.811 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "Execution time: 1013480.6875 miliseconds\n",
      "Execution time: 1013.4806875 seconds\n",
      "Initial Memory Usage: 19.80224609375 MB\n",
      "Final Memory Usage: 50.16748046875 MB\n",
      "Memory Usage Change: 30.365234375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='canp', name='canp_matern',val_seed=100, val_l=None,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50733481-c1df-45f7-a199-6428b0e0d5b4",
   "metadata": {},
   "source": [
    "## NP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b819b05e-feca-40ce-9b3b-d0008b46d4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: np-np_matern\n",
      "Total number of parameters: 232194\n",
      "\n",
      "np:np_matern step 200 lr 5.000e-04 [train_loss] loss 0.6737 (1.976 secs)\n",
      "np:np_matern step 400 lr 5.000e-04 [train_loss] loss 0.5337 (1.900 secs)\n",
      "np:np_matern step 600 lr 5.000e-04 [train_loss] loss 0.5286 (1.930 secs)\n",
      "np:np_matern step 800 lr 4.999e-04 [train_loss] loss 0.5163 (1.990 secs)\n",
      "np:np_matern step 1000 lr 4.999e-04 [train_loss] loss 0.4655 (1.910 secs)\n",
      "np:np_matern step 1200 lr 4.998e-04 [train_loss] loss 0.4743 (1.891 secs)\n",
      "np:np_matern step 1400 lr 4.998e-04 [train_loss] loss 0.4481 (1.858 secs)\n",
      "np:np_matern step 1600 lr 4.997e-04 [train_loss] loss 0.4234 (1.765 secs)\n",
      "np:np_matern step 1800 lr 4.996e-04 [train_loss] loss 0.4259 (1.760 secs)\n",
      "np:np_matern step 2000 lr 4.995e-04 [train_loss] loss 0.4234 (1.844 secs)\n",
      "np:np_matern step 2200 lr 4.994e-04 [train_loss] loss 0.4238 (1.766 secs)\n",
      "np:np_matern step 2400 lr 4.993e-04 [train_loss] loss 0.3908 (1.740 secs)\n",
      "np:np_matern step 2600 lr 4.992e-04 [train_loss] loss 0.3974 (1.733 secs)\n",
      "np:np_matern step 2800 lr 4.990e-04 [train_loss] loss 0.3728 (1.739 secs)\n",
      "np:np_matern step 3000 lr 4.989e-04 [train_loss] loss 0.3383 (1.687 secs)\n",
      "np:np_matern step 3200 lr 4.987e-04 [train_loss] loss 0.3422 (1.674 secs)\n",
      "np:np_matern step 3400 lr 4.986e-04 [train_loss] loss 0.2993 (1.690 secs)\n",
      "np:np_matern step 3600 lr 4.984e-04 [train_loss] loss 0.3048 (1.670 secs)\n",
      "np:np_matern step 3800 lr 4.982e-04 [train_loss] loss 0.2872 (1.781 secs)\n",
      "np:np_matern step 4000 lr 4.980e-04 [train_loss] loss 0.2896 (1.693 secs)\n",
      "np:np_matern step 4200 lr 4.978e-04 [train_loss] loss 0.2843 (1.753 secs)\n",
      "np:np_matern step 4400 lr 4.976e-04 [train_loss] loss 0.2777 (1.770 secs)\n",
      "np:np_matern step 4600 lr 4.974e-04 [train_loss] loss 0.2794 (1.781 secs)\n",
      "np:np_matern step 4800 lr 4.972e-04 [train_loss] loss 0.2696 (1.702 secs)\n",
      "np:np_matern step 5000 lr 4.969e-04 [train_loss] loss 0.2634 (1.775 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 401.13it/s]\n",
      "np:np_matern matern ctx_ll -0.1638 tar_ll -0.4575 (7.479 secs)\n",
      "\n",
      "np:np_matern step 5200 lr 4.967e-04 [train_loss] loss 0.2619 (1.727 secs)\n",
      "np:np_matern step 5400 lr 4.964e-04 [train_loss] loss 0.2501 (1.795 secs)\n",
      "np:np_matern step 5600 lr 4.961e-04 [train_loss] loss 0.2205 (1.795 secs)\n",
      "np:np_matern step 5800 lr 4.959e-04 [train_loss] loss 0.2543 (1.770 secs)\n",
      "np:np_matern step 6000 lr 4.956e-04 [train_loss] loss 0.2356 (1.810 secs)\n",
      "np:np_matern step 6200 lr 4.953e-04 [train_loss] loss 0.2527 (1.740 secs)\n",
      "np:np_matern step 6400 lr 4.950e-04 [train_loss] loss 0.2357 (1.760 secs)\n",
      "np:np_matern step 6600 lr 4.946e-04 [train_loss] loss 0.2521 (1.730 secs)\n",
      "np:np_matern step 6800 lr 4.943e-04 [train_loss] loss 0.2361 (1.660 secs)\n",
      "np:np_matern step 7000 lr 4.940e-04 [train_loss] loss 0.2093 (1.730 secs)\n",
      "np:np_matern step 7200 lr 4.936e-04 [train_loss] loss 0.2225 (1.715 secs)\n",
      "np:np_matern step 7400 lr 4.933e-04 [train_loss] loss 0.1905 (1.720 secs)\n",
      "np:np_matern step 7600 lr 4.929e-04 [train_loss] loss 0.2166 (1.695 secs)\n",
      "np:np_matern step 7800 lr 4.925e-04 [train_loss] loss 0.1842 (1.800 secs)\n",
      "np:np_matern step 8000 lr 4.921e-04 [train_loss] loss 0.1906 (1.835 secs)\n",
      "np:np_matern step 8200 lr 4.918e-04 [train_loss] loss 0.2006 (1.799 secs)\n",
      "np:np_matern step 8400 lr 4.913e-04 [train_loss] loss 0.1784 (1.775 secs)\n",
      "np:np_matern step 8600 lr 4.909e-04 [train_loss] loss 0.1588 (1.825 secs)\n",
      "np:np_matern step 8800 lr 4.905e-04 [train_loss] loss 0.1750 (1.805 secs)\n",
      "np:np_matern step 9000 lr 4.901e-04 [train_loss] loss 0.1777 (1.645 secs)\n",
      "np:np_matern step 9200 lr 4.896e-04 [train_loss] loss 0.1609 (1.650 secs)\n",
      "np:np_matern step 9400 lr 4.892e-04 [train_loss] loss 0.1406 (1.710 secs)\n",
      "np:np_matern step 9600 lr 4.887e-04 [train_loss] loss 0.1266 (1.675 secs)\n",
      "np:np_matern step 9800 lr 4.882e-04 [train_loss] loss 0.1530 (1.665 secs)\n",
      "np:np_matern step 10000 lr 4.878e-04 [train_loss] loss 0.1251 (1.715 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 394.24it/s]\n",
      "np:np_matern matern ctx_ll -0.0438 tar_ll -0.3624 (7.610 secs)\n",
      "\n",
      "np:np_matern step 10200 lr 4.873e-04 [train_loss] loss 0.1202 (1.822 secs)\n",
      "np:np_matern step 10400 lr 4.868e-04 [train_loss] loss 0.1041 (1.795 secs)\n",
      "np:np_matern step 10600 lr 4.863e-04 [train_loss] loss 0.1167 (1.805 secs)\n",
      "np:np_matern step 10800 lr 4.857e-04 [train_loss] loss 0.0955 (1.775 secs)\n",
      "np:np_matern step 11000 lr 4.852e-04 [train_loss] loss 0.1073 (1.925 secs)\n",
      "np:np_matern step 11200 lr 4.847e-04 [train_loss] loss 0.0997 (1.795 secs)\n",
      "np:np_matern step 11400 lr 4.841e-04 [train_loss] loss 0.0699 (2.009 secs)\n",
      "np:np_matern step 11600 lr 4.836e-04 [train_loss] loss 0.0658 (1.990 secs)\n",
      "np:np_matern step 11800 lr 4.830e-04 [train_loss] loss 0.0948 (2.269 secs)\n",
      "np:np_matern step 12000 lr 4.824e-04 [train_loss] loss 0.0693 (2.083 secs)\n",
      "np:np_matern step 12200 lr 4.819e-04 [train_loss] loss 0.0660 (1.992 secs)\n",
      "np:np_matern step 12400 lr 4.813e-04 [train_loss] loss 0.0844 (1.897 secs)\n",
      "np:np_matern step 12600 lr 4.807e-04 [train_loss] loss 0.0529 (1.782 secs)\n",
      "np:np_matern step 12800 lr 4.801e-04 [train_loss] loss 0.0401 (2.075 secs)\n",
      "np:np_matern step 13000 lr 4.794e-04 [train_loss] loss 0.0817 (2.115 secs)\n",
      "np:np_matern step 13200 lr 4.788e-04 [train_loss] loss 0.0485 (1.930 secs)\n",
      "np:np_matern step 13400 lr 4.782e-04 [train_loss] loss 0.0678 (1.760 secs)\n",
      "np:np_matern step 13600 lr 4.775e-04 [train_loss] loss 0.0382 (1.845 secs)\n",
      "np:np_matern step 13800 lr 4.769e-04 [train_loss] loss 0.0121 (1.695 secs)\n",
      "np:np_matern step 14000 lr 4.762e-04 [train_loss] loss 0.0199 (1.675 secs)\n",
      "np:np_matern step 14200 lr 4.755e-04 [train_loss] loss 0.0245 (1.840 secs)\n",
      "np:np_matern step 14400 lr 4.749e-04 [train_loss] loss 0.0025 (1.862 secs)\n",
      "np:np_matern step 14600 lr 4.742e-04 [train_loss] loss 0.0114 (1.884 secs)\n",
      "np:np_matern step 14800 lr 4.735e-04 [train_loss] loss -0.0123 (1.820 secs)\n",
      "np:np_matern step 15000 lr 4.728e-04 [train_loss] loss 0.0278 (1.870 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 377.70it/s]\n",
      "np:np_matern matern ctx_ll 0.0723 tar_ll -0.2262 (7.953 secs)\n",
      "\n",
      "np:np_matern step 15200 lr 4.720e-04 [train_loss] loss 0.0140 (1.900 secs)\n",
      "np:np_matern step 15400 lr 4.713e-04 [train_loss] loss -0.0144 (1.720 secs)\n",
      "np:np_matern step 15600 lr 4.706e-04 [train_loss] loss -0.0469 (1.801 secs)\n",
      "np:np_matern step 15800 lr 4.698e-04 [train_loss] loss -0.0500 (1.779 secs)\n",
      "np:np_matern step 16000 lr 4.691e-04 [train_loss] loss -0.0485 (1.821 secs)\n",
      "np:np_matern step 16200 lr 4.683e-04 [train_loss] loss -0.0282 (1.970 secs)\n",
      "np:np_matern step 16400 lr 4.675e-04 [train_loss] loss -0.0336 (1.916 secs)\n",
      "np:np_matern step 16600 lr 4.668e-04 [train_loss] loss -0.0097 (1.924 secs)\n",
      "np:np_matern step 16800 lr 4.660e-04 [train_loss] loss -0.0599 (1.750 secs)\n",
      "np:np_matern step 17000 lr 4.652e-04 [train_loss] loss -0.0743 (1.796 secs)\n",
      "np:np_matern step 17200 lr 4.644e-04 [train_loss] loss -0.0791 (1.843 secs)\n",
      "np:np_matern step 17400 lr 4.636e-04 [train_loss] loss -0.0648 (1.860 secs)\n",
      "np:np_matern step 17600 lr 4.627e-04 [train_loss] loss -0.0262 (1.780 secs)\n",
      "np:np_matern step 17800 lr 4.619e-04 [train_loss] loss -0.0571 (1.830 secs)\n",
      "np:np_matern step 18000 lr 4.611e-04 [train_loss] loss -0.0959 (1.920 secs)\n",
      "np:np_matern step 18200 lr 4.602e-04 [train_loss] loss -0.0882 (1.885 secs)\n",
      "np:np_matern step 18400 lr 4.594e-04 [train_loss] loss -0.0722 (1.903 secs)\n",
      "np:np_matern step 18600 lr 4.585e-04 [train_loss] loss -0.0871 (1.900 secs)\n",
      "np:np_matern step 18800 lr 4.576e-04 [train_loss] loss -0.0558 (1.900 secs)\n",
      "np:np_matern step 19000 lr 4.568e-04 [train_loss] loss -0.1033 (1.870 secs)\n",
      "np:np_matern step 19200 lr 4.559e-04 [train_loss] loss -0.0763 (1.859 secs)\n",
      "np:np_matern step 19400 lr 4.550e-04 [train_loss] loss -0.0635 (2.040 secs)\n",
      "np:np_matern step 19600 lr 4.541e-04 [train_loss] loss -0.0889 (2.215 secs)\n",
      "np:np_matern step 19800 lr 4.532e-04 [train_loss] loss -0.0721 (2.265 secs)\n",
      "np:np_matern step 20000 lr 4.523e-04 [train_loss] loss -0.0574 (2.150 secs)\n",
      "100%|##########| 3000/3000 [00:08<00:00, 346.87it/s]\n",
      "np:np_matern matern ctx_ll 0.1823 tar_ll -0.1731 (8.650 secs)\n",
      "\n",
      "np:np_matern step 20200 lr 4.513e-04 [train_loss] loss -0.1243 (1.801 secs)\n",
      "np:np_matern step 20400 lr 4.504e-04 [train_loss] loss -0.1092 (1.809 secs)\n",
      "np:np_matern step 20600 lr 4.494e-04 [train_loss] loss -0.1114 (1.804 secs)\n",
      "np:np_matern step 20800 lr 4.485e-04 [train_loss] loss -0.1292 (1.783 secs)\n",
      "np:np_matern step 21000 lr 4.475e-04 [train_loss] loss -0.1052 (1.831 secs)\n",
      "np:np_matern step 21200 lr 4.466e-04 [train_loss] loss -0.1047 (1.874 secs)\n",
      "np:np_matern step 21400 lr 4.456e-04 [train_loss] loss -0.1365 (1.755 secs)\n",
      "np:np_matern step 21600 lr 4.446e-04 [train_loss] loss -0.1073 (1.835 secs)\n",
      "np:np_matern step 21800 lr 4.436e-04 [train_loss] loss -0.1058 (1.787 secs)\n",
      "np:np_matern step 22000 lr 4.426e-04 [train_loss] loss -0.1517 (1.872 secs)\n",
      "np:np_matern step 22200 lr 4.416e-04 [train_loss] loss -0.1703 (1.881 secs)\n",
      "np:np_matern step 22400 lr 4.406e-04 [train_loss] loss -0.1343 (1.779 secs)\n",
      "np:np_matern step 22600 lr 4.396e-04 [train_loss] loss -0.1582 (1.762 secs)\n",
      "np:np_matern step 22800 lr 4.386e-04 [train_loss] loss -0.1157 (1.847 secs)\n",
      "np:np_matern step 23000 lr 4.375e-04 [train_loss] loss -0.1501 (2.015 secs)\n",
      "np:np_matern step 23200 lr 4.365e-04 [train_loss] loss -0.1584 (1.770 secs)\n",
      "np:np_matern step 23400 lr 4.354e-04 [train_loss] loss -0.1425 (1.981 secs)\n",
      "np:np_matern step 23600 lr 4.344e-04 [train_loss] loss -0.1320 (2.085 secs)\n",
      "np:np_matern step 23800 lr 4.333e-04 [train_loss] loss -0.1444 (1.935 secs)\n",
      "np:np_matern step 24000 lr 4.322e-04 [train_loss] loss -0.1891 (1.814 secs)\n",
      "np:np_matern step 24200 lr 4.312e-04 [train_loss] loss -0.1513 (1.627 secs)\n",
      "np:np_matern step 24400 lr 4.301e-04 [train_loss] loss -0.1473 (1.640 secs)\n",
      "np:np_matern step 24600 lr 4.290e-04 [train_loss] loss -0.1479 (1.610 secs)\n",
      "np:np_matern step 24800 lr 4.279e-04 [train_loss] loss -0.1960 (1.700 secs)\n",
      "np:np_matern step 25000 lr 4.268e-04 [train_loss] loss -0.1948 (1.655 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 377.13it/s]\n",
      "np:np_matern matern ctx_ll 0.2733 tar_ll -0.1448 (7.955 secs)\n",
      "\n",
      "np:np_matern step 25200 lr 4.257e-04 [train_loss] loss -0.1565 (1.738 secs)\n",
      "np:np_matern step 25400 lr 4.245e-04 [train_loss] loss -0.1849 (1.680 secs)\n",
      "np:np_matern step 25600 lr 4.234e-04 [train_loss] loss -0.1694 (1.645 secs)\n",
      "np:np_matern step 25800 lr 4.223e-04 [train_loss] loss -0.1782 (1.644 secs)\n",
      "np:np_matern step 26000 lr 4.211e-04 [train_loss] loss -0.1665 (1.679 secs)\n",
      "np:np_matern step 26200 lr 4.200e-04 [train_loss] loss -0.2288 (1.881 secs)\n",
      "np:np_matern step 26400 lr 4.188e-04 [train_loss] loss -0.1669 (1.811 secs)\n",
      "np:np_matern step 26600 lr 4.177e-04 [train_loss] loss -0.1560 (1.781 secs)\n",
      "np:np_matern step 26800 lr 4.165e-04 [train_loss] loss -0.1961 (1.972 secs)\n",
      "np:np_matern step 27000 lr 4.153e-04 [train_loss] loss -0.2075 (1.685 secs)\n",
      "np:np_matern step 27200 lr 4.141e-04 [train_loss] loss -0.1997 (1.734 secs)\n",
      "np:np_matern step 27400 lr 4.130e-04 [train_loss] loss -0.2036 (1.779 secs)\n",
      "np:np_matern step 27600 lr 4.118e-04 [train_loss] loss -0.2080 (1.715 secs)\n",
      "np:np_matern step 27800 lr 4.106e-04 [train_loss] loss -0.2672 (1.665 secs)\n",
      "np:np_matern step 28000 lr 4.094e-04 [train_loss] loss -0.1746 (1.670 secs)\n",
      "np:np_matern step 28200 lr 4.081e-04 [train_loss] loss -0.1792 (1.690 secs)\n",
      "np:np_matern step 28400 lr 4.069e-04 [train_loss] loss -0.1914 (1.595 secs)\n",
      "np:np_matern step 28600 lr 4.057e-04 [train_loss] loss -0.2194 (1.615 secs)\n",
      "np:np_matern step 28800 lr 4.045e-04 [train_loss] loss -0.2351 (1.605 secs)\n",
      "np:np_matern step 29000 lr 4.032e-04 [train_loss] loss -0.2040 (1.645 secs)\n",
      "np:np_matern step 29200 lr 4.020e-04 [train_loss] loss -0.2089 (1.680 secs)\n",
      "np:np_matern step 29400 lr 4.007e-04 [train_loss] loss -0.2262 (1.650 secs)\n",
      "np:np_matern step 29600 lr 3.995e-04 [train_loss] loss -0.2524 (1.641 secs)\n",
      "np:np_matern step 29800 lr 3.982e-04 [train_loss] loss -0.2346 (1.644 secs)\n",
      "np:np_matern step 30000 lr 3.969e-04 [train_loss] loss -0.2141 (1.714 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 393.20it/s]\n",
      "np:np_matern matern ctx_ll 0.2973 tar_ll -0.1104 (7.630 secs)\n",
      "\n",
      "np:np_matern step 30200 lr 3.957e-04 [train_loss] loss -0.2376 (1.819 secs)\n",
      "np:np_matern step 30400 lr 3.944e-04 [train_loss] loss -0.2300 (1.781 secs)\n",
      "np:np_matern step 30600 lr 3.931e-04 [train_loss] loss -0.2008 (1.898 secs)\n",
      "np:np_matern step 30800 lr 3.918e-04 [train_loss] loss -0.2120 (1.943 secs)\n",
      "np:np_matern step 31000 lr 3.905e-04 [train_loss] loss -0.2641 (1.942 secs)\n",
      "np:np_matern step 31200 lr 3.892e-04 [train_loss] loss -0.2115 (1.845 secs)\n",
      "np:np_matern step 31400 lr 3.879e-04 [train_loss] loss -0.2246 (1.710 secs)\n",
      "np:np_matern step 31600 lr 3.866e-04 [train_loss] loss -0.2490 (1.720 secs)\n",
      "np:np_matern step 31800 lr 3.853e-04 [train_loss] loss -0.2544 (1.734 secs)\n",
      "np:np_matern step 32000 lr 3.840e-04 [train_loss] loss -0.2532 (1.910 secs)\n",
      "np:np_matern step 32200 lr 3.826e-04 [train_loss] loss -0.2568 (1.926 secs)\n",
      "np:np_matern step 32400 lr 3.813e-04 [train_loss] loss -0.2415 (1.870 secs)\n",
      "np:np_matern step 32600 lr 3.800e-04 [train_loss] loss -0.2412 (1.772 secs)\n",
      "np:np_matern step 32800 lr 3.786e-04 [train_loss] loss -0.2654 (1.922 secs)\n",
      "np:np_matern step 33000 lr 3.773e-04 [train_loss] loss -0.2183 (1.861 secs)\n",
      "np:np_matern step 33200 lr 3.759e-04 [train_loss] loss -0.2486 (1.940 secs)\n",
      "np:np_matern step 33400 lr 3.745e-04 [train_loss] loss -0.2245 (1.925 secs)\n",
      "np:np_matern step 33600 lr 3.732e-04 [train_loss] loss -0.2543 (1.945 secs)\n",
      "np:np_matern step 33800 lr 3.718e-04 [train_loss] loss -0.2699 (1.920 secs)\n",
      "np:np_matern step 34000 lr 3.704e-04 [train_loss] loss -0.2853 (2.015 secs)\n",
      "np:np_matern step 34200 lr 3.691e-04 [train_loss] loss -0.2277 (1.871 secs)\n",
      "np:np_matern step 34400 lr 3.677e-04 [train_loss] loss -0.2533 (1.930 secs)\n",
      "np:np_matern step 34600 lr 3.663e-04 [train_loss] loss -0.2634 (1.980 secs)\n",
      "np:np_matern step 34800 lr 3.649e-04 [train_loss] loss -0.2847 (1.975 secs)\n",
      "np:np_matern step 35000 lr 3.635e-04 [train_loss] loss -0.2718 (1.876 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 406.15it/s]\n",
      "np:np_matern matern ctx_ll 0.3416 tar_ll -0.0788 (7.386 secs)\n",
      "\n",
      "np:np_matern step 35200 lr 3.621e-04 [train_loss] loss -0.2704 (1.719 secs)\n",
      "np:np_matern step 35400 lr 3.607e-04 [train_loss] loss -0.3102 (1.713 secs)\n",
      "np:np_matern step 35600 lr 3.593e-04 [train_loss] loss -0.2763 (1.860 secs)\n",
      "np:np_matern step 35800 lr 3.579e-04 [train_loss] loss -0.2675 (1.720 secs)\n",
      "np:np_matern step 36000 lr 3.564e-04 [train_loss] loss -0.2941 (1.720 secs)\n",
      "np:np_matern step 36200 lr 3.550e-04 [train_loss] loss -0.2954 (1.780 secs)\n",
      "np:np_matern step 36400 lr 3.536e-04 [train_loss] loss -0.2856 (1.700 secs)\n",
      "np:np_matern step 36600 lr 3.522e-04 [train_loss] loss -0.2782 (1.700 secs)\n",
      "np:np_matern step 36800 lr 3.507e-04 [train_loss] loss -0.3009 (1.845 secs)\n",
      "np:np_matern step 37000 lr 3.493e-04 [train_loss] loss -0.2717 (1.965 secs)\n",
      "np:np_matern step 37200 lr 3.478e-04 [train_loss] loss -0.2892 (1.910 secs)\n",
      "np:np_matern step 37400 lr 3.464e-04 [train_loss] loss -0.3166 (1.910 secs)\n",
      "np:np_matern step 37600 lr 3.449e-04 [train_loss] loss -0.2309 (1.845 secs)\n",
      "np:np_matern step 37800 lr 3.435e-04 [train_loss] loss -0.2682 (1.815 secs)\n",
      "np:np_matern step 38000 lr 3.420e-04 [train_loss] loss -0.2904 (1.770 secs)\n",
      "np:np_matern step 38200 lr 3.406e-04 [train_loss] loss -0.3429 (1.820 secs)\n",
      "np:np_matern step 38400 lr 3.391e-04 [train_loss] loss -0.3113 (1.730 secs)\n",
      "np:np_matern step 38600 lr 3.376e-04 [train_loss] loss -0.3194 (1.740 secs)\n",
      "np:np_matern step 38800 lr 3.362e-04 [train_loss] loss -0.3156 (1.715 secs)\n",
      "np:np_matern step 39000 lr 3.347e-04 [train_loss] loss -0.3153 (1.640 secs)\n",
      "np:np_matern step 39200 lr 3.332e-04 [train_loss] loss -0.3395 (1.700 secs)\n",
      "np:np_matern step 39400 lr 3.317e-04 [train_loss] loss -0.2988 (1.700 secs)\n",
      "np:np_matern step 39600 lr 3.302e-04 [train_loss] loss -0.2773 (1.720 secs)\n",
      "np:np_matern step 39800 lr 3.287e-04 [train_loss] loss -0.3268 (1.780 secs)\n",
      "np:np_matern step 40000 lr 3.273e-04 [train_loss] loss -0.3234 (1.840 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 403.24it/s]\n",
      "np:np_matern matern ctx_ll 0.4021 tar_ll -0.0705 (7.445 secs)\n",
      "\n",
      "np:np_matern step 40200 lr 3.258e-04 [train_loss] loss -0.3132 (1.883 secs)\n",
      "np:np_matern step 40400 lr 3.243e-04 [train_loss] loss -0.2780 (1.926 secs)\n",
      "np:np_matern step 40600 lr 3.228e-04 [train_loss] loss -0.2918 (1.924 secs)\n",
      "np:np_matern step 40800 lr 3.213e-04 [train_loss] loss -0.3462 (1.895 secs)\n",
      "np:np_matern step 41000 lr 3.197e-04 [train_loss] loss -0.2881 (1.695 secs)\n",
      "np:np_matern step 41200 lr 3.182e-04 [train_loss] loss -0.3381 (1.695 secs)\n",
      "np:np_matern step 41400 lr 3.167e-04 [train_loss] loss -0.3211 (1.740 secs)\n",
      "np:np_matern step 41600 lr 3.152e-04 [train_loss] loss -0.3424 (1.715 secs)\n",
      "np:np_matern step 41800 lr 3.137e-04 [train_loss] loss -0.3379 (1.725 secs)\n",
      "np:np_matern step 42000 lr 3.122e-04 [train_loss] loss -0.3549 (1.810 secs)\n",
      "np:np_matern step 42200 lr 3.106e-04 [train_loss] loss -0.3245 (1.810 secs)\n",
      "np:np_matern step 42400 lr 3.091e-04 [train_loss] loss -0.3059 (1.770 secs)\n",
      "np:np_matern step 42600 lr 3.076e-04 [train_loss] loss -0.3237 (1.804 secs)\n",
      "np:np_matern step 42800 lr 3.061e-04 [train_loss] loss -0.2991 (1.796 secs)\n",
      "np:np_matern step 43000 lr 3.045e-04 [train_loss] loss -0.3263 (1.774 secs)\n",
      "np:np_matern step 43200 lr 3.030e-04 [train_loss] loss -0.2916 (1.885 secs)\n",
      "np:np_matern step 43400 lr 3.015e-04 [train_loss] loss -0.3343 (2.072 secs)\n",
      "np:np_matern step 43600 lr 2.999e-04 [train_loss] loss -0.3266 (1.948 secs)\n",
      "np:np_matern step 43800 lr 2.984e-04 [train_loss] loss -0.3323 (1.982 secs)\n",
      "np:np_matern step 44000 lr 2.968e-04 [train_loss] loss -0.3335 (1.783 secs)\n",
      "np:np_matern step 44200 lr 2.953e-04 [train_loss] loss -0.3300 (1.965 secs)\n",
      "np:np_matern step 44400 lr 2.938e-04 [train_loss] loss -0.3494 (2.035 secs)\n",
      "np:np_matern step 44600 lr 2.922e-04 [train_loss] loss -0.3534 (2.080 secs)\n",
      "np:np_matern step 44800 lr 2.907e-04 [train_loss] loss -0.3328 (2.125 secs)\n",
      "np:np_matern step 45000 lr 2.891e-04 [train_loss] loss -0.3386 (2.010 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 405.63it/s]\n",
      "np:np_matern matern ctx_ll 0.4029 tar_ll -0.0030 (7.396 secs)\n",
      "\n",
      "np:np_matern step 45200 lr 2.876e-04 [train_loss] loss -0.3689 (1.652 secs)\n",
      "np:np_matern step 45400 lr 2.860e-04 [train_loss] loss -0.3367 (1.645 secs)\n",
      "np:np_matern step 45600 lr 2.844e-04 [train_loss] loss -0.3282 (1.871 secs)\n",
      "np:np_matern step 45800 lr 2.829e-04 [train_loss] loss -0.2955 (1.783 secs)\n",
      "np:np_matern step 46000 lr 2.813e-04 [train_loss] loss -0.3570 (1.670 secs)\n",
      "np:np_matern step 46200 lr 2.798e-04 [train_loss] loss -0.3376 (1.735 secs)\n",
      "np:np_matern step 46400 lr 2.782e-04 [train_loss] loss -0.3317 (1.759 secs)\n",
      "np:np_matern step 46600 lr 2.767e-04 [train_loss] loss -0.3382 (1.756 secs)\n",
      "np:np_matern step 46800 lr 2.751e-04 [train_loss] loss -0.3818 (1.750 secs)\n",
      "np:np_matern step 47000 lr 2.735e-04 [train_loss] loss -0.3836 (1.766 secs)\n",
      "np:np_matern step 47200 lr 2.720e-04 [train_loss] loss -0.3736 (1.644 secs)\n",
      "np:np_matern step 47400 lr 2.704e-04 [train_loss] loss -0.3909 (1.740 secs)\n",
      "np:np_matern step 47600 lr 2.688e-04 [train_loss] loss -0.3236 (1.660 secs)\n",
      "np:np_matern step 47800 lr 2.673e-04 [train_loss] loss -0.3224 (1.680 secs)\n",
      "np:np_matern step 48000 lr 2.657e-04 [train_loss] loss -0.3560 (1.690 secs)\n",
      "np:np_matern step 48200 lr 2.641e-04 [train_loss] loss -0.3462 (1.810 secs)\n",
      "np:np_matern step 48400 lr 2.626e-04 [train_loss] loss -0.3427 (1.815 secs)\n",
      "np:np_matern step 48600 lr 2.610e-04 [train_loss] loss -0.3755 (1.865 secs)\n",
      "np:np_matern step 48800 lr 2.594e-04 [train_loss] loss -0.3296 (1.765 secs)\n",
      "np:np_matern step 49000 lr 2.579e-04 [train_loss] loss -0.3528 (1.785 secs)\n",
      "np:np_matern step 49200 lr 2.563e-04 [train_loss] loss -0.3622 (1.890 secs)\n",
      "np:np_matern step 49400 lr 2.547e-04 [train_loss] loss -0.3647 (1.800 secs)\n",
      "np:np_matern step 49600 lr 2.531e-04 [train_loss] loss -0.3622 (1.780 secs)\n",
      "np:np_matern step 49800 lr 2.516e-04 [train_loss] loss -0.3803 (1.765 secs)\n",
      "np:np_matern step 50000 lr 2.500e-04 [train_loss] loss -0.3542 (1.795 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 395.06it/s]\n",
      "np:np_matern matern ctx_ll 0.4402 tar_ll -0.0163 (7.594 secs)\n",
      "\n",
      "np:np_matern step 50200 lr 2.484e-04 [train_loss] loss -0.3470 (1.888 secs)\n",
      "np:np_matern step 50400 lr 2.469e-04 [train_loss] loss -0.3638 (1.760 secs)\n",
      "np:np_matern step 50600 lr 2.453e-04 [train_loss] loss -0.3700 (1.740 secs)\n",
      "np:np_matern step 50800 lr 2.437e-04 [train_loss] loss -0.3565 (1.720 secs)\n",
      "np:np_matern step 51000 lr 2.421e-04 [train_loss] loss -0.3596 (1.810 secs)\n",
      "np:np_matern step 51200 lr 2.406e-04 [train_loss] loss -0.3323 (1.820 secs)\n",
      "np:np_matern step 51400 lr 2.390e-04 [train_loss] loss -0.3656 (1.855 secs)\n",
      "np:np_matern step 51600 lr 2.374e-04 [train_loss] loss -0.3786 (1.840 secs)\n",
      "np:np_matern step 51800 lr 2.359e-04 [train_loss] loss -0.3363 (1.860 secs)\n",
      "np:np_matern step 52000 lr 2.343e-04 [train_loss] loss -0.3567 (1.905 secs)\n",
      "np:np_matern step 52200 lr 2.327e-04 [train_loss] loss -0.3996 (1.775 secs)\n",
      "np:np_matern step 52400 lr 2.312e-04 [train_loss] loss -0.3743 (1.755 secs)\n",
      "np:np_matern step 52600 lr 2.296e-04 [train_loss] loss -0.3639 (1.900 secs)\n",
      "np:np_matern step 52800 lr 2.280e-04 [train_loss] loss -0.3495 (1.730 secs)\n",
      "np:np_matern step 53000 lr 2.265e-04 [train_loss] loss -0.3715 (1.801 secs)\n",
      "np:np_matern step 53200 lr 2.249e-04 [train_loss] loss -0.4322 (1.806 secs)\n",
      "np:np_matern step 53400 lr 2.233e-04 [train_loss] loss -0.3822 (1.862 secs)\n",
      "np:np_matern step 53600 lr 2.218e-04 [train_loss] loss -0.3580 (1.815 secs)\n",
      "np:np_matern step 53800 lr 2.202e-04 [train_loss] loss -0.3867 (1.763 secs)\n",
      "np:np_matern step 54000 lr 2.187e-04 [train_loss] loss -0.3171 (1.780 secs)\n",
      "np:np_matern step 54200 lr 2.171e-04 [train_loss] loss -0.4056 (1.700 secs)\n",
      "np:np_matern step 54400 lr 2.156e-04 [train_loss] loss -0.3642 (1.720 secs)\n",
      "np:np_matern step 54600 lr 2.140e-04 [train_loss] loss -0.4014 (1.681 secs)\n",
      "np:np_matern step 54800 lr 2.124e-04 [train_loss] loss -0.4075 (1.834 secs)\n",
      "np:np_matern step 55000 lr 2.109e-04 [train_loss] loss -0.4015 (1.780 secs)\n",
      "100%|##########| 3000/3000 [00:08<00:00, 367.34it/s]\n",
      "np:np_matern matern ctx_ll 0.4767 tar_ll -0.0179 (8.168 secs)\n",
      "\n",
      "np:np_matern step 55200 lr 2.093e-04 [train_loss] loss -0.3941 (1.973 secs)\n",
      "np:np_matern step 55400 lr 2.078e-04 [train_loss] loss -0.4071 (1.885 secs)\n",
      "np:np_matern step 55600 lr 2.062e-04 [train_loss] loss -0.3929 (1.809 secs)\n",
      "np:np_matern step 55800 lr 2.047e-04 [train_loss] loss -0.3574 (1.851 secs)\n",
      "np:np_matern step 56000 lr 2.032e-04 [train_loss] loss -0.4142 (1.855 secs)\n",
      "np:np_matern step 56200 lr 2.016e-04 [train_loss] loss -0.3857 (1.835 secs)\n",
      "np:np_matern step 56400 lr 2.001e-04 [train_loss] loss -0.3762 (1.850 secs)\n",
      "np:np_matern step 56600 lr 1.985e-04 [train_loss] loss -0.4111 (1.820 secs)\n",
      "np:np_matern step 56800 lr 1.970e-04 [train_loss] loss -0.4021 (1.910 secs)\n",
      "np:np_matern step 57000 lr 1.955e-04 [train_loss] loss -0.3959 (1.870 secs)\n",
      "np:np_matern step 57200 lr 1.939e-04 [train_loss] loss -0.3883 (1.670 secs)\n",
      "np:np_matern step 57400 lr 1.924e-04 [train_loss] loss -0.3869 (1.685 secs)\n",
      "np:np_matern step 57600 lr 1.909e-04 [train_loss] loss -0.3777 (1.635 secs)\n",
      "np:np_matern step 57800 lr 1.894e-04 [train_loss] loss -0.3724 (1.680 secs)\n",
      "np:np_matern step 58000 lr 1.878e-04 [train_loss] loss -0.3899 (1.726 secs)\n",
      "np:np_matern step 58200 lr 1.863e-04 [train_loss] loss -0.4316 (1.590 secs)\n",
      "np:np_matern step 58400 lr 1.848e-04 [train_loss] loss -0.3985 (1.620 secs)\n",
      "np:np_matern step 58600 lr 1.833e-04 [train_loss] loss -0.3980 (1.555 secs)\n",
      "np:np_matern step 58800 lr 1.818e-04 [train_loss] loss -0.3939 (1.615 secs)\n",
      "np:np_matern step 59000 lr 1.803e-04 [train_loss] loss -0.4267 (1.716 secs)\n",
      "np:np_matern step 59200 lr 1.787e-04 [train_loss] loss -0.3983 (1.783 secs)\n",
      "np:np_matern step 59400 lr 1.772e-04 [train_loss] loss -0.3976 (1.880 secs)\n",
      "np:np_matern step 59600 lr 1.757e-04 [train_loss] loss -0.4030 (1.740 secs)\n",
      "np:np_matern step 59800 lr 1.742e-04 [train_loss] loss -0.3841 (1.715 secs)\n",
      "np:np_matern step 60000 lr 1.727e-04 [train_loss] loss -0.4023 (1.709 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 407.89it/s]\n",
      "np:np_matern matern ctx_ll 0.5033 tar_ll -0.0286 (7.355 secs)\n",
      "\n",
      "np:np_matern step 60200 lr 1.713e-04 [train_loss] loss -0.4177 (1.627 secs)\n",
      "np:np_matern step 60400 lr 1.698e-04 [train_loss] loss -0.4147 (1.570 secs)\n",
      "np:np_matern step 60600 lr 1.683e-04 [train_loss] loss -0.3998 (1.616 secs)\n",
      "np:np_matern step 60800 lr 1.668e-04 [train_loss] loss -0.4127 (1.699 secs)\n",
      "np:np_matern step 61000 lr 1.653e-04 [train_loss] loss -0.4199 (1.645 secs)\n",
      "np:np_matern step 61200 lr 1.638e-04 [train_loss] loss -0.4405 (1.570 secs)\n",
      "np:np_matern step 61400 lr 1.624e-04 [train_loss] loss -0.4166 (1.605 secs)\n",
      "np:np_matern step 61600 lr 1.609e-04 [train_loss] loss -0.3829 (1.615 secs)\n",
      "np:np_matern step 61800 lr 1.594e-04 [train_loss] loss -0.4110 (1.590 secs)\n",
      "np:np_matern step 62000 lr 1.580e-04 [train_loss] loss -0.4774 (1.710 secs)\n",
      "np:np_matern step 62200 lr 1.565e-04 [train_loss] loss -0.4262 (1.665 secs)\n",
      "np:np_matern step 62400 lr 1.551e-04 [train_loss] loss -0.3951 (1.680 secs)\n",
      "np:np_matern step 62600 lr 1.536e-04 [train_loss] loss -0.3701 (1.680 secs)\n",
      "np:np_matern step 62800 lr 1.522e-04 [train_loss] loss -0.4159 (1.825 secs)\n",
      "np:np_matern step 63000 lr 1.507e-04 [train_loss] loss -0.4268 (1.699 secs)\n",
      "np:np_matern step 63200 lr 1.493e-04 [train_loss] loss -0.4163 (1.700 secs)\n",
      "np:np_matern step 63400 lr 1.478e-04 [train_loss] loss -0.4144 (1.660 secs)\n",
      "np:np_matern step 63600 lr 1.464e-04 [train_loss] loss -0.3991 (1.750 secs)\n",
      "np:np_matern step 63800 lr 1.450e-04 [train_loss] loss -0.4014 (1.715 secs)\n",
      "np:np_matern step 64000 lr 1.436e-04 [train_loss] loss -0.4241 (1.710 secs)\n",
      "np:np_matern step 64200 lr 1.421e-04 [train_loss] loss -0.4639 (1.760 secs)\n",
      "np:np_matern step 64400 lr 1.407e-04 [train_loss] loss -0.3689 (1.730 secs)\n",
      "np:np_matern step 64600 lr 1.393e-04 [train_loss] loss -0.4267 (1.745 secs)\n",
      "np:np_matern step 64800 lr 1.379e-04 [train_loss] loss -0.4452 (1.707 secs)\n",
      "np:np_matern step 65000 lr 1.365e-04 [train_loss] loss -0.4136 (1.673 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 397.38it/s]\n",
      "np:np_matern matern ctx_ll 0.5294 tar_ll -0.0590 (7.552 secs)\n",
      "\n",
      "np:np_matern step 65200 lr 1.351e-04 [train_loss] loss -0.4094 (1.803 secs)\n",
      "np:np_matern step 65400 lr 1.337e-04 [train_loss] loss -0.3993 (1.910 secs)\n",
      "np:np_matern step 65600 lr 1.323e-04 [train_loss] loss -0.4252 (1.835 secs)\n",
      "np:np_matern step 65800 lr 1.309e-04 [train_loss] loss -0.4397 (1.905 secs)\n",
      "np:np_matern step 66000 lr 1.296e-04 [train_loss] loss -0.3968 (1.900 secs)\n",
      "np:np_matern step 66200 lr 1.282e-04 [train_loss] loss -0.4095 (1.980 secs)\n",
      "np:np_matern step 66400 lr 1.268e-04 [train_loss] loss -0.3976 (1.982 secs)\n",
      "np:np_matern step 66600 lr 1.255e-04 [train_loss] loss -0.4169 (1.788 secs)\n",
      "np:np_matern step 66800 lr 1.241e-04 [train_loss] loss -0.4031 (1.895 secs)\n",
      "np:np_matern step 67000 lr 1.227e-04 [train_loss] loss -0.4272 (1.985 secs)\n",
      "np:np_matern step 67200 lr 1.214e-04 [train_loss] loss -0.3962 (1.759 secs)\n",
      "np:np_matern step 67400 lr 1.200e-04 [train_loss] loss -0.4177 (1.990 secs)\n",
      "np:np_matern step 67600 lr 1.187e-04 [train_loss] loss -0.4444 (2.020 secs)\n",
      "np:np_matern step 67800 lr 1.174e-04 [train_loss] loss -0.4735 (1.950 secs)\n",
      "np:np_matern step 68000 lr 1.160e-04 [train_loss] loss -0.4657 (2.071 secs)\n",
      "np:np_matern step 68200 lr 1.147e-04 [train_loss] loss -0.4387 (2.019 secs)\n",
      "np:np_matern step 68400 lr 1.134e-04 [train_loss] loss -0.4268 (2.000 secs)\n",
      "np:np_matern step 68600 lr 1.121e-04 [train_loss] loss -0.4645 (2.050 secs)\n",
      "np:np_matern step 68800 lr 1.108e-04 [train_loss] loss -0.4104 (1.700 secs)\n",
      "np:np_matern step 69000 lr 1.095e-04 [train_loss] loss -0.4416 (1.660 secs)\n",
      "np:np_matern step 69200 lr 1.082e-04 [train_loss] loss -0.4640 (1.665 secs)\n",
      "np:np_matern step 69400 lr 1.069e-04 [train_loss] loss -0.4500 (1.725 secs)\n",
      "np:np_matern step 69600 lr 1.056e-04 [train_loss] loss -0.4460 (1.641 secs)\n",
      "np:np_matern step 69800 lr 1.043e-04 [train_loss] loss -0.4269 (1.710 secs)\n",
      "np:np_matern step 70000 lr 1.031e-04 [train_loss] loss -0.4288 (1.750 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 379.26it/s]\n",
      "np:np_matern matern ctx_ll 0.5199 tar_ll 0.0366 (7.920 secs)\n",
      "\n",
      "np:np_matern step 70200 lr 1.018e-04 [train_loss] loss -0.4006 (1.787 secs)\n",
      "np:np_matern step 70400 lr 1.005e-04 [train_loss] loss -0.4055 (1.785 secs)\n",
      "np:np_matern step 70600 lr 9.927e-05 [train_loss] loss -0.4699 (1.977 secs)\n",
      "np:np_matern step 70800 lr 9.802e-05 [train_loss] loss -0.4324 (2.215 secs)\n",
      "np:np_matern step 71000 lr 9.677e-05 [train_loss] loss -0.4547 (2.166 secs)\n",
      "np:np_matern step 71200 lr 9.554e-05 [train_loss] loss -0.4561 (2.280 secs)\n",
      "np:np_matern step 71400 lr 9.430e-05 [train_loss] loss -0.4403 (2.066 secs)\n",
      "np:np_matern step 71600 lr 9.308e-05 [train_loss] loss -0.4397 (1.957 secs)\n",
      "np:np_matern step 71800 lr 9.186e-05 [train_loss] loss -0.4247 (1.974 secs)\n",
      "np:np_matern step 72000 lr 9.064e-05 [train_loss] loss -0.4364 (2.042 secs)\n",
      "np:np_matern step 72200 lr 8.944e-05 [train_loss] loss -0.4504 (1.998 secs)\n",
      "np:np_matern step 72400 lr 8.824e-05 [train_loss] loss -0.4515 (1.800 secs)\n",
      "np:np_matern step 72600 lr 8.704e-05 [train_loss] loss -0.4304 (1.875 secs)\n",
      "np:np_matern step 72800 lr 8.585e-05 [train_loss] loss -0.4192 (1.855 secs)\n",
      "np:np_matern step 73000 lr 8.467e-05 [train_loss] loss -0.4269 (1.855 secs)\n",
      "np:np_matern step 73200 lr 8.350e-05 [train_loss] loss -0.4439 (1.780 secs)\n",
      "np:np_matern step 73400 lr 8.233e-05 [train_loss] loss -0.4814 (1.690 secs)\n",
      "np:np_matern step 73600 lr 8.117e-05 [train_loss] loss -0.4247 (1.700 secs)\n",
      "np:np_matern step 73800 lr 8.001e-05 [train_loss] loss -0.4471 (1.695 secs)\n",
      "np:np_matern step 74000 lr 7.886e-05 [train_loss] loss -0.4444 (1.690 secs)\n",
      "np:np_matern step 74200 lr 7.772e-05 [train_loss] loss -0.4780 (1.790 secs)\n",
      "np:np_matern step 74400 lr 7.659e-05 [train_loss] loss -0.4570 (1.842 secs)\n",
      "np:np_matern step 74600 lr 7.546e-05 [train_loss] loss -0.4616 (1.811 secs)\n",
      "np:np_matern step 74800 lr 7.434e-05 [train_loss] loss -0.4499 (1.792 secs)\n",
      "np:np_matern step 75000 lr 7.322e-05 [train_loss] loss -0.4699 (1.775 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 383.12it/s]\n",
      "np:np_matern matern ctx_ll 0.5323 tar_ll 0.0461 (7.830 secs)\n",
      "\n",
      "np:np_matern step 75200 lr 7.212e-05 [train_loss] loss -0.4636 (1.835 secs)\n",
      "np:np_matern step 75400 lr 7.102e-05 [train_loss] loss -0.4416 (1.850 secs)\n",
      "np:np_matern step 75600 lr 6.992e-05 [train_loss] loss -0.4546 (1.870 secs)\n",
      "np:np_matern step 75800 lr 6.884e-05 [train_loss] loss -0.4463 (1.815 secs)\n",
      "np:np_matern step 76000 lr 6.776e-05 [train_loss] loss -0.4803 (1.822 secs)\n",
      "np:np_matern step 76200 lr 6.669e-05 [train_loss] loss -0.4402 (1.825 secs)\n",
      "np:np_matern step 76400 lr 6.562e-05 [train_loss] loss -0.4632 (1.822 secs)\n",
      "np:np_matern step 76600 lr 6.456e-05 [train_loss] loss -0.4901 (1.930 secs)\n",
      "np:np_matern step 76800 lr 6.351e-05 [train_loss] loss -0.4553 (1.849 secs)\n",
      "np:np_matern step 77000 lr 6.247e-05 [train_loss] loss -0.4371 (1.935 secs)\n",
      "np:np_matern step 77200 lr 6.144e-05 [train_loss] loss -0.4485 (2.242 secs)\n",
      "np:np_matern step 77400 lr 6.041e-05 [train_loss] loss -0.4956 (2.302 secs)\n",
      "np:np_matern step 77600 lr 5.939e-05 [train_loss] loss -0.4473 (1.802 secs)\n",
      "np:np_matern step 77800 lr 5.838e-05 [train_loss] loss -0.5001 (1.794 secs)\n",
      "np:np_matern step 78000 lr 5.737e-05 [train_loss] loss -0.4538 (1.673 secs)\n",
      "np:np_matern step 78200 lr 5.637e-05 [train_loss] loss -0.4763 (1.775 secs)\n",
      "np:np_matern step 78400 lr 5.538e-05 [train_loss] loss -0.4580 (1.755 secs)\n",
      "np:np_matern step 78600 lr 5.440e-05 [train_loss] loss -0.4394 (1.795 secs)\n",
      "np:np_matern step 78800 lr 5.343e-05 [train_loss] loss -0.4512 (1.845 secs)\n",
      "np:np_matern step 79000 lr 5.246e-05 [train_loss] loss -0.4282 (1.755 secs)\n",
      "np:np_matern step 79200 lr 5.150e-05 [train_loss] loss -0.4773 (1.797 secs)\n",
      "np:np_matern step 79400 lr 5.055e-05 [train_loss] loss -0.4696 (1.703 secs)\n",
      "np:np_matern step 79600 lr 4.961e-05 [train_loss] loss -0.4435 (1.660 secs)\n",
      "np:np_matern step 79800 lr 4.867e-05 [train_loss] loss -0.4378 (1.670 secs)\n",
      "np:np_matern step 80000 lr 4.775e-05 [train_loss] loss -0.4500 (1.665 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 396.30it/s]\n",
      "np:np_matern matern ctx_ll 0.5364 tar_ll 0.0556 (7.570 secs)\n",
      "\n",
      "np:np_matern step 80200 lr 4.683e-05 [train_loss] loss -0.4545 (1.777 secs)\n",
      "np:np_matern step 80400 lr 4.592e-05 [train_loss] loss -0.4596 (1.804 secs)\n",
      "np:np_matern step 80600 lr 4.501e-05 [train_loss] loss -0.4618 (1.839 secs)\n",
      "np:np_matern step 80800 lr 4.412e-05 [train_loss] loss -0.4166 (1.940 secs)\n",
      "np:np_matern step 81000 lr 4.323e-05 [train_loss] loss -0.4416 (1.810 secs)\n",
      "np:np_matern step 81200 lr 4.235e-05 [train_loss] loss -0.4682 (1.891 secs)\n",
      "np:np_matern step 81400 lr 4.148e-05 [train_loss] loss -0.4583 (1.904 secs)\n",
      "np:np_matern step 81600 lr 4.062e-05 [train_loss] loss -0.4512 (1.966 secs)\n",
      "np:np_matern step 81800 lr 3.976e-05 [train_loss] loss -0.4727 (1.850 secs)\n",
      "np:np_matern step 82000 lr 3.892e-05 [train_loss] loss -0.4861 (1.910 secs)\n",
      "np:np_matern step 82200 lr 3.808e-05 [train_loss] loss -0.4665 (1.910 secs)\n",
      "np:np_matern step 82400 lr 3.725e-05 [train_loss] loss -0.4246 (1.945 secs)\n",
      "np:np_matern step 82600 lr 3.643e-05 [train_loss] loss -0.4667 (1.825 secs)\n",
      "np:np_matern step 82800 lr 3.562e-05 [train_loss] loss -0.4613 (1.905 secs)\n",
      "np:np_matern step 83000 lr 3.481e-05 [train_loss] loss -0.4870 (1.939 secs)\n",
      "np:np_matern step 83200 lr 3.402e-05 [train_loss] loss -0.4743 (1.952 secs)\n",
      "np:np_matern step 83400 lr 3.323e-05 [train_loss] loss -0.4580 (1.892 secs)\n",
      "np:np_matern step 83600 lr 3.245e-05 [train_loss] loss -0.4796 (1.953 secs)\n",
      "np:np_matern step 83800 lr 3.168e-05 [train_loss] loss -0.4754 (1.968 secs)\n",
      "np:np_matern step 84000 lr 3.092e-05 [train_loss] loss -0.4877 (1.853 secs)\n",
      "np:np_matern step 84200 lr 3.017e-05 [train_loss] loss -0.4471 (1.842 secs)\n",
      "np:np_matern step 84400 lr 2.943e-05 [train_loss] loss -0.4787 (1.677 secs)\n",
      "np:np_matern step 84600 lr 2.869e-05 [train_loss] loss -0.4805 (1.830 secs)\n",
      "np:np_matern step 84800 lr 2.797e-05 [train_loss] loss -0.4805 (1.960 secs)\n",
      "np:np_matern step 85000 lr 2.725e-05 [train_loss] loss -0.4395 (1.992 secs)\n",
      "100%|##########| 3000/3000 [00:08<00:00, 365.41it/s]\n",
      "np:np_matern matern ctx_ll 0.5443 tar_ll 0.0540 (8.217 secs)\n",
      "\n",
      "np:np_matern step 85200 lr 2.654e-05 [train_loss] loss -0.4824 (1.977 secs)\n",
      "np:np_matern step 85400 lr 2.584e-05 [train_loss] loss -0.4410 (1.903 secs)\n",
      "np:np_matern step 85600 lr 2.515e-05 [train_loss] loss -0.4691 (2.051 secs)\n",
      "np:np_matern step 85800 lr 2.447e-05 [train_loss] loss -0.4811 (1.869 secs)\n",
      "np:np_matern step 86000 lr 2.379e-05 [train_loss] loss -0.4191 (1.948 secs)\n",
      "np:np_matern step 86200 lr 2.313e-05 [train_loss] loss -0.4684 (1.944 secs)\n",
      "np:np_matern step 86400 lr 2.247e-05 [train_loss] loss -0.4880 (1.853 secs)\n",
      "np:np_matern step 86600 lr 2.183e-05 [train_loss] loss -0.4641 (1.840 secs)\n",
      "np:np_matern step 86800 lr 2.119e-05 [train_loss] loss -0.4690 (1.835 secs)\n",
      "np:np_matern step 87000 lr 2.056e-05 [train_loss] loss -0.4708 (1.935 secs)\n",
      "np:np_matern step 87200 lr 1.994e-05 [train_loss] loss -0.4713 (2.161 secs)\n",
      "np:np_matern step 87400 lr 1.933e-05 [train_loss] loss -0.4588 (2.078 secs)\n",
      "np:np_matern step 87600 lr 1.873e-05 [train_loss] loss -0.4649 (2.044 secs)\n",
      "np:np_matern step 87800 lr 1.814e-05 [train_loss] loss -0.5061 (2.143 secs)\n",
      "np:np_matern step 88000 lr 1.756e-05 [train_loss] loss -0.4717 (2.040 secs)\n",
      "np:np_matern step 88200 lr 1.698e-05 [train_loss] loss -0.4643 (2.060 secs)\n",
      "np:np_matern step 88400 lr 1.642e-05 [train_loss] loss -0.4453 (2.035 secs)\n",
      "np:np_matern step 88600 lr 1.586e-05 [train_loss] loss -0.4411 (2.013 secs)\n",
      "np:np_matern step 88800 lr 1.532e-05 [train_loss] loss -0.4785 (2.209 secs)\n",
      "np:np_matern step 89000 lr 1.478e-05 [train_loss] loss -0.4441 (1.970 secs)\n",
      "np:np_matern step 89200 lr 1.425e-05 [train_loss] loss -0.4804 (1.890 secs)\n",
      "np:np_matern step 89400 lr 1.373e-05 [train_loss] loss -0.4886 (1.815 secs)\n",
      "np:np_matern step 89600 lr 1.323e-05 [train_loss] loss -0.4798 (1.770 secs)\n",
      "np:np_matern step 89800 lr 1.273e-05 [train_loss] loss -0.5136 (1.820 secs)\n",
      "np:np_matern step 90000 lr 1.224e-05 [train_loss] loss -0.4948 (1.790 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 390.90it/s]\n",
      "np:np_matern matern ctx_ll 0.5524 tar_ll 0.0499 (7.675 secs)\n",
      "\n",
      "np:np_matern step 90200 lr 1.176e-05 [train_loss] loss -0.4908 (1.660 secs)\n",
      "np:np_matern step 90400 lr 1.128e-05 [train_loss] loss -0.4720 (1.685 secs)\n",
      "np:np_matern step 90600 lr 1.082e-05 [train_loss] loss -0.5229 (1.725 secs)\n",
      "np:np_matern step 90800 lr 1.037e-05 [train_loss] loss -0.4511 (1.795 secs)\n",
      "np:np_matern step 91000 lr 9.927e-06 [train_loss] loss -0.4746 (1.850 secs)\n",
      "np:np_matern step 91200 lr 9.493e-06 [train_loss] loss -0.4934 (1.951 secs)\n",
      "np:np_matern step 91400 lr 9.069e-06 [train_loss] loss -0.4407 (1.970 secs)\n",
      "np:np_matern step 91600 lr 8.655e-06 [train_loss] loss -0.4832 (1.880 secs)\n",
      "np:np_matern step 91800 lr 8.250e-06 [train_loss] loss -0.4352 (1.772 secs)\n",
      "np:np_matern step 92000 lr 7.854e-06 [train_loss] loss -0.4613 (1.753 secs)\n",
      "np:np_matern step 92200 lr 7.468e-06 [train_loss] loss -0.4751 (1.800 secs)\n",
      "np:np_matern step 92400 lr 7.092e-06 [train_loss] loss -0.4784 (1.840 secs)\n",
      "np:np_matern step 92600 lr 6.725e-06 [train_loss] loss -0.4973 (1.839 secs)\n",
      "np:np_matern step 92800 lr 6.368e-06 [train_loss] loss -0.4455 (1.826 secs)\n",
      "np:np_matern step 93000 lr 6.021e-06 [train_loss] loss -0.5037 (2.050 secs)\n",
      "np:np_matern step 93200 lr 5.683e-06 [train_loss] loss -0.4700 (2.000 secs)\n",
      "np:np_matern step 93400 lr 5.355e-06 [train_loss] loss -0.4733 (1.983 secs)\n",
      "np:np_matern step 93600 lr 5.036e-06 [train_loss] loss -0.4671 (1.764 secs)\n",
      "np:np_matern step 93800 lr 4.727e-06 [train_loss] loss -0.4339 (1.698 secs)\n",
      "np:np_matern step 94000 lr 4.428e-06 [train_loss] loss -0.4789 (1.727 secs)\n",
      "np:np_matern step 94200 lr 4.139e-06 [train_loss] loss -0.4497 (1.825 secs)\n",
      "np:np_matern step 94400 lr 3.859e-06 [train_loss] loss -0.4567 (1.877 secs)\n",
      "np:np_matern step 94600 lr 3.589e-06 [train_loss] loss -0.4491 (1.921 secs)\n",
      "np:np_matern step 94800 lr 3.329e-06 [train_loss] loss -0.4687 (2.233 secs)\n",
      "np:np_matern step 95000 lr 3.078e-06 [train_loss] loss -0.4994 (1.938 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 395.77it/s]\n",
      "np:np_matern matern ctx_ll 0.5554 tar_ll 0.0531 (7.580 secs)\n",
      "\n",
      "np:np_matern step 95200 lr 2.837e-06 [train_loss] loss -0.4472 (1.710 secs)\n",
      "np:np_matern step 95400 lr 2.606e-06 [train_loss] loss -0.4951 (1.995 secs)\n",
      "np:np_matern step 95600 lr 2.385e-06 [train_loss] loss -0.4672 (2.343 secs)\n",
      "np:np_matern step 95800 lr 2.173e-06 [train_loss] loss -0.4779 (1.812 secs)\n",
      "np:np_matern step 96000 lr 1.971e-06 [train_loss] loss -0.4858 (1.996 secs)\n",
      "np:np_matern step 96200 lr 1.779e-06 [train_loss] loss -0.4959 (1.853 secs)\n",
      "np:np_matern step 96400 lr 1.597e-06 [train_loss] loss -0.4745 (1.979 secs)\n",
      "np:np_matern step 96600 lr 1.425e-06 [train_loss] loss -0.5034 (1.895 secs)\n",
      "np:np_matern step 96800 lr 1.262e-06 [train_loss] loss -0.4792 (1.971 secs)\n",
      "np:np_matern step 97000 lr 1.110e-06 [train_loss] loss -0.4506 (1.985 secs)\n",
      "np:np_matern step 97200 lr 9.666e-07 [train_loss] loss -0.4854 (2.049 secs)\n",
      "np:np_matern step 97400 lr 8.335e-07 [train_loss] loss -0.4902 (1.983 secs)\n",
      "np:np_matern step 97600 lr 7.103e-07 [train_loss] loss -0.4741 (2.010 secs)\n",
      "np:np_matern step 97800 lr 5.969e-07 [train_loss] loss -0.4460 (1.976 secs)\n",
      "np:np_matern step 98000 lr 4.933e-07 [train_loss] loss -0.4810 (1.735 secs)\n",
      "np:np_matern step 98200 lr 3.996e-07 [train_loss] loss -0.4955 (1.701 secs)\n",
      "np:np_matern step 98400 lr 3.158e-07 [train_loss] loss -0.4693 (1.720 secs)\n",
      "np:np_matern step 98600 lr 2.418e-07 [train_loss] loss -0.4943 (1.718 secs)\n",
      "np:np_matern step 98800 lr 1.776e-07 [train_loss] loss -0.4778 (1.805 secs)\n",
      "np:np_matern step 99000 lr 1.234e-07 [train_loss] loss -0.4447 (2.014 secs)\n",
      "np:np_matern step 99200 lr 7.895e-08 [train_loss] loss -0.4478 (2.010 secs)\n",
      "np:np_matern step 99400 lr 4.441e-08 [train_loss] loss -0.4836 (2.114 secs)\n",
      "np:np_matern step 99600 lr 1.974e-08 [train_loss] loss -0.4633 (1.922 secs)\n",
      "np:np_matern step 99800 lr 4.935e-09 [train_loss] loss -0.4756 (2.009 secs)\n",
      "np:np_matern step 100000 lr 0.000e+00 [train_loss] loss -0.4477 (1.918 secs)\n",
      "100%|##########| 3000/3000 [00:08<00:00, 373.38it/s]\n",
      "np:np_matern matern ctx_ll 0.5559 tar_ll 0.0543 (8.035 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:08<00:00, 345.12it/s]\n",
      "np:np_matern matern ctx_ll 0.5558 tar_ll 0.0538 (8.693 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1098427.5 miliseconds\n",
      "Execution time: 1098.4275 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 155.63916015625 MB\n",
      "Memory Usage Change: 139.38916015625 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='np', name='np_matern',val_seed=100, val_l=None,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388b66d8-c3ef-4c55-ac1c-2a6f33b17838",
   "metadata": {},
   "source": [
    "## ANP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8441d51-dfaf-4cc0-bdbb-89e276ab26d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: anp-anp_matern\n",
      "Total number of parameters: 348418\n",
      "\n",
      "anp:anp_matern step 200 lr 5.000e-04 [train_loss] loss 0.5197 (10.658 secs)\n",
      "anp:anp_matern step 400 lr 5.000e-04 [train_loss] loss 0.1755 (8.316 secs)\n",
      "anp:anp_matern step 600 lr 5.000e-04 [train_loss] loss -0.2412 (8.652 secs)\n",
      "anp:anp_matern step 800 lr 4.999e-04 [train_loss] loss -0.4086 (8.729 secs)\n",
      "anp:anp_matern step 1000 lr 4.999e-04 [train_loss] loss -0.5491 (8.485 secs)\n",
      "anp:anp_matern step 1200 lr 4.998e-04 [train_loss] loss -0.5166 (9.026 secs)\n",
      "anp:anp_matern step 1400 lr 4.998e-04 [train_loss] loss -0.5090 (8.401 secs)\n",
      "anp:anp_matern step 1600 lr 4.997e-04 [train_loss] loss -0.5670 (8.146 secs)\n",
      "anp:anp_matern step 1800 lr 4.996e-04 [train_loss] loss -0.6044 (8.351 secs)\n",
      "anp:anp_matern step 2000 lr 4.995e-04 [train_loss] loss -0.6355 (8.389 secs)\n",
      "anp:anp_matern step 2200 lr 4.994e-04 [train_loss] loss -0.6294 (7.963 secs)\n",
      "anp:anp_matern step 2400 lr 4.993e-04 [train_loss] loss -0.7183 (8.468 secs)\n",
      "anp:anp_matern step 2600 lr 4.992e-04 [train_loss] loss -0.7742 (8.464 secs)\n",
      "anp:anp_matern step 2800 lr 4.990e-04 [train_loss] loss -0.7418 (8.833 secs)\n",
      "anp:anp_matern step 3000 lr 4.989e-04 [train_loss] loss -0.7826 (9.137 secs)\n",
      "anp:anp_matern step 3200 lr 4.987e-04 [train_loss] loss -0.8011 (9.480 secs)\n",
      "anp:anp_matern step 3400 lr 4.986e-04 [train_loss] loss -0.7763 (8.779 secs)\n",
      "anp:anp_matern step 3600 lr 4.984e-04 [train_loss] loss -0.7789 (8.818 secs)\n",
      "anp:anp_matern step 3800 lr 4.982e-04 [train_loss] loss -0.8306 (8.978 secs)\n",
      "anp:anp_matern step 4000 lr 4.980e-04 [train_loss] loss -0.7629 (8.314 secs)\n",
      "anp:anp_matern step 4200 lr 4.978e-04 [train_loss] loss -0.8579 (8.545 secs)\n",
      "anp:anp_matern step 4400 lr 4.976e-04 [train_loss] loss -0.8181 (7.940 secs)\n",
      "anp:anp_matern step 4600 lr 4.974e-04 [train_loss] loss -0.8538 (8.126 secs)\n",
      "anp:anp_matern step 4800 lr 4.972e-04 [train_loss] loss -0.8982 (8.546 secs)\n",
      "anp:anp_matern step 5000 lr 4.969e-04 [train_loss] loss -0.8908 (8.009 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 83.58it/s]\n",
      "anp:anp_matern matern ctx_ll 1.1967 tar_ll 0.4407 (35.892 secs)\n",
      "\n",
      "anp:anp_matern step 5200 lr 4.967e-04 [train_loss] loss -0.8796 (9.208 secs)\n",
      "anp:anp_matern step 5400 lr 4.964e-04 [train_loss] loss -0.8018 (8.575 secs)\n",
      "anp:anp_matern step 5600 lr 4.961e-04 [train_loss] loss -0.8979 (8.442 secs)\n",
      "anp:anp_matern step 5800 lr 4.959e-04 [train_loss] loss -0.8556 (8.449 secs)\n",
      "anp:anp_matern step 6000 lr 4.956e-04 [train_loss] loss -0.8731 (8.299 secs)\n",
      "anp:anp_matern step 6200 lr 4.953e-04 [train_loss] loss -0.9279 (8.571 secs)\n",
      "anp:anp_matern step 6400 lr 4.950e-04 [train_loss] loss -0.9433 (8.014 secs)\n",
      "anp:anp_matern step 6600 lr 4.946e-04 [train_loss] loss -0.9152 (8.358 secs)\n",
      "anp:anp_matern step 6800 lr 4.943e-04 [train_loss] loss -0.9248 (8.470 secs)\n",
      "anp:anp_matern step 7000 lr 4.940e-04 [train_loss] loss -0.9673 (8.550 secs)\n",
      "anp:anp_matern step 7200 lr 4.936e-04 [train_loss] loss -0.9607 (8.539 secs)\n",
      "anp:anp_matern step 7400 lr 4.933e-04 [train_loss] loss -0.8388 (8.498 secs)\n",
      "anp:anp_matern step 7600 lr 4.929e-04 [train_loss] loss -0.7332 (9.252 secs)\n",
      "anp:anp_matern step 7800 lr 4.925e-04 [train_loss] loss -0.9101 (8.979 secs)\n",
      "anp:anp_matern step 8000 lr 4.921e-04 [train_loss] loss -0.9396 (9.974 secs)\n",
      "anp:anp_matern step 8200 lr 4.918e-04 [train_loss] loss -0.9611 (9.422 secs)\n",
      "anp:anp_matern step 8400 lr 4.913e-04 [train_loss] loss -0.9772 (9.815 secs)\n",
      "anp:anp_matern step 8600 lr 4.909e-04 [train_loss] loss -0.9441 (8.688 secs)\n",
      "anp:anp_matern step 8800 lr 4.905e-04 [train_loss] loss -0.9433 (9.697 secs)\n",
      "anp:anp_matern step 9000 lr 4.901e-04 [train_loss] loss -0.9583 (9.548 secs)\n",
      "anp:anp_matern step 9200 lr 4.896e-04 [train_loss] loss -0.9739 (9.419 secs)\n",
      "anp:anp_matern step 9400 lr 4.892e-04 [train_loss] loss -0.9104 (10.055 secs)\n",
      "anp:anp_matern step 9600 lr 4.887e-04 [train_loss] loss -0.9663 (8.990 secs)\n",
      "anp:anp_matern step 9800 lr 4.882e-04 [train_loss] loss -0.9544 (8.827 secs)\n",
      "anp:anp_matern step 10000 lr 4.878e-04 [train_loss] loss -0.9650 (8.597 secs)\n",
      "100%|##########| 3000/3000 [00:38<00:00, 78.23it/s] \n",
      "anp:anp_matern matern ctx_ll 1.2763 tar_ll 0.5057 (38.348 secs)\n",
      "\n",
      "anp:anp_matern step 10200 lr 4.873e-04 [train_loss] loss -0.9201 (8.788 secs)\n",
      "anp:anp_matern step 10400 lr 4.868e-04 [train_loss] loss -0.9585 (8.907 secs)\n",
      "anp:anp_matern step 10600 lr 4.863e-04 [train_loss] loss -0.9797 (9.282 secs)\n",
      "anp:anp_matern step 10800 lr 4.857e-04 [train_loss] loss -0.9762 (9.223 secs)\n",
      "anp:anp_matern step 11000 lr 4.852e-04 [train_loss] loss -0.8067 (8.815 secs)\n",
      "anp:anp_matern step 11200 lr 4.847e-04 [train_loss] loss -0.9461 (8.858 secs)\n",
      "anp:anp_matern step 11400 lr 4.841e-04 [train_loss] loss -0.9613 (8.592 secs)\n",
      "anp:anp_matern step 11600 lr 4.836e-04 [train_loss] loss -0.9501 (8.288 secs)\n",
      "anp:anp_matern step 11800 lr 4.830e-04 [train_loss] loss -1.0344 (8.954 secs)\n",
      "anp:anp_matern step 12000 lr 4.824e-04 [train_loss] loss -0.9844 (8.972 secs)\n",
      "anp:anp_matern step 12200 lr 4.819e-04 [train_loss] loss -0.7982 (9.163 secs)\n",
      "anp:anp_matern step 12400 lr 4.813e-04 [train_loss] loss -0.8487 (8.793 secs)\n",
      "anp:anp_matern step 12600 lr 4.807e-04 [train_loss] loss -0.9186 (8.372 secs)\n",
      "anp:anp_matern step 12800 lr 4.801e-04 [train_loss] loss -0.9813 (8.200 secs)\n",
      "anp:anp_matern step 13000 lr 4.794e-04 [train_loss] loss -0.9289 (8.375 secs)\n",
      "anp:anp_matern step 13200 lr 4.788e-04 [train_loss] loss -1.0096 (8.608 secs)\n",
      "anp:anp_matern step 13400 lr 4.782e-04 [train_loss] loss -1.0081 (8.381 secs)\n",
      "anp:anp_matern step 13600 lr 4.775e-04 [train_loss] loss -1.0002 (6.400 secs)\n",
      "anp:anp_matern step 13800 lr 4.769e-04 [train_loss] loss -1.0021 (5.338 secs)\n",
      "anp:anp_matern step 14000 lr 4.762e-04 [train_loss] loss -0.9841 (5.321 secs)\n",
      "anp:anp_matern step 14200 lr 4.755e-04 [train_loss] loss -0.9474 (5.423 secs)\n",
      "anp:anp_matern step 14400 lr 4.749e-04 [train_loss] loss -1.0052 (5.298 secs)\n",
      "anp:anp_matern step 14600 lr 4.742e-04 [train_loss] loss -0.9852 (5.321 secs)\n",
      "anp:anp_matern step 14800 lr 4.735e-04 [train_loss] loss -1.0453 (5.228 secs)\n",
      "anp:anp_matern step 15000 lr 4.728e-04 [train_loss] loss -0.9404 (5.656 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 142.59it/s]\n",
      "anp:anp_matern matern ctx_ll 1.1902 tar_ll 0.4168 (21.043 secs)\n",
      "\n",
      "anp:anp_matern step 15200 lr 4.720e-04 [train_loss] loss -0.8084 (5.217 secs)\n",
      "anp:anp_matern step 15400 lr 4.713e-04 [train_loss] loss -0.9886 (5.031 secs)\n",
      "anp:anp_matern step 15600 lr 4.706e-04 [train_loss] loss -0.9563 (5.026 secs)\n",
      "anp:anp_matern step 15800 lr 4.698e-04 [train_loss] loss -1.0266 (4.820 secs)\n",
      "anp:anp_matern step 16000 lr 4.691e-04 [train_loss] loss -0.9605 (4.931 secs)\n",
      "anp:anp_matern step 16200 lr 4.683e-04 [train_loss] loss -0.9994 (5.058 secs)\n",
      "anp:anp_matern step 16400 lr 4.675e-04 [train_loss] loss -0.8951 (4.937 secs)\n",
      "anp:anp_matern step 16600 lr 4.668e-04 [train_loss] loss -0.9152 (4.960 secs)\n",
      "anp:anp_matern step 16800 lr 4.660e-04 [train_loss] loss -1.0067 (4.971 secs)\n",
      "anp:anp_matern step 17000 lr 4.652e-04 [train_loss] loss -0.9732 (5.034 secs)\n",
      "anp:anp_matern step 17200 lr 4.644e-04 [train_loss] loss -0.9604 (5.073 secs)\n",
      "anp:anp_matern step 17400 lr 4.636e-04 [train_loss] loss -1.0574 (4.898 secs)\n",
      "anp:anp_matern step 17600 lr 4.627e-04 [train_loss] loss -0.9978 (5.217 secs)\n",
      "anp:anp_matern step 17800 lr 4.619e-04 [train_loss] loss -0.9810 (5.128 secs)\n",
      "anp:anp_matern step 18000 lr 4.611e-04 [train_loss] loss -0.9706 (4.925 secs)\n",
      "anp:anp_matern step 18200 lr 4.602e-04 [train_loss] loss -1.0560 (4.861 secs)\n",
      "anp:anp_matern step 18400 lr 4.594e-04 [train_loss] loss -0.8855 (4.958 secs)\n",
      "anp:anp_matern step 18600 lr 4.585e-04 [train_loss] loss -1.0303 (4.997 secs)\n",
      "anp:anp_matern step 18800 lr 4.576e-04 [train_loss] loss -0.9848 (4.943 secs)\n",
      "anp:anp_matern step 19000 lr 4.568e-04 [train_loss] loss -0.9667 (4.878 secs)\n",
      "anp:anp_matern step 19200 lr 4.559e-04 [train_loss] loss -1.0003 (4.886 secs)\n",
      "anp:anp_matern step 19400 lr 4.550e-04 [train_loss] loss -1.0079 (5.008 secs)\n",
      "anp:anp_matern step 19600 lr 4.541e-04 [train_loss] loss -1.0373 (5.062 secs)\n",
      "anp:anp_matern step 19800 lr 4.532e-04 [train_loss] loss -1.0051 (5.112 secs)\n",
      "anp:anp_matern step 20000 lr 4.523e-04 [train_loss] loss -0.9661 (5.063 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 147.15it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3169 tar_ll 0.5424 (20.390 secs)\n",
      "\n",
      "anp:anp_matern step 20200 lr 4.513e-04 [train_loss] loss -1.0210 (5.205 secs)\n",
      "anp:anp_matern step 20400 lr 4.504e-04 [train_loss] loss -0.9783 (5.160 secs)\n",
      "anp:anp_matern step 20600 lr 4.494e-04 [train_loss] loss -1.0088 (5.382 secs)\n",
      "anp:anp_matern step 20800 lr 4.485e-04 [train_loss] loss -1.0601 (5.346 secs)\n",
      "anp:anp_matern step 21000 lr 4.475e-04 [train_loss] loss -0.9464 (5.301 secs)\n",
      "anp:anp_matern step 21200 lr 4.466e-04 [train_loss] loss -1.0302 (5.260 secs)\n",
      "anp:anp_matern step 21400 lr 4.456e-04 [train_loss] loss -1.0217 (5.142 secs)\n",
      "anp:anp_matern step 21600 lr 4.446e-04 [train_loss] loss -0.9969 (5.574 secs)\n",
      "anp:anp_matern step 21800 lr 4.436e-04 [train_loss] loss -1.0598 (5.578 secs)\n",
      "anp:anp_matern step 22000 lr 4.426e-04 [train_loss] loss -1.0442 (5.308 secs)\n",
      "anp:anp_matern step 22200 lr 4.416e-04 [train_loss] loss -1.0322 (5.251 secs)\n",
      "anp:anp_matern step 22400 lr 4.406e-04 [train_loss] loss -0.9989 (5.313 secs)\n",
      "anp:anp_matern step 22600 lr 4.396e-04 [train_loss] loss -0.9976 (5.152 secs)\n",
      "anp:anp_matern step 22800 lr 4.386e-04 [train_loss] loss -1.0208 (5.089 secs)\n",
      "anp:anp_matern step 23000 lr 4.375e-04 [train_loss] loss -0.9093 (5.359 secs)\n",
      "anp:anp_matern step 23200 lr 4.365e-04 [train_loss] loss -0.9772 (5.266 secs)\n",
      "anp:anp_matern step 23400 lr 4.354e-04 [train_loss] loss -1.0360 (5.155 secs)\n",
      "anp:anp_matern step 23600 lr 4.344e-04 [train_loss] loss -1.0219 (5.360 secs)\n",
      "anp:anp_matern step 23800 lr 4.333e-04 [train_loss] loss -0.9423 (5.110 secs)\n",
      "anp:anp_matern step 24000 lr 4.322e-04 [train_loss] loss -0.9990 (5.042 secs)\n",
      "anp:anp_matern step 24200 lr 4.312e-04 [train_loss] loss -1.0195 (5.157 secs)\n",
      "anp:anp_matern step 24400 lr 4.301e-04 [train_loss] loss -1.0319 (5.053 secs)\n",
      "anp:anp_matern step 24600 lr 4.290e-04 [train_loss] loss -1.0465 (5.024 secs)\n",
      "anp:anp_matern step 24800 lr 4.279e-04 [train_loss] loss -1.0870 (5.080 secs)\n",
      "anp:anp_matern step 25000 lr 4.268e-04 [train_loss] loss -1.0015 (4.961 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 146.61it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3230 tar_ll 0.5486 (20.464 secs)\n",
      "\n",
      "anp:anp_matern step 25200 lr 4.257e-04 [train_loss] loss -1.0507 (5.090 secs)\n",
      "anp:anp_matern step 25400 lr 4.245e-04 [train_loss] loss -0.9974 (5.071 secs)\n",
      "anp:anp_matern step 25600 lr 4.234e-04 [train_loss] loss -1.0360 (4.940 secs)\n",
      "anp:anp_matern step 25800 lr 4.223e-04 [train_loss] loss -1.0033 (5.120 secs)\n",
      "anp:anp_matern step 26000 lr 4.211e-04 [train_loss] loss -0.9842 (5.069 secs)\n",
      "anp:anp_matern step 26200 lr 4.200e-04 [train_loss] loss -0.9855 (4.963 secs)\n",
      "anp:anp_matern step 26400 lr 4.188e-04 [train_loss] loss -0.9984 (5.049 secs)\n",
      "anp:anp_matern step 26600 lr 4.177e-04 [train_loss] loss -1.0150 (5.080 secs)\n",
      "anp:anp_matern step 26800 lr 4.165e-04 [train_loss] loss -1.0415 (5.119 secs)\n",
      "anp:anp_matern step 27000 lr 4.153e-04 [train_loss] loss -0.9647 (5.169 secs)\n",
      "anp:anp_matern step 27200 lr 4.141e-04 [train_loss] loss -1.0529 (5.116 secs)\n",
      "anp:anp_matern step 27400 lr 4.130e-04 [train_loss] loss -1.0277 (4.984 secs)\n",
      "anp:anp_matern step 27600 lr 4.118e-04 [train_loss] loss -1.0475 (4.902 secs)\n",
      "anp:anp_matern step 27800 lr 4.106e-04 [train_loss] loss -1.0448 (5.104 secs)\n",
      "anp:anp_matern step 28000 lr 4.094e-04 [train_loss] loss -1.0260 (5.116 secs)\n",
      "anp:anp_matern step 28200 lr 4.081e-04 [train_loss] loss -1.0510 (5.155 secs)\n",
      "anp:anp_matern step 28400 lr 4.069e-04 [train_loss] loss -0.9909 (5.127 secs)\n",
      "anp:anp_matern step 28600 lr 4.057e-04 [train_loss] loss -1.0604 (5.164 secs)\n",
      "anp:anp_matern step 28800 lr 4.045e-04 [train_loss] loss -1.0618 (5.056 secs)\n",
      "anp:anp_matern step 29000 lr 4.032e-04 [train_loss] loss -1.0344 (5.284 secs)\n",
      "anp:anp_matern step 29200 lr 4.020e-04 [train_loss] loss -0.9572 (5.177 secs)\n",
      "anp:anp_matern step 29400 lr 4.007e-04 [train_loss] loss -1.0349 (5.166 secs)\n",
      "anp:anp_matern step 29600 lr 3.995e-04 [train_loss] loss -1.0613 (5.352 secs)\n",
      "anp:anp_matern step 29800 lr 3.982e-04 [train_loss] loss -1.0699 (5.294 secs)\n",
      "anp:anp_matern step 30000 lr 3.969e-04 [train_loss] loss -1.0400 (5.268 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 141.29it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3297 tar_ll 0.5484 (21.238 secs)\n",
      "\n",
      "anp:anp_matern step 30200 lr 3.957e-04 [train_loss] loss -1.0405 (5.106 secs)\n",
      "anp:anp_matern step 30400 lr 3.944e-04 [train_loss] loss -1.0251 (5.144 secs)\n",
      "anp:anp_matern step 30600 lr 3.931e-04 [train_loss] loss -1.0002 (5.178 secs)\n",
      "anp:anp_matern step 30800 lr 3.918e-04 [train_loss] loss -1.0145 (5.221 secs)\n",
      "anp:anp_matern step 31000 lr 3.905e-04 [train_loss] loss -1.0557 (5.221 secs)\n",
      "anp:anp_matern step 31200 lr 3.892e-04 [train_loss] loss -1.0377 (5.336 secs)\n",
      "anp:anp_matern step 31400 lr 3.879e-04 [train_loss] loss -0.9796 (5.100 secs)\n",
      "anp:anp_matern step 31600 lr 3.866e-04 [train_loss] loss -1.0218 (5.057 secs)\n",
      "anp:anp_matern step 31800 lr 3.853e-04 [train_loss] loss -0.9986 (5.307 secs)\n",
      "anp:anp_matern step 32000 lr 3.840e-04 [train_loss] loss -0.9941 (5.101 secs)\n",
      "anp:anp_matern step 32200 lr 3.826e-04 [train_loss] loss -1.0556 (4.968 secs)\n",
      "anp:anp_matern step 32400 lr 3.813e-04 [train_loss] loss -1.0174 (5.112 secs)\n",
      "anp:anp_matern step 32600 lr 3.800e-04 [train_loss] loss -1.0397 (5.064 secs)\n",
      "anp:anp_matern step 32800 lr 3.786e-04 [train_loss] loss -1.0672 (5.086 secs)\n",
      "anp:anp_matern step 33000 lr 3.773e-04 [train_loss] loss -1.0952 (5.111 secs)\n",
      "anp:anp_matern step 33200 lr 3.759e-04 [train_loss] loss -1.0390 (5.214 secs)\n",
      "anp:anp_matern step 33400 lr 3.745e-04 [train_loss] loss -1.0371 (5.151 secs)\n",
      "anp:anp_matern step 33600 lr 3.732e-04 [train_loss] loss -1.0614 (5.211 secs)\n",
      "anp:anp_matern step 33800 lr 3.718e-04 [train_loss] loss -1.0239 (5.144 secs)\n",
      "anp:anp_matern step 34000 lr 3.704e-04 [train_loss] loss -1.0543 (5.108 secs)\n",
      "anp:anp_matern step 34200 lr 3.691e-04 [train_loss] loss -1.0624 (5.093 secs)\n",
      "anp:anp_matern step 34400 lr 3.677e-04 [train_loss] loss -1.0463 (5.033 secs)\n",
      "anp:anp_matern step 34600 lr 3.663e-04 [train_loss] loss -1.0869 (5.213 secs)\n",
      "anp:anp_matern step 34800 lr 3.649e-04 [train_loss] loss -1.0563 (5.118 secs)\n",
      "anp:anp_matern step 35000 lr 3.635e-04 [train_loss] loss -1.0153 (5.166 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 147.52it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3501 tar_ll 0.5804 (20.339 secs)\n",
      "\n",
      "anp:anp_matern step 35200 lr 3.621e-04 [train_loss] loss -1.0114 (5.129 secs)\n",
      "anp:anp_matern step 35400 lr 3.607e-04 [train_loss] loss -1.0678 (5.145 secs)\n",
      "anp:anp_matern step 35600 lr 3.593e-04 [train_loss] loss -1.0331 (5.078 secs)\n",
      "anp:anp_matern step 35800 lr 3.579e-04 [train_loss] loss -0.9980 (5.184 secs)\n",
      "anp:anp_matern step 36000 lr 3.564e-04 [train_loss] loss -1.0740 (5.340 secs)\n",
      "anp:anp_matern step 36200 lr 3.550e-04 [train_loss] loss -1.0313 (5.164 secs)\n",
      "anp:anp_matern step 36400 lr 3.536e-04 [train_loss] loss -1.0285 (5.106 secs)\n",
      "anp:anp_matern step 36600 lr 3.522e-04 [train_loss] loss -0.9967 (5.140 secs)\n",
      "anp:anp_matern step 36800 lr 3.507e-04 [train_loss] loss -1.0107 (5.232 secs)\n",
      "anp:anp_matern step 37000 lr 3.493e-04 [train_loss] loss -1.0301 (5.123 secs)\n",
      "anp:anp_matern step 37200 lr 3.478e-04 [train_loss] loss -1.0375 (5.266 secs)\n",
      "anp:anp_matern step 37400 lr 3.464e-04 [train_loss] loss -1.0091 (5.204 secs)\n",
      "anp:anp_matern step 37600 lr 3.449e-04 [train_loss] loss -1.1174 (5.330 secs)\n",
      "anp:anp_matern step 37800 lr 3.435e-04 [train_loss] loss -1.0553 (5.416 secs)\n",
      "anp:anp_matern step 38000 lr 3.420e-04 [train_loss] loss -1.0626 (5.215 secs)\n",
      "anp:anp_matern step 38200 lr 3.406e-04 [train_loss] loss -1.0637 (5.233 secs)\n",
      "anp:anp_matern step 38400 lr 3.391e-04 [train_loss] loss -1.0308 (5.127 secs)\n",
      "anp:anp_matern step 38600 lr 3.376e-04 [train_loss] loss -1.0332 (5.152 secs)\n",
      "anp:anp_matern step 38800 lr 3.362e-04 [train_loss] loss -1.0036 (4.916 secs)\n",
      "anp:anp_matern step 39000 lr 3.347e-04 [train_loss] loss -1.0460 (5.226 secs)\n",
      "anp:anp_matern step 39200 lr 3.332e-04 [train_loss] loss -1.0571 (5.254 secs)\n",
      "anp:anp_matern step 39400 lr 3.317e-04 [train_loss] loss -1.0608 (5.085 secs)\n",
      "anp:anp_matern step 39600 lr 3.302e-04 [train_loss] loss -0.9909 (5.133 secs)\n",
      "anp:anp_matern step 39800 lr 3.287e-04 [train_loss] loss -1.0480 (5.097 secs)\n",
      "anp:anp_matern step 40000 lr 3.273e-04 [train_loss] loss -1.0238 (5.136 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 146.27it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3549 tar_ll 0.5888 (20.513 secs)\n",
      "\n",
      "anp:anp_matern step 40200 lr 3.258e-04 [train_loss] loss -1.0443 (5.044 secs)\n",
      "anp:anp_matern step 40400 lr 3.243e-04 [train_loss] loss -1.0650 (5.009 secs)\n",
      "anp:anp_matern step 40600 lr 3.228e-04 [train_loss] loss -1.0463 (5.032 secs)\n",
      "anp:anp_matern step 40800 lr 3.213e-04 [train_loss] loss -1.0388 (5.160 secs)\n",
      "anp:anp_matern step 41000 lr 3.197e-04 [train_loss] loss -1.0258 (5.118 secs)\n",
      "anp:anp_matern step 41200 lr 3.182e-04 [train_loss] loss -1.0204 (5.223 secs)\n",
      "anp:anp_matern step 41400 lr 3.167e-04 [train_loss] loss -1.0364 (5.075 secs)\n",
      "anp:anp_matern step 41600 lr 3.152e-04 [train_loss] loss -1.0647 (5.051 secs)\n",
      "anp:anp_matern step 41800 lr 3.137e-04 [train_loss] loss -1.0792 (5.050 secs)\n",
      "anp:anp_matern step 42000 lr 3.122e-04 [train_loss] loss -1.0982 (5.143 secs)\n",
      "anp:anp_matern step 42200 lr 3.106e-04 [train_loss] loss -1.0620 (5.118 secs)\n",
      "anp:anp_matern step 42400 lr 3.091e-04 [train_loss] loss -1.0128 (5.057 secs)\n",
      "anp:anp_matern step 42600 lr 3.076e-04 [train_loss] loss -1.0281 (5.058 secs)\n",
      "anp:anp_matern step 42800 lr 3.061e-04 [train_loss] loss -0.9976 (5.059 secs)\n",
      "anp:anp_matern step 43000 lr 3.045e-04 [train_loss] loss -1.0404 (5.153 secs)\n",
      "anp:anp_matern step 43200 lr 3.030e-04 [train_loss] loss -1.0991 (5.236 secs)\n",
      "anp:anp_matern step 43400 lr 3.015e-04 [train_loss] loss -1.0430 (5.124 secs)\n",
      "anp:anp_matern step 43600 lr 2.999e-04 [train_loss] loss -1.0361 (5.113 secs)\n",
      "anp:anp_matern step 43800 lr 2.984e-04 [train_loss] loss -1.0085 (5.022 secs)\n",
      "anp:anp_matern step 44000 lr 2.968e-04 [train_loss] loss -1.0590 (5.148 secs)\n",
      "anp:anp_matern step 44200 lr 2.953e-04 [train_loss] loss -1.0699 (5.223 secs)\n",
      "anp:anp_matern step 44400 lr 2.938e-04 [train_loss] loss -1.1140 (5.323 secs)\n",
      "anp:anp_matern step 44600 lr 2.922e-04 [train_loss] loss -1.1058 (5.279 secs)\n",
      "anp:anp_matern step 44800 lr 2.907e-04 [train_loss] loss -1.0211 (5.234 secs)\n",
      "anp:anp_matern step 45000 lr 2.891e-04 [train_loss] loss -1.0526 (5.312 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 142.84it/s]\n",
      "anp:anp_matern matern ctx_ll 1.2889 tar_ll 0.5567 (21.008 secs)\n",
      "\n",
      "anp:anp_matern step 45200 lr 2.876e-04 [train_loss] loss -1.1049 (5.062 secs)\n",
      "anp:anp_matern step 45400 lr 2.860e-04 [train_loss] loss -1.0581 (5.260 secs)\n",
      "anp:anp_matern step 45600 lr 2.844e-04 [train_loss] loss -0.9757 (5.125 secs)\n",
      "anp:anp_matern step 45800 lr 2.829e-04 [train_loss] loss -1.0601 (5.136 secs)\n",
      "anp:anp_matern step 46000 lr 2.813e-04 [train_loss] loss -1.0310 (5.285 secs)\n",
      "anp:anp_matern step 46200 lr 2.798e-04 [train_loss] loss -1.0676 (5.129 secs)\n",
      "anp:anp_matern step 46400 lr 2.782e-04 [train_loss] loss -1.0490 (5.089 secs)\n",
      "anp:anp_matern step 46600 lr 2.767e-04 [train_loss] loss -1.0237 (5.241 secs)\n",
      "anp:anp_matern step 46800 lr 2.751e-04 [train_loss] loss -1.0739 (5.274 secs)\n",
      "anp:anp_matern step 47000 lr 2.735e-04 [train_loss] loss -1.0595 (5.118 secs)\n",
      "anp:anp_matern step 47200 lr 2.720e-04 [train_loss] loss -1.0405 (5.313 secs)\n",
      "anp:anp_matern step 47400 lr 2.704e-04 [train_loss] loss -1.0043 (5.253 secs)\n",
      "anp:anp_matern step 47600 lr 2.688e-04 [train_loss] loss -1.0576 (5.278 secs)\n",
      "anp:anp_matern step 47800 lr 2.673e-04 [train_loss] loss -1.0217 (5.055 secs)\n",
      "anp:anp_matern step 48000 lr 2.657e-04 [train_loss] loss -1.0325 (5.021 secs)\n",
      "anp:anp_matern step 48200 lr 2.641e-04 [train_loss] loss -1.0458 (5.025 secs)\n",
      "anp:anp_matern step 48400 lr 2.626e-04 [train_loss] loss -1.0910 (5.070 secs)\n",
      "anp:anp_matern step 48600 lr 2.610e-04 [train_loss] loss -1.0892 (5.172 secs)\n",
      "anp:anp_matern step 48800 lr 2.594e-04 [train_loss] loss -1.0239 (5.062 secs)\n",
      "anp:anp_matern step 49000 lr 2.579e-04 [train_loss] loss -1.0203 (5.078 secs)\n",
      "anp:anp_matern step 49200 lr 2.563e-04 [train_loss] loss -1.0543 (5.142 secs)\n",
      "anp:anp_matern step 49400 lr 2.547e-04 [train_loss] loss -1.0886 (5.083 secs)\n",
      "anp:anp_matern step 49600 lr 2.531e-04 [train_loss] loss -1.0279 (5.127 secs)\n",
      "anp:anp_matern step 49800 lr 2.516e-04 [train_loss] loss -1.0426 (5.228 secs)\n",
      "anp:anp_matern step 50000 lr 2.500e-04 [train_loss] loss -1.0341 (5.084 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 143.19it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3558 tar_ll 0.5920 (20.953 secs)\n",
      "\n",
      "anp:anp_matern step 50200 lr 2.484e-04 [train_loss] loss -1.0074 (5.132 secs)\n",
      "anp:anp_matern step 50400 lr 2.469e-04 [train_loss] loss -0.9978 (4.996 secs)\n",
      "anp:anp_matern step 50600 lr 2.453e-04 [train_loss] loss -1.0543 (5.043 secs)\n",
      "anp:anp_matern step 50800 lr 2.437e-04 [train_loss] loss -1.0693 (5.156 secs)\n",
      "anp:anp_matern step 51000 lr 2.421e-04 [train_loss] loss -1.0286 (5.013 secs)\n",
      "anp:anp_matern step 51200 lr 2.406e-04 [train_loss] loss -1.0947 (5.026 secs)\n",
      "anp:anp_matern step 51400 lr 2.390e-04 [train_loss] loss -1.1025 (5.117 secs)\n",
      "anp:anp_matern step 51600 lr 2.374e-04 [train_loss] loss -1.0236 (5.014 secs)\n",
      "anp:anp_matern step 51800 lr 2.359e-04 [train_loss] loss -1.0230 (5.164 secs)\n",
      "anp:anp_matern step 52000 lr 2.343e-04 [train_loss] loss -1.0896 (5.221 secs)\n",
      "anp:anp_matern step 52200 lr 2.327e-04 [train_loss] loss -1.0435 (5.092 secs)\n",
      "anp:anp_matern step 52400 lr 2.312e-04 [train_loss] loss -1.0365 (4.959 secs)\n",
      "anp:anp_matern step 52600 lr 2.296e-04 [train_loss] loss -1.0089 (5.255 secs)\n",
      "anp:anp_matern step 52800 lr 2.280e-04 [train_loss] loss -1.0957 (5.216 secs)\n",
      "anp:anp_matern step 53000 lr 2.265e-04 [train_loss] loss -1.0972 (5.238 secs)\n",
      "anp:anp_matern step 53200 lr 2.249e-04 [train_loss] loss -1.0837 (5.338 secs)\n",
      "anp:anp_matern step 53400 lr 2.233e-04 [train_loss] loss -1.1013 (5.151 secs)\n",
      "anp:anp_matern step 53600 lr 2.218e-04 [train_loss] loss -1.0519 (5.122 secs)\n",
      "anp:anp_matern step 53800 lr 2.202e-04 [train_loss] loss -1.0535 (5.243 secs)\n",
      "anp:anp_matern step 54000 lr 2.187e-04 [train_loss] loss -1.0927 (5.131 secs)\n",
      "anp:anp_matern step 54200 lr 2.171e-04 [train_loss] loss -1.0759 (5.214 secs)\n",
      "anp:anp_matern step 54400 lr 2.156e-04 [train_loss] loss -1.0944 (5.001 secs)\n",
      "anp:anp_matern step 54600 lr 2.140e-04 [train_loss] loss -1.0803 (5.224 secs)\n",
      "anp:anp_matern step 54800 lr 2.124e-04 [train_loss] loss -1.0749 (5.142 secs)\n",
      "anp:anp_matern step 55000 lr 2.109e-04 [train_loss] loss -1.0500 (5.055 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 145.69it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3617 tar_ll 0.6100 (20.592 secs)\n",
      "\n",
      "anp:anp_matern step 55200 lr 2.093e-04 [train_loss] loss -1.0756 (5.240 secs)\n",
      "anp:anp_matern step 55400 lr 2.078e-04 [train_loss] loss -1.0677 (5.101 secs)\n",
      "anp:anp_matern step 55600 lr 2.062e-04 [train_loss] loss -1.0475 (5.225 secs)\n",
      "anp:anp_matern step 55800 lr 2.047e-04 [train_loss] loss -1.0465 (4.968 secs)\n",
      "anp:anp_matern step 56000 lr 2.032e-04 [train_loss] loss -1.1126 (4.959 secs)\n",
      "anp:anp_matern step 56200 lr 2.016e-04 [train_loss] loss -1.0895 (5.251 secs)\n",
      "anp:anp_matern step 56400 lr 2.001e-04 [train_loss] loss -1.0602 (5.044 secs)\n",
      "anp:anp_matern step 56600 lr 1.985e-04 [train_loss] loss -1.0170 (5.170 secs)\n",
      "anp:anp_matern step 56800 lr 1.970e-04 [train_loss] loss -1.1158 (5.174 secs)\n",
      "anp:anp_matern step 57000 lr 1.955e-04 [train_loss] loss -1.0772 (5.170 secs)\n",
      "anp:anp_matern step 57200 lr 1.939e-04 [train_loss] loss -1.0317 (5.174 secs)\n",
      "anp:anp_matern step 57400 lr 1.924e-04 [train_loss] loss -1.0559 (5.085 secs)\n",
      "anp:anp_matern step 57600 lr 1.909e-04 [train_loss] loss -1.0473 (5.065 secs)\n",
      "anp:anp_matern step 57800 lr 1.894e-04 [train_loss] loss -1.0965 (5.402 secs)\n",
      "anp:anp_matern step 58000 lr 1.878e-04 [train_loss] loss -1.0738 (5.330 secs)\n",
      "anp:anp_matern step 58200 lr 1.863e-04 [train_loss] loss -1.0398 (5.212 secs)\n",
      "anp:anp_matern step 58400 lr 1.848e-04 [train_loss] loss -1.0956 (5.080 secs)\n",
      "anp:anp_matern step 58600 lr 1.833e-04 [train_loss] loss -1.0523 (5.206 secs)\n",
      "anp:anp_matern step 58800 lr 1.818e-04 [train_loss] loss -1.0933 (5.018 secs)\n",
      "anp:anp_matern step 59000 lr 1.803e-04 [train_loss] loss -1.0260 (5.019 secs)\n",
      "anp:anp_matern step 59200 lr 1.787e-04 [train_loss] loss -1.1019 (5.263 secs)\n",
      "anp:anp_matern step 59400 lr 1.772e-04 [train_loss] loss -1.0309 (4.954 secs)\n",
      "anp:anp_matern step 59600 lr 1.757e-04 [train_loss] loss -1.0252 (5.066 secs)\n",
      "anp:anp_matern step 59800 lr 1.742e-04 [train_loss] loss -1.0388 (5.149 secs)\n",
      "anp:anp_matern step 60000 lr 1.727e-04 [train_loss] loss -1.0670 (4.996 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 141.24it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3523 tar_ll 0.5942 (21.243 secs)\n",
      "\n",
      "anp:anp_matern step 60200 lr 1.713e-04 [train_loss] loss -1.0704 (5.181 secs)\n",
      "anp:anp_matern step 60400 lr 1.698e-04 [train_loss] loss -1.0753 (5.151 secs)\n",
      "anp:anp_matern step 60600 lr 1.683e-04 [train_loss] loss -1.0588 (5.184 secs)\n",
      "anp:anp_matern step 60800 lr 1.668e-04 [train_loss] loss -1.0758 (5.241 secs)\n",
      "anp:anp_matern step 61000 lr 1.653e-04 [train_loss] loss -1.0864 (5.348 secs)\n",
      "anp:anp_matern step 61200 lr 1.638e-04 [train_loss] loss -1.0511 (5.307 secs)\n",
      "anp:anp_matern step 61400 lr 1.624e-04 [train_loss] loss -1.0319 (5.242 secs)\n",
      "anp:anp_matern step 61600 lr 1.609e-04 [train_loss] loss -1.0403 (5.186 secs)\n",
      "anp:anp_matern step 61800 lr 1.594e-04 [train_loss] loss -1.0871 (5.149 secs)\n",
      "anp:anp_matern step 62000 lr 1.580e-04 [train_loss] loss -1.0780 (5.097 secs)\n",
      "anp:anp_matern step 62200 lr 1.565e-04 [train_loss] loss -1.0416 (5.162 secs)\n",
      "anp:anp_matern step 62400 lr 1.551e-04 [train_loss] loss -1.0829 (5.129 secs)\n",
      "anp:anp_matern step 62600 lr 1.536e-04 [train_loss] loss -1.1190 (5.208 secs)\n",
      "anp:anp_matern step 62800 lr 1.522e-04 [train_loss] loss -1.0707 (5.359 secs)\n",
      "anp:anp_matern step 63000 lr 1.507e-04 [train_loss] loss -1.0326 (5.029 secs)\n",
      "anp:anp_matern step 63200 lr 1.493e-04 [train_loss] loss -1.0627 (5.163 secs)\n",
      "anp:anp_matern step 63400 lr 1.478e-04 [train_loss] loss -1.0575 (5.257 secs)\n",
      "anp:anp_matern step 63600 lr 1.464e-04 [train_loss] loss -1.1139 (5.162 secs)\n",
      "anp:anp_matern step 63800 lr 1.450e-04 [train_loss] loss -1.0295 (5.077 secs)\n",
      "anp:anp_matern step 64000 lr 1.436e-04 [train_loss] loss -1.1120 (5.222 secs)\n",
      "anp:anp_matern step 64200 lr 1.421e-04 [train_loss] loss -0.9982 (4.989 secs)\n",
      "anp:anp_matern step 64400 lr 1.407e-04 [train_loss] loss -1.0169 (5.039 secs)\n",
      "anp:anp_matern step 64600 lr 1.393e-04 [train_loss] loss -1.0490 (5.113 secs)\n",
      "anp:anp_matern step 64800 lr 1.379e-04 [train_loss] loss -1.0432 (4.832 secs)\n",
      "anp:anp_matern step 65000 lr 1.365e-04 [train_loss] loss -1.0918 (5.123 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 141.04it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3648 tar_ll 0.6098 (21.272 secs)\n",
      "\n",
      "anp:anp_matern step 65200 lr 1.351e-04 [train_loss] loss -1.0649 (5.006 secs)\n",
      "anp:anp_matern step 65400 lr 1.337e-04 [train_loss] loss -1.0733 (4.915 secs)\n",
      "anp:anp_matern step 65600 lr 1.323e-04 [train_loss] loss -1.0477 (5.089 secs)\n",
      "anp:anp_matern step 65800 lr 1.309e-04 [train_loss] loss -1.0810 (5.082 secs)\n",
      "anp:anp_matern step 66000 lr 1.296e-04 [train_loss] loss -1.0800 (5.013 secs)\n",
      "anp:anp_matern step 66200 lr 1.282e-04 [train_loss] loss -1.0510 (5.071 secs)\n",
      "anp:anp_matern step 66400 lr 1.268e-04 [train_loss] loss -1.0564 (4.955 secs)\n",
      "anp:anp_matern step 66600 lr 1.255e-04 [train_loss] loss -1.0743 (4.873 secs)\n",
      "anp:anp_matern step 66800 lr 1.241e-04 [train_loss] loss -1.1234 (4.977 secs)\n",
      "anp:anp_matern step 67000 lr 1.227e-04 [train_loss] loss -1.1116 (5.002 secs)\n",
      "anp:anp_matern step 67200 lr 1.214e-04 [train_loss] loss -1.0831 (5.053 secs)\n",
      "anp:anp_matern step 67400 lr 1.200e-04 [train_loss] loss -1.0743 (5.080 secs)\n",
      "anp:anp_matern step 67600 lr 1.187e-04 [train_loss] loss -1.0951 (5.155 secs)\n",
      "anp:anp_matern step 67800 lr 1.174e-04 [train_loss] loss -1.1045 (5.154 secs)\n",
      "anp:anp_matern step 68000 lr 1.160e-04 [train_loss] loss -1.0877 (5.109 secs)\n",
      "anp:anp_matern step 68200 lr 1.147e-04 [train_loss] loss -1.0219 (5.141 secs)\n",
      "anp:anp_matern step 68400 lr 1.134e-04 [train_loss] loss -1.0805 (5.208 secs)\n",
      "anp:anp_matern step 68600 lr 1.121e-04 [train_loss] loss -1.0836 (5.130 secs)\n",
      "anp:anp_matern step 68800 lr 1.108e-04 [train_loss] loss -1.0815 (5.237 secs)\n",
      "anp:anp_matern step 69000 lr 1.095e-04 [train_loss] loss -1.1231 (5.123 secs)\n",
      "anp:anp_matern step 69200 lr 1.082e-04 [train_loss] loss -1.0521 (5.052 secs)\n",
      "anp:anp_matern step 69400 lr 1.069e-04 [train_loss] loss -1.0712 (5.289 secs)\n",
      "anp:anp_matern step 69600 lr 1.056e-04 [train_loss] loss -1.0592 (5.069 secs)\n",
      "anp:anp_matern step 69800 lr 1.043e-04 [train_loss] loss -1.0433 (5.082 secs)\n",
      "anp:anp_matern step 70000 lr 1.031e-04 [train_loss] loss -1.1196 (5.333 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 143.01it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3647 tar_ll 0.6188 (20.981 secs)\n",
      "\n",
      "anp:anp_matern step 70200 lr 1.018e-04 [train_loss] loss -1.1124 (5.098 secs)\n",
      "anp:anp_matern step 70400 lr 1.005e-04 [train_loss] loss -1.1091 (5.288 secs)\n",
      "anp:anp_matern step 70600 lr 9.927e-05 [train_loss] loss -1.0617 (5.103 secs)\n",
      "anp:anp_matern step 70800 lr 9.802e-05 [train_loss] loss -1.0868 (5.119 secs)\n",
      "anp:anp_matern step 71000 lr 9.677e-05 [train_loss] loss -1.1125 (5.343 secs)\n",
      "anp:anp_matern step 71200 lr 9.554e-05 [train_loss] loss -1.0684 (5.111 secs)\n",
      "anp:anp_matern step 71400 lr 9.430e-05 [train_loss] loss -1.1211 (5.107 secs)\n",
      "anp:anp_matern step 71600 lr 9.308e-05 [train_loss] loss -1.0772 (5.140 secs)\n",
      "anp:anp_matern step 71800 lr 9.186e-05 [train_loss] loss -1.1104 (5.134 secs)\n",
      "anp:anp_matern step 72000 lr 9.064e-05 [train_loss] loss -1.0969 (5.076 secs)\n",
      "anp:anp_matern step 72200 lr 8.944e-05 [train_loss] loss -1.0730 (5.262 secs)\n",
      "anp:anp_matern step 72400 lr 8.824e-05 [train_loss] loss -1.0232 (5.032 secs)\n",
      "anp:anp_matern step 72600 lr 8.704e-05 [train_loss] loss -1.1505 (5.038 secs)\n",
      "anp:anp_matern step 72800 lr 8.585e-05 [train_loss] loss -1.0573 (5.161 secs)\n",
      "anp:anp_matern step 73000 lr 8.467e-05 [train_loss] loss -1.0395 (5.108 secs)\n",
      "anp:anp_matern step 73200 lr 8.350e-05 [train_loss] loss -1.0778 (5.120 secs)\n",
      "anp:anp_matern step 73400 lr 8.233e-05 [train_loss] loss -1.0567 (5.175 secs)\n",
      "anp:anp_matern step 73600 lr 8.117e-05 [train_loss] loss -1.1021 (5.179 secs)\n",
      "anp:anp_matern step 73800 lr 8.001e-05 [train_loss] loss -1.1208 (5.035 secs)\n",
      "anp:anp_matern step 74000 lr 7.886e-05 [train_loss] loss -1.0569 (5.140 secs)\n",
      "anp:anp_matern step 74200 lr 7.772e-05 [train_loss] loss -1.0662 (5.291 secs)\n",
      "anp:anp_matern step 74400 lr 7.659e-05 [train_loss] loss -1.1356 (5.111 secs)\n",
      "anp:anp_matern step 74600 lr 7.546e-05 [train_loss] loss -1.1096 (5.162 secs)\n",
      "anp:anp_matern step 74800 lr 7.434e-05 [train_loss] loss -1.1316 (5.382 secs)\n",
      "anp:anp_matern step 75000 lr 7.322e-05 [train_loss] loss -1.1157 (5.128 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 142.58it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3679 tar_ll 0.6178 (21.043 secs)\n",
      "\n",
      "anp:anp_matern step 75200 lr 7.212e-05 [train_loss] loss -1.0770 (5.177 secs)\n",
      "anp:anp_matern step 75400 lr 7.102e-05 [train_loss] loss -1.0792 (5.215 secs)\n",
      "anp:anp_matern step 75600 lr 6.992e-05 [train_loss] loss -1.0879 (5.427 secs)\n",
      "anp:anp_matern step 75800 lr 6.884e-05 [train_loss] loss -1.0641 (5.263 secs)\n",
      "anp:anp_matern step 76000 lr 6.776e-05 [train_loss] loss -1.0775 (5.365 secs)\n",
      "anp:anp_matern step 76200 lr 6.669e-05 [train_loss] loss -1.1046 (5.172 secs)\n",
      "anp:anp_matern step 76400 lr 6.562e-05 [train_loss] loss -1.0423 (5.268 secs)\n",
      "anp:anp_matern step 76600 lr 6.456e-05 [train_loss] loss -1.0338 (5.223 secs)\n",
      "anp:anp_matern step 76800 lr 6.351e-05 [train_loss] loss -1.0920 (5.185 secs)\n",
      "anp:anp_matern step 77000 lr 6.247e-05 [train_loss] loss -1.0953 (5.173 secs)\n",
      "anp:anp_matern step 77200 lr 6.144e-05 [train_loss] loss -1.1047 (5.322 secs)\n",
      "anp:anp_matern step 77400 lr 6.041e-05 [train_loss] loss -1.0733 (5.386 secs)\n",
      "anp:anp_matern step 77600 lr 5.939e-05 [train_loss] loss -1.0420 (5.191 secs)\n",
      "anp:anp_matern step 77800 lr 5.838e-05 [train_loss] loss -1.0881 (5.235 secs)\n",
      "anp:anp_matern step 78000 lr 5.737e-05 [train_loss] loss -1.0640 (5.151 secs)\n",
      "anp:anp_matern step 78200 lr 5.637e-05 [train_loss] loss -1.0744 (5.197 secs)\n",
      "anp:anp_matern step 78400 lr 5.538e-05 [train_loss] loss -1.0985 (5.112 secs)\n",
      "anp:anp_matern step 78600 lr 5.440e-05 [train_loss] loss -1.0902 (4.945 secs)\n",
      "anp:anp_matern step 78800 lr 5.343e-05 [train_loss] loss -1.0971 (5.257 secs)\n",
      "anp:anp_matern step 79000 lr 5.246e-05 [train_loss] loss -1.0763 (5.146 secs)\n",
      "anp:anp_matern step 79200 lr 5.150e-05 [train_loss] loss -1.0803 (5.097 secs)\n",
      "anp:anp_matern step 79400 lr 5.055e-05 [train_loss] loss -1.0717 (5.407 secs)\n",
      "anp:anp_matern step 79600 lr 4.961e-05 [train_loss] loss -1.0912 (5.139 secs)\n",
      "anp:anp_matern step 79800 lr 4.867e-05 [train_loss] loss -1.0531 (5.164 secs)\n",
      "anp:anp_matern step 80000 lr 4.775e-05 [train_loss] loss -1.0628 (5.102 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 143.80it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3691 tar_ll 0.6218 (20.866 secs)\n",
      "\n",
      "anp:anp_matern step 80200 lr 4.683e-05 [train_loss] loss -1.0728 (5.073 secs)\n",
      "anp:anp_matern step 80400 lr 4.592e-05 [train_loss] loss -1.0923 (5.154 secs)\n",
      "anp:anp_matern step 80600 lr 4.501e-05 [train_loss] loss -1.0806 (5.135 secs)\n",
      "anp:anp_matern step 80800 lr 4.412e-05 [train_loss] loss -1.1094 (5.112 secs)\n",
      "anp:anp_matern step 81000 lr 4.323e-05 [train_loss] loss -1.0738 (5.219 secs)\n",
      "anp:anp_matern step 81200 lr 4.235e-05 [train_loss] loss -1.0868 (5.191 secs)\n",
      "anp:anp_matern step 81400 lr 4.148e-05 [train_loss] loss -1.0922 (5.020 secs)\n",
      "anp:anp_matern step 81600 lr 4.062e-05 [train_loss] loss -1.0757 (5.210 secs)\n",
      "anp:anp_matern step 81800 lr 3.976e-05 [train_loss] loss -1.0287 (5.095 secs)\n",
      "anp:anp_matern step 82000 lr 3.892e-05 [train_loss] loss -1.1065 (5.039 secs)\n",
      "anp:anp_matern step 82200 lr 3.808e-05 [train_loss] loss -1.0781 (5.074 secs)\n",
      "anp:anp_matern step 82400 lr 3.725e-05 [train_loss] loss -1.1369 (5.009 secs)\n",
      "anp:anp_matern step 82600 lr 3.643e-05 [train_loss] loss -1.0857 (5.071 secs)\n",
      "anp:anp_matern step 82800 lr 3.562e-05 [train_loss] loss -1.0573 (5.043 secs)\n",
      "anp:anp_matern step 83000 lr 3.481e-05 [train_loss] loss -1.0876 (5.020 secs)\n",
      "anp:anp_matern step 83200 lr 3.402e-05 [train_loss] loss -1.0914 (4.969 secs)\n",
      "anp:anp_matern step 83400 lr 3.323e-05 [train_loss] loss -1.0576 (4.952 secs)\n",
      "anp:anp_matern step 83600 lr 3.245e-05 [train_loss] loss -1.0916 (5.251 secs)\n",
      "anp:anp_matern step 83800 lr 3.168e-05 [train_loss] loss -1.0671 (5.187 secs)\n",
      "anp:anp_matern step 84000 lr 3.092e-05 [train_loss] loss -1.0559 (5.109 secs)\n",
      "anp:anp_matern step 84200 lr 3.017e-05 [train_loss] loss -1.0196 (5.319 secs)\n",
      "anp:anp_matern step 84400 lr 2.943e-05 [train_loss] loss -1.1239 (5.059 secs)\n",
      "anp:anp_matern step 84600 lr 2.869e-05 [train_loss] loss -1.0917 (5.097 secs)\n",
      "anp:anp_matern step 84800 lr 2.797e-05 [train_loss] loss -1.0479 (5.266 secs)\n",
      "anp:anp_matern step 85000 lr 2.725e-05 [train_loss] loss -1.1037 (5.109 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 145.71it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3703 tar_ll 0.6306 (20.592 secs)\n",
      "\n",
      "anp:anp_matern step 85200 lr 2.654e-05 [train_loss] loss -1.0973 (5.373 secs)\n",
      "anp:anp_matern step 85400 lr 2.584e-05 [train_loss] loss -1.0929 (5.266 secs)\n",
      "anp:anp_matern step 85600 lr 2.515e-05 [train_loss] loss -1.0903 (5.172 secs)\n",
      "anp:anp_matern step 85800 lr 2.447e-05 [train_loss] loss -1.1395 (5.256 secs)\n",
      "anp:anp_matern step 86000 lr 2.379e-05 [train_loss] loss -1.1038 (5.069 secs)\n",
      "anp:anp_matern step 86200 lr 2.313e-05 [train_loss] loss -1.1301 (5.169 secs)\n",
      "anp:anp_matern step 86400 lr 2.247e-05 [train_loss] loss -1.1170 (5.358 secs)\n",
      "anp:anp_matern step 86600 lr 2.183e-05 [train_loss] loss -1.1053 (5.099 secs)\n",
      "anp:anp_matern step 86800 lr 2.119e-05 [train_loss] loss -1.1023 (5.051 secs)\n",
      "anp:anp_matern step 87000 lr 2.056e-05 [train_loss] loss -1.0847 (5.145 secs)\n",
      "anp:anp_matern step 87200 lr 1.994e-05 [train_loss] loss -1.1247 (5.156 secs)\n",
      "anp:anp_matern step 87400 lr 1.933e-05 [train_loss] loss -1.1071 (5.031 secs)\n",
      "anp:anp_matern step 87600 lr 1.873e-05 [train_loss] loss -1.0999 (5.152 secs)\n",
      "anp:anp_matern step 87800 lr 1.814e-05 [train_loss] loss -1.0580 (5.015 secs)\n",
      "anp:anp_matern step 88000 lr 1.756e-05 [train_loss] loss -1.0887 (5.151 secs)\n",
      "anp:anp_matern step 88200 lr 1.698e-05 [train_loss] loss -1.1204 (5.097 secs)\n",
      "anp:anp_matern step 88400 lr 1.642e-05 [train_loss] loss -1.1074 (4.948 secs)\n",
      "anp:anp_matern step 88600 lr 1.586e-05 [train_loss] loss -1.0537 (5.103 secs)\n",
      "anp:anp_matern step 88800 lr 1.532e-05 [train_loss] loss -1.0908 (5.194 secs)\n",
      "anp:anp_matern step 89000 lr 1.478e-05 [train_loss] loss -1.1055 (5.184 secs)\n",
      "anp:anp_matern step 89200 lr 1.425e-05 [train_loss] loss -1.0839 (5.225 secs)\n",
      "anp:anp_matern step 89400 lr 1.373e-05 [train_loss] loss -1.1021 (5.280 secs)\n",
      "anp:anp_matern step 89600 lr 1.323e-05 [train_loss] loss -1.0697 (5.188 secs)\n",
      "anp:anp_matern step 89800 lr 1.273e-05 [train_loss] loss -1.1072 (5.335 secs)\n",
      "anp:anp_matern step 90000 lr 1.224e-05 [train_loss] loss -1.1322 (5.296 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 145.31it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3710 tar_ll 0.6312 (20.649 secs)\n",
      "\n",
      "anp:anp_matern step 90200 lr 1.176e-05 [train_loss] loss -1.0478 (5.032 secs)\n",
      "anp:anp_matern step 90400 lr 1.128e-05 [train_loss] loss -1.0680 (5.033 secs)\n",
      "anp:anp_matern step 90600 lr 1.082e-05 [train_loss] loss -1.0461 (5.104 secs)\n",
      "anp:anp_matern step 90800 lr 1.037e-05 [train_loss] loss -1.0640 (5.017 secs)\n",
      "anp:anp_matern step 91000 lr 9.927e-06 [train_loss] loss -1.0727 (5.060 secs)\n",
      "anp:anp_matern step 91200 lr 9.493e-06 [train_loss] loss -1.0933 (5.163 secs)\n",
      "anp:anp_matern step 91400 lr 9.069e-06 [train_loss] loss -1.0512 (5.101 secs)\n",
      "anp:anp_matern step 91600 lr 8.655e-06 [train_loss] loss -1.1243 (5.080 secs)\n",
      "anp:anp_matern step 91800 lr 8.250e-06 [train_loss] loss -1.0933 (5.197 secs)\n",
      "anp:anp_matern step 92000 lr 7.854e-06 [train_loss] loss -1.0876 (5.080 secs)\n",
      "anp:anp_matern step 92200 lr 7.468e-06 [train_loss] loss -1.0824 (5.120 secs)\n",
      "anp:anp_matern step 92400 lr 7.092e-06 [train_loss] loss -1.0903 (5.245 secs)\n",
      "anp:anp_matern step 92600 lr 6.725e-06 [train_loss] loss -1.1439 (5.392 secs)\n",
      "anp:anp_matern step 92800 lr 6.368e-06 [train_loss] loss -1.1135 (5.227 secs)\n",
      "anp:anp_matern step 93000 lr 6.021e-06 [train_loss] loss -1.0770 (5.257 secs)\n",
      "anp:anp_matern step 93200 lr 5.683e-06 [train_loss] loss -1.1132 (5.195 secs)\n",
      "anp:anp_matern step 93400 lr 5.355e-06 [train_loss] loss -1.1098 (5.176 secs)\n",
      "anp:anp_matern step 93600 lr 5.036e-06 [train_loss] loss -1.0827 (5.308 secs)\n",
      "anp:anp_matern step 93800 lr 4.727e-06 [train_loss] loss -1.1229 (5.198 secs)\n",
      "anp:anp_matern step 94000 lr 4.428e-06 [train_loss] loss -1.0886 (5.201 secs)\n",
      "anp:anp_matern step 94200 lr 4.139e-06 [train_loss] loss -1.0916 (5.415 secs)\n",
      "anp:anp_matern step 94400 lr 3.859e-06 [train_loss] loss -1.1260 (5.137 secs)\n",
      "anp:anp_matern step 94600 lr 3.589e-06 [train_loss] loss -1.0915 (5.151 secs)\n",
      "anp:anp_matern step 94800 lr 3.329e-06 [train_loss] loss -1.1060 (5.249 secs)\n",
      "anp:anp_matern step 95000 lr 3.078e-06 [train_loss] loss -1.1180 (5.142 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 140.48it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3717 tar_ll 0.6315 (21.357 secs)\n",
      "\n",
      "anp:anp_matern step 95200 lr 2.837e-06 [train_loss] loss -1.0918 (5.243 secs)\n",
      "anp:anp_matern step 95400 lr 2.606e-06 [train_loss] loss -1.0482 (4.987 secs)\n",
      "anp:anp_matern step 95600 lr 2.385e-06 [train_loss] loss -1.0855 (5.075 secs)\n",
      "anp:anp_matern step 95800 lr 2.173e-06 [train_loss] loss -1.0435 (5.070 secs)\n",
      "anp:anp_matern step 96000 lr 1.971e-06 [train_loss] loss -1.1068 (5.113 secs)\n",
      "anp:anp_matern step 96200 lr 1.779e-06 [train_loss] loss -1.1306 (4.983 secs)\n",
      "anp:anp_matern step 96400 lr 1.597e-06 [train_loss] loss -1.1003 (5.140 secs)\n",
      "anp:anp_matern step 96600 lr 1.425e-06 [train_loss] loss -1.0520 (4.953 secs)\n",
      "anp:anp_matern step 96800 lr 1.262e-06 [train_loss] loss -1.0453 (5.104 secs)\n",
      "anp:anp_matern step 97000 lr 1.110e-06 [train_loss] loss -1.0718 (5.141 secs)\n",
      "anp:anp_matern step 97200 lr 9.666e-07 [train_loss] loss -1.0816 (5.123 secs)\n",
      "anp:anp_matern step 97400 lr 8.335e-07 [train_loss] loss -1.1067 (5.120 secs)\n",
      "anp:anp_matern step 97600 lr 7.103e-07 [train_loss] loss -1.0679 (5.174 secs)\n",
      "anp:anp_matern step 97800 lr 5.969e-07 [train_loss] loss -1.1101 (5.020 secs)\n",
      "anp:anp_matern step 98000 lr 4.933e-07 [train_loss] loss -1.0638 (5.095 secs)\n",
      "anp:anp_matern step 98200 lr 3.996e-07 [train_loss] loss -1.0709 (5.158 secs)\n",
      "anp:anp_matern step 98400 lr 3.158e-07 [train_loss] loss -1.0886 (5.314 secs)\n",
      "anp:anp_matern step 98600 lr 2.418e-07 [train_loss] loss -1.0709 (5.039 secs)\n",
      "anp:anp_matern step 98800 lr 1.776e-07 [train_loss] loss -1.0591 (5.063 secs)\n",
      "anp:anp_matern step 99000 lr 1.234e-07 [train_loss] loss -1.0490 (4.987 secs)\n",
      "anp:anp_matern step 99200 lr 7.895e-08 [train_loss] loss -1.0588 (5.073 secs)\n",
      "anp:anp_matern step 99400 lr 4.441e-08 [train_loss] loss -1.0862 (5.009 secs)\n",
      "anp:anp_matern step 99600 lr 1.974e-08 [train_loss] loss -1.1017 (5.035 secs)\n",
      "anp:anp_matern step 99800 lr 4.935e-09 [train_loss] loss -1.0677 (5.089 secs)\n",
      "anp:anp_matern step 100000 lr 0.000e+00 [train_loss] loss -1.1076 (5.104 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 147.08it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3718 tar_ll 0.6314 (20.399 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:20<00:00, 142.89it/s]\n",
      "anp:anp_matern matern ctx_ll 1.3718 tar_ll 0.6315 (20.998 secs)\n",
      "anp:anp_matern matern ctx_ll 1.3718 tar_ll 0.6315 (20.998 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3318189.5 miliseconds\n",
      "Execution time: 3318.1895 seconds\n",
      "Initial Memory Usage: 0.0 MB\n",
      "Final Memory Usage: 196.251953125 MB\n",
      "Memory Usage Change: 196.251953125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='anp', name='anp_matern',val_seed=100, val_l=None,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc19d8c2-1580-48a2-9290-f5b17e5f290c",
   "metadata": {},
   "source": [
    "## BNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a40d54-5d75-4326-a53a-eea94c9f405a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: bnp-bnp_matern\n",
      "Total number of parameters: 248450\n",
      "\n",
      "bnp:bnp_matern step 200 lr 5.000e-04 [train_loss] ll_base -0.6622 ll -0.6593 loss 1.3215 (4.596 secs)\n",
      "bnp:bnp_matern step 400 lr 5.000e-04 [train_loss] ll_base -0.5809 ll -0.5805 loss 1.1613 (4.563 secs)\n",
      "bnp:bnp_matern step 600 lr 5.000e-04 [train_loss] ll_base -0.4947 ll -0.4885 loss 0.9832 (4.707 secs)\n",
      "bnp:bnp_matern step 800 lr 4.999e-04 [train_loss] ll_base -0.4925 ll -0.4789 loss 0.9714 (4.565 secs)\n",
      "bnp:bnp_matern step 1000 lr 4.999e-04 [train_loss] ll_base -0.4623 ll -0.4467 loss 0.9089 (4.623 secs)\n",
      "bnp:bnp_matern step 1200 lr 4.998e-04 [train_loss] ll_base -0.4381 ll -0.4230 loss 0.8611 (4.477 secs)\n",
      "bnp:bnp_matern step 1400 lr 4.998e-04 [train_loss] ll_base -0.4601 ll -0.4378 loss 0.8980 (4.443 secs)\n",
      "bnp:bnp_matern step 1600 lr 4.997e-04 [train_loss] ll_base -0.4745 ll -0.4571 loss 0.9316 (4.385 secs)\n",
      "bnp:bnp_matern step 1800 lr 4.996e-04 [train_loss] ll_base -0.4321 ll -0.4139 loss 0.8461 (4.649 secs)\n",
      "bnp:bnp_matern step 2000 lr 4.995e-04 [train_loss] ll_base -0.4099 ll -0.3855 loss 0.7954 (4.613 secs)\n",
      "bnp:bnp_matern step 2200 lr 4.994e-04 [train_loss] ll_base -0.3624 ll -0.3361 loss 0.6986 (4.735 secs)\n",
      "bnp:bnp_matern step 2400 lr 4.993e-04 [train_loss] ll_base -0.3669 ll -0.3386 loss 0.7055 (4.831 secs)\n",
      "bnp:bnp_matern step 2600 lr 4.992e-04 [train_loss] ll_base -0.3674 ll -0.3365 loss 0.7039 (4.603 secs)\n",
      "bnp:bnp_matern step 2800 lr 4.990e-04 [train_loss] ll_base -0.3379 ll -0.3023 loss 0.6402 (4.432 secs)\n",
      "bnp:bnp_matern step 3000 lr 4.989e-04 [train_loss] ll_base -0.3515 ll -0.3193 loss 0.6708 (4.502 secs)\n",
      "bnp:bnp_matern step 3200 lr 4.987e-04 [train_loss] ll_base -0.3399 ll -0.3021 loss 0.6420 (4.656 secs)\n",
      "bnp:bnp_matern step 3400 lr 4.986e-04 [train_loss] ll_base -0.3726 ll -0.3350 loss 0.7076 (4.410 secs)\n",
      "bnp:bnp_matern step 3600 lr 4.984e-04 [train_loss] ll_base -0.3368 ll -0.2980 loss 0.6348 (4.409 secs)\n",
      "bnp:bnp_matern step 3800 lr 4.982e-04 [train_loss] ll_base -0.3583 ll -0.3179 loss 0.6761 (4.501 secs)\n",
      "bnp:bnp_matern step 4000 lr 4.980e-04 [train_loss] ll_base -0.3312 ll -0.2947 loss 0.6259 (4.340 secs)\n",
      "bnp:bnp_matern step 4200 lr 4.978e-04 [train_loss] ll_base -0.2771 ll -0.2351 loss 0.5122 (4.313 secs)\n",
      "bnp:bnp_matern step 4400 lr 4.976e-04 [train_loss] ll_base -0.3201 ll -0.2719 loss 0.5920 (4.545 secs)\n",
      "bnp:bnp_matern step 4600 lr 4.974e-04 [train_loss] ll_base -0.2677 ll -0.2212 loss 0.4889 (4.522 secs)\n",
      "bnp:bnp_matern step 4800 lr 4.972e-04 [train_loss] ll_base -0.2763 ll -0.2263 loss 0.5026 (4.503 secs)\n",
      "bnp:bnp_matern step 5000 lr 4.969e-04 [train_loss] ll_base -0.2610 ll -0.2152 loss 0.4763 (4.474 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 109.68it/s]\n",
      "bnp:bnp_matern matern ctx_ll -0.0845 tar_ll -0.3260 (27.355 secs)\n",
      "\n",
      "bnp:bnp_matern step 5200 lr 4.967e-04 [train_loss] ll_base -0.2259 ll -0.1770 loss 0.4029 (4.500 secs)\n",
      "bnp:bnp_matern step 5400 lr 4.964e-04 [train_loss] ll_base -0.2409 ll -0.1865 loss 0.4275 (4.440 secs)\n",
      "bnp:bnp_matern step 5600 lr 4.961e-04 [train_loss] ll_base -0.2373 ll -0.1852 loss 0.4225 (4.486 secs)\n",
      "bnp:bnp_matern step 5800 lr 4.959e-04 [train_loss] ll_base -0.2040 ll -0.1408 loss 0.3447 (4.408 secs)\n",
      "bnp:bnp_matern step 6000 lr 4.956e-04 [train_loss] ll_base -0.1846 ll -0.1317 loss 0.3163 (4.410 secs)\n",
      "bnp:bnp_matern step 6200 lr 4.953e-04 [train_loss] ll_base -0.1914 ll -0.1382 loss 0.3296 (4.604 secs)\n",
      "bnp:bnp_matern step 6400 lr 4.950e-04 [train_loss] ll_base -0.1597 ll -0.1081 loss 0.2678 (4.383 secs)\n",
      "bnp:bnp_matern step 6600 lr 4.946e-04 [train_loss] ll_base -0.1805 ll -0.1274 loss 0.3079 (4.565 secs)\n",
      "bnp:bnp_matern step 6800 lr 4.943e-04 [train_loss] ll_base -0.1867 ll -0.1323 loss 0.3190 (4.467 secs)\n",
      "bnp:bnp_matern step 7000 lr 4.940e-04 [train_loss] ll_base -0.1023 ll -0.0476 loss 0.1499 (4.434 secs)\n",
      "bnp:bnp_matern step 7200 lr 4.936e-04 [train_loss] ll_base -0.1753 ll -0.1154 loss 0.2907 (4.443 secs)\n",
      "bnp:bnp_matern step 7400 lr 4.933e-04 [train_loss] ll_base -0.0952 ll -0.0332 loss 0.1284 (4.519 secs)\n",
      "bnp:bnp_matern step 7600 lr 4.929e-04 [train_loss] ll_base -0.1407 ll -0.0821 loss 0.2229 (4.666 secs)\n",
      "bnp:bnp_matern step 7800 lr 4.925e-04 [train_loss] ll_base -0.1188 ll -0.0560 loss 0.1748 (4.529 secs)\n",
      "bnp:bnp_matern step 8000 lr 4.921e-04 [train_loss] ll_base -0.1230 ll -0.0579 loss 0.1809 (4.379 secs)\n",
      "bnp:bnp_matern step 8200 lr 4.918e-04 [train_loss] ll_base -0.0921 ll -0.0355 loss 0.1277 (4.525 secs)\n",
      "bnp:bnp_matern step 8400 lr 4.913e-04 [train_loss] ll_base -0.1021 ll -0.0459 loss 0.1480 (4.575 secs)\n",
      "bnp:bnp_matern step 8600 lr 4.909e-04 [train_loss] ll_base -0.0846 ll -0.0181 loss 0.1027 (4.463 secs)\n",
      "bnp:bnp_matern step 8800 lr 4.905e-04 [train_loss] ll_base -0.1044 ll -0.0348 loss 0.1393 (4.559 secs)\n",
      "bnp:bnp_matern step 9000 lr 4.901e-04 [train_loss] ll_base -0.0778 ll -0.0114 loss 0.0892 (4.713 secs)\n",
      "bnp:bnp_matern step 9200 lr 4.896e-04 [train_loss] ll_base -0.0587 ll 0.0113 loss 0.0474 (4.528 secs)\n",
      "bnp:bnp_matern step 9400 lr 4.892e-04 [train_loss] ll_base -0.0972 ll -0.0255 loss 0.1226 (4.494 secs)\n",
      "bnp:bnp_matern step 9600 lr 4.887e-04 [train_loss] ll_base -0.0720 ll -0.0009 loss 0.0729 (4.678 secs)\n",
      "bnp:bnp_matern step 9800 lr 4.882e-04 [train_loss] ll_base -0.0362 ll 0.0414 loss -0.0052 (4.800 secs)\n",
      "bnp:bnp_matern step 10000 lr 4.878e-04 [train_loss] ll_base -0.0570 ll 0.0141 loss 0.0430 (4.564 secs)\n",
      "100%|##########| 3000/3000 [00:28<00:00, 106.87it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.0791 tar_ll -0.1962 (28.074 secs)\n",
      "\n",
      "bnp:bnp_matern step 10200 lr 4.873e-04 [train_loss] ll_base -0.0147 ll 0.0488 loss -0.0341 (4.554 secs)\n",
      "bnp:bnp_matern step 10400 lr 4.868e-04 [train_loss] ll_base -0.0184 ll 0.0592 loss -0.0408 (4.494 secs)\n",
      "bnp:bnp_matern step 10600 lr 4.863e-04 [train_loss] ll_base -0.0114 ll 0.0636 loss -0.0523 (4.585 secs)\n",
      "bnp:bnp_matern step 10800 lr 4.857e-04 [train_loss] ll_base -0.0270 ll 0.0440 loss -0.0170 (4.424 secs)\n",
      "bnp:bnp_matern step 11000 lr 4.852e-04 [train_loss] ll_base -0.0395 ll 0.0304 loss 0.0091 (4.489 secs)\n",
      "bnp:bnp_matern step 11200 lr 4.847e-04 [train_loss] ll_base -0.0350 ll 0.0390 loss -0.0040 (4.530 secs)\n",
      "bnp:bnp_matern step 11400 lr 4.841e-04 [train_loss] ll_base 0.0235 ll 0.0941 loss -0.1176 (4.492 secs)\n",
      "bnp:bnp_matern step 11600 lr 4.836e-04 [train_loss] ll_base -0.0212 ll 0.0510 loss -0.0298 (4.343 secs)\n",
      "bnp:bnp_matern step 11800 lr 4.830e-04 [train_loss] ll_base 0.0103 ll 0.0780 loss -0.0883 (4.436 secs)\n",
      "bnp:bnp_matern step 12000 lr 4.824e-04 [train_loss] ll_base -0.0224 ll 0.0430 loss -0.0206 (4.540 secs)\n",
      "bnp:bnp_matern step 12200 lr 4.819e-04 [train_loss] ll_base -0.0145 ll 0.0553 loss -0.0408 (4.386 secs)\n",
      "bnp:bnp_matern step 12400 lr 4.813e-04 [train_loss] ll_base -0.0123 ll 0.0581 loss -0.0458 (4.417 secs)\n",
      "bnp:bnp_matern step 12600 lr 4.807e-04 [train_loss] ll_base 0.0259 ll 0.0997 loss -0.1257 (4.648 secs)\n",
      "bnp:bnp_matern step 12800 lr 4.801e-04 [train_loss] ll_base 0.0332 ll 0.1077 loss -0.1409 (4.461 secs)\n",
      "bnp:bnp_matern step 13000 lr 4.794e-04 [train_loss] ll_base 0.0173 ll 0.0887 loss -0.1060 (4.428 secs)\n",
      "bnp:bnp_matern step 13200 lr 4.788e-04 [train_loss] ll_base 0.0037 ll 0.0708 loss -0.0744 (4.550 secs)\n",
      "bnp:bnp_matern step 13400 lr 4.782e-04 [train_loss] ll_base 0.0476 ll 0.1198 loss -0.1673 (4.667 secs)\n",
      "bnp:bnp_matern step 13600 lr 4.775e-04 [train_loss] ll_base 0.0453 ll 0.1209 loss -0.1662 (4.441 secs)\n",
      "bnp:bnp_matern step 13800 lr 4.769e-04 [train_loss] ll_base 0.0193 ll 0.0904 loss -0.1097 (4.516 secs)\n",
      "bnp:bnp_matern step 14000 lr 4.762e-04 [train_loss] ll_base 0.0610 ll 0.1375 loss -0.1986 (4.463 secs)\n",
      "bnp:bnp_matern step 14200 lr 4.755e-04 [train_loss] ll_base 0.0835 ll 0.1597 loss -0.2432 (4.516 secs)\n",
      "bnp:bnp_matern step 14400 lr 4.749e-04 [train_loss] ll_base 0.0509 ll 0.1189 loss -0.1697 (4.574 secs)\n",
      "bnp:bnp_matern step 14600 lr 4.742e-04 [train_loss] ll_base 0.0391 ll 0.1232 loss -0.1623 (4.459 secs)\n",
      "bnp:bnp_matern step 14800 lr 4.735e-04 [train_loss] ll_base 0.0314 ll 0.1066 loss -0.1380 (4.588 secs)\n",
      "bnp:bnp_matern step 15000 lr 4.728e-04 [train_loss] ll_base 0.0674 ll 0.1401 loss -0.2075 (4.630 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 111.11it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.1956 tar_ll -0.1075 (27.004 secs)\n",
      "\n",
      "bnp:bnp_matern step 15200 lr 4.720e-04 [train_loss] ll_base 0.0334 ll 0.1172 loss -0.1507 (4.516 secs)\n",
      "bnp:bnp_matern step 15400 lr 4.713e-04 [train_loss] ll_base 0.0676 ll 0.1501 loss -0.2177 (4.552 secs)\n",
      "bnp:bnp_matern step 15600 lr 4.706e-04 [train_loss] ll_base 0.0196 ll 0.0960 loss -0.1156 (4.762 secs)\n",
      "bnp:bnp_matern step 15800 lr 4.698e-04 [train_loss] ll_base 0.0514 ll 0.1195 loss -0.1710 (4.552 secs)\n",
      "bnp:bnp_matern step 16000 lr 4.691e-04 [train_loss] ll_base 0.0429 ll 0.1200 loss -0.1629 (4.679 secs)\n",
      "bnp:bnp_matern step 16200 lr 4.683e-04 [train_loss] ll_base 0.0848 ll 0.1652 loss -0.2500 (4.537 secs)\n",
      "bnp:bnp_matern step 16400 lr 4.675e-04 [train_loss] ll_base 0.0465 ll 0.1234 loss -0.1699 (4.759 secs)\n",
      "bnp:bnp_matern step 16600 lr 4.668e-04 [train_loss] ll_base 0.1063 ll 0.1919 loss -0.2982 (4.533 secs)\n",
      "bnp:bnp_matern step 16800 lr 4.660e-04 [train_loss] ll_base 0.0849 ll 0.1708 loss -0.2557 (4.430 secs)\n",
      "bnp:bnp_matern step 17000 lr 4.652e-04 [train_loss] ll_base 0.0782 ll 0.1664 loss -0.2447 (4.627 secs)\n",
      "bnp:bnp_matern step 17200 lr 4.644e-04 [train_loss] ll_base 0.0935 ll 0.1834 loss -0.2769 (4.646 secs)\n",
      "bnp:bnp_matern step 17400 lr 4.636e-04 [train_loss] ll_base 0.0981 ll 0.1815 loss -0.2795 (4.615 secs)\n",
      "bnp:bnp_matern step 17600 lr 4.627e-04 [train_loss] ll_base 0.1462 ll 0.2310 loss -0.3772 (4.568 secs)\n",
      "bnp:bnp_matern step 17800 lr 4.619e-04 [train_loss] ll_base 0.0976 ll 0.1864 loss -0.2840 (4.796 secs)\n",
      "bnp:bnp_matern step 18000 lr 4.611e-04 [train_loss] ll_base 0.1147 ll 0.1999 loss -0.3146 (4.458 secs)\n",
      "bnp:bnp_matern step 18200 lr 4.602e-04 [train_loss] ll_base 0.1002 ll 0.1795 loss -0.2797 (4.762 secs)\n",
      "bnp:bnp_matern step 18400 lr 4.594e-04 [train_loss] ll_base 0.1232 ll 0.2084 loss -0.3316 (4.650 secs)\n",
      "bnp:bnp_matern step 18600 lr 4.585e-04 [train_loss] ll_base 0.1034 ll 0.1830 loss -0.2863 (4.489 secs)\n",
      "bnp:bnp_matern step 18800 lr 4.576e-04 [train_loss] ll_base 0.0953 ll 0.1842 loss -0.2795 (4.530 secs)\n",
      "bnp:bnp_matern step 19000 lr 4.568e-04 [train_loss] ll_base 0.0851 ll 0.1717 loss -0.2568 (4.487 secs)\n",
      "bnp:bnp_matern step 19200 lr 4.559e-04 [train_loss] ll_base 0.1113 ll 0.1913 loss -0.3026 (4.553 secs)\n",
      "bnp:bnp_matern step 19400 lr 4.550e-04 [train_loss] ll_base 0.1377 ll 0.2191 loss -0.3568 (4.486 secs)\n",
      "bnp:bnp_matern step 19600 lr 4.541e-04 [train_loss] ll_base 0.1573 ll 0.2407 loss -0.3980 (4.492 secs)\n",
      "bnp:bnp_matern step 19800 lr 4.532e-04 [train_loss] ll_base 0.1355 ll 0.2263 loss -0.3618 (4.548 secs)\n",
      "bnp:bnp_matern step 20000 lr 4.523e-04 [train_loss] ll_base 0.1512 ll 0.2280 loss -0.3792 (4.362 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 110.30it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.2709 tar_ll -0.0466 (27.201 secs)\n",
      "\n",
      "bnp:bnp_matern step 20200 lr 4.513e-04 [train_loss] ll_base 0.1185 ll 0.2078 loss -0.3263 (4.422 secs)\n",
      "bnp:bnp_matern step 20400 lr 4.504e-04 [train_loss] ll_base 0.1450 ll 0.2259 loss -0.3708 (4.412 secs)\n",
      "bnp:bnp_matern step 20600 lr 4.494e-04 [train_loss] ll_base 0.0858 ll 0.1750 loss -0.2608 (4.519 secs)\n",
      "bnp:bnp_matern step 20800 lr 4.485e-04 [train_loss] ll_base 0.1144 ll 0.1919 loss -0.3063 (4.642 secs)\n",
      "bnp:bnp_matern step 21000 lr 4.475e-04 [train_loss] ll_base 0.1654 ll 0.2410 loss -0.4064 (4.461 secs)\n",
      "bnp:bnp_matern step 21200 lr 4.466e-04 [train_loss] ll_base 0.2033 ll 0.2821 loss -0.4854 (4.405 secs)\n",
      "bnp:bnp_matern step 21400 lr 4.456e-04 [train_loss] ll_base 0.1705 ll 0.2524 loss -0.4229 (4.601 secs)\n",
      "bnp:bnp_matern step 21600 lr 4.446e-04 [train_loss] ll_base 0.1611 ll 0.2393 loss -0.4005 (4.487 secs)\n",
      "bnp:bnp_matern step 21800 lr 4.436e-04 [train_loss] ll_base 0.1545 ll 0.2461 loss -0.4006 (4.473 secs)\n",
      "bnp:bnp_matern step 22000 lr 4.426e-04 [train_loss] ll_base 0.1726 ll 0.2511 loss -0.4237 (4.555 secs)\n",
      "bnp:bnp_matern step 22200 lr 4.416e-04 [train_loss] ll_base 0.2351 ll 0.3181 loss -0.5532 (4.759 secs)\n",
      "bnp:bnp_matern step 22400 lr 4.406e-04 [train_loss] ll_base 0.1504 ll 0.2377 loss -0.3881 (4.500 secs)\n",
      "bnp:bnp_matern step 22600 lr 4.396e-04 [train_loss] ll_base 0.1761 ll 0.2561 loss -0.4323 (4.483 secs)\n",
      "bnp:bnp_matern step 22800 lr 4.386e-04 [train_loss] ll_base 0.1330 ll 0.2233 loss -0.3563 (4.498 secs)\n",
      "bnp:bnp_matern step 23000 lr 4.375e-04 [train_loss] ll_base 0.1752 ll 0.2530 loss -0.4281 (4.546 secs)\n",
      "bnp:bnp_matern step 23200 lr 4.365e-04 [train_loss] ll_base 0.1597 ll 0.2470 loss -0.4067 (4.354 secs)\n",
      "bnp:bnp_matern step 23400 lr 4.354e-04 [train_loss] ll_base 0.1316 ll 0.2136 loss -0.3452 (4.420 secs)\n",
      "bnp:bnp_matern step 23600 lr 4.344e-04 [train_loss] ll_base 0.2388 ll 0.3187 loss -0.5576 (4.505 secs)\n",
      "bnp:bnp_matern step 23800 lr 4.333e-04 [train_loss] ll_base 0.1826 ll 0.2582 loss -0.4407 (4.347 secs)\n",
      "bnp:bnp_matern step 24000 lr 4.322e-04 [train_loss] ll_base 0.1899 ll 0.2759 loss -0.4659 (4.480 secs)\n",
      "bnp:bnp_matern step 24200 lr 4.312e-04 [train_loss] ll_base 0.1820 ll 0.2638 loss -0.4458 (4.487 secs)\n",
      "bnp:bnp_matern step 24400 lr 4.301e-04 [train_loss] ll_base 0.1673 ll 0.2494 loss -0.4167 (4.535 secs)\n",
      "bnp:bnp_matern step 24600 lr 4.290e-04 [train_loss] ll_base 0.2183 ll 0.3046 loss -0.5229 (4.496 secs)\n",
      "bnp:bnp_matern step 24800 lr 4.279e-04 [train_loss] ll_base 0.1686 ll 0.2514 loss -0.4199 (4.595 secs)\n",
      "bnp:bnp_matern step 25000 lr 4.268e-04 [train_loss] ll_base 0.1666 ll 0.2500 loss -0.4165 (4.507 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 108.19it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.3682 tar_ll -0.0048 (27.732 secs)\n",
      "\n",
      "bnp:bnp_matern step 25200 lr 4.257e-04 [train_loss] ll_base 0.2017 ll 0.2813 loss -0.4831 (4.656 secs)\n",
      "bnp:bnp_matern step 25400 lr 4.245e-04 [train_loss] ll_base 0.2097 ll 0.2875 loss -0.4972 (4.603 secs)\n",
      "bnp:bnp_matern step 25600 lr 4.234e-04 [train_loss] ll_base 0.1998 ll 0.2832 loss -0.4830 (4.588 secs)\n",
      "bnp:bnp_matern step 25800 lr 4.223e-04 [train_loss] ll_base 0.2318 ll 0.3067 loss -0.5386 (4.750 secs)\n",
      "bnp:bnp_matern step 26000 lr 4.211e-04 [train_loss] ll_base 0.2396 ll 0.3159 loss -0.5555 (4.435 secs)\n",
      "bnp:bnp_matern step 26200 lr 4.200e-04 [train_loss] ll_base 0.2387 ll 0.3124 loss -0.5511 (4.534 secs)\n",
      "bnp:bnp_matern step 26400 lr 4.188e-04 [train_loss] ll_base 0.2028 ll 0.2859 loss -0.4887 (4.530 secs)\n",
      "bnp:bnp_matern step 26600 lr 4.177e-04 [train_loss] ll_base 0.1767 ll 0.2560 loss -0.4327 (4.607 secs)\n",
      "bnp:bnp_matern step 26800 lr 4.165e-04 [train_loss] ll_base 0.2492 ll 0.3336 loss -0.5828 (4.477 secs)\n",
      "bnp:bnp_matern step 27000 lr 4.153e-04 [train_loss] ll_base 0.2213 ll 0.3078 loss -0.5291 (4.476 secs)\n",
      "bnp:bnp_matern step 27200 lr 4.141e-04 [train_loss] ll_base 0.1850 ll 0.2691 loss -0.4541 (4.700 secs)\n",
      "bnp:bnp_matern step 27400 lr 4.130e-04 [train_loss] ll_base 0.1704 ll 0.2552 loss -0.4256 (4.458 secs)\n",
      "bnp:bnp_matern step 27600 lr 4.118e-04 [train_loss] ll_base 0.2196 ll 0.2975 loss -0.5170 (4.432 secs)\n",
      "bnp:bnp_matern step 27800 lr 4.106e-04 [train_loss] ll_base 0.2384 ll 0.3156 loss -0.5540 (4.528 secs)\n",
      "bnp:bnp_matern step 28000 lr 4.094e-04 [train_loss] ll_base 0.2552 ll 0.3350 loss -0.5902 (4.598 secs)\n",
      "bnp:bnp_matern step 28200 lr 4.081e-04 [train_loss] ll_base 0.2527 ll 0.3347 loss -0.5874 (4.488 secs)\n",
      "bnp:bnp_matern step 28400 lr 4.069e-04 [train_loss] ll_base 0.2070 ll 0.2793 loss -0.4863 (4.466 secs)\n",
      "bnp:bnp_matern step 28600 lr 4.057e-04 [train_loss] ll_base 0.2185 ll 0.3001 loss -0.5185 (4.505 secs)\n",
      "bnp:bnp_matern step 28800 lr 4.045e-04 [train_loss] ll_base 0.2538 ll 0.3284 loss -0.5822 (4.423 secs)\n",
      "bnp:bnp_matern step 29000 lr 4.032e-04 [train_loss] ll_base 0.2113 ll 0.2862 loss -0.4975 (4.456 secs)\n",
      "bnp:bnp_matern step 29200 lr 4.020e-04 [train_loss] ll_base 0.2334 ll 0.3149 loss -0.5483 (4.508 secs)\n",
      "bnp:bnp_matern step 29400 lr 4.007e-04 [train_loss] ll_base 0.2423 ll 0.3260 loss -0.5683 (4.469 secs)\n",
      "bnp:bnp_matern step 29600 lr 3.995e-04 [train_loss] ll_base 0.2060 ll 0.2932 loss -0.4993 (4.447 secs)\n",
      "bnp:bnp_matern step 29800 lr 3.982e-04 [train_loss] ll_base 0.2449 ll 0.3204 loss -0.5653 (4.461 secs)\n",
      "bnp:bnp_matern step 30000 lr 3.969e-04 [train_loss] ll_base 0.2432 ll 0.3282 loss -0.5714 (4.505 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 110.15it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.4261 tar_ll 0.0292 (27.240 secs)\n",
      "\n",
      "bnp:bnp_matern step 30200 lr 3.957e-04 [train_loss] ll_base 0.2385 ll 0.3159 loss -0.5544 (4.550 secs)\n",
      "bnp:bnp_matern step 30400 lr 3.944e-04 [train_loss] ll_base 0.2667 ll 0.3384 loss -0.6051 (4.553 secs)\n",
      "bnp:bnp_matern step 30600 lr 3.931e-04 [train_loss] ll_base 0.2409 ll 0.3312 loss -0.5722 (4.279 secs)\n",
      "bnp:bnp_matern step 30800 lr 3.918e-04 [train_loss] ll_base 0.2458 ll 0.3350 loss -0.5807 (4.463 secs)\n",
      "bnp:bnp_matern step 31000 lr 3.905e-04 [train_loss] ll_base 0.2335 ll 0.3203 loss -0.5538 (4.672 secs)\n",
      "bnp:bnp_matern step 31200 lr 3.892e-04 [train_loss] ll_base 0.2820 ll 0.3564 loss -0.6384 (4.371 secs)\n",
      "bnp:bnp_matern step 31400 lr 3.879e-04 [train_loss] ll_base 0.2708 ll 0.3440 loss -0.6148 (4.564 secs)\n",
      "bnp:bnp_matern step 31600 lr 3.866e-04 [train_loss] ll_base 0.3139 ll 0.3911 loss -0.7050 (4.835 secs)\n",
      "bnp:bnp_matern step 31800 lr 3.853e-04 [train_loss] ll_base 0.2348 ll 0.3167 loss -0.5515 (4.990 secs)\n",
      "bnp:bnp_matern step 32000 lr 3.840e-04 [train_loss] ll_base 0.2973 ll 0.3763 loss -0.6736 (4.617 secs)\n",
      "bnp:bnp_matern step 32200 lr 3.826e-04 [train_loss] ll_base 0.2691 ll 0.3559 loss -0.6250 (4.431 secs)\n",
      "bnp:bnp_matern step 32400 lr 3.813e-04 [train_loss] ll_base 0.2638 ll 0.3462 loss -0.6100 (4.758 secs)\n",
      "bnp:bnp_matern step 32600 lr 3.800e-04 [train_loss] ll_base 0.2634 ll 0.3518 loss -0.6152 (4.731 secs)\n",
      "bnp:bnp_matern step 32800 lr 3.786e-04 [train_loss] ll_base 0.2865 ll 0.3637 loss -0.6502 (4.741 secs)\n",
      "bnp:bnp_matern step 33000 lr 3.773e-04 [train_loss] ll_base 0.2829 ll 0.3586 loss -0.6415 (4.826 secs)\n",
      "bnp:bnp_matern step 33200 lr 3.759e-04 [train_loss] ll_base 0.3104 ll 0.3849 loss -0.6953 (4.730 secs)\n",
      "bnp:bnp_matern step 33400 lr 3.745e-04 [train_loss] ll_base 0.2850 ll 0.3594 loss -0.6445 (4.511 secs)\n",
      "bnp:bnp_matern step 33600 lr 3.732e-04 [train_loss] ll_base 0.2661 ll 0.3402 loss -0.6062 (4.696 secs)\n",
      "bnp:bnp_matern step 33800 lr 3.718e-04 [train_loss] ll_base 0.2727 ll 0.3462 loss -0.6189 (4.550 secs)\n",
      "bnp:bnp_matern step 34000 lr 3.704e-04 [train_loss] ll_base 0.3213 ll 0.3964 loss -0.7176 (4.595 secs)\n",
      "bnp:bnp_matern step 34200 lr 3.691e-04 [train_loss] ll_base 0.2726 ll 0.3456 loss -0.6182 (4.539 secs)\n",
      "bnp:bnp_matern step 34400 lr 3.677e-04 [train_loss] ll_base 0.2803 ll 0.3621 loss -0.6424 (4.652 secs)\n",
      "bnp:bnp_matern step 34600 lr 3.663e-04 [train_loss] ll_base 0.2548 ll 0.3413 loss -0.5961 (4.551 secs)\n",
      "bnp:bnp_matern step 34800 lr 3.649e-04 [train_loss] ll_base 0.2963 ll 0.3719 loss -0.6682 (4.668 secs)\n",
      "bnp:bnp_matern step 35000 lr 3.635e-04 [train_loss] ll_base 0.2945 ll 0.3694 loss -0.6639 (4.760 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 108.64it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.4796 tar_ll 0.0496 (27.618 secs)\n",
      "\n",
      "bnp:bnp_matern step 35200 lr 3.621e-04 [train_loss] ll_base 0.2913 ll 0.3773 loss -0.6687 (4.784 secs)\n",
      "bnp:bnp_matern step 35400 lr 3.607e-04 [train_loss] ll_base 0.3074 ll 0.3832 loss -0.6905 (4.455 secs)\n",
      "bnp:bnp_matern step 35600 lr 3.593e-04 [train_loss] ll_base 0.2820 ll 0.3643 loss -0.6463 (4.467 secs)\n",
      "bnp:bnp_matern step 35800 lr 3.579e-04 [train_loss] ll_base 0.2791 ll 0.3608 loss -0.6398 (4.695 secs)\n",
      "bnp:bnp_matern step 36000 lr 3.564e-04 [train_loss] ll_base 0.2897 ll 0.3739 loss -0.6636 (4.491 secs)\n",
      "bnp:bnp_matern step 36200 lr 3.550e-04 [train_loss] ll_base 0.3277 ll 0.4054 loss -0.7330 (4.567 secs)\n",
      "bnp:bnp_matern step 36400 lr 3.536e-04 [train_loss] ll_base 0.3195 ll 0.4083 loss -0.7278 (4.533 secs)\n",
      "bnp:bnp_matern step 36600 lr 3.522e-04 [train_loss] ll_base 0.3009 ll 0.3740 loss -0.6749 (4.628 secs)\n",
      "bnp:bnp_matern step 36800 lr 3.507e-04 [train_loss] ll_base 0.2974 ll 0.3746 loss -0.6720 (4.504 secs)\n",
      "bnp:bnp_matern step 37000 lr 3.493e-04 [train_loss] ll_base 0.3282 ll 0.4025 loss -0.7307 (4.460 secs)\n",
      "bnp:bnp_matern step 37200 lr 3.478e-04 [train_loss] ll_base 0.3579 ll 0.4390 loss -0.7969 (4.578 secs)\n",
      "bnp:bnp_matern step 37400 lr 3.464e-04 [train_loss] ll_base 0.3067 ll 0.3882 loss -0.6949 (4.498 secs)\n",
      "bnp:bnp_matern step 37600 lr 3.449e-04 [train_loss] ll_base 0.2703 ll 0.3515 loss -0.6218 (4.454 secs)\n",
      "bnp:bnp_matern step 37800 lr 3.435e-04 [train_loss] ll_base 0.3198 ll 0.4062 loss -0.7260 (4.675 secs)\n",
      "bnp:bnp_matern step 38000 lr 3.420e-04 [train_loss] ll_base 0.3042 ll 0.3891 loss -0.6932 (4.538 secs)\n",
      "bnp:bnp_matern step 38200 lr 3.406e-04 [train_loss] ll_base 0.3211 ll 0.3991 loss -0.7202 (4.544 secs)\n",
      "bnp:bnp_matern step 38400 lr 3.391e-04 [train_loss] ll_base 0.3413 ll 0.4106 loss -0.7519 (4.570 secs)\n",
      "bnp:bnp_matern step 38600 lr 3.376e-04 [train_loss] ll_base 0.3064 ll 0.3796 loss -0.6861 (4.541 secs)\n",
      "bnp:bnp_matern step 38800 lr 3.362e-04 [train_loss] ll_base 0.3431 ll 0.4201 loss -0.7633 (4.617 secs)\n",
      "bnp:bnp_matern step 39000 lr 3.347e-04 [train_loss] ll_base 0.3120 ll 0.3928 loss -0.7048 (4.533 secs)\n",
      "bnp:bnp_matern step 39200 lr 3.332e-04 [train_loss] ll_base 0.3282 ll 0.4012 loss -0.7294 (4.402 secs)\n",
      "bnp:bnp_matern step 39400 lr 3.317e-04 [train_loss] ll_base 0.3116 ll 0.3912 loss -0.7027 (4.642 secs)\n",
      "bnp:bnp_matern step 39600 lr 3.302e-04 [train_loss] ll_base 0.3225 ll 0.3991 loss -0.7216 (4.603 secs)\n",
      "bnp:bnp_matern step 39800 lr 3.287e-04 [train_loss] ll_base 0.3237 ll 0.4017 loss -0.7253 (4.562 secs)\n",
      "bnp:bnp_matern step 40000 lr 3.273e-04 [train_loss] ll_base 0.3539 ll 0.4297 loss -0.7836 (4.379 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 109.77it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.5197 tar_ll 0.0783 (27.335 secs)\n",
      "\n",
      "bnp:bnp_matern step 40200 lr 3.258e-04 [train_loss] ll_base 0.3288 ll 0.4097 loss -0.7385 (4.658 secs)\n",
      "bnp:bnp_matern step 40400 lr 3.243e-04 [train_loss] ll_base 0.3308 ll 0.4157 loss -0.7465 (4.431 secs)\n",
      "bnp:bnp_matern step 40600 lr 3.228e-04 [train_loss] ll_base 0.3428 ll 0.4211 loss -0.7639 (4.620 secs)\n",
      "bnp:bnp_matern step 40800 lr 3.213e-04 [train_loss] ll_base 0.3376 ll 0.4145 loss -0.7520 (4.559 secs)\n",
      "bnp:bnp_matern step 41000 lr 3.197e-04 [train_loss] ll_base 0.3317 ll 0.4170 loss -0.7488 (4.827 secs)\n",
      "bnp:bnp_matern step 41200 lr 3.182e-04 [train_loss] ll_base 0.3690 ll 0.4493 loss -0.8183 (4.567 secs)\n",
      "bnp:bnp_matern step 41400 lr 3.167e-04 [train_loss] ll_base 0.3106 ll 0.3894 loss -0.6999 (4.567 secs)\n",
      "bnp:bnp_matern step 41600 lr 3.152e-04 [train_loss] ll_base 0.3230 ll 0.4085 loss -0.7315 (4.698 secs)\n",
      "bnp:bnp_matern step 41800 lr 3.137e-04 [train_loss] ll_base 0.3718 ll 0.4482 loss -0.8200 (4.554 secs)\n",
      "bnp:bnp_matern step 42000 lr 3.122e-04 [train_loss] ll_base 0.3385 ll 0.4136 loss -0.7521 (4.472 secs)\n",
      "bnp:bnp_matern step 42200 lr 3.106e-04 [train_loss] ll_base 0.2964 ll 0.3775 loss -0.6739 (4.454 secs)\n",
      "bnp:bnp_matern step 42400 lr 3.091e-04 [train_loss] ll_base 0.3856 ll 0.4608 loss -0.8464 (4.597 secs)\n",
      "bnp:bnp_matern step 42600 lr 3.076e-04 [train_loss] ll_base 0.3211 ll 0.3999 loss -0.7211 (4.486 secs)\n",
      "bnp:bnp_matern step 42800 lr 3.061e-04 [train_loss] ll_base 0.3573 ll 0.4370 loss -0.7943 (4.532 secs)\n",
      "bnp:bnp_matern step 43000 lr 3.045e-04 [train_loss] ll_base 0.3917 ll 0.4658 loss -0.8575 (4.702 secs)\n",
      "bnp:bnp_matern step 43200 lr 3.030e-04 [train_loss] ll_base 0.3471 ll 0.4226 loss -0.7698 (4.505 secs)\n",
      "bnp:bnp_matern step 43400 lr 3.015e-04 [train_loss] ll_base 0.3210 ll 0.3962 loss -0.7172 (4.467 secs)\n",
      "bnp:bnp_matern step 43600 lr 2.999e-04 [train_loss] ll_base 0.3697 ll 0.4447 loss -0.8144 (4.525 secs)\n",
      "bnp:bnp_matern step 43800 lr 2.984e-04 [train_loss] ll_base 0.3260 ll 0.4102 loss -0.7361 (4.626 secs)\n",
      "bnp:bnp_matern step 44000 lr 2.968e-04 [train_loss] ll_base 0.3972 ll 0.4716 loss -0.8688 (4.474 secs)\n",
      "bnp:bnp_matern step 44200 lr 2.953e-04 [train_loss] ll_base 0.3475 ll 0.4292 loss -0.7767 (4.638 secs)\n",
      "bnp:bnp_matern step 44400 lr 2.938e-04 [train_loss] ll_base 0.3291 ll 0.4098 loss -0.7390 (4.620 secs)\n",
      "bnp:bnp_matern step 44600 lr 2.922e-04 [train_loss] ll_base 0.3524 ll 0.4316 loss -0.7840 (4.414 secs)\n",
      "bnp:bnp_matern step 44800 lr 2.907e-04 [train_loss] ll_base 0.3387 ll 0.4180 loss -0.7567 (4.484 secs)\n",
      "bnp:bnp_matern step 45000 lr 2.891e-04 [train_loss] ll_base 0.3144 ll 0.3988 loss -0.7132 (4.571 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 107.73it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.5249 tar_ll 0.0878 (27.851 secs)\n",
      "\n",
      "bnp:bnp_matern step 45200 lr 2.876e-04 [train_loss] ll_base 0.4046 ll 0.4767 loss -0.8813 (4.462 secs)\n",
      "bnp:bnp_matern step 45400 lr 2.860e-04 [train_loss] ll_base 0.3750 ll 0.4558 loss -0.8307 (4.454 secs)\n",
      "bnp:bnp_matern step 45600 lr 2.844e-04 [train_loss] ll_base 0.3207 ll 0.4018 loss -0.7226 (4.385 secs)\n",
      "bnp:bnp_matern step 45800 lr 2.829e-04 [train_loss] ll_base 0.3878 ll 0.4582 loss -0.8460 (4.537 secs)\n",
      "bnp:bnp_matern step 46000 lr 2.813e-04 [train_loss] ll_base 0.4116 ll 0.4785 loss -0.8901 (4.714 secs)\n",
      "bnp:bnp_matern step 46200 lr 2.798e-04 [train_loss] ll_base 0.3479 ll 0.4239 loss -0.7718 (4.523 secs)\n",
      "bnp:bnp_matern step 46400 lr 2.782e-04 [train_loss] ll_base 0.3219 ll 0.3954 loss -0.7173 (4.578 secs)\n",
      "bnp:bnp_matern step 46600 lr 2.767e-04 [train_loss] ll_base 0.3452 ll 0.4217 loss -0.7670 (4.454 secs)\n",
      "bnp:bnp_matern step 46800 lr 2.751e-04 [train_loss] ll_base 0.3481 ll 0.4179 loss -0.7660 (4.424 secs)\n",
      "bnp:bnp_matern step 47000 lr 2.735e-04 [train_loss] ll_base 0.3673 ll 0.4494 loss -0.8167 (4.484 secs)\n",
      "bnp:bnp_matern step 47200 lr 2.720e-04 [train_loss] ll_base 0.3699 ll 0.4424 loss -0.8123 (4.596 secs)\n",
      "bnp:bnp_matern step 47400 lr 2.704e-04 [train_loss] ll_base 0.3865 ll 0.4622 loss -0.8487 (4.435 secs)\n",
      "bnp:bnp_matern step 47600 lr 2.688e-04 [train_loss] ll_base 0.3765 ll 0.4553 loss -0.8318 (4.556 secs)\n",
      "bnp:bnp_matern step 47800 lr 2.673e-04 [train_loss] ll_base 0.3763 ll 0.4571 loss -0.8334 (4.351 secs)\n",
      "bnp:bnp_matern step 48000 lr 2.657e-04 [train_loss] ll_base 0.3791 ll 0.4480 loss -0.8271 (4.583 secs)\n",
      "bnp:bnp_matern step 48200 lr 2.641e-04 [train_loss] ll_base 0.3831 ll 0.4564 loss -0.8395 (4.493 secs)\n",
      "bnp:bnp_matern step 48400 lr 2.626e-04 [train_loss] ll_base 0.3265 ll 0.4033 loss -0.7298 (4.432 secs)\n",
      "bnp:bnp_matern step 48600 lr 2.610e-04 [train_loss] ll_base 0.3809 ll 0.4555 loss -0.8364 (4.392 secs)\n",
      "bnp:bnp_matern step 48800 lr 2.594e-04 [train_loss] ll_base 0.3643 ll 0.4355 loss -0.7998 (4.625 secs)\n",
      "bnp:bnp_matern step 49000 lr 2.579e-04 [train_loss] ll_base 0.3763 ll 0.4463 loss -0.8227 (4.391 secs)\n",
      "bnp:bnp_matern step 49200 lr 2.563e-04 [train_loss] ll_base 0.3620 ll 0.4280 loss -0.7900 (4.390 secs)\n",
      "bnp:bnp_matern step 49400 lr 2.547e-04 [train_loss] ll_base 0.3719 ll 0.4428 loss -0.8147 (4.270 secs)\n",
      "bnp:bnp_matern step 49600 lr 2.531e-04 [train_loss] ll_base 0.4353 ll 0.5010 loss -0.9364 (4.514 secs)\n",
      "bnp:bnp_matern step 49800 lr 2.516e-04 [train_loss] ll_base 0.4042 ll 0.4735 loss -0.8777 (4.428 secs)\n",
      "bnp:bnp_matern step 50000 lr 2.500e-04 [train_loss] ll_base 0.3622 ll 0.4485 loss -0.8106 (4.482 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 107.67it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.5668 tar_ll 0.1071 (27.866 secs)\n",
      "\n",
      "bnp:bnp_matern step 50200 lr 2.484e-04 [train_loss] ll_base 0.3673 ll 0.4400 loss -0.8073 (4.603 secs)\n",
      "bnp:bnp_matern step 50400 lr 2.469e-04 [train_loss] ll_base 0.3733 ll 0.4520 loss -0.8253 (4.668 secs)\n",
      "bnp:bnp_matern step 50600 lr 2.453e-04 [train_loss] ll_base 0.4194 ll 0.4964 loss -0.9159 (4.571 secs)\n",
      "bnp:bnp_matern step 50800 lr 2.437e-04 [train_loss] ll_base 0.3361 ll 0.4083 loss -0.7444 (4.543 secs)\n",
      "bnp:bnp_matern step 51000 lr 2.421e-04 [train_loss] ll_base 0.4119 ll 0.4762 loss -0.8881 (4.780 secs)\n",
      "bnp:bnp_matern step 51200 lr 2.406e-04 [train_loss] ll_base 0.3597 ll 0.4350 loss -0.7947 (4.498 secs)\n",
      "bnp:bnp_matern step 51400 lr 2.390e-04 [train_loss] ll_base 0.3846 ll 0.4567 loss -0.8414 (4.446 secs)\n",
      "bnp:bnp_matern step 51600 lr 2.374e-04 [train_loss] ll_base 0.3917 ll 0.4618 loss -0.8535 (4.564 secs)\n",
      "bnp:bnp_matern step 51800 lr 2.359e-04 [train_loss] ll_base 0.4032 ll 0.4844 loss -0.8876 (4.704 secs)\n",
      "bnp:bnp_matern step 52000 lr 2.343e-04 [train_loss] ll_base 0.3872 ll 0.4619 loss -0.8491 (4.453 secs)\n",
      "bnp:bnp_matern step 52200 lr 2.327e-04 [train_loss] ll_base 0.4212 ll 0.4935 loss -0.9146 (4.423 secs)\n",
      "bnp:bnp_matern step 52400 lr 2.312e-04 [train_loss] ll_base 0.3922 ll 0.4678 loss -0.8600 (4.675 secs)\n",
      "bnp:bnp_matern step 52600 lr 2.296e-04 [train_loss] ll_base 0.3960 ll 0.4768 loss -0.8729 (4.491 secs)\n",
      "bnp:bnp_matern step 52800 lr 2.280e-04 [train_loss] ll_base 0.4212 ll 0.4880 loss -0.9092 (4.458 secs)\n",
      "bnp:bnp_matern step 53000 lr 2.265e-04 [train_loss] ll_base 0.4079 ll 0.4775 loss -0.8855 (4.545 secs)\n",
      "bnp:bnp_matern step 53200 lr 2.249e-04 [train_loss] ll_base 0.3699 ll 0.4448 loss -0.8147 (4.680 secs)\n",
      "bnp:bnp_matern step 53400 lr 2.233e-04 [train_loss] ll_base 0.4298 ll 0.4912 loss -0.9210 (4.527 secs)\n",
      "bnp:bnp_matern step 53600 lr 2.218e-04 [train_loss] ll_base 0.4183 ll 0.4813 loss -0.8996 (4.427 secs)\n",
      "bnp:bnp_matern step 53800 lr 2.202e-04 [train_loss] ll_base 0.3694 ll 0.4495 loss -0.8188 (4.518 secs)\n",
      "bnp:bnp_matern step 54000 lr 2.187e-04 [train_loss] ll_base 0.4433 ll 0.5215 loss -0.9648 (4.567 secs)\n",
      "bnp:bnp_matern step 54200 lr 2.171e-04 [train_loss] ll_base 0.4148 ll 0.4950 loss -0.9098 (4.492 secs)\n",
      "bnp:bnp_matern step 54400 lr 2.156e-04 [train_loss] ll_base 0.4361 ll 0.5084 loss -0.9444 (4.435 secs)\n",
      "bnp:bnp_matern step 54600 lr 2.140e-04 [train_loss] ll_base 0.4632 ll 0.5311 loss -0.9942 (4.386 secs)\n",
      "bnp:bnp_matern step 54800 lr 2.124e-04 [train_loss] ll_base 0.3737 ll 0.4505 loss -0.8242 (4.375 secs)\n",
      "bnp:bnp_matern step 55000 lr 2.109e-04 [train_loss] ll_base 0.3831 ll 0.4526 loss -0.8358 (4.572 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 108.93it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.5788 tar_ll 0.1434 (27.544 secs)\n",
      "\n",
      "bnp:bnp_matern step 55200 lr 2.093e-04 [train_loss] ll_base 0.3442 ll 0.4211 loss -0.7653 (4.465 secs)\n",
      "bnp:bnp_matern step 55400 lr 2.078e-04 [train_loss] ll_base 0.3564 ll 0.4267 loss -0.7831 (4.560 secs)\n",
      "bnp:bnp_matern step 55600 lr 2.062e-04 [train_loss] ll_base 0.4151 ll 0.4878 loss -0.9029 (4.511 secs)\n",
      "bnp:bnp_matern step 55800 lr 2.047e-04 [train_loss] ll_base 0.3940 ll 0.4665 loss -0.8605 (4.644 secs)\n",
      "bnp:bnp_matern step 56000 lr 2.032e-04 [train_loss] ll_base 0.4176 ll 0.4847 loss -0.9023 (4.553 secs)\n",
      "bnp:bnp_matern step 56200 lr 2.016e-04 [train_loss] ll_base 0.3918 ll 0.4630 loss -0.8548 (4.565 secs)\n",
      "bnp:bnp_matern step 56400 lr 2.001e-04 [train_loss] ll_base 0.4323 ll 0.4967 loss -0.9290 (4.453 secs)\n",
      "bnp:bnp_matern step 56600 lr 1.985e-04 [train_loss] ll_base 0.3831 ll 0.4560 loss -0.8392 (4.493 secs)\n",
      "bnp:bnp_matern step 56800 lr 1.970e-04 [train_loss] ll_base 0.4526 ll 0.5258 loss -0.9784 (4.603 secs)\n",
      "bnp:bnp_matern step 57000 lr 1.955e-04 [train_loss] ll_base 0.3534 ll 0.4289 loss -0.7824 (4.625 secs)\n",
      "bnp:bnp_matern step 57200 lr 1.939e-04 [train_loss] ll_base 0.3942 ll 0.4670 loss -0.8612 (4.474 secs)\n",
      "bnp:bnp_matern step 57400 lr 1.924e-04 [train_loss] ll_base 0.4288 ll 0.4985 loss -0.9272 (4.530 secs)\n",
      "bnp:bnp_matern step 57600 lr 1.909e-04 [train_loss] ll_base 0.4711 ll 0.5349 loss -1.0060 (4.593 secs)\n",
      "bnp:bnp_matern step 57800 lr 1.894e-04 [train_loss] ll_base 0.3962 ll 0.4727 loss -0.8689 (4.560 secs)\n",
      "bnp:bnp_matern step 58000 lr 1.878e-04 [train_loss] ll_base 0.4102 ll 0.4845 loss -0.8947 (4.587 secs)\n",
      "bnp:bnp_matern step 58200 lr 1.863e-04 [train_loss] ll_base 0.4271 ll 0.5082 loss -0.9352 (4.587 secs)\n",
      "bnp:bnp_matern step 58400 lr 1.848e-04 [train_loss] ll_base 0.4697 ll 0.5438 loss -1.0135 (4.413 secs)\n",
      "bnp:bnp_matern step 58600 lr 1.833e-04 [train_loss] ll_base 0.4029 ll 0.4760 loss -0.8790 (4.460 secs)\n",
      "bnp:bnp_matern step 58800 lr 1.818e-04 [train_loss] ll_base 0.4184 ll 0.4812 loss -0.8996 (4.635 secs)\n",
      "bnp:bnp_matern step 59000 lr 1.803e-04 [train_loss] ll_base 0.4219 ll 0.4950 loss -0.9169 (4.738 secs)\n",
      "bnp:bnp_matern step 59200 lr 1.787e-04 [train_loss] ll_base 0.4237 ll 0.4875 loss -0.9112 (4.695 secs)\n",
      "bnp:bnp_matern step 59400 lr 1.772e-04 [train_loss] ll_base 0.4323 ll 0.5020 loss -0.9343 (4.719 secs)\n",
      "bnp:bnp_matern step 59600 lr 1.757e-04 [train_loss] ll_base 0.4439 ll 0.5211 loss -0.9650 (4.702 secs)\n",
      "bnp:bnp_matern step 59800 lr 1.742e-04 [train_loss] ll_base 0.4217 ll 0.4902 loss -0.9119 (4.677 secs)\n",
      "bnp:bnp_matern step 60000 lr 1.727e-04 [train_loss] ll_base 0.4189 ll 0.4840 loss -0.9029 (4.514 secs)\n",
      "100%|##########| 3000/3000 [00:28<00:00, 106.05it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.6156 tar_ll 0.1475 (28.291 secs)\n",
      "\n",
      "bnp:bnp_matern step 60200 lr 1.713e-04 [train_loss] ll_base 0.4588 ll 0.5255 loss -0.9842 (4.477 secs)\n",
      "bnp:bnp_matern step 60400 lr 1.698e-04 [train_loss] ll_base 0.4168 ll 0.4985 loss -0.9153 (4.600 secs)\n",
      "bnp:bnp_matern step 60600 lr 1.683e-04 [train_loss] ll_base 0.3816 ll 0.4533 loss -0.8349 (4.472 secs)\n",
      "bnp:bnp_matern step 60800 lr 1.668e-04 [train_loss] ll_base 0.4256 ll 0.4993 loss -0.9249 (4.457 secs)\n",
      "bnp:bnp_matern step 61000 lr 1.653e-04 [train_loss] ll_base 0.4175 ll 0.4909 loss -0.9085 (4.491 secs)\n",
      "bnp:bnp_matern step 61200 lr 1.638e-04 [train_loss] ll_base 0.4412 ll 0.5137 loss -0.9549 (4.722 secs)\n",
      "bnp:bnp_matern step 61400 lr 1.624e-04 [train_loss] ll_base 0.4189 ll 0.4853 loss -0.9042 (4.423 secs)\n",
      "bnp:bnp_matern step 61600 lr 1.609e-04 [train_loss] ll_base 0.4413 ll 0.5026 loss -0.9440 (4.406 secs)\n",
      "bnp:bnp_matern step 61800 lr 1.594e-04 [train_loss] ll_base 0.4219 ll 0.4931 loss -0.9150 (4.477 secs)\n",
      "bnp:bnp_matern step 62000 lr 1.580e-04 [train_loss] ll_base 0.4623 ll 0.5264 loss -0.9886 (4.448 secs)\n",
      "bnp:bnp_matern step 62200 lr 1.565e-04 [train_loss] ll_base 0.4241 ll 0.4957 loss -0.9198 (4.457 secs)\n",
      "bnp:bnp_matern step 62400 lr 1.551e-04 [train_loss] ll_base 0.4164 ll 0.4905 loss -0.9069 (4.431 secs)\n",
      "bnp:bnp_matern step 62600 lr 1.536e-04 [train_loss] ll_base 0.4774 ll 0.5440 loss -1.0213 (4.527 secs)\n",
      "bnp:bnp_matern step 62800 lr 1.522e-04 [train_loss] ll_base 0.4651 ll 0.5307 loss -0.9958 (4.343 secs)\n",
      "bnp:bnp_matern step 63000 lr 1.507e-04 [train_loss] ll_base 0.4515 ll 0.5190 loss -0.9705 (4.309 secs)\n",
      "bnp:bnp_matern step 63200 lr 1.493e-04 [train_loss] ll_base 0.4218 ll 0.4909 loss -0.9127 (4.470 secs)\n",
      "bnp:bnp_matern step 63400 lr 1.478e-04 [train_loss] ll_base 0.4908 ll 0.5555 loss -1.0463 (4.505 secs)\n",
      "bnp:bnp_matern step 63600 lr 1.464e-04 [train_loss] ll_base 0.4279 ll 0.4868 loss -0.9147 (4.511 secs)\n",
      "bnp:bnp_matern step 63800 lr 1.450e-04 [train_loss] ll_base 0.4173 ll 0.4888 loss -0.9062 (4.546 secs)\n",
      "bnp:bnp_matern step 64000 lr 1.436e-04 [train_loss] ll_base 0.4435 ll 0.5136 loss -0.9571 (4.515 secs)\n",
      "bnp:bnp_matern step 64200 lr 1.421e-04 [train_loss] ll_base 0.4439 ll 0.5062 loss -0.9501 (4.515 secs)\n",
      "bnp:bnp_matern step 64400 lr 1.407e-04 [train_loss] ll_base 0.4736 ll 0.5451 loss -1.0188 (4.467 secs)\n",
      "bnp:bnp_matern step 64600 lr 1.393e-04 [train_loss] ll_base 0.4646 ll 0.5281 loss -0.9927 (4.352 secs)\n",
      "bnp:bnp_matern step 64800 lr 1.379e-04 [train_loss] ll_base 0.4479 ll 0.5143 loss -0.9622 (4.513 secs)\n",
      "bnp:bnp_matern step 65000 lr 1.365e-04 [train_loss] ll_base 0.4488 ll 0.5127 loss -0.9616 (4.525 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 108.78it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.6270 tar_ll 0.1456 (27.582 secs)\n",
      "\n",
      "bnp:bnp_matern step 65200 lr 1.351e-04 [train_loss] ll_base 0.4330 ll 0.4975 loss -0.9305 (4.399 secs)\n",
      "bnp:bnp_matern step 65400 lr 1.337e-04 [train_loss] ll_base 0.4610 ll 0.5318 loss -0.9929 (4.280 secs)\n",
      "bnp:bnp_matern step 65600 lr 1.323e-04 [train_loss] ll_base 0.4182 ll 0.4864 loss -0.9046 (4.387 secs)\n",
      "bnp:bnp_matern step 65800 lr 1.309e-04 [train_loss] ll_base 0.4670 ll 0.5342 loss -1.0012 (4.403 secs)\n",
      "bnp:bnp_matern step 66000 lr 1.296e-04 [train_loss] ll_base 0.4625 ll 0.5343 loss -0.9968 (4.494 secs)\n",
      "bnp:bnp_matern step 66200 lr 1.282e-04 [train_loss] ll_base 0.4218 ll 0.4975 loss -0.9194 (4.388 secs)\n",
      "bnp:bnp_matern step 66400 lr 1.268e-04 [train_loss] ll_base 0.4781 ll 0.5430 loss -1.0210 (4.606 secs)\n",
      "bnp:bnp_matern step 66600 lr 1.255e-04 [train_loss] ll_base 0.4314 ll 0.5046 loss -0.9360 (4.428 secs)\n",
      "bnp:bnp_matern step 66800 lr 1.241e-04 [train_loss] ll_base 0.4855 ll 0.5506 loss -1.0361 (4.516 secs)\n",
      "bnp:bnp_matern step 67000 lr 1.227e-04 [train_loss] ll_base 0.4848 ll 0.5492 loss -1.0339 (4.578 secs)\n",
      "bnp:bnp_matern step 67200 lr 1.214e-04 [train_loss] ll_base 0.4532 ll 0.5171 loss -0.9703 (4.645 secs)\n",
      "bnp:bnp_matern step 67400 lr 1.200e-04 [train_loss] ll_base 0.4560 ll 0.5229 loss -0.9789 (4.500 secs)\n",
      "bnp:bnp_matern step 67600 lr 1.187e-04 [train_loss] ll_base 0.4514 ll 0.5203 loss -0.9716 (4.505 secs)\n",
      "bnp:bnp_matern step 67800 lr 1.174e-04 [train_loss] ll_base 0.4399 ll 0.5116 loss -0.9515 (4.679 secs)\n",
      "bnp:bnp_matern step 68000 lr 1.160e-04 [train_loss] ll_base 0.4814 ll 0.5456 loss -1.0270 (4.537 secs)\n",
      "bnp:bnp_matern step 68200 lr 1.147e-04 [train_loss] ll_base 0.4540 ll 0.5245 loss -0.9785 (4.474 secs)\n",
      "bnp:bnp_matern step 68400 lr 1.134e-04 [train_loss] ll_base 0.4232 ll 0.4843 loss -0.9076 (4.599 secs)\n",
      "bnp:bnp_matern step 68600 lr 1.121e-04 [train_loss] ll_base 0.4279 ll 0.5028 loss -0.9307 (4.658 secs)\n",
      "bnp:bnp_matern step 68800 lr 1.108e-04 [train_loss] ll_base 0.4906 ll 0.5515 loss -1.0421 (4.510 secs)\n",
      "bnp:bnp_matern step 69000 lr 1.095e-04 [train_loss] ll_base 0.4623 ll 0.5301 loss -0.9924 (4.710 secs)\n",
      "bnp:bnp_matern step 69200 lr 1.082e-04 [train_loss] ll_base 0.4566 ll 0.5183 loss -0.9749 (4.832 secs)\n",
      "bnp:bnp_matern step 69400 lr 1.069e-04 [train_loss] ll_base 0.4772 ll 0.5500 loss -1.0272 (4.587 secs)\n",
      "bnp:bnp_matern step 69600 lr 1.056e-04 [train_loss] ll_base 0.4776 ll 0.5373 loss -1.0150 (4.392 secs)\n",
      "bnp:bnp_matern step 69800 lr 1.043e-04 [train_loss] ll_base 0.4777 ll 0.5448 loss -1.0226 (4.607 secs)\n",
      "bnp:bnp_matern step 70000 lr 1.031e-04 [train_loss] ll_base 0.4457 ll 0.5124 loss -0.9581 (4.534 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 108.59it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.6264 tar_ll 0.1699 (27.631 secs)\n",
      "\n",
      "bnp:bnp_matern step 70200 lr 1.018e-04 [train_loss] ll_base 0.4833 ll 0.5464 loss -1.0297 (4.418 secs)\n",
      "bnp:bnp_matern step 70400 lr 1.005e-04 [train_loss] ll_base 0.4629 ll 0.5246 loss -0.9875 (4.386 secs)\n",
      "bnp:bnp_matern step 70600 lr 9.927e-05 [train_loss] ll_base 0.4422 ll 0.5068 loss -0.9490 (4.395 secs)\n",
      "bnp:bnp_matern step 70800 lr 9.802e-05 [train_loss] ll_base 0.4918 ll 0.5584 loss -1.0502 (4.559 secs)\n",
      "bnp:bnp_matern step 71000 lr 9.677e-05 [train_loss] ll_base 0.4722 ll 0.5418 loss -1.0140 (4.397 secs)\n",
      "bnp:bnp_matern step 71200 lr 9.554e-05 [train_loss] ll_base 0.4999 ll 0.5695 loss -1.0695 (4.392 secs)\n",
      "bnp:bnp_matern step 71400 lr 9.430e-05 [train_loss] ll_base 0.4534 ll 0.5243 loss -0.9778 (4.659 secs)\n",
      "bnp:bnp_matern step 71600 lr 9.308e-05 [train_loss] ll_base 0.4850 ll 0.5452 loss -1.0302 (4.516 secs)\n",
      "bnp:bnp_matern step 71800 lr 9.186e-05 [train_loss] ll_base 0.4773 ll 0.5375 loss -1.0149 (4.498 secs)\n",
      "bnp:bnp_matern step 72000 lr 9.064e-05 [train_loss] ll_base 0.4379 ll 0.5106 loss -0.9485 (4.437 secs)\n",
      "bnp:bnp_matern step 72200 lr 8.944e-05 [train_loss] ll_base 0.5007 ll 0.5634 loss -1.0641 (4.671 secs)\n",
      "bnp:bnp_matern step 72400 lr 8.824e-05 [train_loss] ll_base 0.4339 ll 0.5068 loss -0.9407 (4.540 secs)\n",
      "bnp:bnp_matern step 72600 lr 8.704e-05 [train_loss] ll_base 0.4983 ll 0.5641 loss -1.0625 (4.462 secs)\n",
      "bnp:bnp_matern step 72800 lr 8.585e-05 [train_loss] ll_base 0.4180 ll 0.4913 loss -0.9093 (4.459 secs)\n",
      "bnp:bnp_matern step 73000 lr 8.467e-05 [train_loss] ll_base 0.4584 ll 0.5312 loss -0.9897 (4.619 secs)\n",
      "bnp:bnp_matern step 73200 lr 8.350e-05 [train_loss] ll_base 0.4423 ll 0.5173 loss -0.9596 (4.555 secs)\n",
      "bnp:bnp_matern step 73400 lr 8.233e-05 [train_loss] ll_base 0.4634 ll 0.5240 loss -0.9874 (4.600 secs)\n",
      "bnp:bnp_matern step 73600 lr 8.117e-05 [train_loss] ll_base 0.4360 ll 0.5000 loss -0.9361 (4.593 secs)\n",
      "bnp:bnp_matern step 73800 lr 8.001e-05 [train_loss] ll_base 0.4696 ll 0.5289 loss -0.9985 (4.442 secs)\n",
      "bnp:bnp_matern step 74000 lr 7.886e-05 [train_loss] ll_base 0.4901 ll 0.5511 loss -1.0412 (4.330 secs)\n",
      "bnp:bnp_matern step 74200 lr 7.772e-05 [train_loss] ll_base 0.4675 ll 0.5353 loss -1.0027 (4.596 secs)\n",
      "bnp:bnp_matern step 74400 lr 7.659e-05 [train_loss] ll_base 0.5014 ll 0.5723 loss -1.0737 (4.464 secs)\n",
      "bnp:bnp_matern step 74600 lr 7.546e-05 [train_loss] ll_base 0.4976 ll 0.5690 loss -1.0667 (4.330 secs)\n",
      "bnp:bnp_matern step 74800 lr 7.434e-05 [train_loss] ll_base 0.4604 ll 0.5285 loss -0.9889 (4.437 secs)\n",
      "bnp:bnp_matern step 75000 lr 7.322e-05 [train_loss] ll_base 0.4589 ll 0.5201 loss -0.9790 (4.458 secs)\n",
      "100%|##########| 3000/3000 [00:28<00:00, 105.15it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.6525 tar_ll 0.1770 (28.535 secs)\n",
      "\n",
      "bnp:bnp_matern step 75200 lr 7.212e-05 [train_loss] ll_base 0.4599 ll 0.5241 loss -0.9839 (4.594 secs)\n",
      "bnp:bnp_matern step 75400 lr 7.102e-05 [train_loss] ll_base 0.4690 ll 0.5363 loss -1.0053 (4.799 secs)\n",
      "bnp:bnp_matern step 75600 lr 6.992e-05 [train_loss] ll_base 0.4955 ll 0.5625 loss -1.0580 (4.565 secs)\n",
      "bnp:bnp_matern step 75800 lr 6.884e-05 [train_loss] ll_base 0.5037 ll 0.5608 loss -1.0645 (4.709 secs)\n",
      "bnp:bnp_matern step 76000 lr 6.776e-05 [train_loss] ll_base 0.4561 ll 0.5184 loss -0.9745 (4.497 secs)\n",
      "bnp:bnp_matern step 76200 lr 6.669e-05 [train_loss] ll_base 0.4852 ll 0.5450 loss -1.0302 (4.508 secs)\n",
      "bnp:bnp_matern step 76400 lr 6.562e-05 [train_loss] ll_base 0.4872 ll 0.5514 loss -1.0386 (4.680 secs)\n",
      "bnp:bnp_matern step 76600 lr 6.456e-05 [train_loss] ll_base 0.4715 ll 0.5395 loss -1.0110 (4.518 secs)\n",
      "bnp:bnp_matern step 76800 lr 6.351e-05 [train_loss] ll_base 0.5102 ll 0.5719 loss -1.0820 (4.428 secs)\n",
      "bnp:bnp_matern step 77000 lr 6.247e-05 [train_loss] ll_base 0.4922 ll 0.5544 loss -1.0466 (4.526 secs)\n",
      "bnp:bnp_matern step 77200 lr 6.144e-05 [train_loss] ll_base 0.4560 ll 0.5270 loss -0.9830 (4.721 secs)\n",
      "bnp:bnp_matern step 77400 lr 6.041e-05 [train_loss] ll_base 0.4784 ll 0.5384 loss -1.0168 (4.516 secs)\n",
      "bnp:bnp_matern step 77600 lr 5.939e-05 [train_loss] ll_base 0.4825 ll 0.5499 loss -1.0324 (4.473 secs)\n",
      "bnp:bnp_matern step 77800 lr 5.838e-05 [train_loss] ll_base 0.4968 ll 0.5645 loss -1.0612 (4.502 secs)\n",
      "bnp:bnp_matern step 78000 lr 5.737e-05 [train_loss] ll_base 0.4439 ll 0.5090 loss -0.9529 (4.524 secs)\n",
      "bnp:bnp_matern step 78200 lr 5.637e-05 [train_loss] ll_base 0.5615 ll 0.6218 loss -1.1833 (4.544 secs)\n",
      "bnp:bnp_matern step 78400 lr 5.538e-05 [train_loss] ll_base 0.5123 ll 0.5778 loss -1.0902 (4.457 secs)\n",
      "bnp:bnp_matern step 78600 lr 5.440e-05 [train_loss] ll_base 0.5142 ll 0.5825 loss -1.0966 (4.765 secs)\n",
      "bnp:bnp_matern step 78800 lr 5.343e-05 [train_loss] ll_base 0.5268 ll 0.5911 loss -1.1179 (4.427 secs)\n",
      "bnp:bnp_matern step 79000 lr 5.246e-05 [train_loss] ll_base 0.4594 ll 0.5215 loss -0.9808 (4.548 secs)\n",
      "bnp:bnp_matern step 79200 lr 5.150e-05 [train_loss] ll_base 0.4725 ll 0.5469 loss -1.0193 (4.466 secs)\n",
      "bnp:bnp_matern step 79400 lr 5.055e-05 [train_loss] ll_base 0.4554 ll 0.5204 loss -0.9758 (4.442 secs)\n",
      "bnp:bnp_matern step 79600 lr 4.961e-05 [train_loss] ll_base 0.4853 ll 0.5471 loss -1.0325 (4.342 secs)\n",
      "bnp:bnp_matern step 79800 lr 4.867e-05 [train_loss] ll_base 0.4984 ll 0.5691 loss -1.0676 (4.403 secs)\n",
      "bnp:bnp_matern step 80000 lr 4.775e-05 [train_loss] ll_base 0.5007 ll 0.5644 loss -1.0651 (4.541 secs)\n",
      "100%|##########| 3000/3000 [00:28<00:00, 106.88it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.6538 tar_ll 0.1838 (28.071 secs)\n",
      "\n",
      "bnp:bnp_matern step 80200 lr 4.683e-05 [train_loss] ll_base 0.5106 ll 0.5795 loss -1.0901 (4.638 secs)\n",
      "bnp:bnp_matern step 80400 lr 4.592e-05 [train_loss] ll_base 0.5464 ll 0.6126 loss -1.1590 (4.415 secs)\n",
      "bnp:bnp_matern step 80600 lr 4.501e-05 [train_loss] ll_base 0.5027 ll 0.5583 loss -1.0609 (4.370 secs)\n",
      "bnp:bnp_matern step 80800 lr 4.412e-05 [train_loss] ll_base 0.4988 ll 0.5538 loss -1.0526 (4.481 secs)\n",
      "bnp:bnp_matern step 81000 lr 4.323e-05 [train_loss] ll_base 0.5765 ll 0.6410 loss -1.2175 (4.376 secs)\n",
      "bnp:bnp_matern step 81200 lr 4.235e-05 [train_loss] ll_base 0.4912 ll 0.5598 loss -1.0510 (4.402 secs)\n",
      "bnp:bnp_matern step 81400 lr 4.148e-05 [train_loss] ll_base 0.4754 ll 0.5414 loss -1.0168 (4.491 secs)\n",
      "bnp:bnp_matern step 81600 lr 4.062e-05 [train_loss] ll_base 0.4825 ll 0.5435 loss -1.0260 (4.604 secs)\n",
      "bnp:bnp_matern step 81800 lr 3.976e-05 [train_loss] ll_base 0.4797 ll 0.5457 loss -1.0254 (4.394 secs)\n",
      "bnp:bnp_matern step 82000 lr 3.892e-05 [train_loss] ll_base 0.5233 ll 0.5937 loss -1.1170 (4.309 secs)\n",
      "bnp:bnp_matern step 82200 lr 3.808e-05 [train_loss] ll_base 0.5049 ll 0.5658 loss -1.0707 (4.440 secs)\n",
      "bnp:bnp_matern step 82400 lr 3.725e-05 [train_loss] ll_base 0.5107 ll 0.5682 loss -1.0789 (4.348 secs)\n",
      "bnp:bnp_matern step 82600 lr 3.643e-05 [train_loss] ll_base 0.5166 ll 0.5736 loss -1.0902 (4.376 secs)\n",
      "bnp:bnp_matern step 82800 lr 3.562e-05 [train_loss] ll_base 0.5034 ll 0.5686 loss -1.0721 (4.358 secs)\n",
      "bnp:bnp_matern step 83000 lr 3.481e-05 [train_loss] ll_base 0.5167 ll 0.5752 loss -1.0919 (4.445 secs)\n",
      "bnp:bnp_matern step 83200 lr 3.402e-05 [train_loss] ll_base 0.5072 ll 0.5698 loss -1.0770 (4.323 secs)\n",
      "bnp:bnp_matern step 83400 lr 3.323e-05 [train_loss] ll_base 0.4903 ll 0.5479 loss -1.0382 (4.367 secs)\n",
      "bnp:bnp_matern step 83600 lr 3.245e-05 [train_loss] ll_base 0.5068 ll 0.5677 loss -1.0744 (4.450 secs)\n",
      "bnp:bnp_matern step 83800 lr 3.168e-05 [train_loss] ll_base 0.5048 ll 0.5666 loss -1.0713 (4.642 secs)\n",
      "bnp:bnp_matern step 84000 lr 3.092e-05 [train_loss] ll_base 0.4866 ll 0.5586 loss -1.0452 (4.567 secs)\n",
      "bnp:bnp_matern step 84200 lr 3.017e-05 [train_loss] ll_base 0.5287 ll 0.5876 loss -1.1163 (4.522 secs)\n",
      "bnp:bnp_matern step 84400 lr 2.943e-05 [train_loss] ll_base 0.4898 ll 0.5498 loss -1.0395 (4.638 secs)\n",
      "bnp:bnp_matern step 84600 lr 2.869e-05 [train_loss] ll_base 0.5116 ll 0.5836 loss -1.0952 (4.560 secs)\n",
      "bnp:bnp_matern step 84800 lr 2.797e-05 [train_loss] ll_base 0.4791 ll 0.5430 loss -1.0220 (4.585 secs)\n",
      "bnp:bnp_matern step 85000 lr 2.725e-05 [train_loss] ll_base 0.5229 ll 0.5855 loss -1.1084 (4.573 secs)\n",
      "100%|##########| 3000/3000 [00:28<00:00, 106.85it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.6764 tar_ll 0.1740 (28.081 secs)\n",
      "\n",
      "bnp:bnp_matern step 85200 lr 2.654e-05 [train_loss] ll_base 0.5214 ll 0.5835 loss -1.1049 (4.786 secs)\n",
      "bnp:bnp_matern step 85400 lr 2.584e-05 [train_loss] ll_base 0.4783 ll 0.5419 loss -1.0203 (4.398 secs)\n",
      "bnp:bnp_matern step 85600 lr 2.515e-05 [train_loss] ll_base 0.5194 ll 0.5772 loss -1.0966 (4.502 secs)\n",
      "bnp:bnp_matern step 85800 lr 2.447e-05 [train_loss] ll_base 0.4750 ll 0.5441 loss -1.0191 (4.566 secs)\n",
      "bnp:bnp_matern step 86000 lr 2.379e-05 [train_loss] ll_base 0.4681 ll 0.5285 loss -0.9966 (4.681 secs)\n",
      "bnp:bnp_matern step 86200 lr 2.313e-05 [train_loss] ll_base 0.4839 ll 0.5528 loss -1.0368 (4.634 secs)\n",
      "bnp:bnp_matern step 86400 lr 2.247e-05 [train_loss] ll_base 0.4788 ll 0.5443 loss -1.0230 (4.458 secs)\n",
      "bnp:bnp_matern step 86600 lr 2.183e-05 [train_loss] ll_base 0.4712 ll 0.5390 loss -1.0102 (4.627 secs)\n",
      "bnp:bnp_matern step 86800 lr 2.119e-05 [train_loss] ll_base 0.4863 ll 0.5426 loss -1.0289 (4.474 secs)\n",
      "bnp:bnp_matern step 87000 lr 2.056e-05 [train_loss] ll_base 0.5571 ll 0.6224 loss -1.1795 (4.498 secs)\n",
      "bnp:bnp_matern step 87200 lr 1.994e-05 [train_loss] ll_base 0.5268 ll 0.5911 loss -1.1179 (4.401 secs)\n",
      "bnp:bnp_matern step 87400 lr 1.933e-05 [train_loss] ll_base 0.4647 ll 0.5359 loss -1.0006 (4.600 secs)\n",
      "bnp:bnp_matern step 87600 lr 1.873e-05 [train_loss] ll_base 0.5128 ll 0.5756 loss -1.0883 (4.467 secs)\n",
      "bnp:bnp_matern step 87800 lr 1.814e-05 [train_loss] ll_base 0.5439 ll 0.6067 loss -1.1506 (4.473 secs)\n",
      "bnp:bnp_matern step 88000 lr 1.756e-05 [train_loss] ll_base 0.4602 ll 0.5229 loss -0.9831 (4.621 secs)\n",
      "bnp:bnp_matern step 88200 lr 1.698e-05 [train_loss] ll_base 0.5136 ll 0.5825 loss -1.0961 (4.442 secs)\n",
      "bnp:bnp_matern step 88400 lr 1.642e-05 [train_loss] ll_base 0.5127 ll 0.5822 loss -1.0949 (4.463 secs)\n",
      "bnp:bnp_matern step 88600 lr 1.586e-05 [train_loss] ll_base 0.4524 ll 0.5184 loss -0.9708 (4.408 secs)\n",
      "bnp:bnp_matern step 88800 lr 1.532e-05 [train_loss] ll_base 0.4843 ll 0.5497 loss -1.0340 (4.586 secs)\n",
      "bnp:bnp_matern step 89000 lr 1.478e-05 [train_loss] ll_base 0.5282 ll 0.5973 loss -1.1256 (4.523 secs)\n",
      "bnp:bnp_matern step 89200 lr 1.425e-05 [train_loss] ll_base 0.5441 ll 0.6016 loss -1.1457 (4.537 secs)\n",
      "bnp:bnp_matern step 89400 lr 1.373e-05 [train_loss] ll_base 0.5125 ll 0.5665 loss -1.0790 (4.557 secs)\n",
      "bnp:bnp_matern step 89600 lr 1.323e-05 [train_loss] ll_base 0.4894 ll 0.5489 loss -1.0383 (4.524 secs)\n",
      "bnp:bnp_matern step 89800 lr 1.273e-05 [train_loss] ll_base 0.4921 ll 0.5509 loss -1.0430 (4.494 secs)\n",
      "bnp:bnp_matern step 90000 lr 1.224e-05 [train_loss] ll_base 0.5392 ll 0.6001 loss -1.1393 (4.431 secs)\n",
      "100%|##########| 3000/3000 [00:28<00:00, 107.06it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.6725 tar_ll 0.1890 (28.026 secs)\n",
      "\n",
      "bnp:bnp_matern step 90200 lr 1.176e-05 [train_loss] ll_base 0.4509 ll 0.5155 loss -0.9664 (4.447 secs)\n",
      "bnp:bnp_matern step 90400 lr 1.128e-05 [train_loss] ll_base 0.4885 ll 0.5511 loss -1.0396 (4.556 secs)\n",
      "bnp:bnp_matern step 90600 lr 1.082e-05 [train_loss] ll_base 0.4909 ll 0.5573 loss -1.0483 (4.402 secs)\n",
      "bnp:bnp_matern step 90800 lr 1.037e-05 [train_loss] ll_base 0.4631 ll 0.5242 loss -0.9873 (4.430 secs)\n",
      "bnp:bnp_matern step 91000 lr 9.927e-06 [train_loss] ll_base 0.4661 ll 0.5334 loss -0.9995 (4.514 secs)\n",
      "bnp:bnp_matern step 91200 lr 9.493e-06 [train_loss] ll_base 0.5324 ll 0.5949 loss -1.1273 (4.471 secs)\n",
      "bnp:bnp_matern step 91400 lr 9.069e-06 [train_loss] ll_base 0.5247 ll 0.5848 loss -1.1094 (4.414 secs)\n",
      "bnp:bnp_matern step 91600 lr 8.655e-06 [train_loss] ll_base 0.5067 ll 0.5666 loss -1.0733 (4.462 secs)\n",
      "bnp:bnp_matern step 91800 lr 8.250e-06 [train_loss] ll_base 0.4988 ll 0.5564 loss -1.0552 (4.642 secs)\n",
      "bnp:bnp_matern step 92000 lr 7.854e-06 [train_loss] ll_base 0.5316 ll 0.5937 loss -1.1252 (4.413 secs)\n",
      "bnp:bnp_matern step 92200 lr 7.468e-06 [train_loss] ll_base 0.5403 ll 0.5989 loss -1.1392 (4.508 secs)\n",
      "bnp:bnp_matern step 92400 lr 7.092e-06 [train_loss] ll_base 0.5072 ll 0.5733 loss -1.0805 (4.619 secs)\n",
      "bnp:bnp_matern step 92600 lr 6.725e-06 [train_loss] ll_base 0.5155 ll 0.5779 loss -1.0934 (4.520 secs)\n",
      "bnp:bnp_matern step 92800 lr 6.368e-06 [train_loss] ll_base 0.4454 ll 0.5251 loss -0.9705 (4.455 secs)\n",
      "bnp:bnp_matern step 93000 lr 6.021e-06 [train_loss] ll_base 0.5507 ll 0.6107 loss -1.1615 (4.529 secs)\n",
      "bnp:bnp_matern step 93200 lr 5.683e-06 [train_loss] ll_base 0.5429 ll 0.6065 loss -1.1494 (4.567 secs)\n",
      "bnp:bnp_matern step 93400 lr 5.355e-06 [train_loss] ll_base 0.4592 ll 0.5260 loss -0.9852 (4.567 secs)\n",
      "bnp:bnp_matern step 93600 lr 5.036e-06 [train_loss] ll_base 0.5176 ll 0.5759 loss -1.0935 (4.432 secs)\n",
      "bnp:bnp_matern step 93800 lr 4.727e-06 [train_loss] ll_base 0.4703 ll 0.5290 loss -0.9993 (4.593 secs)\n",
      "bnp:bnp_matern step 94000 lr 4.428e-06 [train_loss] ll_base 0.5088 ll 0.5719 loss -1.0807 (4.600 secs)\n",
      "bnp:bnp_matern step 94200 lr 4.139e-06 [train_loss] ll_base 0.5064 ll 0.5682 loss -1.0746 (4.425 secs)\n",
      "bnp:bnp_matern step 94400 lr 3.859e-06 [train_loss] ll_base 0.5069 ll 0.5656 loss -1.0725 (4.507 secs)\n",
      "bnp:bnp_matern step 94600 lr 3.589e-06 [train_loss] ll_base 0.4849 ll 0.5435 loss -1.0284 (4.606 secs)\n",
      "bnp:bnp_matern step 94800 lr 3.329e-06 [train_loss] ll_base 0.4617 ll 0.5259 loss -0.9876 (4.446 secs)\n",
      "bnp:bnp_matern step 95000 lr 3.078e-06 [train_loss] ll_base 0.5014 ll 0.5682 loss -1.0695 (4.429 secs)\n",
      "100%|##########| 3000/3000 [00:28<00:00, 106.14it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.6708 tar_ll 0.1928 (28.267 secs)\n",
      "\n",
      "bnp:bnp_matern step 95200 lr 2.837e-06 [train_loss] ll_base 0.5387 ll 0.5956 loss -1.1343 (4.464 secs)\n",
      "bnp:bnp_matern step 95400 lr 2.606e-06 [train_loss] ll_base 0.5059 ll 0.5711 loss -1.0771 (4.565 secs)\n",
      "bnp:bnp_matern step 95600 lr 2.385e-06 [train_loss] ll_base 0.5337 ll 0.6004 loss -1.1341 (4.466 secs)\n",
      "bnp:bnp_matern step 95800 lr 2.173e-06 [train_loss] ll_base 0.5023 ll 0.5645 loss -1.0668 (4.498 secs)\n",
      "bnp:bnp_matern step 96000 lr 1.971e-06 [train_loss] ll_base 0.4915 ll 0.5592 loss -1.0507 (4.470 secs)\n",
      "bnp:bnp_matern step 96200 lr 1.779e-06 [train_loss] ll_base 0.5114 ll 0.5680 loss -1.0794 (4.606 secs)\n",
      "bnp:bnp_matern step 96400 lr 1.597e-06 [train_loss] ll_base 0.5170 ll 0.5734 loss -1.0904 (4.397 secs)\n",
      "bnp:bnp_matern step 96600 lr 1.425e-06 [train_loss] ll_base 0.5066 ll 0.5708 loss -1.0774 (4.384 secs)\n",
      "bnp:bnp_matern step 96800 lr 1.262e-06 [train_loss] ll_base 0.5125 ll 0.5761 loss -1.0886 (4.461 secs)\n",
      "bnp:bnp_matern step 97000 lr 1.110e-06 [train_loss] ll_base 0.5237 ll 0.5855 loss -1.1092 (4.466 secs)\n",
      "bnp:bnp_matern step 97200 lr 9.666e-07 [train_loss] ll_base 0.5486 ll 0.6068 loss -1.1553 (4.513 secs)\n",
      "bnp:bnp_matern step 97400 lr 8.335e-07 [train_loss] ll_base 0.4976 ll 0.5593 loss -1.0569 (4.307 secs)\n",
      "bnp:bnp_matern step 97600 lr 7.103e-07 [train_loss] ll_base 0.4882 ll 0.5553 loss -1.0435 (4.541 secs)\n",
      "bnp:bnp_matern step 97800 lr 5.969e-07 [train_loss] ll_base 0.4973 ll 0.5576 loss -1.0548 (4.482 secs)\n",
      "bnp:bnp_matern step 98000 lr 4.933e-07 [train_loss] ll_base 0.4653 ll 0.5382 loss -1.0035 (4.346 secs)\n",
      "bnp:bnp_matern step 98200 lr 3.996e-07 [train_loss] ll_base 0.4833 ll 0.5443 loss -1.0276 (4.585 secs)\n",
      "bnp:bnp_matern step 98400 lr 3.158e-07 [train_loss] ll_base 0.4855 ll 0.5422 loss -1.0278 (4.556 secs)\n",
      "bnp:bnp_matern step 98600 lr 2.418e-07 [train_loss] ll_base 0.5000 ll 0.5616 loss -1.0616 (4.558 secs)\n",
      "bnp:bnp_matern step 98800 lr 1.776e-07 [train_loss] ll_base 0.4730 ll 0.5369 loss -1.0099 (4.592 secs)\n",
      "bnp:bnp_matern step 99000 lr 1.234e-07 [train_loss] ll_base 0.5137 ll 0.5775 loss -1.0912 (4.714 secs)\n",
      "bnp:bnp_matern step 99200 lr 7.895e-08 [train_loss] ll_base 0.4474 ll 0.5110 loss -0.9584 (4.646 secs)\n",
      "bnp:bnp_matern step 99400 lr 4.441e-08 [train_loss] ll_base 0.4911 ll 0.5614 loss -1.0525 (4.485 secs)\n",
      "bnp:bnp_matern step 99600 lr 1.974e-08 [train_loss] ll_base 0.5136 ll 0.5773 loss -1.0909 (4.509 secs)\n",
      "bnp:bnp_matern step 99800 lr 4.935e-09 [train_loss] ll_base 0.4524 ll 0.5167 loss -0.9691 (4.426 secs)\n",
      "bnp:bnp_matern step 100000 lr 0.000e+00 [train_loss] ll_base 0.4537 ll 0.5242 loss -0.9779 (4.541 secs)\n",
      "100%|##########| 3000/3000 [00:28<00:00, 104.83it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.6756 tar_ll 0.1903 (28.621 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:29<00:00, 101.07it/s]\n",
      "bnp:bnp_matern matern ctx_ll 0.6759 tar_ll 0.1900 (29.687 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 2877796.25 miliseconds\n",
      "Execution time: 2877.79625 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 250.07958984375 MB\n",
      "Memory Usage Change: 233.82958984375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='bnp', name='bnp_matern',val_seed=100, val_l=None,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58c243-b431-4844-8bec-43a94d72e1d4",
   "metadata": {},
   "source": [
    "## BANP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aa5711-d558-4e69-ad25-8d192ef99696",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mod(modelo='banp', name='banp_matern',val_seed=100, val_l=None,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa1f6d-9ff2-484f-baf9-060cacb2a9b1",
   "metadata": {},
   "source": [
    "## TNP-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "059ad821-e9ce-4878-a290-787010238819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: tnpd-tnpd_matern\n",
      "Total number of parameters: 222082\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tnpd:tnpd_matern step 200 lr 5.000e-04 [train_loss] tar_ll -0.6242 loss 0.6242 (5.393 secs)\n",
      "tnpd:tnpd_matern step 400 lr 5.000e-04 [train_loss] tar_ll -0.4247 loss 0.4247 (5.368 secs)\n",
      "tnpd:tnpd_matern step 600 lr 5.000e-04 [train_loss] tar_ll -0.0790 loss 0.0790 (5.233 secs)\n",
      "tnpd:tnpd_matern step 800 lr 4.999e-04 [train_loss] tar_ll 0.1056 loss -0.1056 (5.339 secs)\n",
      "tnpd:tnpd_matern step 1000 lr 4.999e-04 [train_loss] tar_ll 0.3834 loss -0.3834 (5.986 secs)\n",
      "tnpd:tnpd_matern step 1200 lr 4.998e-04 [train_loss] tar_ll 0.4009 loss -0.4009 (5.999 secs)\n",
      "tnpd:tnpd_matern step 1400 lr 4.998e-04 [train_loss] tar_ll 0.5215 loss -0.5215 (5.555 secs)\n",
      "tnpd:tnpd_matern step 1600 lr 4.997e-04 [train_loss] tar_ll 0.4782 loss -0.4782 (5.745 secs)\n",
      "tnpd:tnpd_matern step 1800 lr 4.996e-04 [train_loss] tar_ll 0.5204 loss -0.5204 (5.397 secs)\n",
      "tnpd:tnpd_matern step 2000 lr 4.995e-04 [train_loss] tar_ll 0.5357 loss -0.5357 (5.320 secs)\n",
      "tnpd:tnpd_matern step 2200 lr 4.994e-04 [train_loss] tar_ll 0.7144 loss -0.7144 (5.756 secs)\n",
      "tnpd:tnpd_matern step 2400 lr 4.993e-04 [train_loss] tar_ll 0.7298 loss -0.7298 (5.591 secs)\n",
      "tnpd:tnpd_matern step 2600 lr 4.992e-04 [train_loss] tar_ll 0.7388 loss -0.7388 (5.315 secs)\n",
      "tnpd:tnpd_matern step 2800 lr 4.990e-04 [train_loss] tar_ll 0.7452 loss -0.7452 (5.434 secs)\n",
      "tnpd:tnpd_matern step 3000 lr 4.989e-04 [train_loss] tar_ll 0.7551 loss -0.7551 (5.281 secs)\n",
      "tnpd:tnpd_matern step 3200 lr 4.987e-04 [train_loss] tar_ll 0.7670 loss -0.7670 (5.446 secs)\n",
      "tnpd:tnpd_matern step 3400 lr 4.986e-04 [train_loss] tar_ll 0.8528 loss -0.8528 (5.338 secs)\n",
      "tnpd:tnpd_matern step 3600 lr 4.984e-04 [train_loss] tar_ll 0.7777 loss -0.7777 (5.224 secs)\n",
      "tnpd:tnpd_matern step 3800 lr 4.982e-04 [train_loss] tar_ll 0.8712 loss -0.8712 (5.410 secs)\n",
      "tnpd:tnpd_matern step 4000 lr 4.980e-04 [train_loss] tar_ll 0.8017 loss -0.8017 (5.455 secs)\n",
      "tnpd:tnpd_matern step 4200 lr 4.978e-04 [train_loss] tar_ll 0.9025 loss -0.9025 (5.425 secs)\n",
      "tnpd:tnpd_matern step 4400 lr 4.976e-04 [train_loss] tar_ll 0.7321 loss -0.7321 (5.390 secs)\n",
      "tnpd:tnpd_matern step 4600 lr 4.974e-04 [train_loss] tar_ll 0.8701 loss -0.8701 (5.460 secs)\n",
      "tnpd:tnpd_matern step 4800 lr 4.972e-04 [train_loss] tar_ll 0.9274 loss -0.9274 (5.327 secs)\n",
      "tnpd:tnpd_matern step 5000 lr 4.969e-04 [train_loss] tar_ll 0.8592 loss -0.8592 (5.297 secs)\n",
      "  0%|          | 0/3000 [00:00<?, ?it/s]C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n",
      "100%|##########| 3000/3000 [00:16<00:00, 183.24it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.6516 loss -0.6516 (16.373 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 5200 lr 4.967e-04 [train_loss] tar_ll 0.9682 loss -0.9682 (5.253 secs)\n",
      "tnpd:tnpd_matern step 5400 lr 4.964e-04 [train_loss] tar_ll 0.9348 loss -0.9348 (5.237 secs)\n",
      "tnpd:tnpd_matern step 5600 lr 4.961e-04 [train_loss] tar_ll 0.8727 loss -0.8727 (5.378 secs)\n",
      "tnpd:tnpd_matern step 5800 lr 4.959e-04 [train_loss] tar_ll 0.8332 loss -0.8332 (5.254 secs)\n",
      "tnpd:tnpd_matern step 6000 lr 4.956e-04 [train_loss] tar_ll 0.9285 loss -0.9285 (5.160 secs)\n",
      "tnpd:tnpd_matern step 6200 lr 4.953e-04 [train_loss] tar_ll 0.9414 loss -0.9414 (5.323 secs)\n",
      "tnpd:tnpd_matern step 6400 lr 4.950e-04 [train_loss] tar_ll 0.9657 loss -0.9657 (5.315 secs)\n",
      "tnpd:tnpd_matern step 6600 lr 4.946e-04 [train_loss] tar_ll 0.9567 loss -0.9567 (5.249 secs)\n",
      "tnpd:tnpd_matern step 6800 lr 4.943e-04 [train_loss] tar_ll 0.8946 loss -0.8946 (5.529 secs)\n",
      "tnpd:tnpd_matern step 7000 lr 4.940e-04 [train_loss] tar_ll 0.9637 loss -0.9637 (5.433 secs)\n",
      "tnpd:tnpd_matern step 7200 lr 4.936e-04 [train_loss] tar_ll 1.0225 loss -1.0225 (5.237 secs)\n",
      "tnpd:tnpd_matern step 7400 lr 4.933e-04 [train_loss] tar_ll 0.8917 loss -0.8917 (5.317 secs)\n",
      "tnpd:tnpd_matern step 7600 lr 4.929e-04 [train_loss] tar_ll 0.9298 loss -0.9298 (5.377 secs)\n",
      "tnpd:tnpd_matern step 7800 lr 4.925e-04 [train_loss] tar_ll 0.9011 loss -0.9011 (5.254 secs)\n",
      "tnpd:tnpd_matern step 8000 lr 4.921e-04 [train_loss] tar_ll 0.9728 loss -0.9728 (5.468 secs)\n",
      "tnpd:tnpd_matern step 8200 lr 4.918e-04 [train_loss] tar_ll 0.9789 loss -0.9789 (5.450 secs)\n",
      "tnpd:tnpd_matern step 8400 lr 4.913e-04 [train_loss] tar_ll 0.8505 loss -0.8505 (5.236 secs)\n",
      "tnpd:tnpd_matern step 8600 lr 4.909e-04 [train_loss] tar_ll 0.8656 loss -0.8656 (5.652 secs)\n",
      "tnpd:tnpd_matern step 8800 lr 4.905e-04 [train_loss] tar_ll 0.8963 loss -0.8963 (5.411 secs)\n",
      "tnpd:tnpd_matern step 9000 lr 4.901e-04 [train_loss] tar_ll 0.9339 loss -0.9339 (5.283 secs)\n",
      "tnpd:tnpd_matern step 9200 lr 4.896e-04 [train_loss] tar_ll 0.9226 loss -0.9226 (5.567 secs)\n",
      "tnpd:tnpd_matern step 9400 lr 4.892e-04 [train_loss] tar_ll 1.0351 loss -1.0351 (5.385 secs)\n",
      "tnpd:tnpd_matern step 9600 lr 4.887e-04 [train_loss] tar_ll 0.9463 loss -0.9463 (5.371 secs)\n",
      "tnpd:tnpd_matern step 9800 lr 4.882e-04 [train_loss] tar_ll 1.0161 loss -1.0161 (5.439 secs)\n",
      "tnpd:tnpd_matern step 10000 lr 4.878e-04 [train_loss] tar_ll 1.0283 loss -1.0283 (5.444 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 178.23it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.7230 loss -0.7230 (16.832 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 10200 lr 4.873e-04 [train_loss] tar_ll 0.9202 loss -0.9202 (5.366 secs)\n",
      "tnpd:tnpd_matern step 10400 lr 4.868e-04 [train_loss] tar_ll 0.9761 loss -0.9761 (5.417 secs)\n",
      "tnpd:tnpd_matern step 10600 lr 4.863e-04 [train_loss] tar_ll 0.9723 loss -0.9723 (5.374 secs)\n",
      "tnpd:tnpd_matern step 10800 lr 4.857e-04 [train_loss] tar_ll 0.9567 loss -0.9567 (5.384 secs)\n",
      "tnpd:tnpd_matern step 11000 lr 4.852e-04 [train_loss] tar_ll 0.8807 loss -0.8807 (5.454 secs)\n",
      "tnpd:tnpd_matern step 11200 lr 4.847e-04 [train_loss] tar_ll 0.8546 loss -0.8546 (5.237 secs)\n",
      "tnpd:tnpd_matern step 11400 lr 4.841e-04 [train_loss] tar_ll 1.0118 loss -1.0118 (5.341 secs)\n",
      "tnpd:tnpd_matern step 11600 lr 4.836e-04 [train_loss] tar_ll 1.0183 loss -1.0183 (5.316 secs)\n",
      "tnpd:tnpd_matern step 11800 lr 4.830e-04 [train_loss] tar_ll 0.9890 loss -0.9890 (5.284 secs)\n",
      "tnpd:tnpd_matern step 12000 lr 4.824e-04 [train_loss] tar_ll 0.9527 loss -0.9527 (5.303 secs)\n",
      "tnpd:tnpd_matern step 12200 lr 4.819e-04 [train_loss] tar_ll 1.0649 loss -1.0649 (5.526 secs)\n",
      "tnpd:tnpd_matern step 12400 lr 4.813e-04 [train_loss] tar_ll 1.0260 loss -1.0260 (5.419 secs)\n",
      "tnpd:tnpd_matern step 12600 lr 4.807e-04 [train_loss] tar_ll 0.9557 loss -0.9557 (5.243 secs)\n",
      "tnpd:tnpd_matern step 12800 lr 4.801e-04 [train_loss] tar_ll 1.0618 loss -1.0618 (5.408 secs)\n",
      "tnpd:tnpd_matern step 13000 lr 4.794e-04 [train_loss] tar_ll 0.9754 loss -0.9754 (5.420 secs)\n",
      "tnpd:tnpd_matern step 13200 lr 4.788e-04 [train_loss] tar_ll 1.0674 loss -1.0674 (5.413 secs)\n",
      "tnpd:tnpd_matern step 13400 lr 4.782e-04 [train_loss] tar_ll 1.0903 loss -1.0903 (5.397 secs)\n",
      "tnpd:tnpd_matern step 13600 lr 4.775e-04 [train_loss] tar_ll 1.0446 loss -1.0446 (5.163 secs)\n",
      "tnpd:tnpd_matern step 13800 lr 4.769e-04 [train_loss] tar_ll 1.0970 loss -1.0970 (5.227 secs)\n",
      "tnpd:tnpd_matern step 14000 lr 4.762e-04 [train_loss] tar_ll 1.0976 loss -1.0976 (5.423 secs)\n",
      "tnpd:tnpd_matern step 14200 lr 4.755e-04 [train_loss] tar_ll 1.0035 loss -1.0035 (5.588 secs)\n",
      "tnpd:tnpd_matern step 14400 lr 4.749e-04 [train_loss] tar_ll 1.0736 loss -1.0736 (5.385 secs)\n",
      "tnpd:tnpd_matern step 14600 lr 4.742e-04 [train_loss] tar_ll 0.9147 loss -0.9147 (5.333 secs)\n",
      "tnpd:tnpd_matern step 14800 lr 4.735e-04 [train_loss] tar_ll 1.0480 loss -1.0480 (5.359 secs)\n",
      "tnpd:tnpd_matern step 15000 lr 4.728e-04 [train_loss] tar_ll 1.1132 loss -1.1132 (5.403 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 182.65it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.6707 loss -0.6707 (16.432 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 15200 lr 4.720e-04 [train_loss] tar_ll 1.0652 loss -1.0652 (5.293 secs)\n",
      "tnpd:tnpd_matern step 15400 lr 4.713e-04 [train_loss] tar_ll 1.1002 loss -1.1002 (5.274 secs)\n",
      "tnpd:tnpd_matern step 15600 lr 4.706e-04 [train_loss] tar_ll 0.9273 loss -0.9273 (5.484 secs)\n",
      "tnpd:tnpd_matern step 15800 lr 4.698e-04 [train_loss] tar_ll 0.9947 loss -0.9947 (5.359 secs)\n",
      "tnpd:tnpd_matern step 16000 lr 4.691e-04 [train_loss] tar_ll 1.1246 loss -1.1246 (5.388 secs)\n",
      "tnpd:tnpd_matern step 16200 lr 4.683e-04 [train_loss] tar_ll 1.0434 loss -1.0434 (5.396 secs)\n",
      "tnpd:tnpd_matern step 16400 lr 4.675e-04 [train_loss] tar_ll 1.0965 loss -1.0965 (5.340 secs)\n",
      "tnpd:tnpd_matern step 16600 lr 4.668e-04 [train_loss] tar_ll 1.0992 loss -1.0992 (5.439 secs)\n",
      "tnpd:tnpd_matern step 16800 lr 4.660e-04 [train_loss] tar_ll 1.0759 loss -1.0759 (5.568 secs)\n",
      "tnpd:tnpd_matern step 17000 lr 4.652e-04 [train_loss] tar_ll 1.1094 loss -1.1094 (5.391 secs)\n",
      "tnpd:tnpd_matern step 17200 lr 4.644e-04 [train_loss] tar_ll 0.7796 loss -0.7796 (5.293 secs)\n",
      "tnpd:tnpd_matern step 17400 lr 4.636e-04 [train_loss] tar_ll 0.8637 loss -0.8637 (5.668 secs)\n",
      "tnpd:tnpd_matern step 17600 lr 4.627e-04 [train_loss] tar_ll 1.0594 loss -1.0594 (5.211 secs)\n",
      "tnpd:tnpd_matern step 17800 lr 4.619e-04 [train_loss] tar_ll 1.0455 loss -1.0455 (5.377 secs)\n",
      "tnpd:tnpd_matern step 18000 lr 4.611e-04 [train_loss] tar_ll 1.0777 loss -1.0777 (5.545 secs)\n",
      "tnpd:tnpd_matern step 18200 lr 4.602e-04 [train_loss] tar_ll 1.0150 loss -1.0150 (5.251 secs)\n",
      "tnpd:tnpd_matern step 18400 lr 4.594e-04 [train_loss] tar_ll 1.1117 loss -1.1117 (5.347 secs)\n",
      "tnpd:tnpd_matern step 18600 lr 4.585e-04 [train_loss] tar_ll 1.1133 loss -1.1133 (5.214 secs)\n",
      "tnpd:tnpd_matern step 18800 lr 4.576e-04 [train_loss] tar_ll 1.0838 loss -1.0838 (5.401 secs)\n",
      "tnpd:tnpd_matern step 19000 lr 4.568e-04 [train_loss] tar_ll 1.0117 loss -1.0117 (5.214 secs)\n",
      "tnpd:tnpd_matern step 19200 lr 4.559e-04 [train_loss] tar_ll 1.1308 loss -1.1308 (5.266 secs)\n",
      "tnpd:tnpd_matern step 19400 lr 4.550e-04 [train_loss] tar_ll 1.1217 loss -1.1217 (5.161 secs)\n",
      "tnpd:tnpd_matern step 19600 lr 4.541e-04 [train_loss] tar_ll 1.0385 loss -1.0385 (5.080 secs)\n",
      "tnpd:tnpd_matern step 19800 lr 4.532e-04 [train_loss] tar_ll 1.0826 loss -1.0826 (5.317 secs)\n",
      "tnpd:tnpd_matern step 20000 lr 4.523e-04 [train_loss] tar_ll 1.1444 loss -1.1444 (5.142 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 179.29it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.8376 loss -0.8376 (16.736 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 20200 lr 4.513e-04 [train_loss] tar_ll 1.1053 loss -1.1053 (5.376 secs)\n",
      "tnpd:tnpd_matern step 20400 lr 4.504e-04 [train_loss] tar_ll 1.0916 loss -1.0916 (5.474 secs)\n",
      "tnpd:tnpd_matern step 20600 lr 4.494e-04 [train_loss] tar_ll 1.0804 loss -1.0804 (5.268 secs)\n",
      "tnpd:tnpd_matern step 20800 lr 4.485e-04 [train_loss] tar_ll 1.1040 loss -1.1040 (5.396 secs)\n",
      "tnpd:tnpd_matern step 21000 lr 4.475e-04 [train_loss] tar_ll 1.1223 loss -1.1223 (5.436 secs)\n",
      "tnpd:tnpd_matern step 21200 lr 4.466e-04 [train_loss] tar_ll 1.1546 loss -1.1546 (5.240 secs)\n",
      "tnpd:tnpd_matern step 21400 lr 4.456e-04 [train_loss] tar_ll 1.1217 loss -1.1217 (5.421 secs)\n",
      "tnpd:tnpd_matern step 21600 lr 4.446e-04 [train_loss] tar_ll 0.9971 loss -0.9971 (5.486 secs)\n",
      "tnpd:tnpd_matern step 21800 lr 4.436e-04 [train_loss] tar_ll 1.0338 loss -1.0338 (5.424 secs)\n",
      "tnpd:tnpd_matern step 22000 lr 4.426e-04 [train_loss] tar_ll 1.0128 loss -1.0128 (5.306 secs)\n",
      "tnpd:tnpd_matern step 22200 lr 4.416e-04 [train_loss] tar_ll 1.1037 loss -1.1037 (5.421 secs)\n",
      "tnpd:tnpd_matern step 22400 lr 4.406e-04 [train_loss] tar_ll 1.1101 loss -1.1101 (5.351 secs)\n",
      "tnpd:tnpd_matern step 22600 lr 4.396e-04 [train_loss] tar_ll 1.1087 loss -1.1087 (5.353 secs)\n",
      "tnpd:tnpd_matern step 22800 lr 4.386e-04 [train_loss] tar_ll 1.0506 loss -1.0506 (5.478 secs)\n",
      "tnpd:tnpd_matern step 23000 lr 4.375e-04 [train_loss] tar_ll 0.9880 loss -0.9880 (5.511 secs)\n",
      "tnpd:tnpd_matern step 23200 lr 4.365e-04 [train_loss] tar_ll 1.1052 loss -1.1052 (5.357 secs)\n",
      "tnpd:tnpd_matern step 23400 lr 4.354e-04 [train_loss] tar_ll 1.1190 loss -1.1190 (5.547 secs)\n",
      "tnpd:tnpd_matern step 23600 lr 4.344e-04 [train_loss] tar_ll 1.1334 loss -1.1334 (5.647 secs)\n",
      "tnpd:tnpd_matern step 23800 lr 4.333e-04 [train_loss] tar_ll 1.1962 loss -1.1962 (5.689 secs)\n",
      "tnpd:tnpd_matern step 24000 lr 4.322e-04 [train_loss] tar_ll 1.0854 loss -1.0854 (5.474 secs)\n",
      "tnpd:tnpd_matern step 24200 lr 4.312e-04 [train_loss] tar_ll 1.1640 loss -1.1640 (5.460 secs)\n",
      "tnpd:tnpd_matern step 24400 lr 4.301e-04 [train_loss] tar_ll 1.2440 loss -1.2440 (5.521 secs)\n",
      "tnpd:tnpd_matern step 24600 lr 4.290e-04 [train_loss] tar_ll 1.1999 loss -1.1999 (5.320 secs)\n",
      "tnpd:tnpd_matern step 24800 lr 4.279e-04 [train_loss] tar_ll 1.1315 loss -1.1315 (5.241 secs)\n",
      "tnpd:tnpd_matern step 25000 lr 4.268e-04 [train_loss] tar_ll 1.1170 loss -1.1170 (5.350 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 175.75it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.7969 loss -0.7969 (17.078 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 25200 lr 4.257e-04 [train_loss] tar_ll 1.1640 loss -1.1640 (5.420 secs)\n",
      "tnpd:tnpd_matern step 25400 lr 4.245e-04 [train_loss] tar_ll 1.1786 loss -1.1786 (5.431 secs)\n",
      "tnpd:tnpd_matern step 25600 lr 4.234e-04 [train_loss] tar_ll 1.0620 loss -1.0620 (5.579 secs)\n",
      "tnpd:tnpd_matern step 25800 lr 4.223e-04 [train_loss] tar_ll 1.0104 loss -1.0104 (5.362 secs)\n",
      "tnpd:tnpd_matern step 26000 lr 4.211e-04 [train_loss] tar_ll 1.0884 loss -1.0884 (5.310 secs)\n",
      "tnpd:tnpd_matern step 26200 lr 4.200e-04 [train_loss] tar_ll 1.0548 loss -1.0548 (5.306 secs)\n",
      "tnpd:tnpd_matern step 26400 lr 4.188e-04 [train_loss] tar_ll 1.2054 loss -1.2054 (5.097 secs)\n",
      "tnpd:tnpd_matern step 26600 lr 4.177e-04 [train_loss] tar_ll 1.1211 loss -1.1211 (5.226 secs)\n",
      "tnpd:tnpd_matern step 26800 lr 4.165e-04 [train_loss] tar_ll 1.1421 loss -1.1421 (5.287 secs)\n",
      "tnpd:tnpd_matern step 27000 lr 4.153e-04 [train_loss] tar_ll 1.1499 loss -1.1499 (5.321 secs)\n",
      "tnpd:tnpd_matern step 27200 lr 4.141e-04 [train_loss] tar_ll 1.0727 loss -1.0727 (5.228 secs)\n",
      "tnpd:tnpd_matern step 27400 lr 4.130e-04 [train_loss] tar_ll 1.2373 loss -1.2373 (5.408 secs)\n",
      "tnpd:tnpd_matern step 27600 lr 4.118e-04 [train_loss] tar_ll 1.1884 loss -1.1884 (5.425 secs)\n",
      "tnpd:tnpd_matern step 27800 lr 4.106e-04 [train_loss] tar_ll 1.1550 loss -1.1550 (5.186 secs)\n",
      "tnpd:tnpd_matern step 28000 lr 4.094e-04 [train_loss] tar_ll 1.1638 loss -1.1638 (5.332 secs)\n",
      "tnpd:tnpd_matern step 28200 lr 4.081e-04 [train_loss] tar_ll 1.1781 loss -1.1781 (5.419 secs)\n",
      "tnpd:tnpd_matern step 28400 lr 4.069e-04 [train_loss] tar_ll 1.1426 loss -1.1426 (5.205 secs)\n",
      "tnpd:tnpd_matern step 28600 lr 4.057e-04 [train_loss] tar_ll 1.0718 loss -1.0718 (5.398 secs)\n",
      "tnpd:tnpd_matern step 28800 lr 4.045e-04 [train_loss] tar_ll 1.0695 loss -1.0695 (5.190 secs)\n",
      "tnpd:tnpd_matern step 29000 lr 4.032e-04 [train_loss] tar_ll 1.1504 loss -1.1504 (5.233 secs)\n",
      "tnpd:tnpd_matern step 29200 lr 4.020e-04 [train_loss] tar_ll 0.9929 loss -0.9929 (5.419 secs)\n",
      "tnpd:tnpd_matern step 29400 lr 4.007e-04 [train_loss] tar_ll 1.1828 loss -1.1828 (5.283 secs)\n",
      "tnpd:tnpd_matern step 29600 lr 3.995e-04 [train_loss] tar_ll 1.1860 loss -1.1860 (5.243 secs)\n",
      "tnpd:tnpd_matern step 29800 lr 3.982e-04 [train_loss] tar_ll 1.2069 loss -1.2069 (5.172 secs)\n",
      "tnpd:tnpd_matern step 30000 lr 3.969e-04 [train_loss] tar_ll 1.0183 loss -1.0183 (5.300 secs)\n",
      "100%|##########| 3000/3000 [00:15<00:00, 187.64it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.7400 loss -0.7400 (15.990 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 30200 lr 3.957e-04 [train_loss] tar_ll 1.0208 loss -1.0208 (5.329 secs)\n",
      "tnpd:tnpd_matern step 30400 lr 3.944e-04 [train_loss] tar_ll 1.1262 loss -1.1262 (5.482 secs)\n",
      "tnpd:tnpd_matern step 30600 lr 3.931e-04 [train_loss] tar_ll 1.1842 loss -1.1842 (5.393 secs)\n",
      "tnpd:tnpd_matern step 30800 lr 3.918e-04 [train_loss] tar_ll 1.1615 loss -1.1615 (5.415 secs)\n",
      "tnpd:tnpd_matern step 31000 lr 3.905e-04 [train_loss] tar_ll 1.0555 loss -1.0555 (5.404 secs)\n",
      "tnpd:tnpd_matern step 31200 lr 3.892e-04 [train_loss] tar_ll 1.2347 loss -1.2347 (5.355 secs)\n",
      "tnpd:tnpd_matern step 31400 lr 3.879e-04 [train_loss] tar_ll 1.1929 loss -1.1929 (5.517 secs)\n",
      "tnpd:tnpd_matern step 31600 lr 3.866e-04 [train_loss] tar_ll 1.0866 loss -1.0866 (5.596 secs)\n",
      "tnpd:tnpd_matern step 31800 lr 3.853e-04 [train_loss] tar_ll 1.1763 loss -1.1763 (5.286 secs)\n",
      "tnpd:tnpd_matern step 32000 lr 3.840e-04 [train_loss] tar_ll 1.1806 loss -1.1806 (5.431 secs)\n",
      "tnpd:tnpd_matern step 32200 lr 3.826e-04 [train_loss] tar_ll 1.1558 loss -1.1558 (5.470 secs)\n",
      "tnpd:tnpd_matern step 32400 lr 3.813e-04 [train_loss] tar_ll 1.1881 loss -1.1881 (5.341 secs)\n",
      "tnpd:tnpd_matern step 32600 lr 3.800e-04 [train_loss] tar_ll 1.1379 loss -1.1379 (5.446 secs)\n",
      "tnpd:tnpd_matern step 32800 lr 3.786e-04 [train_loss] tar_ll 1.2157 loss -1.2157 (5.580 secs)\n",
      "tnpd:tnpd_matern step 33000 lr 3.773e-04 [train_loss] tar_ll 1.2275 loss -1.2275 (5.501 secs)\n",
      "tnpd:tnpd_matern step 33200 lr 3.759e-04 [train_loss] tar_ll 1.1858 loss -1.1858 (5.608 secs)\n",
      "tnpd:tnpd_matern step 33400 lr 3.745e-04 [train_loss] tar_ll 1.2551 loss -1.2551 (5.231 secs)\n",
      "tnpd:tnpd_matern step 33600 lr 3.732e-04 [train_loss] tar_ll 1.1848 loss -1.1848 (5.453 secs)\n",
      "tnpd:tnpd_matern step 33800 lr 3.718e-04 [train_loss] tar_ll 1.2341 loss -1.2341 (5.580 secs)\n",
      "tnpd:tnpd_matern step 34000 lr 3.704e-04 [train_loss] tar_ll 1.1567 loss -1.1567 (5.570 secs)\n",
      "tnpd:tnpd_matern step 34200 lr 3.691e-04 [train_loss] tar_ll 1.2673 loss -1.2673 (5.359 secs)\n",
      "tnpd:tnpd_matern step 34400 lr 3.677e-04 [train_loss] tar_ll 1.1183 loss -1.1183 (5.419 secs)\n",
      "tnpd:tnpd_matern step 34600 lr 3.663e-04 [train_loss] tar_ll 1.2757 loss -1.2757 (5.290 secs)\n",
      "tnpd:tnpd_matern step 34800 lr 3.649e-04 [train_loss] tar_ll 1.1927 loss -1.1927 (5.258 secs)\n",
      "tnpd:tnpd_matern step 35000 lr 3.635e-04 [train_loss] tar_ll 1.2197 loss -1.2197 (5.387 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 180.99it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.8493 loss -0.8493 (16.578 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 35200 lr 3.621e-04 [train_loss] tar_ll 1.2251 loss -1.2251 (5.470 secs)\n",
      "tnpd:tnpd_matern step 35400 lr 3.607e-04 [train_loss] tar_ll 1.2486 loss -1.2486 (5.406 secs)\n",
      "tnpd:tnpd_matern step 35600 lr 3.593e-04 [train_loss] tar_ll 1.1982 loss -1.1982 (5.481 secs)\n",
      "tnpd:tnpd_matern step 35800 lr 3.579e-04 [train_loss] tar_ll 1.0608 loss -1.0608 (5.368 secs)\n",
      "tnpd:tnpd_matern step 36000 lr 3.564e-04 [train_loss] tar_ll 1.2102 loss -1.2102 (5.292 secs)\n",
      "tnpd:tnpd_matern step 36200 lr 3.550e-04 [train_loss] tar_ll 1.2535 loss -1.2535 (5.521 secs)\n",
      "tnpd:tnpd_matern step 36400 lr 3.536e-04 [train_loss] tar_ll 1.2218 loss -1.2218 (5.380 secs)\n",
      "tnpd:tnpd_matern step 36600 lr 3.522e-04 [train_loss] tar_ll 1.1699 loss -1.1699 (5.449 secs)\n",
      "tnpd:tnpd_matern step 36800 lr 3.507e-04 [train_loss] tar_ll 1.1856 loss -1.1856 (5.439 secs)\n",
      "tnpd:tnpd_matern step 37000 lr 3.493e-04 [train_loss] tar_ll 1.2141 loss -1.2141 (5.392 secs)\n",
      "tnpd:tnpd_matern step 37200 lr 3.478e-04 [train_loss] tar_ll 1.2467 loss -1.2467 (5.325 secs)\n",
      "tnpd:tnpd_matern step 37400 lr 3.464e-04 [train_loss] tar_ll 1.2067 loss -1.2067 (5.436 secs)\n",
      "tnpd:tnpd_matern step 37600 lr 3.449e-04 [train_loss] tar_ll 1.2133 loss -1.2133 (5.447 secs)\n",
      "tnpd:tnpd_matern step 37800 lr 3.435e-04 [train_loss] tar_ll 1.2268 loss -1.2268 (5.487 secs)\n",
      "tnpd:tnpd_matern step 38000 lr 3.420e-04 [train_loss] tar_ll 1.1392 loss -1.1392 (5.471 secs)\n",
      "tnpd:tnpd_matern step 38200 lr 3.406e-04 [train_loss] tar_ll 1.1818 loss -1.1818 (5.508 secs)\n",
      "tnpd:tnpd_matern step 38400 lr 3.391e-04 [train_loss] tar_ll 1.2343 loss -1.2343 (5.378 secs)\n",
      "tnpd:tnpd_matern step 38600 lr 3.376e-04 [train_loss] tar_ll 1.2590 loss -1.2590 (5.457 secs)\n",
      "tnpd:tnpd_matern step 38800 lr 3.362e-04 [train_loss] tar_ll 1.1381 loss -1.1381 (5.415 secs)\n",
      "tnpd:tnpd_matern step 39000 lr 3.347e-04 [train_loss] tar_ll 1.2232 loss -1.2232 (5.569 secs)\n",
      "tnpd:tnpd_matern step 39200 lr 3.332e-04 [train_loss] tar_ll 1.1233 loss -1.1233 (5.487 secs)\n",
      "tnpd:tnpd_matern step 39400 lr 3.317e-04 [train_loss] tar_ll 1.2059 loss -1.2059 (5.490 secs)\n",
      "tnpd:tnpd_matern step 39600 lr 3.302e-04 [train_loss] tar_ll 1.1824 loss -1.1824 (5.458 secs)\n",
      "tnpd:tnpd_matern step 39800 lr 3.287e-04 [train_loss] tar_ll 1.1971 loss -1.1971 (5.371 secs)\n",
      "tnpd:tnpd_matern step 40000 lr 3.273e-04 [train_loss] tar_ll 1.2573 loss -1.2573 (5.521 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 176.14it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.8710 loss -0.8710 (17.032 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 40200 lr 3.258e-04 [train_loss] tar_ll 1.1769 loss -1.1769 (5.538 secs)\n",
      "tnpd:tnpd_matern step 40400 lr 3.243e-04 [train_loss] tar_ll 1.1219 loss -1.1219 (5.453 secs)\n",
      "tnpd:tnpd_matern step 40600 lr 3.228e-04 [train_loss] tar_ll 1.2608 loss -1.2608 (5.411 secs)\n",
      "tnpd:tnpd_matern step 40800 lr 3.213e-04 [train_loss] tar_ll 1.2477 loss -1.2477 (5.499 secs)\n",
      "tnpd:tnpd_matern step 41000 lr 3.197e-04 [train_loss] tar_ll 1.2450 loss -1.2450 (5.376 secs)\n",
      "tnpd:tnpd_matern step 41200 lr 3.182e-04 [train_loss] tar_ll 1.2663 loss -1.2663 (5.500 secs)\n",
      "tnpd:tnpd_matern step 41400 lr 3.167e-04 [train_loss] tar_ll 1.1739 loss -1.1739 (5.528 secs)\n",
      "tnpd:tnpd_matern step 41600 lr 3.152e-04 [train_loss] tar_ll 1.2632 loss -1.2632 (5.453 secs)\n",
      "tnpd:tnpd_matern step 41800 lr 3.137e-04 [train_loss] tar_ll 1.2356 loss -1.2356 (5.435 secs)\n",
      "tnpd:tnpd_matern step 42000 lr 3.122e-04 [train_loss] tar_ll 1.1616 loss -1.1616 (5.434 secs)\n",
      "tnpd:tnpd_matern step 42200 lr 3.106e-04 [train_loss] tar_ll 1.2406 loss -1.2406 (5.374 secs)\n",
      "tnpd:tnpd_matern step 42400 lr 3.091e-04 [train_loss] tar_ll 1.1505 loss -1.1505 (5.506 secs)\n",
      "tnpd:tnpd_matern step 42600 lr 3.076e-04 [train_loss] tar_ll 1.2685 loss -1.2685 (5.361 secs)\n",
      "tnpd:tnpd_matern step 42800 lr 3.061e-04 [train_loss] tar_ll 1.1987 loss -1.1987 (5.237 secs)\n",
      "tnpd:tnpd_matern step 43000 lr 3.045e-04 [train_loss] tar_ll 1.2950 loss -1.2950 (5.184 secs)\n",
      "tnpd:tnpd_matern step 43200 lr 3.030e-04 [train_loss] tar_ll 1.2696 loss -1.2696 (5.338 secs)\n",
      "tnpd:tnpd_matern step 43400 lr 3.015e-04 [train_loss] tar_ll 1.2661 loss -1.2661 (5.161 secs)\n",
      "tnpd:tnpd_matern step 43600 lr 2.999e-04 [train_loss] tar_ll 1.2405 loss -1.2405 (5.083 secs)\n",
      "tnpd:tnpd_matern step 43800 lr 2.984e-04 [train_loss] tar_ll 1.2980 loss -1.2980 (5.303 secs)\n",
      "tnpd:tnpd_matern step 44000 lr 2.968e-04 [train_loss] tar_ll 1.3084 loss -1.3084 (5.331 secs)\n",
      "tnpd:tnpd_matern step 44200 lr 2.953e-04 [train_loss] tar_ll 1.1709 loss -1.1709 (5.330 secs)\n",
      "tnpd:tnpd_matern step 44400 lr 2.938e-04 [train_loss] tar_ll 1.3248 loss -1.3248 (5.250 secs)\n",
      "tnpd:tnpd_matern step 44600 lr 2.922e-04 [train_loss] tar_ll 1.2370 loss -1.2370 (5.314 secs)\n",
      "tnpd:tnpd_matern step 44800 lr 2.907e-04 [train_loss] tar_ll 1.3249 loss -1.3249 (5.247 secs)\n",
      "tnpd:tnpd_matern step 45000 lr 2.891e-04 [train_loss] tar_ll 1.3001 loss -1.3001 (5.672 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 186.11it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.8124 loss -0.8124 (16.124 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 45200 lr 2.876e-04 [train_loss] tar_ll 1.2059 loss -1.2059 (5.259 secs)\n",
      "tnpd:tnpd_matern step 45400 lr 2.860e-04 [train_loss] tar_ll 1.2070 loss -1.2070 (5.214 secs)\n",
      "tnpd:tnpd_matern step 45600 lr 2.844e-04 [train_loss] tar_ll 1.2322 loss -1.2322 (5.174 secs)\n",
      "tnpd:tnpd_matern step 45800 lr 2.829e-04 [train_loss] tar_ll 1.2150 loss -1.2150 (5.309 secs)\n",
      "tnpd:tnpd_matern step 46000 lr 2.813e-04 [train_loss] tar_ll 1.2870 loss -1.2870 (5.536 secs)\n",
      "tnpd:tnpd_matern step 46200 lr 2.798e-04 [train_loss] tar_ll 1.1935 loss -1.1935 (5.478 secs)\n",
      "tnpd:tnpd_matern step 46400 lr 2.782e-04 [train_loss] tar_ll 1.2002 loss -1.2002 (5.327 secs)\n",
      "tnpd:tnpd_matern step 46600 lr 2.767e-04 [train_loss] tar_ll 1.2651 loss -1.2651 (5.454 secs)\n",
      "tnpd:tnpd_matern step 46800 lr 2.751e-04 [train_loss] tar_ll 1.2801 loss -1.2801 (5.429 secs)\n",
      "tnpd:tnpd_matern step 47000 lr 2.735e-04 [train_loss] tar_ll 1.3264 loss -1.3264 (5.365 secs)\n",
      "tnpd:tnpd_matern step 47200 lr 2.720e-04 [train_loss] tar_ll 1.3670 loss -1.3670 (5.452 secs)\n",
      "tnpd:tnpd_matern step 47400 lr 2.704e-04 [train_loss] tar_ll 1.2391 loss -1.2391 (5.315 secs)\n",
      "tnpd:tnpd_matern step 47600 lr 2.688e-04 [train_loss] tar_ll 1.2610 loss -1.2610 (5.428 secs)\n",
      "tnpd:tnpd_matern step 47800 lr 2.673e-04 [train_loss] tar_ll 1.2519 loss -1.2519 (5.427 secs)\n",
      "tnpd:tnpd_matern step 48000 lr 2.657e-04 [train_loss] tar_ll 1.2581 loss -1.2581 (5.145 secs)\n",
      "tnpd:tnpd_matern step 48200 lr 2.641e-04 [train_loss] tar_ll 1.2992 loss -1.2992 (5.331 secs)\n",
      "tnpd:tnpd_matern step 48400 lr 2.626e-04 [train_loss] tar_ll 1.2454 loss -1.2454 (5.354 secs)\n",
      "tnpd:tnpd_matern step 48600 lr 2.610e-04 [train_loss] tar_ll 1.1911 loss -1.1911 (5.306 secs)\n",
      "tnpd:tnpd_matern step 48800 lr 2.594e-04 [train_loss] tar_ll 1.1576 loss -1.1576 (5.734 secs)\n",
      "tnpd:tnpd_matern step 49000 lr 2.579e-04 [train_loss] tar_ll 1.2531 loss -1.2531 (5.415 secs)\n",
      "tnpd:tnpd_matern step 49200 lr 2.563e-04 [train_loss] tar_ll 1.1618 loss -1.1618 (5.467 secs)\n",
      "tnpd:tnpd_matern step 49400 lr 2.547e-04 [train_loss] tar_ll 1.3362 loss -1.3362 (5.225 secs)\n",
      "tnpd:tnpd_matern step 49600 lr 2.531e-04 [train_loss] tar_ll 1.2233 loss -1.2233 (5.466 secs)\n",
      "tnpd:tnpd_matern step 49800 lr 2.516e-04 [train_loss] tar_ll 1.2877 loss -1.2877 (5.435 secs)\n",
      "tnpd:tnpd_matern step 50000 lr 2.500e-04 [train_loss] tar_ll 1.2087 loss -1.2087 (5.357 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 180.03it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.9092 loss -0.9092 (16.671 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 50200 lr 2.484e-04 [train_loss] tar_ll 1.1644 loss -1.1644 (5.467 secs)\n",
      "tnpd:tnpd_matern step 50400 lr 2.469e-04 [train_loss] tar_ll 1.3171 loss -1.3171 (5.230 secs)\n",
      "tnpd:tnpd_matern step 50600 lr 2.453e-04 [train_loss] tar_ll 1.2308 loss -1.2308 (5.219 secs)\n",
      "tnpd:tnpd_matern step 50800 lr 2.437e-04 [train_loss] tar_ll 1.2177 loss -1.2177 (5.543 secs)\n",
      "tnpd:tnpd_matern step 51000 lr 2.421e-04 [train_loss] tar_ll 1.2796 loss -1.2796 (5.230 secs)\n",
      "tnpd:tnpd_matern step 51200 lr 2.406e-04 [train_loss] tar_ll 1.2810 loss -1.2810 (5.378 secs)\n",
      "tnpd:tnpd_matern step 51400 lr 2.390e-04 [train_loss] tar_ll 1.3936 loss -1.3936 (5.283 secs)\n",
      "tnpd:tnpd_matern step 51600 lr 2.374e-04 [train_loss] tar_ll 1.2878 loss -1.2878 (5.286 secs)\n",
      "tnpd:tnpd_matern step 51800 lr 2.359e-04 [train_loss] tar_ll 1.3385 loss -1.3385 (5.336 secs)\n",
      "tnpd:tnpd_matern step 52000 lr 2.343e-04 [train_loss] tar_ll 1.3110 loss -1.3110 (5.406 secs)\n",
      "tnpd:tnpd_matern step 52200 lr 2.327e-04 [train_loss] tar_ll 1.2939 loss -1.2939 (4.999 secs)\n",
      "tnpd:tnpd_matern step 52400 lr 2.312e-04 [train_loss] tar_ll 1.3148 loss -1.3148 (5.103 secs)\n",
      "tnpd:tnpd_matern step 52600 lr 2.296e-04 [train_loss] tar_ll 1.2803 loss -1.2803 (5.041 secs)\n",
      "tnpd:tnpd_matern step 52800 lr 2.280e-04 [train_loss] tar_ll 1.1985 loss -1.1985 (5.041 secs)\n",
      "tnpd:tnpd_matern step 53000 lr 2.265e-04 [train_loss] tar_ll 1.2445 loss -1.2445 (4.938 secs)\n",
      "tnpd:tnpd_matern step 53200 lr 2.249e-04 [train_loss] tar_ll 1.2149 loss -1.2149 (5.005 secs)\n",
      "tnpd:tnpd_matern step 53400 lr 2.233e-04 [train_loss] tar_ll 1.3216 loss -1.3216 (4.950 secs)\n",
      "tnpd:tnpd_matern step 53600 lr 2.218e-04 [train_loss] tar_ll 1.3202 loss -1.3202 (4.932 secs)\n",
      "tnpd:tnpd_matern step 53800 lr 2.202e-04 [train_loss] tar_ll 1.3026 loss -1.3026 (5.029 secs)\n",
      "tnpd:tnpd_matern step 54000 lr 2.187e-04 [train_loss] tar_ll 1.1620 loss -1.1620 (4.963 secs)\n",
      "tnpd:tnpd_matern step 54200 lr 2.171e-04 [train_loss] tar_ll 1.3231 loss -1.3231 (5.126 secs)\n",
      "tnpd:tnpd_matern step 54400 lr 2.156e-04 [train_loss] tar_ll 1.2478 loss -1.2478 (5.204 secs)\n",
      "tnpd:tnpd_matern step 54600 lr 2.140e-04 [train_loss] tar_ll 1.2412 loss -1.2412 (5.059 secs)\n",
      "tnpd:tnpd_matern step 54800 lr 2.124e-04 [train_loss] tar_ll 1.3785 loss -1.3785 (4.986 secs)\n",
      "tnpd:tnpd_matern step 55000 lr 2.109e-04 [train_loss] tar_ll 1.2412 loss -1.2412 (5.101 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 179.14it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.9064 loss -0.9064 (16.753 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 55200 lr 2.093e-04 [train_loss] tar_ll 1.2668 loss -1.2668 (5.343 secs)\n",
      "tnpd:tnpd_matern step 55400 lr 2.078e-04 [train_loss] tar_ll 1.3245 loss -1.3245 (5.229 secs)\n",
      "tnpd:tnpd_matern step 55600 lr 2.062e-04 [train_loss] tar_ll 1.3135 loss -1.3135 (5.296 secs)\n",
      "tnpd:tnpd_matern step 55800 lr 2.047e-04 [train_loss] tar_ll 1.2779 loss -1.2779 (5.144 secs)\n",
      "tnpd:tnpd_matern step 56000 lr 2.032e-04 [train_loss] tar_ll 1.3477 loss -1.3477 (5.308 secs)\n",
      "tnpd:tnpd_matern step 56200 lr 2.016e-04 [train_loss] tar_ll 1.2734 loss -1.2734 (5.308 secs)\n",
      "tnpd:tnpd_matern step 56400 lr 2.001e-04 [train_loss] tar_ll 1.3018 loss -1.3018 (5.132 secs)\n",
      "tnpd:tnpd_matern step 56600 lr 1.985e-04 [train_loss] tar_ll 1.1857 loss -1.1857 (5.162 secs)\n",
      "tnpd:tnpd_matern step 56800 lr 1.970e-04 [train_loss] tar_ll 1.2906 loss -1.2906 (5.266 secs)\n",
      "tnpd:tnpd_matern step 57000 lr 1.955e-04 [train_loss] tar_ll 1.2268 loss -1.2268 (5.163 secs)\n",
      "tnpd:tnpd_matern step 57200 lr 1.939e-04 [train_loss] tar_ll 1.4222 loss -1.4222 (5.051 secs)\n",
      "tnpd:tnpd_matern step 57400 lr 1.924e-04 [train_loss] tar_ll 1.3046 loss -1.3046 (5.132 secs)\n",
      "tnpd:tnpd_matern step 57600 lr 1.909e-04 [train_loss] tar_ll 1.2292 loss -1.2292 (5.083 secs)\n",
      "tnpd:tnpd_matern step 57800 lr 1.894e-04 [train_loss] tar_ll 1.2865 loss -1.2865 (5.436 secs)\n",
      "tnpd:tnpd_matern step 58000 lr 1.878e-04 [train_loss] tar_ll 1.2936 loss -1.2936 (5.091 secs)\n",
      "tnpd:tnpd_matern step 58200 lr 1.863e-04 [train_loss] tar_ll 1.2783 loss -1.2783 (5.138 secs)\n",
      "tnpd:tnpd_matern step 58400 lr 1.848e-04 [train_loss] tar_ll 1.2469 loss -1.2469 (4.848 secs)\n",
      "tnpd:tnpd_matern step 58600 lr 1.833e-04 [train_loss] tar_ll 1.1522 loss -1.1522 (5.077 secs)\n",
      "tnpd:tnpd_matern step 58800 lr 1.818e-04 [train_loss] tar_ll 1.3011 loss -1.3011 (5.163 secs)\n",
      "tnpd:tnpd_matern step 59000 lr 1.803e-04 [train_loss] tar_ll 1.3209 loss -1.3209 (5.120 secs)\n",
      "tnpd:tnpd_matern step 59200 lr 1.787e-04 [train_loss] tar_ll 1.2784 loss -1.2784 (5.309 secs)\n",
      "tnpd:tnpd_matern step 59400 lr 1.772e-04 [train_loss] tar_ll 1.2465 loss -1.2465 (5.295 secs)\n",
      "tnpd:tnpd_matern step 59600 lr 1.757e-04 [train_loss] tar_ll 1.2981 loss -1.2981 (5.241 secs)\n",
      "tnpd:tnpd_matern step 59800 lr 1.742e-04 [train_loss] tar_ll 1.2455 loss -1.2455 (5.211 secs)\n",
      "tnpd:tnpd_matern step 60000 lr 1.727e-04 [train_loss] tar_ll 1.2592 loss -1.2592 (5.350 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 180.28it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.9530 loss -0.9530 (16.647 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 60200 lr 1.713e-04 [train_loss] tar_ll 1.3096 loss -1.3096 (5.017 secs)\n",
      "tnpd:tnpd_matern step 60400 lr 1.698e-04 [train_loss] tar_ll 1.3078 loss -1.3078 (5.139 secs)\n",
      "tnpd:tnpd_matern step 60600 lr 1.683e-04 [train_loss] tar_ll 1.2769 loss -1.2769 (5.174 secs)\n",
      "tnpd:tnpd_matern step 60800 lr 1.668e-04 [train_loss] tar_ll 1.2798 loss -1.2798 (5.132 secs)\n",
      "tnpd:tnpd_matern step 61000 lr 1.653e-04 [train_loss] tar_ll 1.2677 loss -1.2677 (5.005 secs)\n",
      "tnpd:tnpd_matern step 61200 lr 1.638e-04 [train_loss] tar_ll 1.3366 loss -1.3366 (5.005 secs)\n",
      "tnpd:tnpd_matern step 61400 lr 1.624e-04 [train_loss] tar_ll 1.3649 loss -1.3649 (5.066 secs)\n",
      "tnpd:tnpd_matern step 61600 lr 1.609e-04 [train_loss] tar_ll 1.2968 loss -1.2968 (4.993 secs)\n",
      "tnpd:tnpd_matern step 61800 lr 1.594e-04 [train_loss] tar_ll 1.3279 loss -1.3279 (5.016 secs)\n",
      "tnpd:tnpd_matern step 62000 lr 1.580e-04 [train_loss] tar_ll 1.2923 loss -1.2923 (5.120 secs)\n",
      "tnpd:tnpd_matern step 62200 lr 1.565e-04 [train_loss] tar_ll 1.3524 loss -1.3524 (5.145 secs)\n",
      "tnpd:tnpd_matern step 62400 lr 1.551e-04 [train_loss] tar_ll 1.2071 loss -1.2071 (5.169 secs)\n",
      "tnpd:tnpd_matern step 62600 lr 1.536e-04 [train_loss] tar_ll 1.3021 loss -1.3021 (5.247 secs)\n",
      "tnpd:tnpd_matern step 62800 lr 1.522e-04 [train_loss] tar_ll 1.2877 loss -1.2877 (5.120 secs)\n",
      "tnpd:tnpd_matern step 63000 lr 1.507e-04 [train_loss] tar_ll 1.3055 loss -1.3055 (5.283 secs)\n",
      "tnpd:tnpd_matern step 63200 lr 1.493e-04 [train_loss] tar_ll 1.3960 loss -1.3960 (5.308 secs)\n",
      "tnpd:tnpd_matern step 63400 lr 1.478e-04 [train_loss] tar_ll 1.3082 loss -1.3082 (5.163 secs)\n",
      "tnpd:tnpd_matern step 63600 lr 1.464e-04 [train_loss] tar_ll 1.2256 loss -1.2256 (5.459 secs)\n",
      "tnpd:tnpd_matern step 63800 lr 1.450e-04 [train_loss] tar_ll 1.3043 loss -1.3043 (5.108 secs)\n",
      "tnpd:tnpd_matern step 64000 lr 1.436e-04 [train_loss] tar_ll 1.3635 loss -1.3635 (5.175 secs)\n",
      "tnpd:tnpd_matern step 64200 lr 1.421e-04 [train_loss] tar_ll 1.2920 loss -1.2920 (5.399 secs)\n",
      "tnpd:tnpd_matern step 64400 lr 1.407e-04 [train_loss] tar_ll 1.3302 loss -1.3302 (5.188 secs)\n",
      "tnpd:tnpd_matern step 64600 lr 1.393e-04 [train_loss] tar_ll 1.2390 loss -1.2390 (5.137 secs)\n",
      "tnpd:tnpd_matern step 64800 lr 1.379e-04 [train_loss] tar_ll 1.3864 loss -1.3864 (5.374 secs)\n",
      "tnpd:tnpd_matern step 65000 lr 1.365e-04 [train_loss] tar_ll 1.2880 loss -1.2880 (5.187 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 181.96it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.9248 loss -0.9248 (16.487 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 65200 lr 1.351e-04 [train_loss] tar_ll 1.2992 loss -1.2992 (5.447 secs)\n",
      "tnpd:tnpd_matern step 65400 lr 1.337e-04 [train_loss] tar_ll 1.3345 loss -1.3345 (5.465 secs)\n",
      "tnpd:tnpd_matern step 65600 lr 1.323e-04 [train_loss] tar_ll 1.3228 loss -1.3228 (5.429 secs)\n",
      "tnpd:tnpd_matern step 65800 lr 1.309e-04 [train_loss] tar_ll 1.3674 loss -1.3674 (5.241 secs)\n",
      "tnpd:tnpd_matern step 66000 lr 1.296e-04 [train_loss] tar_ll 1.2482 loss -1.2482 (5.023 secs)\n",
      "tnpd:tnpd_matern step 66200 lr 1.282e-04 [train_loss] tar_ll 1.3029 loss -1.3029 (5.090 secs)\n",
      "tnpd:tnpd_matern step 66400 lr 1.268e-04 [train_loss] tar_ll 1.3250 loss -1.3250 (5.035 secs)\n",
      "tnpd:tnpd_matern step 66600 lr 1.255e-04 [train_loss] tar_ll 1.2577 loss -1.2577 (5.029 secs)\n",
      "tnpd:tnpd_matern step 66800 lr 1.241e-04 [train_loss] tar_ll 1.2678 loss -1.2678 (5.030 secs)\n",
      "tnpd:tnpd_matern step 67000 lr 1.227e-04 [train_loss] tar_ll 1.2979 loss -1.2979 (4.878 secs)\n",
      "tnpd:tnpd_matern step 67200 lr 1.214e-04 [train_loss] tar_ll 1.3510 loss -1.3510 (5.036 secs)\n",
      "tnpd:tnpd_matern step 67400 lr 1.200e-04 [train_loss] tar_ll 1.4602 loss -1.4602 (5.047 secs)\n",
      "tnpd:tnpd_matern step 67600 lr 1.187e-04 [train_loss] tar_ll 1.3187 loss -1.3187 (5.108 secs)\n",
      "tnpd:tnpd_matern step 67800 lr 1.174e-04 [train_loss] tar_ll 1.3633 loss -1.3633 (5.059 secs)\n",
      "tnpd:tnpd_matern step 68000 lr 1.160e-04 [train_loss] tar_ll 1.3699 loss -1.3699 (4.981 secs)\n",
      "tnpd:tnpd_matern step 68200 lr 1.147e-04 [train_loss] tar_ll 1.3010 loss -1.3010 (4.972 secs)\n",
      "tnpd:tnpd_matern step 68400 lr 1.134e-04 [train_loss] tar_ll 1.2680 loss -1.2680 (5.005 secs)\n",
      "tnpd:tnpd_matern step 68600 lr 1.121e-04 [train_loss] tar_ll 1.2951 loss -1.2951 (5.169 secs)\n",
      "tnpd:tnpd_matern step 68800 lr 1.108e-04 [train_loss] tar_ll 1.2788 loss -1.2788 (5.078 secs)\n",
      "tnpd:tnpd_matern step 69000 lr 1.095e-04 [train_loss] tar_ll 1.3376 loss -1.3376 (4.922 secs)\n",
      "tnpd:tnpd_matern step 69200 lr 1.082e-04 [train_loss] tar_ll 1.3662 loss -1.3662 (5.180 secs)\n",
      "tnpd:tnpd_matern step 69400 lr 1.069e-04 [train_loss] tar_ll 1.3822 loss -1.3822 (5.162 secs)\n",
      "tnpd:tnpd_matern step 69600 lr 1.056e-04 [train_loss] tar_ll 1.3421 loss -1.3421 (5.150 secs)\n",
      "tnpd:tnpd_matern step 69800 lr 1.043e-04 [train_loss] tar_ll 1.3130 loss -1.3130 (5.289 secs)\n",
      "tnpd:tnpd_matern step 70000 lr 1.031e-04 [train_loss] tar_ll 1.2252 loss -1.2252 (5.241 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 181.90it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.9506 loss -0.9506 (16.498 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 70200 lr 1.018e-04 [train_loss] tar_ll 1.3328 loss -1.3328 (5.296 secs)\n",
      "tnpd:tnpd_matern step 70400 lr 1.005e-04 [train_loss] tar_ll 1.3009 loss -1.3009 (5.362 secs)\n",
      "tnpd:tnpd_matern step 70600 lr 9.927e-05 [train_loss] tar_ll 1.3328 loss -1.3328 (5.333 secs)\n",
      "tnpd:tnpd_matern step 70800 lr 9.802e-05 [train_loss] tar_ll 1.3556 loss -1.3556 (4.931 secs)\n",
      "tnpd:tnpd_matern step 71000 lr 9.677e-05 [train_loss] tar_ll 1.4324 loss -1.4324 (5.205 secs)\n",
      "tnpd:tnpd_matern step 71200 lr 9.554e-05 [train_loss] tar_ll 1.4465 loss -1.4465 (5.169 secs)\n",
      "tnpd:tnpd_matern step 71400 lr 9.430e-05 [train_loss] tar_ll 1.2950 loss -1.2950 (5.078 secs)\n",
      "tnpd:tnpd_matern step 71600 lr 9.308e-05 [train_loss] tar_ll 1.3535 loss -1.3535 (5.271 secs)\n",
      "tnpd:tnpd_matern step 71800 lr 9.186e-05 [train_loss] tar_ll 1.3346 loss -1.3346 (5.162 secs)\n",
      "tnpd:tnpd_matern step 72000 lr 9.064e-05 [train_loss] tar_ll 1.3170 loss -1.3170 (5.059 secs)\n",
      "tnpd:tnpd_matern step 72200 lr 8.944e-05 [train_loss] tar_ll 1.3016 loss -1.3016 (5.314 secs)\n",
      "tnpd:tnpd_matern step 72400 lr 8.824e-05 [train_loss] tar_ll 1.3310 loss -1.3310 (5.162 secs)\n",
      "tnpd:tnpd_matern step 72600 lr 8.704e-05 [train_loss] tar_ll 1.3542 loss -1.3542 (5.060 secs)\n",
      "tnpd:tnpd_matern step 72800 lr 8.585e-05 [train_loss] tar_ll 1.3651 loss -1.3651 (5.326 secs)\n",
      "tnpd:tnpd_matern step 73000 lr 8.467e-05 [train_loss] tar_ll 1.3843 loss -1.3843 (5.247 secs)\n",
      "tnpd:tnpd_matern step 73200 lr 8.350e-05 [train_loss] tar_ll 1.3630 loss -1.3630 (5.090 secs)\n",
      "tnpd:tnpd_matern step 73400 lr 8.233e-05 [train_loss] tar_ll 1.3523 loss -1.3523 (5.090 secs)\n",
      "tnpd:tnpd_matern step 73600 lr 8.117e-05 [train_loss] tar_ll 1.3884 loss -1.3884 (5.308 secs)\n",
      "tnpd:tnpd_matern step 73800 lr 8.001e-05 [train_loss] tar_ll 1.3377 loss -1.3377 (5.338 secs)\n",
      "tnpd:tnpd_matern step 74000 lr 7.886e-05 [train_loss] tar_ll 1.2959 loss -1.2959 (5.198 secs)\n",
      "tnpd:tnpd_matern step 74200 lr 7.772e-05 [train_loss] tar_ll 1.3439 loss -1.3439 (5.369 secs)\n",
      "tnpd:tnpd_matern step 74400 lr 7.659e-05 [train_loss] tar_ll 1.2593 loss -1.2593 (5.000 secs)\n",
      "tnpd:tnpd_matern step 74600 lr 7.546e-05 [train_loss] tar_ll 1.2965 loss -1.2965 (4.960 secs)\n",
      "tnpd:tnpd_matern step 74800 lr 7.434e-05 [train_loss] tar_ll 1.4047 loss -1.4047 (4.969 secs)\n",
      "tnpd:tnpd_matern step 75000 lr 7.322e-05 [train_loss] tar_ll 1.4328 loss -1.4328 (5.052 secs)\n",
      "100%|##########| 3000/3000 [00:15<00:00, 197.34it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.9317 loss -0.9317 (15.207 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 75200 lr 7.212e-05 [train_loss] tar_ll 1.3653 loss -1.3653 (5.108 secs)\n",
      "tnpd:tnpd_matern step 75400 lr 7.102e-05 [train_loss] tar_ll 1.3476 loss -1.3476 (5.028 secs)\n",
      "tnpd:tnpd_matern step 75600 lr 6.992e-05 [train_loss] tar_ll 1.3986 loss -1.3986 (5.387 secs)\n",
      "tnpd:tnpd_matern step 75800 lr 6.884e-05 [train_loss] tar_ll 1.3814 loss -1.3814 (5.253 secs)\n",
      "tnpd:tnpd_matern step 76000 lr 6.776e-05 [train_loss] tar_ll 1.3474 loss -1.3474 (5.126 secs)\n",
      "tnpd:tnpd_matern step 76200 lr 6.669e-05 [train_loss] tar_ll 1.3421 loss -1.3421 (5.060 secs)\n",
      "tnpd:tnpd_matern step 76400 lr 6.562e-05 [train_loss] tar_ll 1.3986 loss -1.3986 (5.096 secs)\n",
      "tnpd:tnpd_matern step 76600 lr 6.456e-05 [train_loss] tar_ll 1.3645 loss -1.3645 (5.113 secs)\n",
      "tnpd:tnpd_matern step 76800 lr 6.351e-05 [train_loss] tar_ll 1.3628 loss -1.3628 (5.084 secs)\n",
      "tnpd:tnpd_matern step 77000 lr 6.247e-05 [train_loss] tar_ll 1.3813 loss -1.3813 (5.084 secs)\n",
      "tnpd:tnpd_matern step 77200 lr 6.144e-05 [train_loss] tar_ll 1.3667 loss -1.3667 (5.211 secs)\n",
      "tnpd:tnpd_matern step 77400 lr 6.041e-05 [train_loss] tar_ll 1.3226 loss -1.3226 (4.987 secs)\n",
      "tnpd:tnpd_matern step 77600 lr 5.939e-05 [train_loss] tar_ll 1.1973 loss -1.1973 (5.102 secs)\n",
      "tnpd:tnpd_matern step 77800 lr 5.838e-05 [train_loss] tar_ll 1.4027 loss -1.4027 (5.132 secs)\n",
      "tnpd:tnpd_matern step 78000 lr 5.737e-05 [train_loss] tar_ll 1.3597 loss -1.3597 (5.138 secs)\n",
      "tnpd:tnpd_matern step 78200 lr 5.637e-05 [train_loss] tar_ll 1.3384 loss -1.3384 (5.181 secs)\n",
      "tnpd:tnpd_matern step 78400 lr 5.538e-05 [train_loss] tar_ll 1.3326 loss -1.3326 (5.181 secs)\n",
      "tnpd:tnpd_matern step 78600 lr 5.440e-05 [train_loss] tar_ll 1.3461 loss -1.3461 (5.187 secs)\n",
      "tnpd:tnpd_matern step 78800 lr 5.343e-05 [train_loss] tar_ll 1.3055 loss -1.3055 (5.186 secs)\n",
      "tnpd:tnpd_matern step 79000 lr 5.246e-05 [train_loss] tar_ll 1.2290 loss -1.2290 (5.375 secs)\n",
      "tnpd:tnpd_matern step 79200 lr 5.150e-05 [train_loss] tar_ll 1.3807 loss -1.3807 (5.223 secs)\n",
      "tnpd:tnpd_matern step 79400 lr 5.055e-05 [train_loss] tar_ll 1.3673 loss -1.3673 (5.175 secs)\n",
      "tnpd:tnpd_matern step 79600 lr 4.961e-05 [train_loss] tar_ll 1.3288 loss -1.3288 (5.423 secs)\n",
      "tnpd:tnpd_matern step 79800 lr 4.867e-05 [train_loss] tar_ll 1.3385 loss -1.3385 (5.277 secs)\n",
      "tnpd:tnpd_matern step 80000 lr 4.775e-05 [train_loss] tar_ll 1.3050 loss -1.3050 (5.224 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 184.62it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.9531 loss -0.9531 (16.251 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 80200 lr 4.683e-05 [train_loss] tar_ll 1.2703 loss -1.2703 (5.519 secs)\n",
      "tnpd:tnpd_matern step 80400 lr 4.592e-05 [train_loss] tar_ll 1.3386 loss -1.3386 (5.272 secs)\n",
      "tnpd:tnpd_matern step 80600 lr 4.501e-05 [train_loss] tar_ll 1.2959 loss -1.2959 (5.174 secs)\n",
      "tnpd:tnpd_matern step 80800 lr 4.412e-05 [train_loss] tar_ll 1.3171 loss -1.3171 (5.393 secs)\n",
      "tnpd:tnpd_matern step 81000 lr 4.323e-05 [train_loss] tar_ll 1.3680 loss -1.3680 (5.277 secs)\n",
      "tnpd:tnpd_matern step 81200 lr 4.235e-05 [train_loss] tar_ll 1.3501 loss -1.3501 (5.284 secs)\n",
      "tnpd:tnpd_matern step 81400 lr 4.148e-05 [train_loss] tar_ll 1.3078 loss -1.3078 (5.332 secs)\n",
      "tnpd:tnpd_matern step 81600 lr 4.062e-05 [train_loss] tar_ll 1.3983 loss -1.3983 (5.302 secs)\n",
      "tnpd:tnpd_matern step 81800 lr 3.976e-05 [train_loss] tar_ll 1.3257 loss -1.3257 (5.241 secs)\n",
      "tnpd:tnpd_matern step 82000 lr 3.892e-05 [train_loss] tar_ll 1.4142 loss -1.4142 (5.423 secs)\n",
      "tnpd:tnpd_matern step 82200 lr 3.808e-05 [train_loss] tar_ll 1.3479 loss -1.3479 (5.326 secs)\n",
      "tnpd:tnpd_matern step 82400 lr 3.725e-05 [train_loss] tar_ll 1.2594 loss -1.2594 (5.338 secs)\n",
      "tnpd:tnpd_matern step 82600 lr 3.643e-05 [train_loss] tar_ll 1.4494 loss -1.4494 (5.332 secs)\n",
      "tnpd:tnpd_matern step 82800 lr 3.562e-05 [train_loss] tar_ll 1.3964 loss -1.3964 (5.284 secs)\n",
      "tnpd:tnpd_matern step 83000 lr 3.481e-05 [train_loss] tar_ll 1.3733 loss -1.3733 (5.035 secs)\n",
      "tnpd:tnpd_matern step 83200 lr 3.402e-05 [train_loss] tar_ll 1.3889 loss -1.3889 (5.374 secs)\n",
      "tnpd:tnpd_matern step 83400 lr 3.323e-05 [train_loss] tar_ll 1.3649 loss -1.3649 (5.253 secs)\n",
      "tnpd:tnpd_matern step 83600 lr 3.245e-05 [train_loss] tar_ll 1.3146 loss -1.3146 (5.162 secs)\n",
      "tnpd:tnpd_matern step 83800 lr 3.168e-05 [train_loss] tar_ll 1.3908 loss -1.3908 (5.265 secs)\n",
      "tnpd:tnpd_matern step 84000 lr 3.092e-05 [train_loss] tar_ll 1.3587 loss -1.3587 (5.242 secs)\n",
      "tnpd:tnpd_matern step 84200 lr 3.017e-05 [train_loss] tar_ll 1.3804 loss -1.3804 (5.180 secs)\n",
      "tnpd:tnpd_matern step 84400 lr 2.943e-05 [train_loss] tar_ll 1.3673 loss -1.3673 (5.187 secs)\n",
      "tnpd:tnpd_matern step 84600 lr 2.869e-05 [train_loss] tar_ll 1.3471 loss -1.3471 (5.241 secs)\n",
      "tnpd:tnpd_matern step 84800 lr 2.797e-05 [train_loss] tar_ll 1.4075 loss -1.4075 (4.981 secs)\n",
      "tnpd:tnpd_matern step 85000 lr 2.725e-05 [train_loss] tar_ll 1.2838 loss -1.2838 (5.387 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 185.31it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.9604 loss -0.9604 (16.196 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 85200 lr 2.654e-05 [train_loss] tar_ll 1.3959 loss -1.3959 (5.053 secs)\n",
      "tnpd:tnpd_matern step 85400 lr 2.584e-05 [train_loss] tar_ll 1.4381 loss -1.4381 (5.223 secs)\n",
      "tnpd:tnpd_matern step 85600 lr 2.515e-05 [train_loss] tar_ll 1.3628 loss -1.3628 (5.272 secs)\n",
      "tnpd:tnpd_matern step 85800 lr 2.447e-05 [train_loss] tar_ll 1.4525 loss -1.4525 (5.211 secs)\n",
      "tnpd:tnpd_matern step 86000 lr 2.379e-05 [train_loss] tar_ll 1.4355 loss -1.4355 (5.071 secs)\n",
      "tnpd:tnpd_matern step 86200 lr 2.313e-05 [train_loss] tar_ll 1.4848 loss -1.4848 (5.459 secs)\n",
      "tnpd:tnpd_matern step 86400 lr 2.247e-05 [train_loss] tar_ll 1.4843 loss -1.4843 (5.308 secs)\n",
      "tnpd:tnpd_matern step 86600 lr 2.183e-05 [train_loss] tar_ll 1.4695 loss -1.4695 (5.435 secs)\n",
      "tnpd:tnpd_matern step 86800 lr 2.119e-05 [train_loss] tar_ll 1.4221 loss -1.4221 (5.351 secs)\n",
      "tnpd:tnpd_matern step 87000 lr 2.056e-05 [train_loss] tar_ll 1.4144 loss -1.4144 (5.283 secs)\n",
      "tnpd:tnpd_matern step 87200 lr 1.994e-05 [train_loss] tar_ll 1.3139 loss -1.3139 (5.393 secs)\n",
      "tnpd:tnpd_matern step 87400 lr 1.933e-05 [train_loss] tar_ll 1.4159 loss -1.4159 (5.356 secs)\n",
      "tnpd:tnpd_matern step 87600 lr 1.873e-05 [train_loss] tar_ll 1.4078 loss -1.4078 (5.247 secs)\n",
      "tnpd:tnpd_matern step 87800 lr 1.814e-05 [train_loss] tar_ll 1.3891 loss -1.3891 (5.217 secs)\n",
      "tnpd:tnpd_matern step 88000 lr 1.756e-05 [train_loss] tar_ll 1.3435 loss -1.3435 (5.399 secs)\n",
      "tnpd:tnpd_matern step 88200 lr 1.698e-05 [train_loss] tar_ll 1.4045 loss -1.4045 (5.368 secs)\n",
      "tnpd:tnpd_matern step 88400 lr 1.642e-05 [train_loss] tar_ll 1.3860 loss -1.3860 (5.217 secs)\n",
      "tnpd:tnpd_matern step 88600 lr 1.586e-05 [train_loss] tar_ll 1.4357 loss -1.4357 (5.411 secs)\n",
      "tnpd:tnpd_matern step 88800 lr 1.532e-05 [train_loss] tar_ll 1.4228 loss -1.4228 (5.260 secs)\n",
      "tnpd:tnpd_matern step 89000 lr 1.478e-05 [train_loss] tar_ll 1.3946 loss -1.3946 (5.307 secs)\n",
      "tnpd:tnpd_matern step 89200 lr 1.425e-05 [train_loss] tar_ll 1.4367 loss -1.4367 (5.532 secs)\n",
      "tnpd:tnpd_matern step 89400 lr 1.373e-05 [train_loss] tar_ll 1.3070 loss -1.3070 (5.256 secs)\n",
      "tnpd:tnpd_matern step 89600 lr 1.323e-05 [train_loss] tar_ll 1.3423 loss -1.3423 (5.340 secs)\n",
      "tnpd:tnpd_matern step 89800 lr 1.273e-05 [train_loss] tar_ll 1.4104 loss -1.4104 (5.265 secs)\n",
      "tnpd:tnpd_matern step 90000 lr 1.224e-05 [train_loss] tar_ll 1.3587 loss -1.3587 (5.302 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 178.67it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.9562 loss -0.9562 (16.790 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 90200 lr 1.176e-05 [train_loss] tar_ll 1.3500 loss -1.3500 (5.351 secs)\n",
      "tnpd:tnpd_matern step 90400 lr 1.128e-05 [train_loss] tar_ll 1.3598 loss -1.3598 (5.484 secs)\n",
      "tnpd:tnpd_matern step 90600 lr 1.082e-05 [train_loss] tar_ll 1.4210 loss -1.4210 (5.431 secs)\n",
      "tnpd:tnpd_matern step 90800 lr 1.037e-05 [train_loss] tar_ll 1.4181 loss -1.4181 (5.285 secs)\n",
      "tnpd:tnpd_matern step 91000 lr 9.927e-06 [train_loss] tar_ll 1.3878 loss -1.3878 (5.372 secs)\n",
      "tnpd:tnpd_matern step 91200 lr 9.493e-06 [train_loss] tar_ll 1.3332 loss -1.3332 (5.260 secs)\n",
      "tnpd:tnpd_matern step 91400 lr 9.069e-06 [train_loss] tar_ll 1.3783 loss -1.3783 (5.223 secs)\n",
      "tnpd:tnpd_matern step 91600 lr 8.655e-06 [train_loss] tar_ll 1.3501 loss -1.3501 (5.248 secs)\n",
      "tnpd:tnpd_matern step 91800 lr 8.250e-06 [train_loss] tar_ll 1.3773 loss -1.3773 (5.256 secs)\n",
      "tnpd:tnpd_matern step 92000 lr 7.854e-06 [train_loss] tar_ll 1.3997 loss -1.3997 (5.364 secs)\n",
      "tnpd:tnpd_matern step 92200 lr 7.468e-06 [train_loss] tar_ll 1.3049 loss -1.3049 (5.166 secs)\n",
      "tnpd:tnpd_matern step 92400 lr 7.092e-06 [train_loss] tar_ll 1.3870 loss -1.3870 (5.286 secs)\n",
      "tnpd:tnpd_matern step 92600 lr 6.725e-06 [train_loss] tar_ll 1.3369 loss -1.3369 (5.019 secs)\n",
      "tnpd:tnpd_matern step 92800 lr 6.368e-06 [train_loss] tar_ll 1.3162 loss -1.3162 (5.322 secs)\n",
      "tnpd:tnpd_matern step 93000 lr 6.021e-06 [train_loss] tar_ll 1.4030 loss -1.4030 (5.369 secs)\n",
      "tnpd:tnpd_matern step 93200 lr 5.683e-06 [train_loss] tar_ll 1.3472 loss -1.3472 (5.731 secs)\n",
      "tnpd:tnpd_matern step 93400 lr 5.355e-06 [train_loss] tar_ll 1.3807 loss -1.3807 (5.348 secs)\n",
      "tnpd:tnpd_matern step 93600 lr 5.036e-06 [train_loss] tar_ll 1.3660 loss -1.3660 (6.153 secs)\n",
      "tnpd:tnpd_matern step 93800 lr 4.727e-06 [train_loss] tar_ll 1.3152 loss -1.3152 (5.338 secs)\n",
      "tnpd:tnpd_matern step 94000 lr 4.428e-06 [train_loss] tar_ll 1.3207 loss -1.3207 (5.489 secs)\n",
      "tnpd:tnpd_matern step 94200 lr 4.139e-06 [train_loss] tar_ll 1.3745 loss -1.3745 (5.154 secs)\n",
      "tnpd:tnpd_matern step 94400 lr 3.859e-06 [train_loss] tar_ll 1.3965 loss -1.3965 (5.413 secs)\n",
      "tnpd:tnpd_matern step 94600 lr 3.589e-06 [train_loss] tar_ll 1.3454 loss -1.3454 (5.206 secs)\n",
      "tnpd:tnpd_matern step 94800 lr 3.329e-06 [train_loss] tar_ll 1.4003 loss -1.4003 (5.677 secs)\n",
      "tnpd:tnpd_matern step 95000 lr 3.078e-06 [train_loss] tar_ll 1.3932 loss -1.3932 (5.641 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 176.97it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.9643 loss -0.9643 (16.955 secs)\n",
      "\n",
      "tnpd:tnpd_matern step 95200 lr 2.837e-06 [train_loss] tar_ll 1.2389 loss -1.2389 (5.899 secs)\n",
      "tnpd:tnpd_matern step 95400 lr 2.606e-06 [train_loss] tar_ll 1.4164 loss -1.4164 (5.398 secs)\n",
      "tnpd:tnpd_matern step 95600 lr 2.385e-06 [train_loss] tar_ll 1.4247 loss -1.4247 (5.553 secs)\n",
      "tnpd:tnpd_matern step 95800 lr 2.173e-06 [train_loss] tar_ll 1.3705 loss -1.3705 (5.500 secs)\n",
      "tnpd:tnpd_matern step 96000 lr 1.971e-06 [train_loss] tar_ll 1.3988 loss -1.3988 (5.409 secs)\n",
      "tnpd:tnpd_matern step 96200 lr 1.779e-06 [train_loss] tar_ll 1.3725 loss -1.3725 (5.604 secs)\n",
      "tnpd:tnpd_matern step 96400 lr 1.597e-06 [train_loss] tar_ll 1.3820 loss -1.3820 (5.370 secs)\n",
      "tnpd:tnpd_matern step 96600 lr 1.425e-06 [train_loss] tar_ll 1.4262 loss -1.4262 (5.764 secs)\n",
      "tnpd:tnpd_matern step 96800 lr 1.262e-06 [train_loss] tar_ll 1.3717 loss -1.3717 (5.148 secs)\n",
      "tnpd:tnpd_matern step 97000 lr 1.110e-06 [train_loss] tar_ll 1.4193 loss -1.4193 (5.686 secs)\n",
      "tnpd:tnpd_matern step 97200 lr 9.666e-07 [train_loss] tar_ll 1.3516 loss -1.3516 (5.439 secs)\n",
      "tnpd:tnpd_matern step 97400 lr 8.335e-07 [train_loss] tar_ll 1.4057 loss -1.4057 (5.727 secs)\n",
      "tnpd:tnpd_matern step 97600 lr 7.103e-07 [train_loss] tar_ll 1.3780 loss -1.3780 (5.542 secs)\n",
      "tnpd:tnpd_matern step 97800 lr 5.969e-07 [train_loss] tar_ll 1.4014 loss -1.4014 (5.731 secs)\n",
      "tnpd:tnpd_matern step 98000 lr 4.933e-07 [train_loss] tar_ll 1.3937 loss -1.3937 (5.462 secs)\n",
      "tnpd:tnpd_matern step 98200 lr 3.996e-07 [train_loss] tar_ll 1.3533 loss -1.3533 (5.443 secs)\n",
      "tnpd:tnpd_matern step 98400 lr 3.158e-07 [train_loss] tar_ll 1.4133 loss -1.4133 (5.706 secs)\n",
      "tnpd:tnpd_matern step 98600 lr 2.418e-07 [train_loss] tar_ll 1.3467 loss -1.3467 (5.171 secs)\n",
      "tnpd:tnpd_matern step 98800 lr 1.776e-07 [train_loss] tar_ll 1.3835 loss -1.3835 (5.602 secs)\n",
      "tnpd:tnpd_matern step 99000 lr 1.234e-07 [train_loss] tar_ll 1.4206 loss -1.4206 (5.125 secs)\n",
      "tnpd:tnpd_matern step 99200 lr 7.895e-08 [train_loss] tar_ll 1.3343 loss -1.3343 (5.792 secs)\n",
      "tnpd:tnpd_matern step 99400 lr 4.441e-08 [train_loss] tar_ll 1.3105 loss -1.3105 (4.991 secs)\n",
      "tnpd:tnpd_matern step 99600 lr 1.974e-08 [train_loss] tar_ll 1.3403 loss -1.3403 (5.753 secs)\n",
      "tnpd:tnpd_matern step 99800 lr 4.935e-09 [train_loss] tar_ll 1.4428 loss -1.4428 (5.196 secs)\n",
      "tnpd:tnpd_matern step 100000 lr 0.000e+00 [train_loss] tar_ll 1.4159 loss -1.4159 (5.635 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 174.88it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.9595 loss -0.9595 (17.161 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:16<00:00, 177.07it/s]\n",
      "tnpd:tnpd_matern matern tar_ll 0.9595 loss -0.9595 (16.946 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3035494.25 miliseconds\n",
      "Execution time: 3035.49425 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 39.80517578125 MB\n",
      "Memory Usage Change: 23.55517578125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='tnpd', name='tnpd_matern',val_seed=100, val_l=None,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d8b5b-80eb-493d-a7d2-5b5d3534a358",
   "metadata": {},
   "source": [
    "## EQTNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec8daa-d1d0-4c04-bf76-ee763bf22170",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mod(modelo='eqtnp', name='eqtnp_matern',val_seed=100, val_l=None,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852561b-ef76-4503-b71f-3c02e41f8e7c",
   "metadata": {},
   "source": [
    "## LBANP (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985f1bce-7126-478a-ac28-fcd8d246becd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: lbanp-lbanp-num_latents-8_matern\n",
      "Total number of parameters: 784834\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "matern-seed100.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 200 lr 5.000e-04 [train_loss] tar_ll -0.6944 loss 0.6944 (6.468 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 400 lr 5.000e-04 [train_loss] tar_ll -0.6916 loss 0.6916 (6.293 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 600 lr 5.000e-04 [train_loss] tar_ll -0.5983 loss 0.5983 (6.284 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 800 lr 4.999e-04 [train_loss] tar_ll -0.5857 loss 0.5857 (6.193 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 1000 lr 4.999e-04 [train_loss] tar_ll -0.5898 loss 0.5898 (6.519 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 1200 lr 4.998e-04 [train_loss] tar_ll -0.5698 loss 0.5698 (6.353 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 1400 lr 4.998e-04 [train_loss] tar_ll -0.4593 loss 0.4593 (6.415 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 1600 lr 4.997e-04 [train_loss] tar_ll -0.3471 loss 0.3471 (6.398 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 1800 lr 4.996e-04 [train_loss] tar_ll -0.2678 loss 0.2678 (6.134 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 2000 lr 4.995e-04 [train_loss] tar_ll -0.2215 loss 0.2215 (6.106 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 2200 lr 4.994e-04 [train_loss] tar_ll -0.1172 loss 0.1172 (6.062 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 2400 lr 4.993e-04 [train_loss] tar_ll -0.0522 loss 0.0522 (5.972 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 2600 lr 4.992e-04 [train_loss] tar_ll 0.0028 loss -0.0028 (6.088 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 2800 lr 4.990e-04 [train_loss] tar_ll 0.0430 loss -0.0430 (6.059 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 3000 lr 4.989e-04 [train_loss] tar_ll 0.0971 loss -0.0971 (6.228 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 3200 lr 4.987e-04 [train_loss] tar_ll 0.0552 loss -0.0552 (6.139 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 3400 lr 4.986e-04 [train_loss] tar_ll 0.1028 loss -0.1028 (6.232 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 3600 lr 4.984e-04 [train_loss] tar_ll 0.0744 loss -0.0744 (6.224 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 3800 lr 4.982e-04 [train_loss] tar_ll 0.1043 loss -0.1043 (5.988 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 4000 lr 4.980e-04 [train_loss] tar_ll 0.1484 loss -0.1484 (5.831 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 4200 lr 4.978e-04 [train_loss] tar_ll 0.1337 loss -0.1337 (5.987 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 4400 lr 4.976e-04 [train_loss] tar_ll 0.1172 loss -0.1172 (5.815 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 4600 lr 4.974e-04 [train_loss] tar_ll 0.2092 loss -0.2092 (5.932 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 4800 lr 4.972e-04 [train_loss] tar_ll 0.2480 loss -0.2480 (5.834 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 5000 lr 4.969e-04 [train_loss] tar_ll 0.2655 loss -0.2655 (5.808 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 108.20it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.1340 loss -0.1340 (27.731 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 5200 lr 4.967e-04 [train_loss] tar_ll 0.2753 loss -0.2753 (5.919 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 5400 lr 4.964e-04 [train_loss] tar_ll 0.2460 loss -0.2460 (5.892 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 5600 lr 4.961e-04 [train_loss] tar_ll 0.3181 loss -0.3181 (5.827 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 5800 lr 4.959e-04 [train_loss] tar_ll 0.2921 loss -0.2921 (5.862 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 6000 lr 4.956e-04 [train_loss] tar_ll 0.2597 loss -0.2597 (5.900 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 6200 lr 4.953e-04 [train_loss] tar_ll 0.2924 loss -0.2924 (5.803 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 6400 lr 4.950e-04 [train_loss] tar_ll 0.2786 loss -0.2786 (6.496 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 6600 lr 4.946e-04 [train_loss] tar_ll 0.2368 loss -0.2368 (6.587 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 6800 lr 4.943e-04 [train_loss] tar_ll 0.3223 loss -0.3223 (6.300 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 7000 lr 4.940e-04 [train_loss] tar_ll 0.3300 loss -0.3300 (5.948 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 7200 lr 4.936e-04 [train_loss] tar_ll 0.3529 loss -0.3529 (5.827 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 7400 lr 4.933e-04 [train_loss] tar_ll 0.3033 loss -0.3033 (5.872 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 7600 lr 4.929e-04 [train_loss] tar_ll 0.3991 loss -0.3991 (5.803 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 7800 lr 4.925e-04 [train_loss] tar_ll 0.3826 loss -0.3826 (5.796 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 8000 lr 4.921e-04 [train_loss] tar_ll 0.4116 loss -0.4116 (5.946 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 8200 lr 4.918e-04 [train_loss] tar_ll 0.4301 loss -0.4301 (5.881 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 8400 lr 4.913e-04 [train_loss] tar_ll 0.3785 loss -0.3785 (5.815 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 8600 lr 4.909e-04 [train_loss] tar_ll 0.3094 loss -0.3094 (5.909 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 8800 lr 4.905e-04 [train_loss] tar_ll 0.3876 loss -0.3876 (5.801 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 9000 lr 4.901e-04 [train_loss] tar_ll 0.3797 loss -0.3797 (6.228 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 9200 lr 4.896e-04 [train_loss] tar_ll 0.3698 loss -0.3698 (5.946 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 9400 lr 4.892e-04 [train_loss] tar_ll 0.5011 loss -0.5011 (5.839 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 9600 lr 4.887e-04 [train_loss] tar_ll 0.4973 loss -0.4973 (5.941 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 9800 lr 4.882e-04 [train_loss] tar_ll 0.4725 loss -0.4725 (5.812 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 10000 lr 4.878e-04 [train_loss] tar_ll 0.4227 loss -0.4227 (5.843 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 107.91it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.2428 loss -0.2428 (27.802 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 10200 lr 4.873e-04 [train_loss] tar_ll 0.3637 loss -0.3637 (5.989 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 10400 lr 4.868e-04 [train_loss] tar_ll 0.4785 loss -0.4785 (6.028 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 10600 lr 4.863e-04 [train_loss] tar_ll 0.4617 loss -0.4617 (5.774 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 10800 lr 4.857e-04 [train_loss] tar_ll 0.5171 loss -0.5171 (5.924 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 11000 lr 4.852e-04 [train_loss] tar_ll 0.4553 loss -0.4553 (5.862 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 11200 lr 4.847e-04 [train_loss] tar_ll 0.4918 loss -0.4918 (6.035 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 11400 lr 4.841e-04 [train_loss] tar_ll 0.6014 loss -0.6014 (6.129 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 11600 lr 4.836e-04 [train_loss] tar_ll 0.5273 loss -0.5273 (6.020 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 11800 lr 4.830e-04 [train_loss] tar_ll 0.5053 loss -0.5053 (6.056 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 12000 lr 4.824e-04 [train_loss] tar_ll 0.5180 loss -0.5180 (6.046 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 12200 lr 4.819e-04 [train_loss] tar_ll 0.4841 loss -0.4841 (6.077 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 12400 lr 4.813e-04 [train_loss] tar_ll 0.4903 loss -0.4903 (6.034 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 12600 lr 4.807e-04 [train_loss] tar_ll 0.4899 loss -0.4899 (5.873 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 12800 lr 4.801e-04 [train_loss] tar_ll 0.5281 loss -0.5281 (5.743 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 13000 lr 4.794e-04 [train_loss] tar_ll 0.4672 loss -0.4672 (5.785 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 13200 lr 4.788e-04 [train_loss] tar_ll 0.5385 loss -0.5385 (5.778 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 13400 lr 4.782e-04 [train_loss] tar_ll 0.5283 loss -0.5283 (5.676 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 13600 lr 4.775e-04 [train_loss] tar_ll 0.5768 loss -0.5768 (5.887 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 13800 lr 4.769e-04 [train_loss] tar_ll 0.4835 loss -0.4835 (5.793 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 14000 lr 4.762e-04 [train_loss] tar_ll 0.5559 loss -0.5559 (5.898 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 14200 lr 4.755e-04 [train_loss] tar_ll 0.5662 loss -0.5662 (5.957 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 14400 lr 4.749e-04 [train_loss] tar_ll 0.5496 loss -0.5496 (5.987 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 14600 lr 4.742e-04 [train_loss] tar_ll 0.4743 loss -0.4743 (5.937 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 14800 lr 4.735e-04 [train_loss] tar_ll 0.6192 loss -0.6192 (5.853 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 15000 lr 4.728e-04 [train_loss] tar_ll 0.5372 loss -0.5372 (5.833 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 107.59it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.3628 loss -0.3628 (27.887 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 15200 lr 4.720e-04 [train_loss] tar_ll 0.6336 loss -0.6336 (6.002 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 15400 lr 4.713e-04 [train_loss] tar_ll 0.5513 loss -0.5513 (5.868 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 15600 lr 4.706e-04 [train_loss] tar_ll 0.6203 loss -0.6203 (5.791 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 15800 lr 4.698e-04 [train_loss] tar_ll 0.6107 loss -0.6107 (5.943 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 16000 lr 4.691e-04 [train_loss] tar_ll 0.5006 loss -0.5006 (5.837 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 16200 lr 4.683e-04 [train_loss] tar_ll 0.5380 loss -0.5380 (5.844 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 16400 lr 4.675e-04 [train_loss] tar_ll 0.5275 loss -0.5275 (5.929 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 16600 lr 4.668e-04 [train_loss] tar_ll 0.5189 loss -0.5189 (6.427 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 16800 lr 4.660e-04 [train_loss] tar_ll 0.6563 loss -0.6563 (6.328 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 17000 lr 4.652e-04 [train_loss] tar_ll 0.6483 loss -0.6483 (6.164 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 17200 lr 4.644e-04 [train_loss] tar_ll 0.5499 loss -0.5499 (6.422 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 17400 lr 4.636e-04 [train_loss] tar_ll 0.5879 loss -0.5879 (6.598 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 17600 lr 4.627e-04 [train_loss] tar_ll 0.5750 loss -0.5750 (6.006 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 17800 lr 4.619e-04 [train_loss] tar_ll 0.6278 loss -0.6278 (5.812 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 18000 lr 4.611e-04 [train_loss] tar_ll 0.5410 loss -0.5410 (5.927 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 18200 lr 4.602e-04 [train_loss] tar_ll 0.6189 loss -0.6189 (5.959 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 18400 lr 4.594e-04 [train_loss] tar_ll 0.5576 loss -0.5576 (6.140 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 18600 lr 4.585e-04 [train_loss] tar_ll 0.5420 loss -0.5420 (6.383 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 18800 lr 4.576e-04 [train_loss] tar_ll 0.6099 loss -0.6099 (6.665 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 19000 lr 4.568e-04 [train_loss] tar_ll 0.5986 loss -0.5986 (6.371 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 19200 lr 4.559e-04 [train_loss] tar_ll 0.6209 loss -0.6209 (6.097 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 19400 lr 4.550e-04 [train_loss] tar_ll 0.7267 loss -0.7267 (6.102 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 19600 lr 4.541e-04 [train_loss] tar_ll 0.5842 loss -0.5842 (5.842 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 19800 lr 4.532e-04 [train_loss] tar_ll 0.6354 loss -0.6354 (5.815 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 20000 lr 4.523e-04 [train_loss] tar_ll 0.6147 loss -0.6147 (5.889 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 108.38it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.3977 loss -0.3977 (27.681 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 20200 lr 4.513e-04 [train_loss] tar_ll 0.6395 loss -0.6395 (6.146 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 20400 lr 4.504e-04 [train_loss] tar_ll 0.6278 loss -0.6278 (6.078 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 20600 lr 4.494e-04 [train_loss] tar_ll 0.5947 loss -0.5947 (6.145 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 20800 lr 4.485e-04 [train_loss] tar_ll 0.6701 loss -0.6701 (5.813 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 21000 lr 4.475e-04 [train_loss] tar_ll 0.6206 loss -0.6206 (5.798 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 21200 lr 4.466e-04 [train_loss] tar_ll 0.7037 loss -0.7037 (6.160 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 21400 lr 4.456e-04 [train_loss] tar_ll 0.6661 loss -0.6661 (6.008 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 21600 lr 4.446e-04 [train_loss] tar_ll 0.6840 loss -0.6840 (5.962 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 21800 lr 4.436e-04 [train_loss] tar_ll 0.6182 loss -0.6182 (6.095 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 22000 lr 4.426e-04 [train_loss] tar_ll 0.6401 loss -0.6401 (5.985 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 22200 lr 4.416e-04 [train_loss] tar_ll 0.6690 loss -0.6690 (6.179 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 22400 lr 4.406e-04 [train_loss] tar_ll 0.6830 loss -0.6830 (6.122 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 22600 lr 4.396e-04 [train_loss] tar_ll 0.5899 loss -0.5899 (6.019 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 22800 lr 4.386e-04 [train_loss] tar_ll 0.6563 loss -0.6563 (5.873 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 23000 lr 4.375e-04 [train_loss] tar_ll 0.6427 loss -0.6427 (5.709 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 23200 lr 4.365e-04 [train_loss] tar_ll 0.6646 loss -0.6646 (5.776 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 23400 lr 4.354e-04 [train_loss] tar_ll 0.6836 loss -0.6836 (5.799 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 23600 lr 4.344e-04 [train_loss] tar_ll 0.6139 loss -0.6139 (5.715 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 23800 lr 4.333e-04 [train_loss] tar_ll 0.6400 loss -0.6400 (5.824 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 24000 lr 4.322e-04 [train_loss] tar_ll 0.7145 loss -0.7145 (5.747 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 24200 lr 4.312e-04 [train_loss] tar_ll 0.7086 loss -0.7086 (5.820 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 24400 lr 4.301e-04 [train_loss] tar_ll 0.7425 loss -0.7425 (5.838 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 24600 lr 4.290e-04 [train_loss] tar_ll 0.6627 loss -0.6627 (5.746 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 24800 lr 4.279e-04 [train_loss] tar_ll 0.7201 loss -0.7201 (5.771 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 25000 lr 4.268e-04 [train_loss] tar_ll 0.6289 loss -0.6289 (5.868 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:26<00:00, 111.34it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.4107 loss -0.4107 (26.948 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 25200 lr 4.257e-04 [train_loss] tar_ll 0.6543 loss -0.6543 (5.926 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 25400 lr 4.245e-04 [train_loss] tar_ll 0.7346 loss -0.7346 (5.693 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 25600 lr 4.234e-04 [train_loss] tar_ll 0.7376 loss -0.7376 (6.018 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 25800 lr 4.223e-04 [train_loss] tar_ll 0.6611 loss -0.6611 (5.742 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 26000 lr 4.211e-04 [train_loss] tar_ll 0.7887 loss -0.7887 (5.665 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 26200 lr 4.200e-04 [train_loss] tar_ll 0.6672 loss -0.6672 (5.826 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 26400 lr 4.188e-04 [train_loss] tar_ll 0.7291 loss -0.7291 (5.777 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 26600 lr 4.177e-04 [train_loss] tar_ll 0.6847 loss -0.6847 (5.760 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 26800 lr 4.165e-04 [train_loss] tar_ll 0.6992 loss -0.6992 (5.883 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 27000 lr 4.153e-04 [train_loss] tar_ll 0.6510 loss -0.6510 (5.735 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 27200 lr 4.141e-04 [train_loss] tar_ll 0.7036 loss -0.7036 (5.808 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 27400 lr 4.130e-04 [train_loss] tar_ll 0.6518 loss -0.6518 (5.927 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 27600 lr 4.118e-04 [train_loss] tar_ll 0.6455 loss -0.6455 (5.860 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 27800 lr 4.106e-04 [train_loss] tar_ll 0.7901 loss -0.7901 (5.876 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 28000 lr 4.094e-04 [train_loss] tar_ll 0.7355 loss -0.7355 (5.859 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 28200 lr 4.081e-04 [train_loss] tar_ll 0.6842 loss -0.6842 (5.833 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 28400 lr 4.069e-04 [train_loss] tar_ll 0.7132 loss -0.7132 (5.880 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 28600 lr 4.057e-04 [train_loss] tar_ll 0.7862 loss -0.7862 (5.952 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 28800 lr 4.045e-04 [train_loss] tar_ll 0.8082 loss -0.8082 (6.038 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 29000 lr 4.032e-04 [train_loss] tar_ll 0.6796 loss -0.6796 (6.616 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 29200 lr 4.020e-04 [train_loss] tar_ll 0.7821 loss -0.7821 (6.663 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 29400 lr 4.007e-04 [train_loss] tar_ll 0.8025 loss -0.8025 (6.641 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 29600 lr 3.995e-04 [train_loss] tar_ll 0.7671 loss -0.7671 (5.892 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 29800 lr 3.982e-04 [train_loss] tar_ll 0.6852 loss -0.6852 (5.810 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 30000 lr 3.969e-04 [train_loss] tar_ll 0.8108 loss -0.8108 (5.897 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 101.79it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.4776 loss -0.4776 (29.474 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 30200 lr 3.957e-04 [train_loss] tar_ll 0.7044 loss -0.7044 (5.926 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 30400 lr 3.944e-04 [train_loss] tar_ll 0.6718 loss -0.6718 (5.756 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 30600 lr 3.931e-04 [train_loss] tar_ll 0.7570 loss -0.7570 (5.892 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 30800 lr 3.918e-04 [train_loss] tar_ll 0.8021 loss -0.8021 (5.788 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 31000 lr 3.905e-04 [train_loss] tar_ll 0.7546 loss -0.7546 (5.813 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 31200 lr 3.892e-04 [train_loss] tar_ll 0.7879 loss -0.7879 (6.019 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 31400 lr 3.879e-04 [train_loss] tar_ll 0.7606 loss -0.7606 (5.972 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 31600 lr 3.866e-04 [train_loss] tar_ll 0.7249 loss -0.7249 (5.984 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 31800 lr 3.853e-04 [train_loss] tar_ll 0.7858 loss -0.7858 (5.977 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 32000 lr 3.840e-04 [train_loss] tar_ll 0.7865 loss -0.7865 (5.948 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 32200 lr 3.826e-04 [train_loss] tar_ll 0.7624 loss -0.7624 (6.168 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 32400 lr 3.813e-04 [train_loss] tar_ll 0.7896 loss -0.7896 (6.044 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 32600 lr 3.800e-04 [train_loss] tar_ll 0.8048 loss -0.8048 (6.173 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 32800 lr 3.786e-04 [train_loss] tar_ll 0.8314 loss -0.8314 (6.196 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 33000 lr 3.773e-04 [train_loss] tar_ll 0.7669 loss -0.7669 (5.932 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 33200 lr 3.759e-04 [train_loss] tar_ll 0.8014 loss -0.8014 (6.135 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 33400 lr 3.745e-04 [train_loss] tar_ll 0.8436 loss -0.8436 (6.053 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 33600 lr 3.732e-04 [train_loss] tar_ll 0.7920 loss -0.7920 (6.102 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 33800 lr 3.718e-04 [train_loss] tar_ll 0.8001 loss -0.8001 (6.134 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 34000 lr 3.704e-04 [train_loss] tar_ll 0.8177 loss -0.8177 (6.049 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 34200 lr 3.691e-04 [train_loss] tar_ll 0.7363 loss -0.7363 (6.106 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 34400 lr 3.677e-04 [train_loss] tar_ll 0.8207 loss -0.8207 (5.980 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 34600 lr 3.663e-04 [train_loss] tar_ll 0.8536 loss -0.8536 (5.900 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 34800 lr 3.649e-04 [train_loss] tar_ll 0.7502 loss -0.7502 (5.897 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 35000 lr 3.635e-04 [train_loss] tar_ll 0.8625 loss -0.8625 (5.891 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:26<00:00, 111.75it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.5217 loss -0.5217 (26.847 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 35200 lr 3.621e-04 [train_loss] tar_ll 0.8109 loss -0.8109 (5.729 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 35400 lr 3.607e-04 [train_loss] tar_ll 0.9202 loss -0.9202 (5.602 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 35600 lr 3.593e-04 [train_loss] tar_ll 0.7547 loss -0.7547 (5.811 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 35800 lr 3.579e-04 [train_loss] tar_ll 0.8437 loss -0.8437 (5.832 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 36000 lr 3.564e-04 [train_loss] tar_ll 0.8556 loss -0.8556 (5.624 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 36200 lr 3.550e-04 [train_loss] tar_ll 0.8432 loss -0.8432 (5.841 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 36400 lr 3.536e-04 [train_loss] tar_ll 0.7745 loss -0.7745 (5.710 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 36600 lr 3.522e-04 [train_loss] tar_ll 0.8178 loss -0.8178 (6.683 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 36800 lr 3.507e-04 [train_loss] tar_ll 0.8545 loss -0.8545 (6.478 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 37000 lr 3.493e-04 [train_loss] tar_ll 0.7926 loss -0.7926 (6.631 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 37200 lr 3.478e-04 [train_loss] tar_ll 0.8638 loss -0.8638 (6.561 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 37400 lr 3.464e-04 [train_loss] tar_ll 0.7633 loss -0.7633 (6.217 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 37600 lr 3.449e-04 [train_loss] tar_ll 0.8951 loss -0.8951 (6.043 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 37800 lr 3.435e-04 [train_loss] tar_ll 0.8923 loss -0.8923 (6.005 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 38000 lr 3.420e-04 [train_loss] tar_ll 0.8121 loss -0.8121 (5.949 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 38200 lr 3.406e-04 [train_loss] tar_ll 0.7234 loss -0.7234 (6.011 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 38400 lr 3.391e-04 [train_loss] tar_ll 0.8419 loss -0.8419 (5.836 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 38600 lr 3.376e-04 [train_loss] tar_ll 0.8461 loss -0.8461 (5.916 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 38800 lr 3.362e-04 [train_loss] tar_ll 0.8644 loss -0.8644 (5.903 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 39000 lr 3.347e-04 [train_loss] tar_ll 0.8714 loss -0.8714 (6.251 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 39200 lr 3.332e-04 [train_loss] tar_ll 0.8430 loss -0.8430 (6.684 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 39400 lr 3.317e-04 [train_loss] tar_ll 0.8512 loss -0.8512 (6.789 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 39600 lr 3.302e-04 [train_loss] tar_ll 0.8052 loss -0.8052 (5.909 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 39800 lr 3.287e-04 [train_loss] tar_ll 0.8778 loss -0.8778 (6.019 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 40000 lr 3.273e-04 [train_loss] tar_ll 0.8848 loss -0.8848 (5.799 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 107.24it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.5432 loss -0.5432 (27.977 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 40200 lr 3.258e-04 [train_loss] tar_ll 0.8558 loss -0.8558 (5.863 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 40400 lr 3.243e-04 [train_loss] tar_ll 0.8929 loss -0.8929 (5.862 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 40600 lr 3.228e-04 [train_loss] tar_ll 0.9202 loss -0.9202 (5.769 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 40800 lr 3.213e-04 [train_loss] tar_ll 0.7768 loss -0.7768 (5.822 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 41000 lr 3.197e-04 [train_loss] tar_ll 0.7888 loss -0.7888 (5.998 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 41200 lr 3.182e-04 [train_loss] tar_ll 0.9025 loss -0.9025 (5.888 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 41400 lr 3.167e-04 [train_loss] tar_ll 0.8852 loss -0.8852 (6.012 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 41600 lr 3.152e-04 [train_loss] tar_ll 0.8098 loss -0.8098 (5.967 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 41800 lr 3.137e-04 [train_loss] tar_ll 0.8984 loss -0.8984 (5.926 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 42000 lr 3.122e-04 [train_loss] tar_ll 0.8643 loss -0.8643 (5.950 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 42200 lr 3.106e-04 [train_loss] tar_ll 0.9056 loss -0.9056 (5.924 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 42400 lr 3.091e-04 [train_loss] tar_ll 0.8845 loss -0.8845 (5.900 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 42600 lr 3.076e-04 [train_loss] tar_ll 0.8379 loss -0.8379 (6.062 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 42800 lr 3.061e-04 [train_loss] tar_ll 0.8135 loss -0.8135 (6.009 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 43000 lr 3.045e-04 [train_loss] tar_ll 0.8709 loss -0.8709 (5.906 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 43200 lr 3.030e-04 [train_loss] tar_ll 0.9030 loss -0.9030 (6.019 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 43400 lr 3.015e-04 [train_loss] tar_ll 0.8879 loss -0.8879 (5.998 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 43600 lr 2.999e-04 [train_loss] tar_ll 0.9169 loss -0.9169 (6.091 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 43800 lr 2.984e-04 [train_loss] tar_ll 0.9139 loss -0.9139 (5.976 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 44000 lr 2.968e-04 [train_loss] tar_ll 0.8570 loss -0.8570 (6.104 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 44200 lr 2.953e-04 [train_loss] tar_ll 0.9280 loss -0.9280 (6.153 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 44400 lr 2.938e-04 [train_loss] tar_ll 0.8832 loss -0.8832 (5.990 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 44600 lr 2.922e-04 [train_loss] tar_ll 0.9181 loss -0.9181 (5.870 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 44800 lr 2.907e-04 [train_loss] tar_ll 0.8824 loss -0.8824 (5.818 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 45000 lr 2.891e-04 [train_loss] tar_ll 0.8634 loss -0.8634 (5.873 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 109.48it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.5194 loss -0.5194 (27.405 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 45200 lr 2.876e-04 [train_loss] tar_ll 0.9388 loss -0.9388 (5.804 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 45400 lr 2.860e-04 [train_loss] tar_ll 0.9280 loss -0.9280 (5.874 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 45600 lr 2.844e-04 [train_loss] tar_ll 0.9330 loss -0.9330 (5.774 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 45800 lr 2.829e-04 [train_loss] tar_ll 0.8707 loss -0.8707 (5.840 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 46000 lr 2.813e-04 [train_loss] tar_ll 0.9324 loss -0.9324 (5.940 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 46200 lr 2.798e-04 [train_loss] tar_ll 0.8625 loss -0.8625 (5.917 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 46400 lr 2.782e-04 [train_loss] tar_ll 0.9645 loss -0.9645 (5.908 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 46600 lr 2.767e-04 [train_loss] tar_ll 0.8980 loss -0.8980 (5.767 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 46800 lr 2.751e-04 [train_loss] tar_ll 0.8895 loss -0.8895 (5.739 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 47000 lr 2.735e-04 [train_loss] tar_ll 0.9755 loss -0.9755 (5.883 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 47200 lr 2.720e-04 [train_loss] tar_ll 0.8867 loss -0.8867 (5.797 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 47400 lr 2.704e-04 [train_loss] tar_ll 0.9402 loss -0.9402 (5.747 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 47600 lr 2.688e-04 [train_loss] tar_ll 0.9761 loss -0.9761 (5.967 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 47800 lr 2.673e-04 [train_loss] tar_ll 0.8855 loss -0.8855 (6.014 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 48000 lr 2.657e-04 [train_loss] tar_ll 0.8669 loss -0.8669 (5.989 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 48200 lr 2.641e-04 [train_loss] tar_ll 1.0007 loss -1.0007 (6.079 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 48400 lr 2.626e-04 [train_loss] tar_ll 0.8630 loss -0.8630 (5.974 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 48600 lr 2.610e-04 [train_loss] tar_ll 0.8926 loss -0.8926 (6.046 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 48800 lr 2.594e-04 [train_loss] tar_ll 0.8184 loss -0.8184 (5.950 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 49000 lr 2.579e-04 [train_loss] tar_ll 0.8463 loss -0.8463 (5.976 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 49200 lr 2.563e-04 [train_loss] tar_ll 0.9443 loss -0.9443 (5.966 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 49400 lr 2.547e-04 [train_loss] tar_ll 0.7662 loss -0.7662 (5.921 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 49600 lr 2.531e-04 [train_loss] tar_ll 0.9647 loss -0.9647 (5.929 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 49800 lr 2.516e-04 [train_loss] tar_ll 0.9127 loss -0.9127 (5.966 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 50000 lr 2.500e-04 [train_loss] tar_ll 0.9821 loss -0.9821 (5.897 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 108.21it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.6069 loss -0.6069 (27.724 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 50200 lr 2.484e-04 [train_loss] tar_ll 0.9107 loss -0.9107 (6.206 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 50400 lr 2.469e-04 [train_loss] tar_ll 0.9210 loss -0.9210 (6.216 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 50600 lr 2.453e-04 [train_loss] tar_ll 0.9500 loss -0.9500 (6.056 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 50800 lr 2.437e-04 [train_loss] tar_ll 0.9103 loss -0.9103 (5.933 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 51000 lr 2.421e-04 [train_loss] tar_ll 0.9875 loss -0.9875 (5.872 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 51200 lr 2.406e-04 [train_loss] tar_ll 1.1038 loss -1.1038 (5.872 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 51400 lr 2.390e-04 [train_loss] tar_ll 0.9311 loss -0.9311 (5.977 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 51600 lr 2.374e-04 [train_loss] tar_ll 0.8846 loss -0.8846 (5.839 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 51800 lr 2.359e-04 [train_loss] tar_ll 0.9809 loss -0.9809 (5.844 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 52000 lr 2.343e-04 [train_loss] tar_ll 0.9001 loss -0.9001 (5.963 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 52200 lr 2.327e-04 [train_loss] tar_ll 0.8949 loss -0.8949 (6.021 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 52400 lr 2.312e-04 [train_loss] tar_ll 0.9795 loss -0.9795 (5.880 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 52600 lr 2.296e-04 [train_loss] tar_ll 0.9406 loss -0.9406 (5.989 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 52800 lr 2.280e-04 [train_loss] tar_ll 0.9731 loss -0.9731 (5.888 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 53000 lr 2.265e-04 [train_loss] tar_ll 1.0610 loss -1.0610 (6.082 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 53200 lr 2.249e-04 [train_loss] tar_ll 1.0233 loss -1.0233 (6.091 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 53400 lr 2.233e-04 [train_loss] tar_ll 1.0017 loss -1.0017 (5.871 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 53600 lr 2.218e-04 [train_loss] tar_ll 0.9924 loss -0.9924 (6.066 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 53800 lr 2.202e-04 [train_loss] tar_ll 0.9777 loss -0.9777 (6.105 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 54000 lr 2.187e-04 [train_loss] tar_ll 1.0106 loss -1.0106 (6.123 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 54200 lr 2.171e-04 [train_loss] tar_ll 0.9276 loss -0.9276 (6.182 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 54400 lr 2.156e-04 [train_loss] tar_ll 0.9084 loss -0.9084 (6.084 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 54600 lr 2.140e-04 [train_loss] tar_ll 1.0173 loss -1.0173 (5.923 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 54800 lr 2.124e-04 [train_loss] tar_ll 0.9895 loss -0.9895 (5.893 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 55000 lr 2.109e-04 [train_loss] tar_ll 1.0239 loss -1.0239 (5.854 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 109.15it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.6082 loss -0.6082 (27.487 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 55200 lr 2.093e-04 [train_loss] tar_ll 0.9167 loss -0.9167 (5.804 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 55400 lr 2.078e-04 [train_loss] tar_ll 1.0711 loss -1.0711 (5.848 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 55600 lr 2.062e-04 [train_loss] tar_ll 0.9904 loss -0.9904 (5.732 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 55800 lr 2.047e-04 [train_loss] tar_ll 1.0297 loss -1.0297 (5.818 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 56000 lr 2.032e-04 [train_loss] tar_ll 1.0176 loss -1.0176 (5.849 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 56200 lr 2.016e-04 [train_loss] tar_ll 1.0489 loss -1.0489 (5.809 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 56400 lr 2.001e-04 [train_loss] tar_ll 1.0173 loss -1.0173 (5.848 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 56600 lr 1.985e-04 [train_loss] tar_ll 0.9173 loss -0.9173 (5.776 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0525 loss -1.0525 (5.713 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 57000 lr 1.955e-04 [train_loss] tar_ll 1.0337 loss -1.0337 (5.781 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 57200 lr 1.939e-04 [train_loss] tar_ll 0.9787 loss -0.9787 (6.384 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 57400 lr 1.924e-04 [train_loss] tar_ll 1.0410 loss -1.0410 (6.661 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 57600 lr 1.909e-04 [train_loss] tar_ll 0.9867 loss -0.9867 (5.691 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 57800 lr 1.894e-04 [train_loss] tar_ll 0.9774 loss -0.9774 (5.860 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 58000 lr 1.878e-04 [train_loss] tar_ll 0.9699 loss -0.9699 (5.874 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 58200 lr 1.863e-04 [train_loss] tar_ll 1.0240 loss -1.0240 (5.805 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 58400 lr 1.848e-04 [train_loss] tar_ll 1.0475 loss -1.0475 (5.863 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 58600 lr 1.833e-04 [train_loss] tar_ll 1.0046 loss -1.0046 (5.877 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 58800 lr 1.818e-04 [train_loss] tar_ll 0.9882 loss -0.9882 (5.773 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 59000 lr 1.803e-04 [train_loss] tar_ll 0.9855 loss -0.9855 (5.991 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 59200 lr 1.787e-04 [train_loss] tar_ll 1.0044 loss -1.0044 (5.994 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 59400 lr 1.772e-04 [train_loss] tar_ll 1.0325 loss -1.0325 (5.745 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 59600 lr 1.757e-04 [train_loss] tar_ll 1.0507 loss -1.0507 (5.832 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 59800 lr 1.742e-04 [train_loss] tar_ll 0.9974 loss -0.9974 (5.751 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 60000 lr 1.727e-04 [train_loss] tar_ll 1.0504 loss -1.0504 (5.803 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 109.02it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.6534 loss -0.6534 (27.520 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 60200 lr 1.713e-04 [train_loss] tar_ll 1.0492 loss -1.0492 (5.865 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 60400 lr 1.698e-04 [train_loss] tar_ll 1.0548 loss -1.0548 (5.867 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 60600 lr 1.683e-04 [train_loss] tar_ll 0.9923 loss -0.9923 (5.750 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 60800 lr 1.668e-04 [train_loss] tar_ll 1.0547 loss -1.0547 (5.792 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 61000 lr 1.653e-04 [train_loss] tar_ll 0.9359 loss -0.9359 (5.799 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 61200 lr 1.638e-04 [train_loss] tar_ll 1.0219 loss -1.0219 (5.768 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 61400 lr 1.624e-04 [train_loss] tar_ll 1.0556 loss -1.0556 (5.917 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 61600 lr 1.609e-04 [train_loss] tar_ll 1.0160 loss -1.0160 (5.763 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 61800 lr 1.594e-04 [train_loss] tar_ll 1.0708 loss -1.0708 (5.768 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 62000 lr 1.580e-04 [train_loss] tar_ll 1.0836 loss -1.0836 (6.058 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 62200 lr 1.565e-04 [train_loss] tar_ll 1.0539 loss -1.0539 (5.895 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 62400 lr 1.551e-04 [train_loss] tar_ll 1.0540 loss -1.0540 (5.864 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 62600 lr 1.536e-04 [train_loss] tar_ll 0.9947 loss -0.9947 (6.034 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 62800 lr 1.522e-04 [train_loss] tar_ll 1.0801 loss -1.0801 (5.870 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 63000 lr 1.507e-04 [train_loss] tar_ll 0.9729 loss -0.9729 (5.976 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 63200 lr 1.493e-04 [train_loss] tar_ll 1.0720 loss -1.0720 (6.149 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 63400 lr 1.478e-04 [train_loss] tar_ll 1.0967 loss -1.0967 (5.931 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 63600 lr 1.464e-04 [train_loss] tar_ll 1.1082 loss -1.1082 (6.185 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 63800 lr 1.450e-04 [train_loss] tar_ll 1.0545 loss -1.0545 (6.020 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 64000 lr 1.436e-04 [train_loss] tar_ll 1.0479 loss -1.0479 (6.211 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 64200 lr 1.421e-04 [train_loss] tar_ll 1.0560 loss -1.0560 (6.177 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 64400 lr 1.407e-04 [train_loss] tar_ll 0.9707 loss -0.9707 (6.066 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 64600 lr 1.393e-04 [train_loss] tar_ll 1.1124 loss -1.1124 (6.101 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 64800 lr 1.379e-04 [train_loss] tar_ll 1.0698 loss -1.0698 (5.930 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 65000 lr 1.365e-04 [train_loss] tar_ll 1.0396 loss -1.0396 (5.810 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 108.65it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.6833 loss -0.6833 (27.616 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 65200 lr 1.351e-04 [train_loss] tar_ll 1.0710 loss -1.0710 (5.895 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 65400 lr 1.337e-04 [train_loss] tar_ll 1.0844 loss -1.0844 (5.862 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 65600 lr 1.323e-04 [train_loss] tar_ll 1.1306 loss -1.1306 (5.708 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 65800 lr 1.309e-04 [train_loss] tar_ll 0.9969 loss -0.9969 (5.822 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 66000 lr 1.296e-04 [train_loss] tar_ll 1.0844 loss -1.0844 (5.887 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 66200 lr 1.282e-04 [train_loss] tar_ll 1.1020 loss -1.1020 (6.084 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 66400 lr 1.268e-04 [train_loss] tar_ll 1.1500 loss -1.1500 (6.178 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 66600 lr 1.255e-04 [train_loss] tar_ll 1.0909 loss -1.0909 (6.074 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 66800 lr 1.241e-04 [train_loss] tar_ll 1.0514 loss -1.0514 (6.147 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 67000 lr 1.227e-04 [train_loss] tar_ll 1.0519 loss -1.0519 (6.510 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 67200 lr 1.214e-04 [train_loss] tar_ll 1.1030 loss -1.1030 (6.603 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 67400 lr 1.200e-04 [train_loss] tar_ll 1.0683 loss -1.0683 (6.628 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 67600 lr 1.187e-04 [train_loss] tar_ll 1.0598 loss -1.0598 (6.521 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 67800 lr 1.174e-04 [train_loss] tar_ll 1.1625 loss -1.1625 (6.457 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 68000 lr 1.160e-04 [train_loss] tar_ll 1.1120 loss -1.1120 (5.874 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 68200 lr 1.147e-04 [train_loss] tar_ll 1.1395 loss -1.1395 (5.768 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 68400 lr 1.134e-04 [train_loss] tar_ll 1.1018 loss -1.1018 (5.809 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 68600 lr 1.121e-04 [train_loss] tar_ll 0.9699 loss -0.9699 (5.728 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 68800 lr 1.108e-04 [train_loss] tar_ll 1.0789 loss -1.0789 (5.761 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 69000 lr 1.095e-04 [train_loss] tar_ll 1.1724 loss -1.1724 (6.020 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 69200 lr 1.082e-04 [train_loss] tar_ll 1.0833 loss -1.0833 (5.826 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 69400 lr 1.069e-04 [train_loss] tar_ll 1.1462 loss -1.1462 (5.746 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 69600 lr 1.056e-04 [train_loss] tar_ll 1.0884 loss -1.0884 (5.917 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 69800 lr 1.043e-04 [train_loss] tar_ll 1.0870 loss -1.0870 (5.790 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 70000 lr 1.031e-04 [train_loss] tar_ll 1.0648 loss -1.0648 (5.811 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 109.10it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.6926 loss -0.6926 (27.500 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 70200 lr 1.018e-04 [train_loss] tar_ll 1.0736 loss -1.0736 (6.494 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 70400 lr 1.005e-04 [train_loss] tar_ll 1.0855 loss -1.0855 (5.708 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 70600 lr 9.927e-05 [train_loss] tar_ll 1.1121 loss -1.1121 (5.728 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 70800 lr 9.802e-05 [train_loss] tar_ll 1.0859 loss -1.0859 (5.867 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 71000 lr 9.677e-05 [train_loss] tar_ll 1.1922 loss -1.1922 (5.819 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 71200 lr 9.554e-05 [train_loss] tar_ll 1.1099 loss -1.1099 (5.861 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 71400 lr 9.430e-05 [train_loss] tar_ll 1.0967 loss -1.0967 (6.304 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 71600 lr 9.308e-05 [train_loss] tar_ll 1.0981 loss -1.0981 (5.845 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 71800 lr 9.186e-05 [train_loss] tar_ll 0.9955 loss -0.9955 (5.953 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 72000 lr 9.064e-05 [train_loss] tar_ll 1.1746 loss -1.1746 (5.914 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 72200 lr 8.944e-05 [train_loss] tar_ll 1.1287 loss -1.1287 (6.029 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 72400 lr 8.824e-05 [train_loss] tar_ll 1.0923 loss -1.0923 (5.985 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 72600 lr 8.704e-05 [train_loss] tar_ll 1.1081 loss -1.1081 (6.021 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 72800 lr 8.585e-05 [train_loss] tar_ll 1.1962 loss -1.1962 (5.918 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 73000 lr 8.467e-05 [train_loss] tar_ll 1.2043 loss -1.2043 (6.044 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 73200 lr 8.350e-05 [train_loss] tar_ll 1.1539 loss -1.1539 (5.987 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 73400 lr 8.233e-05 [train_loss] tar_ll 1.1879 loss -1.1879 (6.065 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 73600 lr 8.117e-05 [train_loss] tar_ll 1.1260 loss -1.1260 (5.930 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 73800 lr 8.001e-05 [train_loss] tar_ll 1.1138 loss -1.1138 (5.976 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 74000 lr 7.886e-05 [train_loss] tar_ll 1.0383 loss -1.0383 (6.109 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 74200 lr 7.772e-05 [train_loss] tar_ll 1.0479 loss -1.0479 (6.179 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 74400 lr 7.659e-05 [train_loss] tar_ll 1.1559 loss -1.1559 (6.150 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 74600 lr 7.546e-05 [train_loss] tar_ll 1.1429 loss -1.1429 (6.165 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 74800 lr 7.434e-05 [train_loss] tar_ll 1.1363 loss -1.1363 (6.185 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1931 loss -1.1931 (5.933 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:26<00:00, 111.43it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.7329 loss -0.7329 (26.925 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1147 loss -1.1147 (5.801 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 75400 lr 7.102e-05 [train_loss] tar_ll 1.1177 loss -1.1177 (5.721 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 75600 lr 6.992e-05 [train_loss] tar_ll 1.1340 loss -1.1340 (5.694 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 75800 lr 6.884e-05 [train_loss] tar_ll 1.1842 loss -1.1842 (5.764 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 76000 lr 6.776e-05 [train_loss] tar_ll 1.1547 loss -1.1547 (5.627 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 76200 lr 6.669e-05 [train_loss] tar_ll 1.2943 loss -1.2943 (5.791 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 76400 lr 6.562e-05 [train_loss] tar_ll 1.1659 loss -1.1659 (5.654 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 76600 lr 6.456e-05 [train_loss] tar_ll 1.1753 loss -1.1753 (6.175 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 76800 lr 6.351e-05 [train_loss] tar_ll 1.1412 loss -1.1412 (6.158 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 77000 lr 6.247e-05 [train_loss] tar_ll 1.2151 loss -1.2151 (6.121 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 77200 lr 6.144e-05 [train_loss] tar_ll 1.1660 loss -1.1660 (6.422 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 77400 lr 6.041e-05 [train_loss] tar_ll 1.1691 loss -1.1691 (6.649 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 77600 lr 5.939e-05 [train_loss] tar_ll 1.1209 loss -1.1209 (6.708 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 77800 lr 5.838e-05 [train_loss] tar_ll 1.1051 loss -1.1051 (6.467 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 78000 lr 5.737e-05 [train_loss] tar_ll 1.0812 loss -1.0812 (6.345 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 78200 lr 5.637e-05 [train_loss] tar_ll 1.0890 loss -1.0890 (6.093 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 78400 lr 5.538e-05 [train_loss] tar_ll 1.1497 loss -1.1497 (6.260 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 78600 lr 5.440e-05 [train_loss] tar_ll 1.2145 loss -1.2145 (6.016 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 78800 lr 5.343e-05 [train_loss] tar_ll 1.1653 loss -1.1653 (5.993 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 79000 lr 5.246e-05 [train_loss] tar_ll 1.1338 loss -1.1338 (6.328 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 79200 lr 5.150e-05 [train_loss] tar_ll 1.1645 loss -1.1645 (6.116 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 79400 lr 5.055e-05 [train_loss] tar_ll 1.1961 loss -1.1961 (6.143 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 79600 lr 4.961e-05 [train_loss] tar_ll 1.1027 loss -1.1027 (5.815 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 79800 lr 4.867e-05 [train_loss] tar_ll 1.1196 loss -1.1196 (5.804 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 80000 lr 4.775e-05 [train_loss] tar_ll 1.2077 loss -1.2077 (5.910 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 109.58it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.7685 loss -0.7685 (27.379 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 80200 lr 4.683e-05 [train_loss] tar_ll 1.0396 loss -1.0396 (6.700 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 80400 lr 4.592e-05 [train_loss] tar_ll 1.2432 loss -1.2432 (6.566 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 80600 lr 4.501e-05 [train_loss] tar_ll 1.0960 loss -1.0960 (5.972 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2459 loss -1.2459 (5.776 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 81000 lr 4.323e-05 [train_loss] tar_ll 1.1465 loss -1.1465 (5.894 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 81200 lr 4.235e-05 [train_loss] tar_ll 1.1343 loss -1.1343 (5.838 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 81400 lr 4.148e-05 [train_loss] tar_ll 1.1694 loss -1.1694 (5.900 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 81600 lr 4.062e-05 [train_loss] tar_ll 1.1892 loss -1.1892 (6.019 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 81800 lr 3.976e-05 [train_loss] tar_ll 1.3033 loss -1.3033 (5.675 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 82000 lr 3.892e-05 [train_loss] tar_ll 1.0430 loss -1.0430 (5.768 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 82200 lr 3.808e-05 [train_loss] tar_ll 1.2365 loss -1.2365 (6.011 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 82400 lr 3.725e-05 [train_loss] tar_ll 1.2121 loss -1.2121 (5.849 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 82600 lr 3.643e-05 [train_loss] tar_ll 1.1849 loss -1.1849 (6.047 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 82800 lr 3.562e-05 [train_loss] tar_ll 1.1414 loss -1.1414 (5.960 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 83000 lr 3.481e-05 [train_loss] tar_ll 1.1664 loss -1.1664 (5.824 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 83200 lr 3.402e-05 [train_loss] tar_ll 1.1814 loss -1.1814 (6.203 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 83400 lr 3.323e-05 [train_loss] tar_ll 1.2006 loss -1.2006 (6.681 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 83600 lr 3.245e-05 [train_loss] tar_ll 1.1970 loss -1.1970 (6.302 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 83800 lr 3.168e-05 [train_loss] tar_ll 1.1385 loss -1.1385 (6.369 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 84000 lr 3.092e-05 [train_loss] tar_ll 1.2607 loss -1.2607 (6.248 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2002 loss -1.2002 (6.357 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 84400 lr 2.943e-05 [train_loss] tar_ll 1.1288 loss -1.1288 (6.143 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 84600 lr 2.869e-05 [train_loss] tar_ll 1.1645 loss -1.1645 (6.083 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 84800 lr 2.797e-05 [train_loss] tar_ll 1.0976 loss -1.0976 (6.173 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 85000 lr 2.725e-05 [train_loss] tar_ll 1.1893 loss -1.1893 (5.839 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 110.52it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.7736 loss -0.7736 (27.146 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 85200 lr 2.654e-05 [train_loss] tar_ll 1.1569 loss -1.1569 (5.876 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 85400 lr 2.584e-05 [train_loss] tar_ll 1.3162 loss -1.3162 (5.843 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 85600 lr 2.515e-05 [train_loss] tar_ll 1.0928 loss -1.0928 (5.730 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 85800 lr 2.447e-05 [train_loss] tar_ll 1.2450 loss -1.2450 (5.748 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 86000 lr 2.379e-05 [train_loss] tar_ll 1.2751 loss -1.2751 (5.906 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 86200 lr 2.313e-05 [train_loss] tar_ll 1.0938 loss -1.0938 (5.763 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 86400 lr 2.247e-05 [train_loss] tar_ll 1.2552 loss -1.2552 (5.793 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 86600 lr 2.183e-05 [train_loss] tar_ll 1.1727 loss -1.1727 (5.833 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 86800 lr 2.119e-05 [train_loss] tar_ll 1.1495 loss -1.1495 (5.764 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 87000 lr 2.056e-05 [train_loss] tar_ll 1.2274 loss -1.2274 (5.754 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 87200 lr 1.994e-05 [train_loss] tar_ll 1.2040 loss -1.2040 (5.965 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 87400 lr 1.933e-05 [train_loss] tar_ll 1.2395 loss -1.2395 (5.757 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 87600 lr 1.873e-05 [train_loss] tar_ll 1.0871 loss -1.0871 (5.877 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 87800 lr 1.814e-05 [train_loss] tar_ll 1.1682 loss -1.1682 (5.713 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 88000 lr 1.756e-05 [train_loss] tar_ll 1.1825 loss -1.1825 (5.726 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 88200 lr 1.698e-05 [train_loss] tar_ll 1.1878 loss -1.1878 (5.801 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 88400 lr 1.642e-05 [train_loss] tar_ll 1.1483 loss -1.1483 (5.693 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 88600 lr 1.586e-05 [train_loss] tar_ll 1.1635 loss -1.1635 (5.738 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 88800 lr 1.532e-05 [train_loss] tar_ll 1.1898 loss -1.1898 (5.842 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 89000 lr 1.478e-05 [train_loss] tar_ll 1.2016 loss -1.2016 (5.738 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 89200 lr 1.425e-05 [train_loss] tar_ll 1.2572 loss -1.2572 (5.807 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 89400 lr 1.373e-05 [train_loss] tar_ll 1.2075 loss -1.2075 (5.979 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 89600 lr 1.323e-05 [train_loss] tar_ll 1.1397 loss -1.1397 (5.781 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2348 loss -1.2348 (5.969 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 90000 lr 1.224e-05 [train_loss] tar_ll 1.1380 loss -1.1380 (5.767 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 105.32it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.7765 loss -0.7765 (28.487 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 90200 lr 1.176e-05 [train_loss] tar_ll 1.1602 loss -1.1602 (6.421 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 90400 lr 1.128e-05 [train_loss] tar_ll 1.1974 loss -1.1974 (6.033 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 90600 lr 1.082e-05 [train_loss] tar_ll 1.1573 loss -1.1573 (5.836 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 90800 lr 1.037e-05 [train_loss] tar_ll 1.2090 loss -1.2090 (5.774 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 91000 lr 9.927e-06 [train_loss] tar_ll 1.2532 loss -1.2532 (5.900 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2902 loss -1.2902 (5.941 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 91400 lr 9.069e-06 [train_loss] tar_ll 1.1456 loss -1.1456 (6.177 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 91600 lr 8.655e-06 [train_loss] tar_ll 1.1700 loss -1.1700 (6.129 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 91800 lr 8.250e-06 [train_loss] tar_ll 1.1893 loss -1.1893 (5.826 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2019 loss -1.2019 (5.778 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 92200 lr 7.468e-06 [train_loss] tar_ll 1.1819 loss -1.1819 (5.980 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 92400 lr 7.092e-06 [train_loss] tar_ll 1.2106 loss -1.2106 (5.940 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 92600 lr 6.725e-06 [train_loss] tar_ll 1.2414 loss -1.2414 (6.150 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 92800 lr 6.368e-06 [train_loss] tar_ll 1.2191 loss -1.2191 (5.995 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 93000 lr 6.021e-06 [train_loss] tar_ll 1.1917 loss -1.1917 (6.036 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 93200 lr 5.683e-06 [train_loss] tar_ll 1.1830 loss -1.1830 (6.156 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 93400 lr 5.355e-06 [train_loss] tar_ll 1.2626 loss -1.2626 (5.891 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 93600 lr 5.036e-06 [train_loss] tar_ll 1.1966 loss -1.1966 (6.138 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 93800 lr 4.727e-06 [train_loss] tar_ll 1.2394 loss -1.2394 (6.181 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 94000 lr 4.428e-06 [train_loss] tar_ll 1.1892 loss -1.1892 (6.041 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 94200 lr 4.139e-06 [train_loss] tar_ll 1.1913 loss -1.1913 (6.164 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 94400 lr 3.859e-06 [train_loss] tar_ll 1.2532 loss -1.2532 (6.282 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 94600 lr 3.589e-06 [train_loss] tar_ll 1.2381 loss -1.2381 (6.143 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 94800 lr 3.329e-06 [train_loss] tar_ll 1.2147 loss -1.2147 (6.200 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 95000 lr 3.078e-06 [train_loss] tar_ll 1.2310 loss -1.2310 (6.172 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 110.32it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.7846 loss -0.7846 (27.198 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_matern step 95200 lr 2.837e-06 [train_loss] tar_ll 1.2523 loss -1.2523 (5.766 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 95400 lr 2.606e-06 [train_loss] tar_ll 1.1067 loss -1.1067 (5.822 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 95600 lr 2.385e-06 [train_loss] tar_ll 1.1808 loss -1.1808 (5.652 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 95800 lr 2.173e-06 [train_loss] tar_ll 1.1933 loss -1.1933 (5.677 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 96000 lr 1.971e-06 [train_loss] tar_ll 1.1712 loss -1.1712 (5.757 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 96200 lr 1.779e-06 [train_loss] tar_ll 1.1848 loss -1.1848 (5.717 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 96400 lr 1.597e-06 [train_loss] tar_ll 1.1356 loss -1.1356 (5.692 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 96600 lr 1.425e-06 [train_loss] tar_ll 1.1540 loss -1.1540 (6.180 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 96800 lr 1.262e-06 [train_loss] tar_ll 1.1830 loss -1.1830 (6.040 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 97000 lr 1.110e-06 [train_loss] tar_ll 1.1911 loss -1.1911 (6.248 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 97200 lr 9.666e-07 [train_loss] tar_ll 1.2060 loss -1.2060 (6.114 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 97400 lr 8.335e-07 [train_loss] tar_ll 1.2525 loss -1.2525 (6.355 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 97600 lr 7.103e-07 [train_loss] tar_ll 1.2318 loss -1.2318 (6.170 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 97800 lr 5.969e-07 [train_loss] tar_ll 1.1842 loss -1.1842 (6.128 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 98000 lr 4.933e-07 [train_loss] tar_ll 1.2324 loss -1.2324 (6.169 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 98200 lr 3.996e-07 [train_loss] tar_ll 1.2689 loss -1.2689 (5.960 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 98400 lr 3.158e-07 [train_loss] tar_ll 1.2401 loss -1.2401 (5.690 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 98600 lr 2.418e-07 [train_loss] tar_ll 1.2501 loss -1.2501 (5.912 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2112 loss -1.2112 (5.737 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 99000 lr 1.234e-07 [train_loss] tar_ll 1.1298 loss -1.1298 (5.868 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 99200 lr 7.895e-08 [train_loss] tar_ll 1.3008 loss -1.3008 (6.041 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2302 loss -1.2302 (5.733 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 99600 lr 1.974e-08 [train_loss] tar_ll 1.2317 loss -1.2317 (5.812 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 99800 lr 4.935e-09 [train_loss] tar_ll 1.1523 loss -1.1523 (5.954 secs)\n",
      "lbanp:lbanp-num_latents-8_matern step 100000 lr 0.000e+00 [train_loss] tar_ll 1.2122 loss -1.2122 (5.816 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 105.71it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.7832 loss -0.7832 (28.383 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 109.01it/s]\n",
      "lbanp:lbanp-num_latents-8_matern matern tar_ll 0.7832 loss -0.7832 (27.523 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "Execution time: 3585226.5 miliseconds\n",
      "Execution time: 3585.2265 seconds\n",
      "Initial Memory Usage: 19.80224609375 MB\n",
      "Final Memory Usage: 58.6845703125 MB\n",
      "Memory Usage Change: 38.88232421875 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='lbanp', name='lbanp-num_latents-8_matern',val_seed=100, val_l=8,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1b49b-92c4-4ff7-bad4-beff827512ff",
   "metadata": {},
   "source": [
    "## LBANP (128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5623988a-4637-4cb4-9a8a-e9c5ef91106c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: lbanp-lbanp-num_latents-128_matern\n",
      "Total number of parameters: 792514\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "matern-seed100.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 200 lr 5.000e-04 [train_loss] tar_ll -0.6961 loss 0.6961 (5.978 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 400 lr 5.000e-04 [train_loss] tar_ll -0.6825 loss 0.6825 (5.966 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 600 lr 5.000e-04 [train_loss] tar_ll -0.5935 loss 0.5935 (6.068 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 800 lr 4.999e-04 [train_loss] tar_ll -0.5389 loss 0.5389 (5.941 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 1000 lr 4.999e-04 [train_loss] tar_ll -0.4901 loss 0.4901 (6.050 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 1200 lr 4.998e-04 [train_loss] tar_ll -0.4771 loss 0.4771 (6.027 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 1400 lr 4.998e-04 [train_loss] tar_ll -0.3103 loss 0.3103 (6.156 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 1600 lr 4.997e-04 [train_loss] tar_ll -0.2149 loss 0.2149 (6.189 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 1800 lr 4.996e-04 [train_loss] tar_ll -0.1341 loss 0.1341 (6.164 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 2000 lr 4.995e-04 [train_loss] tar_ll -0.0986 loss 0.0986 (6.285 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 2200 lr 4.994e-04 [train_loss] tar_ll 0.0680 loss -0.0680 (6.344 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 2400 lr 4.993e-04 [train_loss] tar_ll 0.1092 loss -0.1092 (6.228 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1566 loss -0.1566 (6.482 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 2800 lr 4.990e-04 [train_loss] tar_ll 0.1837 loss -0.1837 (6.108 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 3000 lr 4.989e-04 [train_loss] tar_ll 0.2555 loss -0.2555 (6.187 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 3200 lr 4.987e-04 [train_loss] tar_ll 0.2853 loss -0.2853 (6.291 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 3400 lr 4.986e-04 [train_loss] tar_ll 0.2976 loss -0.2976 (6.319 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2684 loss -0.2684 (6.472 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 3800 lr 4.982e-04 [train_loss] tar_ll 0.3190 loss -0.3190 (6.365 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 4000 lr 4.980e-04 [train_loss] tar_ll 0.1581 loss -0.1581 (6.492 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3142 loss -0.3142 (6.350 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 4400 lr 4.976e-04 [train_loss] tar_ll 0.0483 loss -0.0483 (5.988 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 4600 lr 4.974e-04 [train_loss] tar_ll 0.1843 loss -0.1843 (6.107 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 4800 lr 4.972e-04 [train_loss] tar_ll 0.2893 loss -0.2893 (5.946 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 5000 lr 4.969e-04 [train_loss] tar_ll 0.3997 loss -0.3997 (5.953 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 104.44it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.3045 loss -0.3045 (28.727 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 5200 lr 4.967e-04 [train_loss] tar_ll 0.3366 loss -0.3366 (6.055 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 5400 lr 4.964e-04 [train_loss] tar_ll 0.4699 loss -0.4699 (6.224 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4580 loss -0.4580 (5.862 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 5800 lr 4.959e-04 [train_loss] tar_ll 0.5490 loss -0.5490 (5.989 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4968 loss -0.4968 (5.840 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 6200 lr 4.953e-04 [train_loss] tar_ll 0.5317 loss -0.5317 (5.994 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 6400 lr 4.950e-04 [train_loss] tar_ll 0.4211 loss -0.4211 (6.016 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 6600 lr 4.946e-04 [train_loss] tar_ll 0.4672 loss -0.4672 (6.010 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 6800 lr 4.943e-04 [train_loss] tar_ll 0.3845 loss -0.3845 (6.122 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 7000 lr 4.940e-04 [train_loss] tar_ll 0.4275 loss -0.4275 (5.980 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 7200 lr 4.936e-04 [train_loss] tar_ll 0.4648 loss -0.4648 (6.016 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 7400 lr 4.933e-04 [train_loss] tar_ll 0.4035 loss -0.4035 (6.253 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 7600 lr 4.929e-04 [train_loss] tar_ll 0.4626 loss -0.4626 (6.198 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 7800 lr 4.925e-04 [train_loss] tar_ll 0.5697 loss -0.5697 (6.213 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 8000 lr 4.921e-04 [train_loss] tar_ll 0.5504 loss -0.5504 (6.270 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 8200 lr 4.918e-04 [train_loss] tar_ll 0.5400 loss -0.5400 (6.083 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 8400 lr 4.913e-04 [train_loss] tar_ll 0.5824 loss -0.5824 (6.271 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 8600 lr 4.909e-04 [train_loss] tar_ll 0.5397 loss -0.5397 (6.142 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 8800 lr 4.905e-04 [train_loss] tar_ll 0.5452 loss -0.5452 (6.129 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 9000 lr 4.901e-04 [train_loss] tar_ll 0.6016 loss -0.6016 (6.155 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 9200 lr 4.896e-04 [train_loss] tar_ll 0.5862 loss -0.5862 (6.131 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 9400 lr 4.892e-04 [train_loss] tar_ll 0.5878 loss -0.5878 (6.431 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 9600 lr 4.887e-04 [train_loss] tar_ll 0.2669 loss -0.2669 (6.085 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 9800 lr 4.882e-04 [train_loss] tar_ll 0.4980 loss -0.4980 (6.102 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 10000 lr 4.878e-04 [train_loss] tar_ll 0.5776 loss -0.5776 (6.296 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 102.37it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.3847 loss -0.3847 (29.305 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 10200 lr 4.873e-04 [train_loss] tar_ll 0.6386 loss -0.6386 (6.044 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 10400 lr 4.868e-04 [train_loss] tar_ll 0.6147 loss -0.6147 (6.012 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 10600 lr 4.863e-04 [train_loss] tar_ll 0.5428 loss -0.5428 (6.215 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 10800 lr 4.857e-04 [train_loss] tar_ll 0.6376 loss -0.6376 (6.084 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 11000 lr 4.852e-04 [train_loss] tar_ll 0.6586 loss -0.6586 (6.142 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 11200 lr 4.847e-04 [train_loss] tar_ll 0.6951 loss -0.6951 (6.164 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 11400 lr 4.841e-04 [train_loss] tar_ll 0.6741 loss -0.6741 (6.198 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 11600 lr 4.836e-04 [train_loss] tar_ll 0.6491 loss -0.6491 (6.346 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 11800 lr 4.830e-04 [train_loss] tar_ll 0.6013 loss -0.6013 (6.214 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 12000 lr 4.824e-04 [train_loss] tar_ll 0.6486 loss -0.6486 (6.293 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 12200 lr 4.819e-04 [train_loss] tar_ll 0.5654 loss -0.5654 (6.356 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 12400 lr 4.813e-04 [train_loss] tar_ll 0.5748 loss -0.5748 (6.297 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 12600 lr 4.807e-04 [train_loss] tar_ll 0.7104 loss -0.7104 (6.332 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 12800 lr 4.801e-04 [train_loss] tar_ll 0.6760 loss -0.6760 (6.148 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 13000 lr 4.794e-04 [train_loss] tar_ll 0.5754 loss -0.5754 (6.318 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 13200 lr 4.788e-04 [train_loss] tar_ll 0.3189 loss -0.3189 (6.558 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 13400 lr 4.782e-04 [train_loss] tar_ll 0.2786 loss -0.2786 (6.316 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 13600 lr 4.775e-04 [train_loss] tar_ll 0.4472 loss -0.4472 (6.476 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 13800 lr 4.769e-04 [train_loss] tar_ll 0.5206 loss -0.5206 (6.266 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 14000 lr 4.762e-04 [train_loss] tar_ll 0.4763 loss -0.4763 (6.309 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 14200 lr 4.755e-04 [train_loss] tar_ll 0.6086 loss -0.6086 (6.010 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 14400 lr 4.749e-04 [train_loss] tar_ll 0.6315 loss -0.6315 (6.088 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 14600 lr 4.742e-04 [train_loss] tar_ll 0.5344 loss -0.5344 (6.181 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 14800 lr 4.735e-04 [train_loss] tar_ll 0.6040 loss -0.6040 (6.010 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 15000 lr 4.728e-04 [train_loss] tar_ll 0.5892 loss -0.5892 (5.941 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 103.63it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.4395 loss -0.4395 (28.952 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 15200 lr 4.720e-04 [train_loss] tar_ll 0.6347 loss -0.6347 (6.409 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 15400 lr 4.713e-04 [train_loss] tar_ll 0.6739 loss -0.6739 (6.402 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 15600 lr 4.706e-04 [train_loss] tar_ll 0.6894 loss -0.6894 (6.213 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 15800 lr 4.698e-04 [train_loss] tar_ll 0.5680 loss -0.5680 (6.210 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 16000 lr 4.691e-04 [train_loss] tar_ll 0.4440 loss -0.4440 (6.123 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 16200 lr 4.683e-04 [train_loss] tar_ll 0.5970 loss -0.5970 (6.313 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 16400 lr 4.675e-04 [train_loss] tar_ll 0.6100 loss -0.6100 (6.022 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 16600 lr 4.668e-04 [train_loss] tar_ll 0.6920 loss -0.6920 (6.068 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 16800 lr 4.660e-04 [train_loss] tar_ll 0.7562 loss -0.7562 (6.163 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 17000 lr 4.652e-04 [train_loss] tar_ll 0.6909 loss -0.6909 (6.162 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 17200 lr 4.644e-04 [train_loss] tar_ll 0.7170 loss -0.7170 (6.463 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 17400 lr 4.636e-04 [train_loss] tar_ll 0.8352 loss -0.8352 (6.161 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 17600 lr 4.627e-04 [train_loss] tar_ll 0.8756 loss -0.8756 (6.069 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 17800 lr 4.619e-04 [train_loss] tar_ll 0.7732 loss -0.7732 (6.149 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 18000 lr 4.611e-04 [train_loss] tar_ll 0.9151 loss -0.9151 (6.026 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 18200 lr 4.602e-04 [train_loss] tar_ll 0.8151 loss -0.8151 (6.198 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 18400 lr 4.594e-04 [train_loss] tar_ll 0.8064 loss -0.8064 (6.401 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 18600 lr 4.585e-04 [train_loss] tar_ll 0.6646 loss -0.6646 (6.102 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 18800 lr 4.576e-04 [train_loss] tar_ll 0.7576 loss -0.7576 (6.222 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 19000 lr 4.568e-04 [train_loss] tar_ll 0.8052 loss -0.8052 (6.332 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 19200 lr 4.559e-04 [train_loss] tar_ll 0.7874 loss -0.7874 (6.088 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 19400 lr 4.550e-04 [train_loss] tar_ll 0.7315 loss -0.7315 (6.237 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 19600 lr 4.541e-04 [train_loss] tar_ll 0.6625 loss -0.6625 (6.537 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 19800 lr 4.532e-04 [train_loss] tar_ll 0.7310 loss -0.7310 (6.451 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 20000 lr 4.523e-04 [train_loss] tar_ll 0.7483 loss -0.7483 (6.033 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 101.26it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.4810 loss -0.4810 (29.628 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 20200 lr 4.513e-04 [train_loss] tar_ll 0.7964 loss -0.7964 (6.016 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 20400 lr 4.504e-04 [train_loss] tar_ll 0.8323 loss -0.8323 (6.115 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 20600 lr 4.494e-04 [train_loss] tar_ll 0.6252 loss -0.6252 (5.982 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 20800 lr 4.485e-04 [train_loss] tar_ll 0.7534 loss -0.7534 (6.093 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 21000 lr 4.475e-04 [train_loss] tar_ll 0.8317 loss -0.8317 (6.534 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 21200 lr 4.466e-04 [train_loss] tar_ll 0.8523 loss -0.8523 (6.289 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 21400 lr 4.456e-04 [train_loss] tar_ll 0.8290 loss -0.8290 (6.300 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 21600 lr 4.446e-04 [train_loss] tar_ll 0.8076 loss -0.8076 (6.166 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 21800 lr 4.436e-04 [train_loss] tar_ll 0.8523 loss -0.8523 (6.253 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 22000 lr 4.426e-04 [train_loss] tar_ll 0.8465 loss -0.8465 (6.544 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 22200 lr 4.416e-04 [train_loss] tar_ll 0.8134 loss -0.8134 (6.177 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 22400 lr 4.406e-04 [train_loss] tar_ll 0.8551 loss -0.8551 (5.995 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 22600 lr 4.396e-04 [train_loss] tar_ll 0.8314 loss -0.8314 (6.059 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 22800 lr 4.386e-04 [train_loss] tar_ll 0.8518 loss -0.8518 (5.963 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 23000 lr 4.375e-04 [train_loss] tar_ll 0.8000 loss -0.8000 (6.070 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 23200 lr 4.365e-04 [train_loss] tar_ll 0.7860 loss -0.7860 (5.944 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 23400 lr 4.354e-04 [train_loss] tar_ll 0.8002 loss -0.8002 (5.979 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 23600 lr 4.344e-04 [train_loss] tar_ll 0.8418 loss -0.8418 (6.118 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 23800 lr 4.333e-04 [train_loss] tar_ll 0.8594 loss -0.8594 (5.970 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 24000 lr 4.322e-04 [train_loss] tar_ll 0.8798 loss -0.8798 (6.138 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 24200 lr 4.312e-04 [train_loss] tar_ll 0.7452 loss -0.7452 (6.063 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 24400 lr 4.301e-04 [train_loss] tar_ll 0.7201 loss -0.7201 (6.051 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 24600 lr 4.290e-04 [train_loss] tar_ll 0.7995 loss -0.7995 (6.120 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 24800 lr 4.279e-04 [train_loss] tar_ll 0.8522 loss -0.8522 (6.052 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 25000 lr 4.268e-04 [train_loss] tar_ll 0.6635 loss -0.6635 (5.976 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 104.66it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.4906 loss -0.4906 (28.668 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 25200 lr 4.257e-04 [train_loss] tar_ll 0.6336 loss -0.6336 (6.222 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 25400 lr 4.245e-04 [train_loss] tar_ll 0.6791 loss -0.6791 (6.047 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 25600 lr 4.234e-04 [train_loss] tar_ll 0.7850 loss -0.7850 (6.072 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 25800 lr 4.223e-04 [train_loss] tar_ll 0.6867 loss -0.6867 (6.457 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 26000 lr 4.211e-04 [train_loss] tar_ll 0.8184 loss -0.8184 (6.561 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 26200 lr 4.200e-04 [train_loss] tar_ll 0.7264 loss -0.7264 (6.261 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 26400 lr 4.188e-04 [train_loss] tar_ll 0.7732 loss -0.7732 (6.108 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 26600 lr 4.177e-04 [train_loss] tar_ll 0.5678 loss -0.5678 (6.067 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 26800 lr 4.165e-04 [train_loss] tar_ll 0.6975 loss -0.6975 (6.248 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 27000 lr 4.153e-04 [train_loss] tar_ll 0.7395 loss -0.7395 (5.987 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 27200 lr 4.141e-04 [train_loss] tar_ll 0.7422 loss -0.7422 (6.097 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 27400 lr 4.130e-04 [train_loss] tar_ll 0.4014 loss -0.4014 (6.080 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 27600 lr 4.118e-04 [train_loss] tar_ll 0.6546 loss -0.6546 (5.988 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 27800 lr 4.106e-04 [train_loss] tar_ll 0.7209 loss -0.7209 (6.199 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 28000 lr 4.094e-04 [train_loss] tar_ll 0.6782 loss -0.6782 (6.064 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 28200 lr 4.081e-04 [train_loss] tar_ll 0.8030 loss -0.8030 (6.137 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 28400 lr 4.069e-04 [train_loss] tar_ll 0.7127 loss -0.7127 (6.135 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 28600 lr 4.057e-04 [train_loss] tar_ll 0.8331 loss -0.8331 (6.117 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 28800 lr 4.045e-04 [train_loss] tar_ll 0.8652 loss -0.8652 (6.303 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 29000 lr 4.032e-04 [train_loss] tar_ll 0.9360 loss -0.9360 (6.220 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 29200 lr 4.020e-04 [train_loss] tar_ll 0.8789 loss -0.8789 (6.092 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 29400 lr 4.007e-04 [train_loss] tar_ll 0.8384 loss -0.8384 (6.123 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 29600 lr 3.995e-04 [train_loss] tar_ll 0.6893 loss -0.6893 (6.027 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 29800 lr 3.982e-04 [train_loss] tar_ll 0.7733 loss -0.7733 (6.165 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 30000 lr 3.969e-04 [train_loss] tar_ll 0.8500 loss -0.8500 (6.128 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 101.07it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.5872 loss -0.5872 (29.684 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 30200 lr 3.957e-04 [train_loss] tar_ll 0.8642 loss -0.8642 (6.009 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 30400 lr 3.944e-04 [train_loss] tar_ll 0.8453 loss -0.8453 (6.076 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 30600 lr 3.931e-04 [train_loss] tar_ll 0.7907 loss -0.7907 (6.024 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 30800 lr 3.918e-04 [train_loss] tar_ll 0.7621 loss -0.7621 (6.199 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 31000 lr 3.905e-04 [train_loss] tar_ll 0.9096 loss -0.9096 (6.312 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 31200 lr 3.892e-04 [train_loss] tar_ll 0.8314 loss -0.8314 (6.322 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 31400 lr 3.879e-04 [train_loss] tar_ll 0.8530 loss -0.8530 (6.132 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 31600 lr 3.866e-04 [train_loss] tar_ll 0.8801 loss -0.8801 (6.337 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 31800 lr 3.853e-04 [train_loss] tar_ll 0.9142 loss -0.9142 (6.296 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 32000 lr 3.840e-04 [train_loss] tar_ll 0.9139 loss -0.9139 (6.339 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 32200 lr 3.826e-04 [train_loss] tar_ll 0.8904 loss -0.8904 (6.326 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 32400 lr 3.813e-04 [train_loss] tar_ll 0.7998 loss -0.7998 (6.103 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 32600 lr 3.800e-04 [train_loss] tar_ll 0.8840 loss -0.8840 (6.472 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 32800 lr 3.786e-04 [train_loss] tar_ll 0.7999 loss -0.7999 (6.460 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 33000 lr 3.773e-04 [train_loss] tar_ll 0.8871 loss -0.8871 (6.361 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 33200 lr 3.759e-04 [train_loss] tar_ll 0.7456 loss -0.7456 (6.409 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 33400 lr 3.745e-04 [train_loss] tar_ll 0.7780 loss -0.7780 (6.325 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 33600 lr 3.732e-04 [train_loss] tar_ll 0.9107 loss -0.9107 (6.048 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 33800 lr 3.718e-04 [train_loss] tar_ll 0.8322 loss -0.8322 (6.127 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 34000 lr 3.704e-04 [train_loss] tar_ll 0.9037 loss -0.9037 (6.149 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 34200 lr 3.691e-04 [train_loss] tar_ll 0.9126 loss -0.9126 (6.112 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 34400 lr 3.677e-04 [train_loss] tar_ll 0.8893 loss -0.8893 (5.990 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 34600 lr 3.663e-04 [train_loss] tar_ll 0.9706 loss -0.9706 (6.023 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 34800 lr 3.649e-04 [train_loss] tar_ll 0.9380 loss -0.9380 (5.875 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 35000 lr 3.635e-04 [train_loss] tar_ll 0.9012 loss -0.9012 (6.154 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 103.69it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.6584 loss -0.6584 (28.933 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 35200 lr 3.621e-04 [train_loss] tar_ll 0.8038 loss -0.8038 (6.196 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 35400 lr 3.607e-04 [train_loss] tar_ll 0.8847 loss -0.8847 (6.057 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 35600 lr 3.593e-04 [train_loss] tar_ll 0.9403 loss -0.9403 (6.005 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 35800 lr 3.579e-04 [train_loss] tar_ll 0.7917 loss -0.7917 (6.142 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 36000 lr 3.564e-04 [train_loss] tar_ll 0.9412 loss -0.9412 (6.161 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 36200 lr 3.550e-04 [train_loss] tar_ll 0.9387 loss -0.9387 (6.837 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 36400 lr 3.536e-04 [train_loss] tar_ll 0.9343 loss -0.9343 (6.112 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 36600 lr 3.522e-04 [train_loss] tar_ll 0.9719 loss -0.9719 (6.076 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 36800 lr 3.507e-04 [train_loss] tar_ll 0.9780 loss -0.9780 (6.142 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 37000 lr 3.493e-04 [train_loss] tar_ll 0.8714 loss -0.8714 (6.135 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 37200 lr 3.478e-04 [train_loss] tar_ll 0.8615 loss -0.8615 (6.104 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 37400 lr 3.464e-04 [train_loss] tar_ll 0.8915 loss -0.8915 (6.142 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 37600 lr 3.449e-04 [train_loss] tar_ll 0.9481 loss -0.9481 (6.198 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 37800 lr 3.435e-04 [train_loss] tar_ll 0.9057 loss -0.9057 (6.572 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 38000 lr 3.420e-04 [train_loss] tar_ll 0.9253 loss -0.9253 (6.108 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 38200 lr 3.406e-04 [train_loss] tar_ll 0.8642 loss -0.8642 (6.134 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 38400 lr 3.391e-04 [train_loss] tar_ll 0.9553 loss -0.9553 (6.208 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 38600 lr 3.376e-04 [train_loss] tar_ll 0.8883 loss -0.8883 (6.135 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 38800 lr 3.362e-04 [train_loss] tar_ll 0.9550 loss -0.9550 (6.202 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 39000 lr 3.347e-04 [train_loss] tar_ll 0.9583 loss -0.9583 (6.165 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 39200 lr 3.332e-04 [train_loss] tar_ll 0.9706 loss -0.9706 (6.250 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 39400 lr 3.317e-04 [train_loss] tar_ll 0.7547 loss -0.7547 (6.170 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 39600 lr 3.302e-04 [train_loss] tar_ll 0.5636 loss -0.5636 (6.039 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 39800 lr 3.287e-04 [train_loss] tar_ll 0.7221 loss -0.7221 (6.138 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 40000 lr 3.273e-04 [train_loss] tar_ll 0.8108 loss -0.8108 (6.169 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 102.50it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.5082 loss -0.5082 (29.270 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 40200 lr 3.258e-04 [train_loss] tar_ll 0.8624 loss -0.8624 (6.296 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 40400 lr 3.243e-04 [train_loss] tar_ll 0.7938 loss -0.7938 (6.626 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 40600 lr 3.228e-04 [train_loss] tar_ll 0.9345 loss -0.9345 (6.457 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 40800 lr 3.213e-04 [train_loss] tar_ll 0.9424 loss -0.9424 (6.359 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 41000 lr 3.197e-04 [train_loss] tar_ll 0.5199 loss -0.5199 (6.451 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 41200 lr 3.182e-04 [train_loss] tar_ll 0.8246 loss -0.8246 (6.244 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 41400 lr 3.167e-04 [train_loss] tar_ll 0.7944 loss -0.7944 (6.406 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 41600 lr 3.152e-04 [train_loss] tar_ll 0.8833 loss -0.8833 (6.355 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 41800 lr 3.137e-04 [train_loss] tar_ll 0.9961 loss -0.9961 (6.190 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 42000 lr 3.122e-04 [train_loss] tar_ll 0.8188 loss -0.8188 (6.355 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 42200 lr 3.106e-04 [train_loss] tar_ll 0.8706 loss -0.8706 (6.498 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 42400 lr 3.091e-04 [train_loss] tar_ll 0.9377 loss -0.9377 (6.578 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 42600 lr 3.076e-04 [train_loss] tar_ll 0.8660 loss -0.8660 (6.428 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 42800 lr 3.061e-04 [train_loss] tar_ll 0.8094 loss -0.8094 (6.563 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 43000 lr 3.045e-04 [train_loss] tar_ll 0.9037 loss -0.9037 (6.513 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 43200 lr 3.030e-04 [train_loss] tar_ll 0.9393 loss -0.9393 (6.208 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 43400 lr 3.015e-04 [train_loss] tar_ll 0.9573 loss -0.9573 (6.117 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 43600 lr 2.999e-04 [train_loss] tar_ll 0.9758 loss -0.9758 (6.110 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 43800 lr 2.984e-04 [train_loss] tar_ll 0.9655 loss -0.9655 (5.974 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 44000 lr 2.968e-04 [train_loss] tar_ll 0.9527 loss -0.9527 (6.301 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 44200 lr 2.953e-04 [train_loss] tar_ll 0.9896 loss -0.9896 (6.041 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 44400 lr 2.938e-04 [train_loss] tar_ll 0.7915 loss -0.7915 (6.066 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 44600 lr 2.922e-04 [train_loss] tar_ll 0.9461 loss -0.9461 (6.056 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 44800 lr 2.907e-04 [train_loss] tar_ll 0.9320 loss -0.9320 (6.072 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 45000 lr 2.891e-04 [train_loss] tar_ll 0.9319 loss -0.9319 (6.042 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:30<00:00, 97.61it/s] \n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.6099 loss -0.6099 (30.736 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 45200 lr 2.876e-04 [train_loss] tar_ll 1.0096 loss -1.0096 (7.042 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 45400 lr 2.860e-04 [train_loss] tar_ll 0.9855 loss -0.9855 (6.773 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 45600 lr 2.844e-04 [train_loss] tar_ll 1.0552 loss -1.0552 (6.335 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 45800 lr 2.829e-04 [train_loss] tar_ll 0.8688 loss -0.8688 (6.161 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 46000 lr 2.813e-04 [train_loss] tar_ll 0.8062 loss -0.8062 (6.272 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 46200 lr 2.798e-04 [train_loss] tar_ll 0.9058 loss -0.9058 (6.177 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 46400 lr 2.782e-04 [train_loss] tar_ll 1.0250 loss -1.0250 (6.145 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 46600 lr 2.767e-04 [train_loss] tar_ll 1.0300 loss -1.0300 (6.248 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 46800 lr 2.751e-04 [train_loss] tar_ll 1.0399 loss -1.0399 (6.172 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 47000 lr 2.735e-04 [train_loss] tar_ll 1.0533 loss -1.0533 (6.404 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 47200 lr 2.720e-04 [train_loss] tar_ll 1.0463 loss -1.0463 (6.523 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 47400 lr 2.704e-04 [train_loss] tar_ll 1.0440 loss -1.0440 (6.134 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 47600 lr 2.688e-04 [train_loss] tar_ll 0.9471 loss -0.9471 (6.227 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 47800 lr 2.673e-04 [train_loss] tar_ll 0.9969 loss -0.9969 (6.143 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 48000 lr 2.657e-04 [train_loss] tar_ll 0.9750 loss -0.9750 (6.506 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 48200 lr 2.641e-04 [train_loss] tar_ll 0.9828 loss -0.9828 (6.258 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 48400 lr 2.626e-04 [train_loss] tar_ll 1.0330 loss -1.0330 (6.118 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 48600 lr 2.610e-04 [train_loss] tar_ll 1.0383 loss -1.0383 (6.179 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 48800 lr 2.594e-04 [train_loss] tar_ll 1.0211 loss -1.0211 (6.137 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 49000 lr 2.579e-04 [train_loss] tar_ll 0.9915 loss -0.9915 (6.071 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 49200 lr 2.563e-04 [train_loss] tar_ll 0.9978 loss -0.9978 (6.764 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 49400 lr 2.547e-04 [train_loss] tar_ll 0.8658 loss -0.8658 (6.935 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 49600 lr 2.531e-04 [train_loss] tar_ll 0.7786 loss -0.7786 (6.560 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 49800 lr 2.516e-04 [train_loss] tar_ll 0.8699 loss -0.8699 (6.040 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 50000 lr 2.500e-04 [train_loss] tar_ll 1.0300 loss -1.0300 (6.085 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 101.83it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.6587 loss -0.6587 (29.461 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 50200 lr 2.484e-04 [train_loss] tar_ll 0.9285 loss -0.9285 (6.400 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 50400 lr 2.469e-04 [train_loss] tar_ll 0.9904 loss -0.9904 (6.265 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 50600 lr 2.453e-04 [train_loss] tar_ll 1.0403 loss -1.0403 (6.296 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 50800 lr 2.437e-04 [train_loss] tar_ll 1.0249 loss -1.0249 (6.269 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 51000 lr 2.421e-04 [train_loss] tar_ll 1.0017 loss -1.0017 (6.195 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 51200 lr 2.406e-04 [train_loss] tar_ll 1.0161 loss -1.0161 (6.413 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 51400 lr 2.390e-04 [train_loss] tar_ll 1.0901 loss -1.0901 (6.381 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 51600 lr 2.374e-04 [train_loss] tar_ll 0.9710 loss -0.9710 (6.331 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 51800 lr 2.359e-04 [train_loss] tar_ll 1.0733 loss -1.0733 (6.403 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 52000 lr 2.343e-04 [train_loss] tar_ll 0.9729 loss -0.9729 (6.461 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 52200 lr 2.327e-04 [train_loss] tar_ll 1.0026 loss -1.0026 (6.477 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 52400 lr 2.312e-04 [train_loss] tar_ll 0.9946 loss -0.9946 (6.618 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 52600 lr 2.296e-04 [train_loss] tar_ll 0.9552 loss -0.9552 (6.285 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 52800 lr 2.280e-04 [train_loss] tar_ll 1.0163 loss -1.0163 (6.059 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 53000 lr 2.265e-04 [train_loss] tar_ll 1.0786 loss -1.0786 (6.082 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 53200 lr 2.249e-04 [train_loss] tar_ll 1.1011 loss -1.1011 (6.191 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 53400 lr 2.233e-04 [train_loss] tar_ll 1.0832 loss -1.0832 (6.005 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 53600 lr 2.218e-04 [train_loss] tar_ll 1.0852 loss -1.0852 (5.900 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 53800 lr 2.202e-04 [train_loss] tar_ll 1.1495 loss -1.1495 (6.300 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 54000 lr 2.187e-04 [train_loss] tar_ll 1.0691 loss -1.0691 (6.024 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 54200 lr 2.171e-04 [train_loss] tar_ll 0.9983 loss -0.9983 (6.172 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 54400 lr 2.156e-04 [train_loss] tar_ll 1.0823 loss -1.0823 (5.919 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 54600 lr 2.140e-04 [train_loss] tar_ll 0.9932 loss -0.9932 (5.909 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 54800 lr 2.124e-04 [train_loss] tar_ll 1.0285 loss -1.0285 (6.200 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 55000 lr 2.109e-04 [train_loss] tar_ll 1.1183 loss -1.1183 (6.436 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:31<00:00, 96.41it/s] \n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.7460 loss -0.7460 (31.119 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 55200 lr 2.093e-04 [train_loss] tar_ll 1.0821 loss -1.0821 (7.091 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 55400 lr 2.078e-04 [train_loss] tar_ll 1.0032 loss -1.0032 (7.066 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 55600 lr 2.062e-04 [train_loss] tar_ll 1.0973 loss -1.0973 (6.437 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 55800 lr 2.047e-04 [train_loss] tar_ll 1.1145 loss -1.1145 (6.284 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 56000 lr 2.032e-04 [train_loss] tar_ll 1.0828 loss -1.0828 (6.041 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 56200 lr 2.016e-04 [train_loss] tar_ll 1.0650 loss -1.0650 (6.242 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 56400 lr 2.001e-04 [train_loss] tar_ll 1.0363 loss -1.0363 (6.119 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 56600 lr 1.985e-04 [train_loss] tar_ll 1.0726 loss -1.0726 (6.815 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0355 loss -1.0355 (7.198 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 57000 lr 1.955e-04 [train_loss] tar_ll 1.1449 loss -1.1449 (7.141 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 57200 lr 1.939e-04 [train_loss] tar_ll 1.1242 loss -1.1242 (6.922 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 57400 lr 1.924e-04 [train_loss] tar_ll 1.1168 loss -1.1168 (6.186 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 57600 lr 1.909e-04 [train_loss] tar_ll 1.0319 loss -1.0319 (6.532 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 57800 lr 1.894e-04 [train_loss] tar_ll 0.9817 loss -0.9817 (6.480 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 58000 lr 1.878e-04 [train_loss] tar_ll 1.1023 loss -1.1023 (6.772 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 58200 lr 1.863e-04 [train_loss] tar_ll 1.1041 loss -1.1041 (6.347 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 58400 lr 1.848e-04 [train_loss] tar_ll 1.1579 loss -1.1579 (6.066 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 58600 lr 1.833e-04 [train_loss] tar_ll 1.2148 loss -1.2148 (6.185 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 58800 lr 1.818e-04 [train_loss] tar_ll 1.1803 loss -1.1803 (6.230 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 59000 lr 1.803e-04 [train_loss] tar_ll 1.1721 loss -1.1721 (6.079 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 59200 lr 1.787e-04 [train_loss] tar_ll 1.1147 loss -1.1147 (6.363 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 59400 lr 1.772e-04 [train_loss] tar_ll 1.1345 loss -1.1345 (6.055 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 59600 lr 1.757e-04 [train_loss] tar_ll 1.1196 loss -1.1196 (6.058 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 59800 lr 1.742e-04 [train_loss] tar_ll 1.0773 loss -1.0773 (6.168 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 60000 lr 1.727e-04 [train_loss] tar_ll 1.1447 loss -1.1447 (6.042 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 100.50it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.7951 loss -0.7951 (29.851 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 60200 lr 1.713e-04 [train_loss] tar_ll 1.1313 loss -1.1313 (6.369 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 60400 lr 1.698e-04 [train_loss] tar_ll 1.1136 loss -1.1136 (6.408 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 60600 lr 1.683e-04 [train_loss] tar_ll 1.1111 loss -1.1111 (6.374 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 60800 lr 1.668e-04 [train_loss] tar_ll 1.2023 loss -1.2023 (6.342 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 61000 lr 1.653e-04 [train_loss] tar_ll 1.1365 loss -1.1365 (6.329 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 61200 lr 1.638e-04 [train_loss] tar_ll 1.1292 loss -1.1292 (6.566 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 61400 lr 1.624e-04 [train_loss] tar_ll 1.1350 loss -1.1350 (6.483 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 61600 lr 1.609e-04 [train_loss] tar_ll 1.1377 loss -1.1377 (6.605 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 61800 lr 1.594e-04 [train_loss] tar_ll 1.0704 loss -1.0704 (6.533 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 62000 lr 1.580e-04 [train_loss] tar_ll 1.1338 loss -1.1338 (6.234 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 62200 lr 1.565e-04 [train_loss] tar_ll 1.1142 loss -1.1142 (6.181 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 62400 lr 1.551e-04 [train_loss] tar_ll 1.1362 loss -1.1362 (6.091 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 62600 lr 1.536e-04 [train_loss] tar_ll 1.1055 loss -1.1055 (6.026 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 62800 lr 1.522e-04 [train_loss] tar_ll 1.1170 loss -1.1170 (6.157 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 63000 lr 1.507e-04 [train_loss] tar_ll 1.1310 loss -1.1310 (5.960 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 63200 lr 1.493e-04 [train_loss] tar_ll 1.1124 loss -1.1124 (6.156 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 63400 lr 1.478e-04 [train_loss] tar_ll 1.1672 loss -1.1672 (6.111 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 63600 lr 1.464e-04 [train_loss] tar_ll 1.1887 loss -1.1887 (6.020 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 63800 lr 1.450e-04 [train_loss] tar_ll 1.0929 loss -1.0929 (6.146 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 64000 lr 1.436e-04 [train_loss] tar_ll 1.1420 loss -1.1420 (6.173 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 64200 lr 1.421e-04 [train_loss] tar_ll 1.1436 loss -1.1436 (6.116 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 64400 lr 1.407e-04 [train_loss] tar_ll 1.2002 loss -1.2002 (6.185 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 64600 lr 1.393e-04 [train_loss] tar_ll 1.1455 loss -1.1455 (6.018 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 64800 lr 1.379e-04 [train_loss] tar_ll 1.1611 loss -1.1611 (6.100 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 65000 lr 1.365e-04 [train_loss] tar_ll 1.2143 loss -1.2143 (6.134 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 101.99it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.8096 loss -0.8096 (29.416 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 65200 lr 1.351e-04 [train_loss] tar_ll 1.1035 loss -1.1035 (6.129 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 65400 lr 1.337e-04 [train_loss] tar_ll 1.2328 loss -1.2328 (6.194 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 65600 lr 1.323e-04 [train_loss] tar_ll 1.0243 loss -1.0243 (6.462 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 65800 lr 1.309e-04 [train_loss] tar_ll 1.0717 loss -1.0717 (6.189 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 66000 lr 1.296e-04 [train_loss] tar_ll 1.1877 loss -1.1877 (6.230 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 66200 lr 1.282e-04 [train_loss] tar_ll 1.0748 loss -1.0748 (6.220 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 66400 lr 1.268e-04 [train_loss] tar_ll 1.2064 loss -1.2064 (6.187 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 66600 lr 1.255e-04 [train_loss] tar_ll 1.2181 loss -1.2181 (6.214 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 66800 lr 1.241e-04 [train_loss] tar_ll 1.1627 loss -1.1627 (6.112 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 67000 lr 1.227e-04 [train_loss] tar_ll 1.2405 loss -1.2405 (6.341 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 67200 lr 1.214e-04 [train_loss] tar_ll 1.2390 loss -1.2390 (6.094 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 67400 lr 1.200e-04 [train_loss] tar_ll 1.1853 loss -1.1853 (6.182 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 67600 lr 1.187e-04 [train_loss] tar_ll 1.1243 loss -1.1243 (6.207 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 67800 lr 1.174e-04 [train_loss] tar_ll 1.1276 loss -1.1276 (6.179 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 68000 lr 1.160e-04 [train_loss] tar_ll 1.2483 loss -1.2483 (6.760 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 68200 lr 1.147e-04 [train_loss] tar_ll 1.2227 loss -1.2227 (6.253 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 68400 lr 1.134e-04 [train_loss] tar_ll 1.1936 loss -1.1936 (6.175 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 68600 lr 1.121e-04 [train_loss] tar_ll 1.1774 loss -1.1774 (6.553 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 68800 lr 1.108e-04 [train_loss] tar_ll 1.1977 loss -1.1977 (6.110 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 69000 lr 1.095e-04 [train_loss] tar_ll 1.1877 loss -1.1877 (6.133 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 69200 lr 1.082e-04 [train_loss] tar_ll 1.1498 loss -1.1498 (6.216 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 69400 lr 1.069e-04 [train_loss] tar_ll 1.2678 loss -1.2678 (6.083 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 69600 lr 1.056e-04 [train_loss] tar_ll 1.2083 loss -1.2083 (6.237 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 69800 lr 1.043e-04 [train_loss] tar_ll 1.2302 loss -1.2302 (6.296 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 70000 lr 1.031e-04 [train_loss] tar_ll 1.2272 loss -1.2272 (6.401 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 102.78it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.8604 loss -0.8604 (29.192 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 70200 lr 1.018e-04 [train_loss] tar_ll 1.2700 loss -1.2700 (6.411 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 70400 lr 1.005e-04 [train_loss] tar_ll 1.1528 loss -1.1528 (6.364 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 70600 lr 9.927e-05 [train_loss] tar_ll 1.1283 loss -1.1283 (6.394 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 70800 lr 9.802e-05 [train_loss] tar_ll 1.1737 loss -1.1737 (6.485 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 71000 lr 9.677e-05 [train_loss] tar_ll 1.2078 loss -1.2078 (6.412 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 71200 lr 9.554e-05 [train_loss] tar_ll 1.2516 loss -1.2516 (6.539 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 71400 lr 9.430e-05 [train_loss] tar_ll 1.1890 loss -1.1890 (6.482 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 71600 lr 9.308e-05 [train_loss] tar_ll 1.2330 loss -1.2330 (6.365 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 71800 lr 9.186e-05 [train_loss] tar_ll 1.1139 loss -1.1139 (6.034 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 72000 lr 9.064e-05 [train_loss] tar_ll 1.2467 loss -1.2467 (6.061 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 72200 lr 8.944e-05 [train_loss] tar_ll 1.2342 loss -1.2342 (6.201 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 72400 lr 8.824e-05 [train_loss] tar_ll 1.1189 loss -1.1189 (6.042 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 72600 lr 8.704e-05 [train_loss] tar_ll 1.3135 loss -1.3135 (5.980 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 72800 lr 8.585e-05 [train_loss] tar_ll 1.2330 loss -1.2330 (6.393 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 73000 lr 8.467e-05 [train_loss] tar_ll 1.1772 loss -1.1772 (5.956 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 73200 lr 8.350e-05 [train_loss] tar_ll 1.2102 loss -1.2102 (6.149 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 73400 lr 8.233e-05 [train_loss] tar_ll 1.2057 loss -1.2057 (5.948 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 73600 lr 8.117e-05 [train_loss] tar_ll 1.2562 loss -1.2562 (5.914 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 73800 lr 8.001e-05 [train_loss] tar_ll 1.2174 loss -1.2174 (6.040 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 74000 lr 7.886e-05 [train_loss] tar_ll 1.2855 loss -1.2855 (5.984 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 74200 lr 7.772e-05 [train_loss] tar_ll 1.2023 loss -1.2023 (5.991 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 74400 lr 7.659e-05 [train_loss] tar_ll 1.2302 loss -1.2302 (6.082 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 74600 lr 7.546e-05 [train_loss] tar_ll 1.2770 loss -1.2770 (5.995 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 74800 lr 7.434e-05 [train_loss] tar_ll 1.2189 loss -1.2189 (6.038 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1895 loss -1.1895 (5.942 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 100.50it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.8536 loss -0.8536 (29.850 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 75200 lr 7.212e-05 [train_loss] tar_ll 1.2292 loss -1.2292 (6.231 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 75400 lr 7.102e-05 [train_loss] tar_ll 1.2882 loss -1.2882 (6.325 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 75600 lr 6.992e-05 [train_loss] tar_ll 1.2827 loss -1.2827 (6.465 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 75800 lr 6.884e-05 [train_loss] tar_ll 1.2414 loss -1.2414 (6.465 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 76000 lr 6.776e-05 [train_loss] tar_ll 1.2075 loss -1.2075 (6.194 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 76200 lr 6.669e-05 [train_loss] tar_ll 1.2431 loss -1.2431 (6.025 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 76400 lr 6.562e-05 [train_loss] tar_ll 1.3104 loss -1.3104 (6.112 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 76600 lr 6.456e-05 [train_loss] tar_ll 1.2791 loss -1.2791 (6.119 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 76800 lr 6.351e-05 [train_loss] tar_ll 1.3423 loss -1.3423 (6.060 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 77000 lr 6.247e-05 [train_loss] tar_ll 1.2162 loss -1.2162 (6.250 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 77200 lr 6.144e-05 [train_loss] tar_ll 1.2171 loss -1.2171 (6.441 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 77400 lr 6.041e-05 [train_loss] tar_ll 1.2868 loss -1.2868 (6.373 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 77600 lr 5.939e-05 [train_loss] tar_ll 1.2988 loss -1.2988 (6.109 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2429 loss -1.2429 (6.088 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 78000 lr 5.737e-05 [train_loss] tar_ll 1.1971 loss -1.1971 (6.461 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 78200 lr 5.637e-05 [train_loss] tar_ll 1.2357 loss -1.2357 (6.085 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 78400 lr 5.538e-05 [train_loss] tar_ll 1.3042 loss -1.3042 (6.154 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 78600 lr 5.440e-05 [train_loss] tar_ll 1.2449 loss -1.2449 (6.117 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 78800 lr 5.343e-05 [train_loss] tar_ll 1.1254 loss -1.1254 (5.984 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 79000 lr 5.246e-05 [train_loss] tar_ll 1.2667 loss -1.2667 (6.123 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 79200 lr 5.150e-05 [train_loss] tar_ll 1.3320 loss -1.3320 (6.070 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 79400 lr 5.055e-05 [train_loss] tar_ll 1.1756 loss -1.1756 (6.066 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 79600 lr 4.961e-05 [train_loss] tar_ll 1.2378 loss -1.2378 (6.333 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 79800 lr 4.867e-05 [train_loss] tar_ll 1.1964 loss -1.1964 (6.254 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 80000 lr 4.775e-05 [train_loss] tar_ll 1.2145 loss -1.2145 (6.278 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 105.11it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.8672 loss -0.8672 (28.545 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 80200 lr 4.683e-05 [train_loss] tar_ll 1.3272 loss -1.3272 (6.133 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 80400 lr 4.592e-05 [train_loss] tar_ll 1.2735 loss -1.2735 (6.026 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1828 loss -1.1828 (6.107 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2278 loss -1.2278 (5.983 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 81000 lr 4.323e-05 [train_loss] tar_ll 1.3275 loss -1.3275 (5.964 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 81200 lr 4.235e-05 [train_loss] tar_ll 1.2473 loss -1.2473 (6.193 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 81400 lr 4.148e-05 [train_loss] tar_ll 1.2985 loss -1.2985 (6.025 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 81600 lr 4.062e-05 [train_loss] tar_ll 1.2876 loss -1.2876 (6.132 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 81800 lr 3.976e-05 [train_loss] tar_ll 1.1849 loss -1.1849 (6.118 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 82000 lr 3.892e-05 [train_loss] tar_ll 1.2201 loss -1.2201 (6.062 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 82200 lr 3.808e-05 [train_loss] tar_ll 1.2188 loss -1.2188 (6.198 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 82400 lr 3.725e-05 [train_loss] tar_ll 1.3384 loss -1.3384 (5.974 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 82600 lr 3.643e-05 [train_loss] tar_ll 1.2661 loss -1.2661 (6.068 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 82800 lr 3.562e-05 [train_loss] tar_ll 1.3168 loss -1.3168 (6.134 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 83000 lr 3.481e-05 [train_loss] tar_ll 1.3334 loss -1.3334 (5.965 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 83200 lr 3.402e-05 [train_loss] tar_ll 1.2597 loss -1.2597 (5.992 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 83400 lr 3.323e-05 [train_loss] tar_ll 1.2935 loss -1.2935 (6.056 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 83600 lr 3.245e-05 [train_loss] tar_ll 1.2973 loss -1.2973 (6.014 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 83800 lr 3.168e-05 [train_loss] tar_ll 1.3107 loss -1.3107 (6.296 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 84000 lr 3.092e-05 [train_loss] tar_ll 1.2331 loss -1.2331 (6.182 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2902 loss -1.2902 (6.391 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 84400 lr 2.943e-05 [train_loss] tar_ll 1.2658 loss -1.2658 (6.233 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 84600 lr 2.869e-05 [train_loss] tar_ll 1.3219 loss -1.3219 (6.009 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 84800 lr 2.797e-05 [train_loss] tar_ll 1.2359 loss -1.2359 (6.168 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 85000 lr 2.725e-05 [train_loss] tar_ll 1.2673 loss -1.2673 (6.134 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 102.00it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.8975 loss -0.8975 (29.415 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 85200 lr 2.654e-05 [train_loss] tar_ll 1.2505 loss -1.2505 (6.060 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 85400 lr 2.584e-05 [train_loss] tar_ll 1.2854 loss -1.2854 (6.271 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 85600 lr 2.515e-05 [train_loss] tar_ll 1.3680 loss -1.3680 (6.277 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 85800 lr 2.447e-05 [train_loss] tar_ll 1.2042 loss -1.2042 (6.158 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 86000 lr 2.379e-05 [train_loss] tar_ll 1.2492 loss -1.2492 (6.217 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 86200 lr 2.313e-05 [train_loss] tar_ll 1.2810 loss -1.2810 (6.097 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 86400 lr 2.247e-05 [train_loss] tar_ll 1.2264 loss -1.2264 (6.172 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 86600 lr 2.183e-05 [train_loss] tar_ll 1.2166 loss -1.2166 (6.515 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 86800 lr 2.119e-05 [train_loss] tar_ll 1.2620 loss -1.2620 (6.553 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 87000 lr 2.056e-05 [train_loss] tar_ll 1.2852 loss -1.2852 (6.602 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 87200 lr 1.994e-05 [train_loss] tar_ll 1.3196 loss -1.3196 (6.653 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 87400 lr 1.933e-05 [train_loss] tar_ll 1.2449 loss -1.2449 (6.672 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 87600 lr 1.873e-05 [train_loss] tar_ll 1.3237 loss -1.3237 (6.018 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 87800 lr 1.814e-05 [train_loss] tar_ll 1.2975 loss -1.2975 (6.182 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 88000 lr 1.756e-05 [train_loss] tar_ll 1.2558 loss -1.2558 (6.227 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 88200 lr 1.698e-05 [train_loss] tar_ll 1.3039 loss -1.3039 (6.205 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 88400 lr 1.642e-05 [train_loss] tar_ll 1.2689 loss -1.2689 (5.959 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 88600 lr 1.586e-05 [train_loss] tar_ll 1.2275 loss -1.2275 (6.078 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 88800 lr 1.532e-05 [train_loss] tar_ll 1.2691 loss -1.2691 (5.961 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 89000 lr 1.478e-05 [train_loss] tar_ll 1.3465 loss -1.3465 (6.141 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 89200 lr 1.425e-05 [train_loss] tar_ll 1.2808 loss -1.2808 (6.224 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 89400 lr 1.373e-05 [train_loss] tar_ll 1.3255 loss -1.3255 (6.302 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 89600 lr 1.323e-05 [train_loss] tar_ll 1.3247 loss -1.3247 (6.337 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2621 loss -1.2621 (6.228 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 90000 lr 1.224e-05 [train_loss] tar_ll 1.2463 loss -1.2463 (6.283 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 100.80it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.8933 loss -0.8933 (29.763 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 90200 lr 1.176e-05 [train_loss] tar_ll 1.3010 loss -1.3010 (6.588 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 90400 lr 1.128e-05 [train_loss] tar_ll 1.2701 loss -1.2701 (6.545 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 90600 lr 1.082e-05 [train_loss] tar_ll 1.3592 loss -1.3592 (6.404 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 90800 lr 1.037e-05 [train_loss] tar_ll 1.4447 loss -1.4447 (6.376 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 91000 lr 9.927e-06 [train_loss] tar_ll 1.3527 loss -1.3527 (6.340 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 91200 lr 9.493e-06 [train_loss] tar_ll 1.3320 loss -1.3320 (6.147 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2796 loss -1.2796 (6.056 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 91600 lr 8.655e-06 [train_loss] tar_ll 1.1905 loss -1.1905 (6.121 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2430 loss -1.2430 (5.989 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 92000 lr 7.854e-06 [train_loss] tar_ll 1.3249 loss -1.3249 (5.936 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 92200 lr 7.468e-06 [train_loss] tar_ll 1.3270 loss -1.3270 (6.176 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 92400 lr 7.092e-06 [train_loss] tar_ll 1.2894 loss -1.2894 (6.030 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 92600 lr 6.725e-06 [train_loss] tar_ll 1.2878 loss -1.2878 (6.039 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 92800 lr 6.368e-06 [train_loss] tar_ll 1.3156 loss -1.3156 (5.983 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 93000 lr 6.021e-06 [train_loss] tar_ll 1.3082 loss -1.3082 (6.096 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 93200 lr 5.683e-06 [train_loss] tar_ll 1.3206 loss -1.3206 (6.153 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 93400 lr 5.355e-06 [train_loss] tar_ll 1.2965 loss -1.2965 (6.369 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 93600 lr 5.036e-06 [train_loss] tar_ll 1.3416 loss -1.3416 (6.302 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 93800 lr 4.727e-06 [train_loss] tar_ll 1.2869 loss -1.2869 (6.435 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 94000 lr 4.428e-06 [train_loss] tar_ll 1.3306 loss -1.3306 (6.780 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 94200 lr 4.139e-06 [train_loss] tar_ll 1.3171 loss -1.3171 (6.548 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 94400 lr 3.859e-06 [train_loss] tar_ll 1.2502 loss -1.2502 (6.795 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 94600 lr 3.589e-06 [train_loss] tar_ll 1.3424 loss -1.3424 (6.465 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 94800 lr 3.329e-06 [train_loss] tar_ll 1.3087 loss -1.3087 (6.469 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 95000 lr 3.078e-06 [train_loss] tar_ll 1.2394 loss -1.2394 (6.125 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 103.85it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.9019 loss -0.9019 (28.889 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_matern step 95200 lr 2.837e-06 [train_loss] tar_ll 1.3374 loss -1.3374 (6.615 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 95400 lr 2.606e-06 [train_loss] tar_ll 1.2534 loss -1.2534 (6.133 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 95600 lr 2.385e-06 [train_loss] tar_ll 1.2872 loss -1.2872 (6.210 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 95800 lr 2.173e-06 [train_loss] tar_ll 1.3320 loss -1.3320 (6.269 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 96000 lr 1.971e-06 [train_loss] tar_ll 1.2534 loss -1.2534 (6.161 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2747 loss -1.2747 (6.201 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 96400 lr 1.597e-06 [train_loss] tar_ll 1.3221 loss -1.3221 (6.132 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 96600 lr 1.425e-06 [train_loss] tar_ll 1.2838 loss -1.2838 (6.108 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 96800 lr 1.262e-06 [train_loss] tar_ll 1.3072 loss -1.3072 (6.150 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 97000 lr 1.110e-06 [train_loss] tar_ll 1.3240 loss -1.3240 (6.009 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 97200 lr 9.666e-07 [train_loss] tar_ll 1.3215 loss -1.3215 (6.139 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 97400 lr 8.335e-07 [train_loss] tar_ll 1.2818 loss -1.2818 (6.168 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 97600 lr 7.103e-07 [train_loss] tar_ll 1.2756 loss -1.2756 (6.076 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 97800 lr 5.969e-07 [train_loss] tar_ll 1.2387 loss -1.2387 (6.209 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 98000 lr 4.933e-07 [train_loss] tar_ll 1.3474 loss -1.3474 (6.524 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 98200 lr 3.996e-07 [train_loss] tar_ll 1.3291 loss -1.3291 (6.149 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 98400 lr 3.158e-07 [train_loss] tar_ll 1.2797 loss -1.2797 (6.143 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 98600 lr 2.418e-07 [train_loss] tar_ll 1.2993 loss -1.2993 (6.078 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 98800 lr 1.776e-07 [train_loss] tar_ll 1.3343 loss -1.3343 (6.443 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 99000 lr 1.234e-07 [train_loss] tar_ll 1.2531 loss -1.2531 (6.678 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 99200 lr 7.895e-08 [train_loss] tar_ll 1.3024 loss -1.3024 (6.576 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2508 loss -1.2508 (6.565 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 99600 lr 1.974e-08 [train_loss] tar_ll 1.3723 loss -1.3723 (6.265 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 99800 lr 4.935e-09 [train_loss] tar_ll 1.2329 loss -1.2329 (6.389 secs)\n",
      "lbanp:lbanp-num_latents-128_matern step 100000 lr 0.000e+00 [train_loss] tar_ll 1.3589 loss -1.3589 (6.341 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 105.05it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.8984 loss -0.8984 (28.560 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 105.33it/s]\n",
      "lbanp:lbanp-num_latents-128_matern matern tar_ll 0.8984 loss -0.8984 (28.483 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "Execution time: 3741813.5 miliseconds\n",
      "Execution time: 3741.8135 seconds\n",
      "Initial Memory Usage: 19.80224609375 MB\n",
      "Final Memory Usage: 181.14453125 MB\n",
      "Memory Usage Change: 161.34228515625 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='lbanp', name='lbanp-num_latents-128_matern',val_seed=100, val_l=128,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69551945-b04f-4236-8f72-a16b5d07a139",
   "metadata": {},
   "source": [
    "## TNP-ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7338f-525d-4f83-9142-f34e36c9736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mod(modelo='tnpnd', name='tnpnd_matern',val_seed=100, val_l=None,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3207db3f-fab1-4b73-ace6-ba85e52aec4f",
   "metadata": {},
   "source": [
    "## TNP-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c647030a-7c57-4ebd-ab1f-5e24e89953ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: tnpa-tnpa_matern\n",
      "Total number of parameters: 222082\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument pretrain: False\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tnpa:tnpa_matern step 200 lr 5.000e-04 [train_loss] tar_ll -0.5830 loss 0.5830 (5.250 secs)\n",
      "tnpa:tnpa_matern step 400 lr 5.000e-04 [train_loss] tar_ll -0.1047 loss 0.1047 (5.940 secs)\n",
      "tnpa:tnpa_matern step 600 lr 5.000e-04 [train_loss] tar_ll 0.2404 loss -0.2404 (5.405 secs)\n",
      "tnpa:tnpa_matern step 800 lr 4.999e-04 [train_loss] tar_ll 0.2350 loss -0.2350 (5.402 secs)\n",
      "tnpa:tnpa_matern step 1000 lr 4.999e-04 [train_loss] tar_ll 0.5046 loss -0.5046 (5.236 secs)\n",
      "tnpa:tnpa_matern step 1200 lr 4.998e-04 [train_loss] tar_ll 0.5625 loss -0.5625 (5.200 secs)\n",
      "tnpa:tnpa_matern step 1400 lr 4.998e-04 [train_loss] tar_ll 0.5878 loss -0.5878 (5.356 secs)\n",
      "tnpa:tnpa_matern step 1600 lr 4.997e-04 [train_loss] tar_ll 0.7312 loss -0.7312 (5.293 secs)\n",
      "tnpa:tnpa_matern step 1800 lr 4.996e-04 [train_loss] tar_ll 0.8374 loss -0.8374 (5.507 secs)\n",
      "tnpa:tnpa_matern step 2000 lr 4.995e-04 [train_loss] tar_ll 0.8417 loss -0.8417 (5.333 secs)\n",
      "tnpa:tnpa_matern step 2200 lr 4.994e-04 [train_loss] tar_ll 1.0055 loss -1.0055 (5.259 secs)\n",
      "tnpa:tnpa_matern step 2400 lr 4.993e-04 [train_loss] tar_ll 0.9807 loss -0.9807 (5.453 secs)\n",
      "tnpa:tnpa_matern step 2600 lr 4.992e-04 [train_loss] tar_ll 0.9998 loss -0.9998 (5.247 secs)\n",
      "tnpa:tnpa_matern step 2800 lr 4.990e-04 [train_loss] tar_ll 1.0506 loss -1.0506 (5.321 secs)\n",
      "tnpa:tnpa_matern step 3000 lr 4.989e-04 [train_loss] tar_ll 1.0623 loss -1.0623 (5.383 secs)\n",
      "tnpa:tnpa_matern step 3200 lr 4.987e-04 [train_loss] tar_ll 1.0426 loss -1.0426 (5.303 secs)\n",
      "tnpa:tnpa_matern step 3400 lr 4.986e-04 [train_loss] tar_ll 1.1235 loss -1.1235 (5.372 secs)\n",
      "tnpa:tnpa_matern step 3600 lr 4.984e-04 [train_loss] tar_ll 1.1197 loss -1.1197 (5.547 secs)\n",
      "tnpa:tnpa_matern step 3800 lr 4.982e-04 [train_loss] tar_ll 1.1196 loss -1.1196 (5.418 secs)\n",
      "tnpa:tnpa_matern step 4000 lr 4.980e-04 [train_loss] tar_ll 0.9826 loss -0.9826 (5.498 secs)\n",
      "tnpa:tnpa_matern step 4200 lr 4.978e-04 [train_loss] tar_ll 1.0819 loss -1.0819 (5.440 secs)\n",
      "tnpa:tnpa_matern step 4400 lr 4.976e-04 [train_loss] tar_ll 1.0673 loss -1.0673 (5.310 secs)\n",
      "tnpa:tnpa_matern step 4600 lr 4.974e-04 [train_loss] tar_ll 1.1125 loss -1.1125 (5.347 secs)\n",
      "tnpa:tnpa_matern step 4800 lr 4.972e-04 [train_loss] tar_ll 1.2213 loss -1.2213 (5.423 secs)\n",
      "tnpa:tnpa_matern step 5000 lr 4.969e-04 [train_loss] tar_ll 1.1614 loss -1.1614 (5.396 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 176.69it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 0.9260 loss -0.9260 (16.981 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 5200 lr 4.967e-04 [train_loss] tar_ll 1.1799 loss -1.1799 (5.320 secs)\n",
      "tnpa:tnpa_matern step 5400 lr 4.964e-04 [train_loss] tar_ll 1.1658 loss -1.1658 (5.189 secs)\n",
      "tnpa:tnpa_matern step 5600 lr 4.961e-04 [train_loss] tar_ll 1.2176 loss -1.2176 (5.202 secs)\n",
      "tnpa:tnpa_matern step 5800 lr 4.959e-04 [train_loss] tar_ll 1.2590 loss -1.2590 (5.183 secs)\n",
      "tnpa:tnpa_matern step 6000 lr 4.956e-04 [train_loss] tar_ll 1.2258 loss -1.2258 (5.153 secs)\n",
      "tnpa:tnpa_matern step 6200 lr 4.953e-04 [train_loss] tar_ll 1.1836 loss -1.1836 (5.270 secs)\n",
      "tnpa:tnpa_matern step 6400 lr 4.950e-04 [train_loss] tar_ll 1.1498 loss -1.1498 (5.090 secs)\n",
      "tnpa:tnpa_matern step 6600 lr 4.946e-04 [train_loss] tar_ll 1.2264 loss -1.2264 (5.287 secs)\n",
      "tnpa:tnpa_matern step 6800 lr 4.943e-04 [train_loss] tar_ll 1.2783 loss -1.2783 (5.206 secs)\n",
      "tnpa:tnpa_matern step 7000 lr 4.940e-04 [train_loss] tar_ll 1.2183 loss -1.2183 (5.346 secs)\n",
      "tnpa:tnpa_matern step 7200 lr 4.936e-04 [train_loss] tar_ll 1.1795 loss -1.1795 (5.408 secs)\n",
      "tnpa:tnpa_matern step 7400 lr 4.933e-04 [train_loss] tar_ll 1.1335 loss -1.1335 (5.229 secs)\n",
      "tnpa:tnpa_matern step 7600 lr 4.929e-04 [train_loss] tar_ll 1.1624 loss -1.1624 (5.275 secs)\n",
      "tnpa:tnpa_matern step 7800 lr 4.925e-04 [train_loss] tar_ll 1.2181 loss -1.2181 (5.220 secs)\n",
      "tnpa:tnpa_matern step 8000 lr 4.921e-04 [train_loss] tar_ll 1.2336 loss -1.2336 (5.168 secs)\n",
      "tnpa:tnpa_matern step 8200 lr 4.918e-04 [train_loss] tar_ll 1.2903 loss -1.2903 (5.409 secs)\n",
      "tnpa:tnpa_matern step 8400 lr 4.913e-04 [train_loss] tar_ll 1.2960 loss -1.2960 (5.265 secs)\n",
      "tnpa:tnpa_matern step 8600 lr 4.909e-04 [train_loss] tar_ll 1.2555 loss -1.2555 (5.351 secs)\n",
      "tnpa:tnpa_matern step 8800 lr 4.905e-04 [train_loss] tar_ll 1.3264 loss -1.3264 (5.242 secs)\n",
      "tnpa:tnpa_matern step 9000 lr 4.901e-04 [train_loss] tar_ll 1.2352 loss -1.2352 (5.280 secs)\n",
      "tnpa:tnpa_matern step 9200 lr 4.896e-04 [train_loss] tar_ll 1.1618 loss -1.1618 (5.210 secs)\n",
      "tnpa:tnpa_matern step 9400 lr 4.892e-04 [train_loss] tar_ll 1.2655 loss -1.2655 (5.171 secs)\n",
      "tnpa:tnpa_matern step 9600 lr 4.887e-04 [train_loss] tar_ll 1.2369 loss -1.2369 (5.315 secs)\n",
      "tnpa:tnpa_matern step 9800 lr 4.882e-04 [train_loss] tar_ll 1.3125 loss -1.3125 (5.266 secs)\n",
      "tnpa:tnpa_matern step 10000 lr 4.878e-04 [train_loss] tar_ll 1.2965 loss -1.2965 (5.258 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 180.86it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 0.9832 loss -0.9832 (16.588 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 10200 lr 4.873e-04 [train_loss] tar_ll 1.2809 loss -1.2809 (5.426 secs)\n",
      "tnpa:tnpa_matern step 10400 lr 4.868e-04 [train_loss] tar_ll 1.2676 loss -1.2676 (5.276 secs)\n",
      "tnpa:tnpa_matern step 10600 lr 4.863e-04 [train_loss] tar_ll 1.2499 loss -1.2499 (5.461 secs)\n",
      "tnpa:tnpa_matern step 10800 lr 4.857e-04 [train_loss] tar_ll 1.2745 loss -1.2745 (5.378 secs)\n",
      "tnpa:tnpa_matern step 11000 lr 4.852e-04 [train_loss] tar_ll 1.2850 loss -1.2850 (5.294 secs)\n",
      "tnpa:tnpa_matern step 11200 lr 4.847e-04 [train_loss] tar_ll 1.3283 loss -1.3283 (5.314 secs)\n",
      "tnpa:tnpa_matern step 11400 lr 4.841e-04 [train_loss] tar_ll 1.3280 loss -1.3280 (5.267 secs)\n",
      "tnpa:tnpa_matern step 11600 lr 4.836e-04 [train_loss] tar_ll 1.2362 loss -1.2362 (5.323 secs)\n",
      "tnpa:tnpa_matern step 11800 lr 4.830e-04 [train_loss] tar_ll 1.3447 loss -1.3447 (5.458 secs)\n",
      "tnpa:tnpa_matern step 12000 lr 4.824e-04 [train_loss] tar_ll 1.2895 loss -1.2895 (5.436 secs)\n",
      "tnpa:tnpa_matern step 12200 lr 4.819e-04 [train_loss] tar_ll 1.3242 loss -1.3242 (5.229 secs)\n",
      "tnpa:tnpa_matern step 12400 lr 4.813e-04 [train_loss] tar_ll 1.3185 loss -1.3185 (5.447 secs)\n",
      "tnpa:tnpa_matern step 12600 lr 4.807e-04 [train_loss] tar_ll 1.2880 loss -1.2880 (5.195 secs)\n",
      "tnpa:tnpa_matern step 12800 lr 4.801e-04 [train_loss] tar_ll 1.2932 loss -1.2932 (5.426 secs)\n",
      "tnpa:tnpa_matern step 13000 lr 4.794e-04 [train_loss] tar_ll 1.3177 loss -1.3177 (5.467 secs)\n",
      "tnpa:tnpa_matern step 13200 lr 4.788e-04 [train_loss] tar_ll 1.3129 loss -1.3129 (5.265 secs)\n",
      "tnpa:tnpa_matern step 13400 lr 4.782e-04 [train_loss] tar_ll 1.3583 loss -1.3583 (5.285 secs)\n",
      "tnpa:tnpa_matern step 13600 lr 4.775e-04 [train_loss] tar_ll 1.3875 loss -1.3875 (5.292 secs)\n",
      "tnpa:tnpa_matern step 13800 lr 4.769e-04 [train_loss] tar_ll 1.3839 loss -1.3839 (5.219 secs)\n",
      "tnpa:tnpa_matern step 14000 lr 4.762e-04 [train_loss] tar_ll 1.2996 loss -1.2996 (5.238 secs)\n",
      "tnpa:tnpa_matern step 14200 lr 4.755e-04 [train_loss] tar_ll 1.3952 loss -1.3952 (5.410 secs)\n",
      "tnpa:tnpa_matern step 14400 lr 4.749e-04 [train_loss] tar_ll 1.3364 loss -1.3364 (5.375 secs)\n",
      "tnpa:tnpa_matern step 14600 lr 4.742e-04 [train_loss] tar_ll 1.3120 loss -1.3120 (5.247 secs)\n",
      "tnpa:tnpa_matern step 14800 lr 4.735e-04 [train_loss] tar_ll 1.3220 loss -1.3220 (5.439 secs)\n",
      "tnpa:tnpa_matern step 15000 lr 4.728e-04 [train_loss] tar_ll 1.3336 loss -1.3336 (5.364 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 182.97it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 0.8426 loss -0.8426 (16.400 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 15200 lr 4.720e-04 [train_loss] tar_ll 1.3754 loss -1.3754 (5.371 secs)\n",
      "tnpa:tnpa_matern step 15400 lr 4.713e-04 [train_loss] tar_ll 1.3565 loss -1.3565 (5.468 secs)\n",
      "tnpa:tnpa_matern step 15600 lr 4.706e-04 [train_loss] tar_ll 1.3692 loss -1.3692 (5.204 secs)\n",
      "tnpa:tnpa_matern step 15800 lr 4.698e-04 [train_loss] tar_ll 1.3611 loss -1.3611 (5.324 secs)\n",
      "tnpa:tnpa_matern step 16000 lr 4.691e-04 [train_loss] tar_ll 1.3994 loss -1.3994 (5.356 secs)\n",
      "tnpa:tnpa_matern step 16200 lr 4.683e-04 [train_loss] tar_ll 1.3186 loss -1.3186 (5.388 secs)\n",
      "tnpa:tnpa_matern step 16400 lr 4.675e-04 [train_loss] tar_ll 1.3328 loss -1.3328 (5.372 secs)\n",
      "tnpa:tnpa_matern step 16600 lr 4.668e-04 [train_loss] tar_ll 1.2578 loss -1.2578 (5.516 secs)\n",
      "tnpa:tnpa_matern step 16800 lr 4.660e-04 [train_loss] tar_ll 1.3367 loss -1.3367 (5.436 secs)\n",
      "tnpa:tnpa_matern step 17000 lr 4.652e-04 [train_loss] tar_ll 1.4286 loss -1.4286 (5.362 secs)\n",
      "tnpa:tnpa_matern step 17200 lr 4.644e-04 [train_loss] tar_ll 1.3489 loss -1.3489 (5.565 secs)\n",
      "tnpa:tnpa_matern step 17400 lr 4.636e-04 [train_loss] tar_ll 1.3857 loss -1.3857 (5.469 secs)\n",
      "tnpa:tnpa_matern step 17600 lr 4.627e-04 [train_loss] tar_ll 1.2328 loss -1.2328 (5.294 secs)\n",
      "tnpa:tnpa_matern step 17800 lr 4.619e-04 [train_loss] tar_ll 1.2290 loss -1.2290 (5.577 secs)\n",
      "tnpa:tnpa_matern step 18000 lr 4.611e-04 [train_loss] tar_ll 1.3546 loss -1.3546 (5.292 secs)\n",
      "tnpa:tnpa_matern step 18200 lr 4.602e-04 [train_loss] tar_ll 1.3288 loss -1.3288 (5.474 secs)\n",
      "tnpa:tnpa_matern step 18400 lr 4.594e-04 [train_loss] tar_ll 1.3994 loss -1.3994 (5.609 secs)\n",
      "tnpa:tnpa_matern step 18600 lr 4.585e-04 [train_loss] tar_ll 1.3367 loss -1.3367 (5.468 secs)\n",
      "tnpa:tnpa_matern step 18800 lr 4.576e-04 [train_loss] tar_ll 1.3490 loss -1.3490 (5.441 secs)\n",
      "tnpa:tnpa_matern step 19000 lr 4.568e-04 [train_loss] tar_ll 1.3981 loss -1.3981 (5.572 secs)\n",
      "tnpa:tnpa_matern step 19200 lr 4.559e-04 [train_loss] tar_ll 1.3103 loss -1.3103 (5.498 secs)\n",
      "tnpa:tnpa_matern step 19400 lr 4.550e-04 [train_loss] tar_ll 1.3904 loss -1.3904 (5.480 secs)\n",
      "tnpa:tnpa_matern step 19600 lr 4.541e-04 [train_loss] tar_ll 1.3484 loss -1.3484 (5.350 secs)\n",
      "tnpa:tnpa_matern step 19800 lr 4.532e-04 [train_loss] tar_ll 1.4218 loss -1.4218 (5.447 secs)\n",
      "tnpa:tnpa_matern step 20000 lr 4.523e-04 [train_loss] tar_ll 1.4109 loss -1.4109 (5.426 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 178.70it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.0633 loss -1.0633 (16.790 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 20200 lr 4.513e-04 [train_loss] tar_ll 1.3170 loss -1.3170 (5.376 secs)\n",
      "tnpa:tnpa_matern step 20400 lr 4.504e-04 [train_loss] tar_ll 1.3950 loss -1.3950 (5.306 secs)\n",
      "tnpa:tnpa_matern step 20600 lr 4.494e-04 [train_loss] tar_ll 1.3822 loss -1.3822 (5.515 secs)\n",
      "tnpa:tnpa_matern step 20800 lr 4.485e-04 [train_loss] tar_ll 1.4187 loss -1.4187 (5.287 secs)\n",
      "tnpa:tnpa_matern step 21000 lr 4.475e-04 [train_loss] tar_ll 1.4086 loss -1.4086 (5.390 secs)\n",
      "tnpa:tnpa_matern step 21200 lr 4.466e-04 [train_loss] tar_ll 1.3591 loss -1.3591 (5.375 secs)\n",
      "tnpa:tnpa_matern step 21400 lr 4.456e-04 [train_loss] tar_ll 1.3679 loss -1.3679 (5.310 secs)\n",
      "tnpa:tnpa_matern step 21600 lr 4.446e-04 [train_loss] tar_ll 1.3519 loss -1.3519 (5.338 secs)\n",
      "tnpa:tnpa_matern step 21800 lr 4.436e-04 [train_loss] tar_ll 1.3934 loss -1.3934 (5.333 secs)\n",
      "tnpa:tnpa_matern step 22000 lr 4.426e-04 [train_loss] tar_ll 1.3870 loss -1.3870 (5.196 secs)\n",
      "tnpa:tnpa_matern step 22200 lr 4.416e-04 [train_loss] tar_ll 1.4184 loss -1.4184 (5.077 secs)\n",
      "tnpa:tnpa_matern step 22400 lr 4.406e-04 [train_loss] tar_ll 1.4055 loss -1.4055 (5.365 secs)\n",
      "tnpa:tnpa_matern step 22600 lr 4.396e-04 [train_loss] tar_ll 1.3110 loss -1.3110 (5.237 secs)\n",
      "tnpa:tnpa_matern step 22800 lr 4.386e-04 [train_loss] tar_ll 1.3919 loss -1.3919 (5.391 secs)\n",
      "tnpa:tnpa_matern step 23000 lr 4.375e-04 [train_loss] tar_ll 1.3785 loss -1.3785 (5.277 secs)\n",
      "tnpa:tnpa_matern step 23200 lr 4.365e-04 [train_loss] tar_ll 1.3162 loss -1.3162 (5.228 secs)\n",
      "tnpa:tnpa_matern step 23400 lr 4.354e-04 [train_loss] tar_ll 1.3322 loss -1.3322 (5.189 secs)\n",
      "tnpa:tnpa_matern step 23600 lr 4.344e-04 [train_loss] tar_ll 1.3585 loss -1.3585 (5.381 secs)\n",
      "tnpa:tnpa_matern step 23800 lr 4.333e-04 [train_loss] tar_ll 1.3992 loss -1.3992 (5.235 secs)\n",
      "tnpa:tnpa_matern step 24000 lr 4.322e-04 [train_loss] tar_ll 1.4409 loss -1.4409 (5.222 secs)\n",
      "tnpa:tnpa_matern step 24200 lr 4.312e-04 [train_loss] tar_ll 1.4319 loss -1.4319 (5.270 secs)\n",
      "tnpa:tnpa_matern step 24400 lr 4.301e-04 [train_loss] tar_ll 1.3750 loss -1.3750 (5.294 secs)\n",
      "tnpa:tnpa_matern step 24600 lr 4.290e-04 [train_loss] tar_ll 1.4447 loss -1.4447 (5.111 secs)\n",
      "tnpa:tnpa_matern step 24800 lr 4.279e-04 [train_loss] tar_ll 1.2738 loss -1.2738 (5.197 secs)\n",
      "tnpa:tnpa_matern step 25000 lr 4.268e-04 [train_loss] tar_ll 1.3723 loss -1.3723 (5.150 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 182.06it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.0782 loss -1.0782 (16.481 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 25200 lr 4.257e-04 [train_loss] tar_ll 1.3775 loss -1.3775 (5.268 secs)\n",
      "tnpa:tnpa_matern step 25400 lr 4.245e-04 [train_loss] tar_ll 1.4443 loss -1.4443 (5.458 secs)\n",
      "tnpa:tnpa_matern step 25600 lr 4.234e-04 [train_loss] tar_ll 1.4246 loss -1.4246 (5.252 secs)\n",
      "tnpa:tnpa_matern step 25800 lr 4.223e-04 [train_loss] tar_ll 1.4404 loss -1.4404 (5.342 secs)\n",
      "tnpa:tnpa_matern step 26000 lr 4.211e-04 [train_loss] tar_ll 1.4039 loss -1.4039 (5.353 secs)\n",
      "tnpa:tnpa_matern step 26200 lr 4.200e-04 [train_loss] tar_ll 1.4253 loss -1.4253 (5.428 secs)\n",
      "tnpa:tnpa_matern step 26400 lr 4.188e-04 [train_loss] tar_ll 1.4621 loss -1.4621 (5.334 secs)\n",
      "tnpa:tnpa_matern step 26600 lr 4.177e-04 [train_loss] tar_ll 1.4590 loss -1.4590 (5.511 secs)\n",
      "tnpa:tnpa_matern step 26800 lr 4.165e-04 [train_loss] tar_ll 1.3794 loss -1.3794 (5.378 secs)\n",
      "tnpa:tnpa_matern step 27000 lr 4.153e-04 [train_loss] tar_ll 1.1874 loss -1.1874 (5.310 secs)\n",
      "tnpa:tnpa_matern step 27200 lr 4.141e-04 [train_loss] tar_ll 1.4311 loss -1.4311 (5.584 secs)\n",
      "tnpa:tnpa_matern step 27400 lr 4.130e-04 [train_loss] tar_ll 1.3966 loss -1.3966 (5.412 secs)\n",
      "tnpa:tnpa_matern step 27600 lr 4.118e-04 [train_loss] tar_ll 1.4591 loss -1.4591 (5.323 secs)\n",
      "tnpa:tnpa_matern step 27800 lr 4.106e-04 [train_loss] tar_ll 1.4487 loss -1.4487 (5.419 secs)\n",
      "tnpa:tnpa_matern step 28000 lr 4.094e-04 [train_loss] tar_ll 1.5094 loss -1.5094 (5.395 secs)\n",
      "tnpa:tnpa_matern step 28200 lr 4.081e-04 [train_loss] tar_ll 1.4277 loss -1.4277 (5.215 secs)\n",
      "tnpa:tnpa_matern step 28400 lr 4.069e-04 [train_loss] tar_ll 1.5039 loss -1.5039 (5.341 secs)\n",
      "tnpa:tnpa_matern step 28600 lr 4.057e-04 [train_loss] tar_ll 1.4492 loss -1.4492 (5.293 secs)\n",
      "tnpa:tnpa_matern step 28800 lr 4.045e-04 [train_loss] tar_ll 1.5150 loss -1.5150 (5.334 secs)\n",
      "tnpa:tnpa_matern step 29000 lr 4.032e-04 [train_loss] tar_ll 1.4054 loss -1.4054 (5.580 secs)\n",
      "tnpa:tnpa_matern step 29200 lr 4.020e-04 [train_loss] tar_ll 1.4636 loss -1.4636 (5.434 secs)\n",
      "tnpa:tnpa_matern step 29400 lr 4.007e-04 [train_loss] tar_ll 1.4665 loss -1.4665 (5.387 secs)\n",
      "tnpa:tnpa_matern step 29600 lr 3.995e-04 [train_loss] tar_ll 1.4744 loss -1.4744 (5.360 secs)\n",
      "tnpa:tnpa_matern step 29800 lr 3.982e-04 [train_loss] tar_ll 1.3817 loss -1.3817 (5.301 secs)\n",
      "tnpa:tnpa_matern step 30000 lr 3.969e-04 [train_loss] tar_ll 1.3403 loss -1.3403 (5.245 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 180.85it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.0544 loss -1.0544 (16.592 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 30200 lr 3.957e-04 [train_loss] tar_ll 1.4104 loss -1.4104 (5.333 secs)\n",
      "tnpa:tnpa_matern step 30400 lr 3.944e-04 [train_loss] tar_ll 1.3496 loss -1.3496 (5.154 secs)\n",
      "tnpa:tnpa_matern step 30600 lr 3.931e-04 [train_loss] tar_ll 1.5084 loss -1.5084 (5.448 secs)\n",
      "tnpa:tnpa_matern step 30800 lr 3.918e-04 [train_loss] tar_ll 1.3998 loss -1.3998 (5.370 secs)\n",
      "tnpa:tnpa_matern step 31000 lr 3.905e-04 [train_loss] tar_ll 1.3948 loss -1.3948 (5.163 secs)\n",
      "tnpa:tnpa_matern step 31200 lr 3.892e-04 [train_loss] tar_ll 1.4226 loss -1.4226 (5.204 secs)\n",
      "tnpa:tnpa_matern step 31400 lr 3.879e-04 [train_loss] tar_ll 1.5063 loss -1.5063 (5.255 secs)\n",
      "tnpa:tnpa_matern step 31600 lr 3.866e-04 [train_loss] tar_ll 1.4313 loss -1.4313 (5.300 secs)\n",
      "tnpa:tnpa_matern step 31800 lr 3.853e-04 [train_loss] tar_ll 1.4277 loss -1.4277 (5.236 secs)\n",
      "tnpa:tnpa_matern step 32000 lr 3.840e-04 [train_loss] tar_ll 1.4378 loss -1.4378 (5.297 secs)\n",
      "tnpa:tnpa_matern step 32200 lr 3.826e-04 [train_loss] tar_ll 1.4147 loss -1.4147 (5.261 secs)\n",
      "tnpa:tnpa_matern step 32400 lr 3.813e-04 [train_loss] tar_ll 1.4600 loss -1.4600 (5.315 secs)\n",
      "tnpa:tnpa_matern step 32600 lr 3.800e-04 [train_loss] tar_ll 1.4369 loss -1.4369 (5.358 secs)\n",
      "tnpa:tnpa_matern step 32800 lr 3.786e-04 [train_loss] tar_ll 1.4633 loss -1.4633 (5.355 secs)\n",
      "tnpa:tnpa_matern step 33000 lr 3.773e-04 [train_loss] tar_ll 1.3911 loss -1.3911 (5.312 secs)\n",
      "tnpa:tnpa_matern step 33200 lr 3.759e-04 [train_loss] tar_ll 1.4834 loss -1.4834 (5.440 secs)\n",
      "tnpa:tnpa_matern step 33400 lr 3.745e-04 [train_loss] tar_ll 1.4048 loss -1.4048 (5.348 secs)\n",
      "tnpa:tnpa_matern step 33600 lr 3.732e-04 [train_loss] tar_ll 1.5026 loss -1.5026 (5.398 secs)\n",
      "tnpa:tnpa_matern step 33800 lr 3.718e-04 [train_loss] tar_ll 1.4376 loss -1.4376 (5.405 secs)\n",
      "tnpa:tnpa_matern step 34000 lr 3.704e-04 [train_loss] tar_ll 1.4335 loss -1.4335 (5.189 secs)\n",
      "tnpa:tnpa_matern step 34200 lr 3.691e-04 [train_loss] tar_ll 1.4841 loss -1.4841 (5.503 secs)\n",
      "tnpa:tnpa_matern step 34400 lr 3.677e-04 [train_loss] tar_ll 1.5202 loss -1.5202 (5.283 secs)\n",
      "tnpa:tnpa_matern step 34600 lr 3.663e-04 [train_loss] tar_ll 1.4177 loss -1.4177 (5.333 secs)\n",
      "tnpa:tnpa_matern step 34800 lr 3.649e-04 [train_loss] tar_ll 1.5307 loss -1.5307 (5.297 secs)\n",
      "tnpa:tnpa_matern step 35000 lr 3.635e-04 [train_loss] tar_ll 1.4905 loss -1.4905 (5.335 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 176.07it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.1256 loss -1.1256 (17.041 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 35200 lr 3.621e-04 [train_loss] tar_ll 1.4764 loss -1.4764 (5.189 secs)\n",
      "tnpa:tnpa_matern step 35400 lr 3.607e-04 [train_loss] tar_ll 1.4574 loss -1.4574 (5.363 secs)\n",
      "tnpa:tnpa_matern step 35600 lr 3.593e-04 [train_loss] tar_ll 1.5350 loss -1.5350 (5.248 secs)\n",
      "tnpa:tnpa_matern step 35800 lr 3.579e-04 [train_loss] tar_ll 1.4586 loss -1.4586 (5.271 secs)\n",
      "tnpa:tnpa_matern step 36000 lr 3.564e-04 [train_loss] tar_ll 1.5283 loss -1.5283 (5.441 secs)\n",
      "tnpa:tnpa_matern step 36200 lr 3.550e-04 [train_loss] tar_ll 1.4613 loss -1.4613 (5.281 secs)\n",
      "tnpa:tnpa_matern step 36400 lr 3.536e-04 [train_loss] tar_ll 1.5009 loss -1.5009 (5.383 secs)\n",
      "tnpa:tnpa_matern step 36600 lr 3.522e-04 [train_loss] tar_ll 1.5229 loss -1.5229 (5.312 secs)\n",
      "tnpa:tnpa_matern step 36800 lr 3.507e-04 [train_loss] tar_ll 1.4677 loss -1.4677 (5.263 secs)\n",
      "tnpa:tnpa_matern step 37000 lr 3.493e-04 [train_loss] tar_ll 1.5028 loss -1.5028 (5.263 secs)\n",
      "tnpa:tnpa_matern step 37200 lr 3.478e-04 [train_loss] tar_ll 1.5412 loss -1.5412 (5.210 secs)\n",
      "tnpa:tnpa_matern step 37400 lr 3.464e-04 [train_loss] tar_ll 1.4730 loss -1.4730 (5.125 secs)\n",
      "tnpa:tnpa_matern step 37600 lr 3.449e-04 [train_loss] tar_ll 1.5347 loss -1.5347 (5.252 secs)\n",
      "tnpa:tnpa_matern step 37800 lr 3.435e-04 [train_loss] tar_ll 1.4285 loss -1.4285 (5.398 secs)\n",
      "tnpa:tnpa_matern step 38000 lr 3.420e-04 [train_loss] tar_ll 1.4791 loss -1.4791 (5.111 secs)\n",
      "tnpa:tnpa_matern step 38200 lr 3.406e-04 [train_loss] tar_ll 1.5281 loss -1.5281 (5.256 secs)\n",
      "tnpa:tnpa_matern step 38400 lr 3.391e-04 [train_loss] tar_ll 1.4631 loss -1.4631 (5.447 secs)\n",
      "tnpa:tnpa_matern step 38600 lr 3.376e-04 [train_loss] tar_ll 1.4839 loss -1.4839 (5.341 secs)\n",
      "tnpa:tnpa_matern step 38800 lr 3.362e-04 [train_loss] tar_ll 1.4954 loss -1.4954 (5.255 secs)\n",
      "tnpa:tnpa_matern step 39000 lr 3.347e-04 [train_loss] tar_ll 1.5133 loss -1.5133 (5.275 secs)\n",
      "tnpa:tnpa_matern step 39200 lr 3.332e-04 [train_loss] tar_ll 1.4698 loss -1.4698 (5.312 secs)\n",
      "tnpa:tnpa_matern step 39400 lr 3.317e-04 [train_loss] tar_ll 1.4607 loss -1.4607 (5.305 secs)\n",
      "tnpa:tnpa_matern step 39600 lr 3.302e-04 [train_loss] tar_ll 1.5320 loss -1.5320 (5.188 secs)\n",
      "tnpa:tnpa_matern step 39800 lr 3.287e-04 [train_loss] tar_ll 1.5368 loss -1.5368 (5.288 secs)\n",
      "tnpa:tnpa_matern step 40000 lr 3.273e-04 [train_loss] tar_ll 1.5609 loss -1.5609 (5.201 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 182.49it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.1586 loss -1.1586 (16.443 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 40200 lr 3.258e-04 [train_loss] tar_ll 1.5182 loss -1.5182 (5.228 secs)\n",
      "tnpa:tnpa_matern step 40400 lr 3.243e-04 [train_loss] tar_ll 1.5078 loss -1.5078 (5.196 secs)\n",
      "tnpa:tnpa_matern step 40600 lr 3.228e-04 [train_loss] tar_ll 1.5358 loss -1.5358 (5.129 secs)\n",
      "tnpa:tnpa_matern step 40800 lr 3.213e-04 [train_loss] tar_ll 1.5240 loss -1.5240 (5.136 secs)\n",
      "tnpa:tnpa_matern step 41000 lr 3.197e-04 [train_loss] tar_ll 1.4876 loss -1.4876 (5.157 secs)\n",
      "tnpa:tnpa_matern step 41200 lr 3.182e-04 [train_loss] tar_ll 1.4884 loss -1.4884 (5.251 secs)\n",
      "tnpa:tnpa_matern step 41400 lr 3.167e-04 [train_loss] tar_ll 1.4960 loss -1.4960 (5.484 secs)\n",
      "tnpa:tnpa_matern step 41600 lr 3.152e-04 [train_loss] tar_ll 1.5082 loss -1.5082 (5.312 secs)\n",
      "tnpa:tnpa_matern step 41800 lr 3.137e-04 [train_loss] tar_ll 1.4647 loss -1.4647 (5.253 secs)\n",
      "tnpa:tnpa_matern step 42000 lr 3.122e-04 [train_loss] tar_ll 1.5115 loss -1.5115 (5.450 secs)\n",
      "tnpa:tnpa_matern step 42200 lr 3.106e-04 [train_loss] tar_ll 1.4695 loss -1.4695 (5.293 secs)\n",
      "tnpa:tnpa_matern step 42400 lr 3.091e-04 [train_loss] tar_ll 1.5649 loss -1.5649 (5.423 secs)\n",
      "tnpa:tnpa_matern step 42600 lr 3.076e-04 [train_loss] tar_ll 1.4776 loss -1.4776 (5.538 secs)\n",
      "tnpa:tnpa_matern step 42800 lr 3.061e-04 [train_loss] tar_ll 1.5361 loss -1.5361 (5.457 secs)\n",
      "tnpa:tnpa_matern step 43000 lr 3.045e-04 [train_loss] tar_ll 1.4926 loss -1.4926 (5.393 secs)\n",
      "tnpa:tnpa_matern step 43200 lr 3.030e-04 [train_loss] tar_ll 1.4927 loss -1.4927 (5.569 secs)\n",
      "tnpa:tnpa_matern step 43400 lr 3.015e-04 [train_loss] tar_ll 1.5715 loss -1.5715 (5.282 secs)\n",
      "tnpa:tnpa_matern step 43600 lr 2.999e-04 [train_loss] tar_ll 1.4889 loss -1.4889 (5.257 secs)\n",
      "tnpa:tnpa_matern step 43800 lr 2.984e-04 [train_loss] tar_ll 1.5332 loss -1.5332 (5.216 secs)\n",
      "tnpa:tnpa_matern step 44000 lr 2.968e-04 [train_loss] tar_ll 1.4692 loss -1.4692 (5.382 secs)\n",
      "tnpa:tnpa_matern step 44200 lr 2.953e-04 [train_loss] tar_ll 1.5204 loss -1.5204 (5.289 secs)\n",
      "tnpa:tnpa_matern step 44400 lr 2.938e-04 [train_loss] tar_ll 1.5506 loss -1.5506 (5.495 secs)\n",
      "tnpa:tnpa_matern step 44600 lr 2.922e-04 [train_loss] tar_ll 1.4561 loss -1.4561 (5.259 secs)\n",
      "tnpa:tnpa_matern step 44800 lr 2.907e-04 [train_loss] tar_ll 1.4967 loss -1.4967 (5.548 secs)\n",
      "tnpa:tnpa_matern step 45000 lr 2.891e-04 [train_loss] tar_ll 1.5285 loss -1.5285 (5.760 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 174.70it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.1487 loss -1.1487 (17.175 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 45200 lr 2.876e-04 [train_loss] tar_ll 1.5050 loss -1.5050 (5.228 secs)\n",
      "tnpa:tnpa_matern step 45400 lr 2.860e-04 [train_loss] tar_ll 1.5152 loss -1.5152 (5.430 secs)\n",
      "tnpa:tnpa_matern step 45600 lr 2.844e-04 [train_loss] tar_ll 1.5334 loss -1.5334 (5.231 secs)\n",
      "tnpa:tnpa_matern step 45800 lr 2.829e-04 [train_loss] tar_ll 1.5479 loss -1.5479 (5.172 secs)\n",
      "tnpa:tnpa_matern step 46000 lr 2.813e-04 [train_loss] tar_ll 1.5451 loss -1.5451 (5.274 secs)\n",
      "tnpa:tnpa_matern step 46200 lr 2.798e-04 [train_loss] tar_ll 1.4999 loss -1.4999 (5.344 secs)\n",
      "tnpa:tnpa_matern step 46400 lr 2.782e-04 [train_loss] tar_ll 1.4764 loss -1.4764 (5.269 secs)\n",
      "tnpa:tnpa_matern step 46600 lr 2.767e-04 [train_loss] tar_ll 1.4812 loss -1.4812 (5.482 secs)\n",
      "tnpa:tnpa_matern step 46800 lr 2.751e-04 [train_loss] tar_ll 1.5580 loss -1.5580 (5.302 secs)\n",
      "tnpa:tnpa_matern step 47000 lr 2.735e-04 [train_loss] tar_ll 1.5353 loss -1.5353 (5.187 secs)\n",
      "tnpa:tnpa_matern step 47200 lr 2.720e-04 [train_loss] tar_ll 1.4806 loss -1.4806 (5.272 secs)\n",
      "tnpa:tnpa_matern step 47400 lr 2.704e-04 [train_loss] tar_ll 1.5123 loss -1.5123 (5.277 secs)\n",
      "tnpa:tnpa_matern step 47600 lr 2.688e-04 [train_loss] tar_ll 1.5099 loss -1.5099 (5.188 secs)\n",
      "tnpa:tnpa_matern step 47800 lr 2.673e-04 [train_loss] tar_ll 1.5619 loss -1.5619 (5.225 secs)\n",
      "tnpa:tnpa_matern step 48000 lr 2.657e-04 [train_loss] tar_ll 1.5569 loss -1.5569 (5.297 secs)\n",
      "tnpa:tnpa_matern step 48200 lr 2.641e-04 [train_loss] tar_ll 1.5205 loss -1.5205 (5.268 secs)\n",
      "tnpa:tnpa_matern step 48400 lr 2.626e-04 [train_loss] tar_ll 1.4593 loss -1.4593 (5.175 secs)\n",
      "tnpa:tnpa_matern step 48600 lr 2.610e-04 [train_loss] tar_ll 1.5056 loss -1.5056 (5.190 secs)\n",
      "tnpa:tnpa_matern step 48800 lr 2.594e-04 [train_loss] tar_ll 1.5458 loss -1.5458 (5.083 secs)\n",
      "tnpa:tnpa_matern step 49000 lr 2.579e-04 [train_loss] tar_ll 1.5582 loss -1.5582 (5.304 secs)\n",
      "tnpa:tnpa_matern step 49200 lr 2.563e-04 [train_loss] tar_ll 1.5229 loss -1.5229 (5.297 secs)\n",
      "tnpa:tnpa_matern step 49400 lr 2.547e-04 [train_loss] tar_ll 1.5581 loss -1.5581 (5.305 secs)\n",
      "tnpa:tnpa_matern step 49600 lr 2.531e-04 [train_loss] tar_ll 1.5468 loss -1.5468 (5.273 secs)\n",
      "tnpa:tnpa_matern step 49800 lr 2.516e-04 [train_loss] tar_ll 1.6047 loss -1.6047 (5.422 secs)\n",
      "tnpa:tnpa_matern step 50000 lr 2.500e-04 [train_loss] tar_ll 1.5384 loss -1.5384 (5.453 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 172.56it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.1526 loss -1.1526 (17.388 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 50200 lr 2.484e-04 [train_loss] tar_ll 1.5191 loss -1.5191 (5.452 secs)\n",
      "tnpa:tnpa_matern step 50400 lr 2.469e-04 [train_loss] tar_ll 1.5452 loss -1.5452 (5.294 secs)\n",
      "tnpa:tnpa_matern step 50600 lr 2.453e-04 [train_loss] tar_ll 1.5704 loss -1.5704 (5.140 secs)\n",
      "tnpa:tnpa_matern step 50800 lr 2.437e-04 [train_loss] tar_ll 1.5374 loss -1.5374 (5.661 secs)\n",
      "tnpa:tnpa_matern step 51000 lr 2.421e-04 [train_loss] tar_ll 1.5611 loss -1.5611 (5.308 secs)\n",
      "tnpa:tnpa_matern step 51200 lr 2.406e-04 [train_loss] tar_ll 1.5184 loss -1.5184 (5.282 secs)\n",
      "tnpa:tnpa_matern step 51400 lr 2.390e-04 [train_loss] tar_ll 1.5408 loss -1.5408 (5.510 secs)\n",
      "tnpa:tnpa_matern step 51600 lr 2.374e-04 [train_loss] tar_ll 1.6413 loss -1.6413 (5.354 secs)\n",
      "tnpa:tnpa_matern step 51800 lr 2.359e-04 [train_loss] tar_ll 1.5702 loss -1.5702 (5.404 secs)\n",
      "tnpa:tnpa_matern step 52000 lr 2.343e-04 [train_loss] tar_ll 1.5195 loss -1.5195 (5.490 secs)\n",
      "tnpa:tnpa_matern step 52200 lr 2.327e-04 [train_loss] tar_ll 1.5171 loss -1.5171 (5.430 secs)\n",
      "tnpa:tnpa_matern step 52400 lr 2.312e-04 [train_loss] tar_ll 1.5211 loss -1.5211 (5.237 secs)\n",
      "tnpa:tnpa_matern step 52600 lr 2.296e-04 [train_loss] tar_ll 1.5529 loss -1.5529 (5.425 secs)\n",
      "tnpa:tnpa_matern step 52800 lr 2.280e-04 [train_loss] tar_ll 1.5085 loss -1.5085 (5.300 secs)\n",
      "tnpa:tnpa_matern step 53000 lr 2.265e-04 [train_loss] tar_ll 1.5739 loss -1.5739 (5.371 secs)\n",
      "tnpa:tnpa_matern step 53200 lr 2.249e-04 [train_loss] tar_ll 1.5199 loss -1.5199 (5.455 secs)\n",
      "tnpa:tnpa_matern step 53400 lr 2.233e-04 [train_loss] tar_ll 1.5442 loss -1.5442 (5.191 secs)\n",
      "tnpa:tnpa_matern step 53600 lr 2.218e-04 [train_loss] tar_ll 1.6071 loss -1.6071 (5.243 secs)\n",
      "tnpa:tnpa_matern step 53800 lr 2.202e-04 [train_loss] tar_ll 1.5782 loss -1.5782 (5.461 secs)\n",
      "tnpa:tnpa_matern step 54000 lr 2.187e-04 [train_loss] tar_ll 1.5724 loss -1.5724 (5.306 secs)\n",
      "tnpa:tnpa_matern step 54200 lr 2.171e-04 [train_loss] tar_ll 1.5276 loss -1.5276 (5.251 secs)\n",
      "tnpa:tnpa_matern step 54400 lr 2.156e-04 [train_loss] tar_ll 1.5177 loss -1.5177 (5.555 secs)\n",
      "tnpa:tnpa_matern step 54600 lr 2.140e-04 [train_loss] tar_ll 1.5357 loss -1.5357 (5.351 secs)\n",
      "tnpa:tnpa_matern step 54800 lr 2.124e-04 [train_loss] tar_ll 1.5804 loss -1.5804 (5.324 secs)\n",
      "tnpa:tnpa_matern step 55000 lr 2.109e-04 [train_loss] tar_ll 1.5835 loss -1.5835 (5.401 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 177.43it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.1629 loss -1.1629 (16.911 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 55200 lr 2.093e-04 [train_loss] tar_ll 1.5819 loss -1.5819 (5.440 secs)\n",
      "tnpa:tnpa_matern step 55400 lr 2.078e-04 [train_loss] tar_ll 1.5799 loss -1.5799 (5.476 secs)\n",
      "tnpa:tnpa_matern step 55600 lr 2.062e-04 [train_loss] tar_ll 1.5645 loss -1.5645 (5.331 secs)\n",
      "tnpa:tnpa_matern step 55800 lr 2.047e-04 [train_loss] tar_ll 1.5442 loss -1.5442 (5.371 secs)\n",
      "tnpa:tnpa_matern step 56000 lr 2.032e-04 [train_loss] tar_ll 1.5407 loss -1.5407 (5.149 secs)\n",
      "tnpa:tnpa_matern step 56200 lr 2.016e-04 [train_loss] tar_ll 1.5767 loss -1.5767 (5.356 secs)\n",
      "tnpa:tnpa_matern step 56400 lr 2.001e-04 [train_loss] tar_ll 1.5614 loss -1.5614 (5.193 secs)\n",
      "tnpa:tnpa_matern step 56600 lr 1.985e-04 [train_loss] tar_ll 1.5696 loss -1.5696 (5.492 secs)\n",
      "tnpa:tnpa_matern step 56800 lr 1.970e-04 [train_loss] tar_ll 1.6020 loss -1.6020 (5.276 secs)\n",
      "tnpa:tnpa_matern step 57000 lr 1.955e-04 [train_loss] tar_ll 1.5424 loss -1.5424 (5.287 secs)\n",
      "tnpa:tnpa_matern step 57200 lr 1.939e-04 [train_loss] tar_ll 1.5973 loss -1.5973 (5.548 secs)\n",
      "tnpa:tnpa_matern step 57400 lr 1.924e-04 [train_loss] tar_ll 1.5513 loss -1.5513 (5.370 secs)\n",
      "tnpa:tnpa_matern step 57600 lr 1.909e-04 [train_loss] tar_ll 1.4880 loss -1.4880 (5.327 secs)\n",
      "tnpa:tnpa_matern step 57800 lr 1.894e-04 [train_loss] tar_ll 1.5907 loss -1.5907 (5.400 secs)\n",
      "tnpa:tnpa_matern step 58000 lr 1.878e-04 [train_loss] tar_ll 1.6038 loss -1.6038 (5.399 secs)\n",
      "tnpa:tnpa_matern step 58200 lr 1.863e-04 [train_loss] tar_ll 1.5782 loss -1.5782 (5.402 secs)\n",
      "tnpa:tnpa_matern step 58400 lr 1.848e-04 [train_loss] tar_ll 1.5680 loss -1.5680 (5.284 secs)\n",
      "tnpa:tnpa_matern step 58600 lr 1.833e-04 [train_loss] tar_ll 1.5572 loss -1.5572 (5.323 secs)\n",
      "tnpa:tnpa_matern step 58800 lr 1.818e-04 [train_loss] tar_ll 1.6100 loss -1.6100 (5.339 secs)\n",
      "tnpa:tnpa_matern step 59000 lr 1.803e-04 [train_loss] tar_ll 1.5694 loss -1.5694 (5.574 secs)\n",
      "tnpa:tnpa_matern step 59200 lr 1.787e-04 [train_loss] tar_ll 1.5630 loss -1.5630 (5.455 secs)\n",
      "tnpa:tnpa_matern step 59400 lr 1.772e-04 [train_loss] tar_ll 1.5931 loss -1.5931 (5.387 secs)\n",
      "tnpa:tnpa_matern step 59600 lr 1.757e-04 [train_loss] tar_ll 1.6214 loss -1.6214 (5.467 secs)\n",
      "tnpa:tnpa_matern step 59800 lr 1.742e-04 [train_loss] tar_ll 1.5633 loss -1.5633 (5.406 secs)\n",
      "tnpa:tnpa_matern step 60000 lr 1.727e-04 [train_loss] tar_ll 1.5379 loss -1.5379 (5.313 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 183.12it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.1759 loss -1.1759 (16.383 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 60200 lr 1.713e-04 [train_loss] tar_ll 1.6088 loss -1.6088 (5.494 secs)\n",
      "tnpa:tnpa_matern step 60400 lr 1.698e-04 [train_loss] tar_ll 1.5649 loss -1.5649 (5.343 secs)\n",
      "tnpa:tnpa_matern step 60600 lr 1.683e-04 [train_loss] tar_ll 1.5272 loss -1.5272 (5.209 secs)\n",
      "tnpa:tnpa_matern step 60800 lr 1.668e-04 [train_loss] tar_ll 1.5696 loss -1.5696 (5.465 secs)\n",
      "tnpa:tnpa_matern step 61000 lr 1.653e-04 [train_loss] tar_ll 1.5398 loss -1.5398 (5.191 secs)\n",
      "tnpa:tnpa_matern step 61200 lr 1.638e-04 [train_loss] tar_ll 1.5763 loss -1.5763 (5.275 secs)\n",
      "tnpa:tnpa_matern step 61400 lr 1.624e-04 [train_loss] tar_ll 1.5409 loss -1.5409 (5.401 secs)\n",
      "tnpa:tnpa_matern step 61600 lr 1.609e-04 [train_loss] tar_ll 1.5488 loss -1.5488 (5.109 secs)\n",
      "tnpa:tnpa_matern step 61800 lr 1.594e-04 [train_loss] tar_ll 1.5702 loss -1.5702 (5.197 secs)\n",
      "tnpa:tnpa_matern step 62000 lr 1.580e-04 [train_loss] tar_ll 1.5659 loss -1.5659 (5.498 secs)\n",
      "tnpa:tnpa_matern step 62200 lr 1.565e-04 [train_loss] tar_ll 1.6024 loss -1.6024 (5.397 secs)\n",
      "tnpa:tnpa_matern step 62400 lr 1.551e-04 [train_loss] tar_ll 1.6367 loss -1.6367 (5.218 secs)\n",
      "tnpa:tnpa_matern step 62600 lr 1.536e-04 [train_loss] tar_ll 1.6543 loss -1.6543 (5.325 secs)\n",
      "tnpa:tnpa_matern step 62800 lr 1.522e-04 [train_loss] tar_ll 1.6102 loss -1.6102 (5.248 secs)\n",
      "tnpa:tnpa_matern step 63000 lr 1.507e-04 [train_loss] tar_ll 1.5528 loss -1.5528 (5.168 secs)\n",
      "tnpa:tnpa_matern step 63200 lr 1.493e-04 [train_loss] tar_ll 1.5923 loss -1.5923 (5.366 secs)\n",
      "tnpa:tnpa_matern step 63400 lr 1.478e-04 [train_loss] tar_ll 1.5980 loss -1.5980 (5.387 secs)\n",
      "tnpa:tnpa_matern step 63600 lr 1.464e-04 [train_loss] tar_ll 1.6034 loss -1.6034 (5.272 secs)\n",
      "tnpa:tnpa_matern step 63800 lr 1.450e-04 [train_loss] tar_ll 1.6031 loss -1.6031 (5.431 secs)\n",
      "tnpa:tnpa_matern step 64000 lr 1.436e-04 [train_loss] tar_ll 1.5939 loss -1.5939 (5.143 secs)\n",
      "tnpa:tnpa_matern step 64200 lr 1.421e-04 [train_loss] tar_ll 1.6031 loss -1.6031 (5.186 secs)\n",
      "tnpa:tnpa_matern step 64400 lr 1.407e-04 [train_loss] tar_ll 1.6062 loss -1.6062 (5.403 secs)\n",
      "tnpa:tnpa_matern step 64600 lr 1.393e-04 [train_loss] tar_ll 1.6139 loss -1.6139 (5.121 secs)\n",
      "tnpa:tnpa_matern step 64800 lr 1.379e-04 [train_loss] tar_ll 1.6213 loss -1.6213 (5.092 secs)\n",
      "tnpa:tnpa_matern step 65000 lr 1.365e-04 [train_loss] tar_ll 1.5597 loss -1.5597 (5.535 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 174.69it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.1990 loss -1.1990 (17.177 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 65200 lr 1.351e-04 [train_loss] tar_ll 1.6663 loss -1.6663 (5.498 secs)\n",
      "tnpa:tnpa_matern step 65400 lr 1.337e-04 [train_loss] tar_ll 1.6064 loss -1.6064 (5.417 secs)\n",
      "tnpa:tnpa_matern step 65600 lr 1.323e-04 [train_loss] tar_ll 1.5926 loss -1.5926 (5.321 secs)\n",
      "tnpa:tnpa_matern step 65800 lr 1.309e-04 [train_loss] tar_ll 1.6020 loss -1.6020 (5.315 secs)\n",
      "tnpa:tnpa_matern step 66000 lr 1.296e-04 [train_loss] tar_ll 1.6422 loss -1.6422 (5.354 secs)\n",
      "tnpa:tnpa_matern step 66200 lr 1.282e-04 [train_loss] tar_ll 1.6194 loss -1.6194 (5.430 secs)\n",
      "tnpa:tnpa_matern step 66400 lr 1.268e-04 [train_loss] tar_ll 1.6221 loss -1.6221 (5.352 secs)\n",
      "tnpa:tnpa_matern step 66600 lr 1.255e-04 [train_loss] tar_ll 1.5608 loss -1.5608 (5.441 secs)\n",
      "tnpa:tnpa_matern step 66800 lr 1.241e-04 [train_loss] tar_ll 1.6096 loss -1.6096 (5.459 secs)\n",
      "tnpa:tnpa_matern step 67000 lr 1.227e-04 [train_loss] tar_ll 1.6121 loss -1.6121 (5.392 secs)\n",
      "tnpa:tnpa_matern step 67200 lr 1.214e-04 [train_loss] tar_ll 1.5825 loss -1.5825 (5.339 secs)\n",
      "tnpa:tnpa_matern step 67400 lr 1.200e-04 [train_loss] tar_ll 1.6014 loss -1.6014 (5.424 secs)\n",
      "tnpa:tnpa_matern step 67600 lr 1.187e-04 [train_loss] tar_ll 1.5849 loss -1.5849 (5.114 secs)\n",
      "tnpa:tnpa_matern step 67800 lr 1.174e-04 [train_loss] tar_ll 1.5969 loss -1.5969 (5.461 secs)\n",
      "tnpa:tnpa_matern step 68000 lr 1.160e-04 [train_loss] tar_ll 1.5821 loss -1.5821 (5.369 secs)\n",
      "tnpa:tnpa_matern step 68200 lr 1.147e-04 [train_loss] tar_ll 1.6430 loss -1.6430 (5.245 secs)\n",
      "tnpa:tnpa_matern step 68400 lr 1.134e-04 [train_loss] tar_ll 1.6035 loss -1.6035 (5.372 secs)\n",
      "tnpa:tnpa_matern step 68600 lr 1.121e-04 [train_loss] tar_ll 1.6213 loss -1.6213 (5.434 secs)\n",
      "tnpa:tnpa_matern step 68800 lr 1.108e-04 [train_loss] tar_ll 1.6024 loss -1.6024 (5.369 secs)\n",
      "tnpa:tnpa_matern step 69000 lr 1.095e-04 [train_loss] tar_ll 1.5961 loss -1.5961 (5.247 secs)\n",
      "tnpa:tnpa_matern step 69200 lr 1.082e-04 [train_loss] tar_ll 1.5711 loss -1.5711 (5.230 secs)\n",
      "tnpa:tnpa_matern step 69400 lr 1.069e-04 [train_loss] tar_ll 1.6092 loss -1.6092 (5.071 secs)\n",
      "tnpa:tnpa_matern step 69600 lr 1.056e-04 [train_loss] tar_ll 1.6202 loss -1.6202 (5.208 secs)\n",
      "tnpa:tnpa_matern step 69800 lr 1.043e-04 [train_loss] tar_ll 1.6376 loss -1.6376 (5.159 secs)\n",
      "tnpa:tnpa_matern step 70000 lr 1.031e-04 [train_loss] tar_ll 1.5730 loss -1.5730 (5.216 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 180.28it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.2066 loss -1.2066 (16.644 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 70200 lr 1.018e-04 [train_loss] tar_ll 1.6381 loss -1.6381 (5.266 secs)\n",
      "tnpa:tnpa_matern step 70400 lr 1.005e-04 [train_loss] tar_ll 1.5932 loss -1.5932 (5.214 secs)\n",
      "tnpa:tnpa_matern step 70600 lr 9.927e-05 [train_loss] tar_ll 1.6320 loss -1.6320 (5.140 secs)\n",
      "tnpa:tnpa_matern step 70800 lr 9.802e-05 [train_loss] tar_ll 1.6157 loss -1.6157 (5.447 secs)\n",
      "tnpa:tnpa_matern step 71000 lr 9.677e-05 [train_loss] tar_ll 1.5894 loss -1.5894 (5.308 secs)\n",
      "tnpa:tnpa_matern step 71200 lr 9.554e-05 [train_loss] tar_ll 1.6464 loss -1.6464 (5.408 secs)\n",
      "tnpa:tnpa_matern step 71400 lr 9.430e-05 [train_loss] tar_ll 1.6489 loss -1.6489 (5.277 secs)\n",
      "tnpa:tnpa_matern step 71600 lr 9.308e-05 [train_loss] tar_ll 1.6493 loss -1.6493 (5.186 secs)\n",
      "tnpa:tnpa_matern step 71800 lr 9.186e-05 [train_loss] tar_ll 1.6207 loss -1.6207 (5.113 secs)\n",
      "tnpa:tnpa_matern step 72000 lr 9.064e-05 [train_loss] tar_ll 1.6155 loss -1.6155 (5.206 secs)\n",
      "tnpa:tnpa_matern step 72200 lr 8.944e-05 [train_loss] tar_ll 1.6131 loss -1.6131 (5.272 secs)\n",
      "tnpa:tnpa_matern step 72400 lr 8.824e-05 [train_loss] tar_ll 1.6417 loss -1.6417 (5.151 secs)\n",
      "tnpa:tnpa_matern step 72600 lr 8.704e-05 [train_loss] tar_ll 1.6155 loss -1.6155 (5.338 secs)\n",
      "tnpa:tnpa_matern step 72800 lr 8.585e-05 [train_loss] tar_ll 1.6728 loss -1.6728 (5.137 secs)\n",
      "tnpa:tnpa_matern step 73000 lr 8.467e-05 [train_loss] tar_ll 1.5995 loss -1.5995 (5.449 secs)\n",
      "tnpa:tnpa_matern step 73200 lr 8.350e-05 [train_loss] tar_ll 1.6231 loss -1.6231 (5.506 secs)\n",
      "tnpa:tnpa_matern step 73400 lr 8.233e-05 [train_loss] tar_ll 1.6376 loss -1.6376 (5.395 secs)\n",
      "tnpa:tnpa_matern step 73600 lr 8.117e-05 [train_loss] tar_ll 1.6084 loss -1.6084 (5.153 secs)\n",
      "tnpa:tnpa_matern step 73800 lr 8.001e-05 [train_loss] tar_ll 1.6563 loss -1.6563 (5.407 secs)\n",
      "tnpa:tnpa_matern step 74000 lr 7.886e-05 [train_loss] tar_ll 1.6418 loss -1.6418 (5.398 secs)\n",
      "tnpa:tnpa_matern step 74200 lr 7.772e-05 [train_loss] tar_ll 1.6486 loss -1.6486 (5.442 secs)\n",
      "tnpa:tnpa_matern step 74400 lr 7.659e-05 [train_loss] tar_ll 1.6227 loss -1.6227 (5.617 secs)\n",
      "tnpa:tnpa_matern step 74600 lr 7.546e-05 [train_loss] tar_ll 1.6241 loss -1.6241 (5.317 secs)\n",
      "tnpa:tnpa_matern step 74800 lr 7.434e-05 [train_loss] tar_ll 1.6118 loss -1.6118 (5.348 secs)\n",
      "tnpa:tnpa_matern step 75000 lr 7.322e-05 [train_loss] tar_ll 1.6302 loss -1.6302 (5.648 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 182.02it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.2050 loss -1.2050 (16.485 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 75200 lr 7.212e-05 [train_loss] tar_ll 1.6245 loss -1.6245 (5.424 secs)\n",
      "tnpa:tnpa_matern step 75400 lr 7.102e-05 [train_loss] tar_ll 1.6037 loss -1.6037 (5.177 secs)\n",
      "tnpa:tnpa_matern step 75600 lr 6.992e-05 [train_loss] tar_ll 1.6114 loss -1.6114 (5.393 secs)\n",
      "tnpa:tnpa_matern step 75800 lr 6.884e-05 [train_loss] tar_ll 1.6331 loss -1.6331 (5.282 secs)\n",
      "tnpa:tnpa_matern step 76000 lr 6.776e-05 [train_loss] tar_ll 1.5880 loss -1.5880 (5.381 secs)\n",
      "tnpa:tnpa_matern step 76200 lr 6.669e-05 [train_loss] tar_ll 1.6625 loss -1.6625 (5.635 secs)\n",
      "tnpa:tnpa_matern step 76400 lr 6.562e-05 [train_loss] tar_ll 1.5840 loss -1.5840 (5.247 secs)\n",
      "tnpa:tnpa_matern step 76600 lr 6.456e-05 [train_loss] tar_ll 1.6807 loss -1.6807 (5.380 secs)\n",
      "tnpa:tnpa_matern step 76800 lr 6.351e-05 [train_loss] tar_ll 1.6143 loss -1.6143 (5.438 secs)\n",
      "tnpa:tnpa_matern step 77000 lr 6.247e-05 [train_loss] tar_ll 1.6342 loss -1.6342 (5.347 secs)\n",
      "tnpa:tnpa_matern step 77200 lr 6.144e-05 [train_loss] tar_ll 1.6171 loss -1.6171 (5.130 secs)\n",
      "tnpa:tnpa_matern step 77400 lr 6.041e-05 [train_loss] tar_ll 1.6384 loss -1.6384 (5.396 secs)\n",
      "tnpa:tnpa_matern step 77600 lr 5.939e-05 [train_loss] tar_ll 1.6819 loss -1.6819 (5.206 secs)\n",
      "tnpa:tnpa_matern step 77800 lr 5.838e-05 [train_loss] tar_ll 1.6250 loss -1.6250 (5.321 secs)\n",
      "tnpa:tnpa_matern step 78000 lr 5.737e-05 [train_loss] tar_ll 1.6309 loss -1.6309 (5.419 secs)\n",
      "tnpa:tnpa_matern step 78200 lr 5.637e-05 [train_loss] tar_ll 1.6050 loss -1.6050 (5.340 secs)\n",
      "tnpa:tnpa_matern step 78400 lr 5.538e-05 [train_loss] tar_ll 1.5948 loss -1.5948 (5.244 secs)\n",
      "tnpa:tnpa_matern step 78600 lr 5.440e-05 [train_loss] tar_ll 1.6100 loss -1.6100 (5.371 secs)\n",
      "tnpa:tnpa_matern step 78800 lr 5.343e-05 [train_loss] tar_ll 1.6195 loss -1.6195 (5.402 secs)\n",
      "tnpa:tnpa_matern step 79000 lr 5.246e-05 [train_loss] tar_ll 1.6519 loss -1.6519 (5.137 secs)\n",
      "tnpa:tnpa_matern step 79200 lr 5.150e-05 [train_loss] tar_ll 1.5925 loss -1.5925 (5.317 secs)\n",
      "tnpa:tnpa_matern step 79400 lr 5.055e-05 [train_loss] tar_ll 1.6118 loss -1.6118 (5.059 secs)\n",
      "tnpa:tnpa_matern step 79600 lr 4.961e-05 [train_loss] tar_ll 1.6471 loss -1.6471 (5.293 secs)\n",
      "tnpa:tnpa_matern step 79800 lr 4.867e-05 [train_loss] tar_ll 1.7011 loss -1.7011 (5.240 secs)\n",
      "tnpa:tnpa_matern step 80000 lr 4.775e-05 [train_loss] tar_ll 1.6709 loss -1.6709 (5.227 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 180.96it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.1991 loss -1.1991 (16.582 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 80200 lr 4.683e-05 [train_loss] tar_ll 1.6548 loss -1.6548 (5.221 secs)\n",
      "tnpa:tnpa_matern step 80400 lr 4.592e-05 [train_loss] tar_ll 1.5675 loss -1.5675 (5.519 secs)\n",
      "tnpa:tnpa_matern step 80600 lr 4.501e-05 [train_loss] tar_ll 1.6345 loss -1.6345 (5.406 secs)\n",
      "tnpa:tnpa_matern step 80800 lr 4.412e-05 [train_loss] tar_ll 1.6173 loss -1.6173 (5.289 secs)\n",
      "tnpa:tnpa_matern step 81000 lr 4.323e-05 [train_loss] tar_ll 1.6497 loss -1.6497 (5.499 secs)\n",
      "tnpa:tnpa_matern step 81200 lr 4.235e-05 [train_loss] tar_ll 1.6297 loss -1.6297 (5.366 secs)\n",
      "tnpa:tnpa_matern step 81400 lr 4.148e-05 [train_loss] tar_ll 1.6348 loss -1.6348 (5.289 secs)\n",
      "tnpa:tnpa_matern step 81600 lr 4.062e-05 [train_loss] tar_ll 1.6944 loss -1.6944 (5.457 secs)\n",
      "tnpa:tnpa_matern step 81800 lr 3.976e-05 [train_loss] tar_ll 1.6270 loss -1.6270 (5.266 secs)\n",
      "tnpa:tnpa_matern step 82000 lr 3.892e-05 [train_loss] tar_ll 1.6666 loss -1.6666 (5.458 secs)\n",
      "tnpa:tnpa_matern step 82200 lr 3.808e-05 [train_loss] tar_ll 1.6244 loss -1.6244 (5.438 secs)\n",
      "tnpa:tnpa_matern step 82400 lr 3.725e-05 [train_loss] tar_ll 1.6502 loss -1.6502 (5.283 secs)\n",
      "tnpa:tnpa_matern step 82600 lr 3.643e-05 [train_loss] tar_ll 1.5812 loss -1.5812 (5.371 secs)\n",
      "tnpa:tnpa_matern step 82800 lr 3.562e-05 [train_loss] tar_ll 1.6048 loss -1.6048 (5.492 secs)\n",
      "tnpa:tnpa_matern step 83000 lr 3.481e-05 [train_loss] tar_ll 1.6113 loss -1.6113 (5.375 secs)\n",
      "tnpa:tnpa_matern step 83200 lr 3.402e-05 [train_loss] tar_ll 1.6191 loss -1.6191 (5.326 secs)\n",
      "tnpa:tnpa_matern step 83400 lr 3.323e-05 [train_loss] tar_ll 1.6847 loss -1.6847 (5.465 secs)\n",
      "tnpa:tnpa_matern step 83600 lr 3.245e-05 [train_loss] tar_ll 1.6392 loss -1.6392 (5.233 secs)\n",
      "tnpa:tnpa_matern step 83800 lr 3.168e-05 [train_loss] tar_ll 1.6433 loss -1.6433 (5.408 secs)\n",
      "tnpa:tnpa_matern step 84000 lr 3.092e-05 [train_loss] tar_ll 1.5860 loss -1.5860 (5.415 secs)\n",
      "tnpa:tnpa_matern step 84200 lr 3.017e-05 [train_loss] tar_ll 1.6527 loss -1.6527 (5.443 secs)\n",
      "tnpa:tnpa_matern step 84400 lr 2.943e-05 [train_loss] tar_ll 1.6371 loss -1.6371 (5.411 secs)\n",
      "tnpa:tnpa_matern step 84600 lr 2.869e-05 [train_loss] tar_ll 1.6425 loss -1.6425 (5.326 secs)\n",
      "tnpa:tnpa_matern step 84800 lr 2.797e-05 [train_loss] tar_ll 1.6718 loss -1.6718 (5.264 secs)\n",
      "tnpa:tnpa_matern step 85000 lr 2.725e-05 [train_loss] tar_ll 1.6843 loss -1.6843 (5.614 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 175.83it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.2167 loss -1.2167 (17.066 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 85200 lr 2.654e-05 [train_loss] tar_ll 1.6226 loss -1.6226 (5.297 secs)\n",
      "tnpa:tnpa_matern step 85400 lr 2.584e-05 [train_loss] tar_ll 1.6212 loss -1.6212 (5.307 secs)\n",
      "tnpa:tnpa_matern step 85600 lr 2.515e-05 [train_loss] tar_ll 1.6503 loss -1.6503 (5.390 secs)\n",
      "tnpa:tnpa_matern step 85800 lr 2.447e-05 [train_loss] tar_ll 1.6746 loss -1.6746 (5.329 secs)\n",
      "tnpa:tnpa_matern step 86000 lr 2.379e-05 [train_loss] tar_ll 1.6489 loss -1.6489 (5.355 secs)\n",
      "tnpa:tnpa_matern step 86200 lr 2.313e-05 [train_loss] tar_ll 1.6505 loss -1.6505 (5.433 secs)\n",
      "tnpa:tnpa_matern step 86400 lr 2.247e-05 [train_loss] tar_ll 1.7000 loss -1.7000 (5.298 secs)\n",
      "tnpa:tnpa_matern step 86600 lr 2.183e-05 [train_loss] tar_ll 1.6115 loss -1.6115 (5.214 secs)\n",
      "tnpa:tnpa_matern step 86800 lr 2.119e-05 [train_loss] tar_ll 1.6296 loss -1.6296 (5.179 secs)\n",
      "tnpa:tnpa_matern step 87000 lr 2.056e-05 [train_loss] tar_ll 1.6448 loss -1.6448 (5.382 secs)\n",
      "tnpa:tnpa_matern step 87200 lr 1.994e-05 [train_loss] tar_ll 1.6417 loss -1.6417 (5.305 secs)\n",
      "tnpa:tnpa_matern step 87400 lr 1.933e-05 [train_loss] tar_ll 1.6754 loss -1.6754 (5.494 secs)\n",
      "tnpa:tnpa_matern step 87600 lr 1.873e-05 [train_loss] tar_ll 1.6910 loss -1.6910 (5.175 secs)\n",
      "tnpa:tnpa_matern step 87800 lr 1.814e-05 [train_loss] tar_ll 1.6265 loss -1.6265 (5.326 secs)\n",
      "tnpa:tnpa_matern step 88000 lr 1.756e-05 [train_loss] tar_ll 1.6835 loss -1.6835 (5.346 secs)\n",
      "tnpa:tnpa_matern step 88200 lr 1.698e-05 [train_loss] tar_ll 1.6132 loss -1.6132 (5.355 secs)\n",
      "tnpa:tnpa_matern step 88400 lr 1.642e-05 [train_loss] tar_ll 1.6724 loss -1.6724 (5.133 secs)\n",
      "tnpa:tnpa_matern step 88600 lr 1.586e-05 [train_loss] tar_ll 1.6450 loss -1.6450 (5.409 secs)\n",
      "tnpa:tnpa_matern step 88800 lr 1.532e-05 [train_loss] tar_ll 1.6604 loss -1.6604 (5.410 secs)\n",
      "tnpa:tnpa_matern step 89000 lr 1.478e-05 [train_loss] tar_ll 1.6152 loss -1.6152 (5.362 secs)\n",
      "tnpa:tnpa_matern step 89200 lr 1.425e-05 [train_loss] tar_ll 1.7103 loss -1.7103 (5.484 secs)\n",
      "tnpa:tnpa_matern step 89400 lr 1.373e-05 [train_loss] tar_ll 1.6088 loss -1.6088 (5.332 secs)\n",
      "tnpa:tnpa_matern step 89600 lr 1.323e-05 [train_loss] tar_ll 1.6438 loss -1.6438 (5.299 secs)\n",
      "tnpa:tnpa_matern step 89800 lr 1.273e-05 [train_loss] tar_ll 1.7051 loss -1.7051 (5.597 secs)\n",
      "tnpa:tnpa_matern step 90000 lr 1.224e-05 [train_loss] tar_ll 1.6072 loss -1.6072 (5.239 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 176.52it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.2143 loss -1.2143 (16.997 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 90200 lr 1.176e-05 [train_loss] tar_ll 1.6690 loss -1.6690 (5.425 secs)\n",
      "tnpa:tnpa_matern step 90400 lr 1.128e-05 [train_loss] tar_ll 1.6209 loss -1.6209 (5.318 secs)\n",
      "tnpa:tnpa_matern step 90600 lr 1.082e-05 [train_loss] tar_ll 1.6630 loss -1.6630 (5.348 secs)\n",
      "tnpa:tnpa_matern step 90800 lr 1.037e-05 [train_loss] tar_ll 1.6530 loss -1.6530 (5.399 secs)\n",
      "tnpa:tnpa_matern step 91000 lr 9.927e-06 [train_loss] tar_ll 1.5730 loss -1.5730 (5.328 secs)\n",
      "tnpa:tnpa_matern step 91200 lr 9.493e-06 [train_loss] tar_ll 1.6727 loss -1.6727 (5.343 secs)\n",
      "tnpa:tnpa_matern step 91400 lr 9.069e-06 [train_loss] tar_ll 1.6169 loss -1.6169 (5.321 secs)\n",
      "tnpa:tnpa_matern step 91600 lr 8.655e-06 [train_loss] tar_ll 1.6342 loss -1.6342 (5.467 secs)\n",
      "tnpa:tnpa_matern step 91800 lr 8.250e-06 [train_loss] tar_ll 1.6170 loss -1.6170 (5.318 secs)\n",
      "tnpa:tnpa_matern step 92000 lr 7.854e-06 [train_loss] tar_ll 1.6305 loss -1.6305 (5.475 secs)\n",
      "tnpa:tnpa_matern step 92200 lr 7.468e-06 [train_loss] tar_ll 1.7014 loss -1.7014 (5.342 secs)\n",
      "tnpa:tnpa_matern step 92400 lr 7.092e-06 [train_loss] tar_ll 1.6863 loss -1.6863 (5.401 secs)\n",
      "tnpa:tnpa_matern step 92600 lr 6.725e-06 [train_loss] tar_ll 1.6175 loss -1.6175 (5.448 secs)\n",
      "tnpa:tnpa_matern step 92800 lr 6.368e-06 [train_loss] tar_ll 1.6180 loss -1.6180 (5.285 secs)\n",
      "tnpa:tnpa_matern step 93000 lr 6.021e-06 [train_loss] tar_ll 1.6637 loss -1.6637 (5.226 secs)\n",
      "tnpa:tnpa_matern step 93200 lr 5.683e-06 [train_loss] tar_ll 1.6690 loss -1.6690 (5.263 secs)\n",
      "tnpa:tnpa_matern step 93400 lr 5.355e-06 [train_loss] tar_ll 1.5599 loss -1.5599 (5.294 secs)\n",
      "tnpa:tnpa_matern step 93600 lr 5.036e-06 [train_loss] tar_ll 1.6449 loss -1.6449 (5.225 secs)\n",
      "tnpa:tnpa_matern step 93800 lr 4.727e-06 [train_loss] tar_ll 1.6236 loss -1.6236 (5.378 secs)\n",
      "tnpa:tnpa_matern step 94000 lr 4.428e-06 [train_loss] tar_ll 1.6652 loss -1.6652 (5.388 secs)\n",
      "tnpa:tnpa_matern step 94200 lr 4.139e-06 [train_loss] tar_ll 1.6548 loss -1.6548 (5.056 secs)\n",
      "tnpa:tnpa_matern step 94400 lr 3.859e-06 [train_loss] tar_ll 1.6660 loss -1.6660 (5.383 secs)\n",
      "tnpa:tnpa_matern step 94600 lr 3.589e-06 [train_loss] tar_ll 1.6644 loss -1.6644 (5.186 secs)\n",
      "tnpa:tnpa_matern step 94800 lr 3.329e-06 [train_loss] tar_ll 1.6198 loss -1.6198 (5.288 secs)\n",
      "tnpa:tnpa_matern step 95000 lr 3.078e-06 [train_loss] tar_ll 1.6624 loss -1.6624 (5.245 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 175.60it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.2236 loss -1.2236 (17.088 secs)\n",
      "\n",
      "tnpa:tnpa_matern step 95200 lr 2.837e-06 [train_loss] tar_ll 1.6419 loss -1.6419 (5.295 secs)\n",
      "tnpa:tnpa_matern step 95400 lr 2.606e-06 [train_loss] tar_ll 1.6852 loss -1.6852 (5.192 secs)\n",
      "tnpa:tnpa_matern step 95600 lr 2.385e-06 [train_loss] tar_ll 1.6178 loss -1.6178 (5.442 secs)\n",
      "tnpa:tnpa_matern step 95800 lr 2.173e-06 [train_loss] tar_ll 1.6703 loss -1.6703 (5.299 secs)\n",
      "tnpa:tnpa_matern step 96000 lr 1.971e-06 [train_loss] tar_ll 1.6523 loss -1.6523 (5.285 secs)\n",
      "tnpa:tnpa_matern step 96200 lr 1.779e-06 [train_loss] tar_ll 1.6756 loss -1.6756 (5.566 secs)\n",
      "tnpa:tnpa_matern step 96400 lr 1.597e-06 [train_loss] tar_ll 1.6755 loss -1.6755 (5.276 secs)\n",
      "tnpa:tnpa_matern step 96600 lr 1.425e-06 [train_loss] tar_ll 1.6279 loss -1.6279 (5.573 secs)\n",
      "tnpa:tnpa_matern step 96800 lr 1.262e-06 [train_loss] tar_ll 1.6856 loss -1.6856 (5.534 secs)\n",
      "tnpa:tnpa_matern step 97000 lr 1.110e-06 [train_loss] tar_ll 1.6768 loss -1.6768 (5.249 secs)\n",
      "tnpa:tnpa_matern step 97200 lr 9.666e-07 [train_loss] tar_ll 1.6822 loss -1.6822 (5.398 secs)\n",
      "tnpa:tnpa_matern step 97400 lr 8.335e-07 [train_loss] tar_ll 1.7074 loss -1.7074 (5.475 secs)\n",
      "tnpa:tnpa_matern step 97600 lr 7.103e-07 [train_loss] tar_ll 1.6283 loss -1.6283 (5.599 secs)\n",
      "tnpa:tnpa_matern step 97800 lr 5.969e-07 [train_loss] tar_ll 1.6633 loss -1.6633 (5.357 secs)\n",
      "tnpa:tnpa_matern step 98000 lr 4.933e-07 [train_loss] tar_ll 1.6185 loss -1.6185 (5.438 secs)\n",
      "tnpa:tnpa_matern step 98200 lr 3.996e-07 [train_loss] tar_ll 1.6477 loss -1.6477 (5.310 secs)\n",
      "tnpa:tnpa_matern step 98400 lr 3.158e-07 [train_loss] tar_ll 1.6971 loss -1.6971 (5.274 secs)\n",
      "tnpa:tnpa_matern step 98600 lr 2.418e-07 [train_loss] tar_ll 1.6846 loss -1.6846 (5.478 secs)\n",
      "tnpa:tnpa_matern step 98800 lr 1.776e-07 [train_loss] tar_ll 1.6669 loss -1.6669 (5.354 secs)\n",
      "tnpa:tnpa_matern step 99000 lr 1.234e-07 [train_loss] tar_ll 1.6795 loss -1.6795 (5.751 secs)\n",
      "tnpa:tnpa_matern step 99200 lr 7.895e-08 [train_loss] tar_ll 1.6552 loss -1.6552 (5.300 secs)\n",
      "tnpa:tnpa_matern step 99400 lr 4.441e-08 [train_loss] tar_ll 1.6332 loss -1.6332 (5.319 secs)\n",
      "tnpa:tnpa_matern step 99600 lr 1.974e-08 [train_loss] tar_ll 1.6417 loss -1.6417 (5.307 secs)\n",
      "tnpa:tnpa_matern step 99800 lr 4.935e-09 [train_loss] tar_ll 1.6816 loss -1.6816 (5.502 secs)\n",
      "tnpa:tnpa_matern step 100000 lr 0.000e+00 [train_loss] tar_ll 1.6856 loss -1.6856 (5.254 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 172.72it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.2218 loss -1.2218 (17.372 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:17<00:00, 173.84it/s]\n",
      "tnpa:tnpa_matern matern tar_ll 1.2218 loss -1.2218 (17.262 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3052189.5 miliseconds\n",
      "Execution time: 3052.1895 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 65.8369140625 MB\n",
      "Memory Usage Change: 49.5869140625 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='tnpa', name='tnpa_matern',val_seed=100, val_l=None,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827ef479-8676-4437-8043-0915f4462ec8",
   "metadata": {},
   "source": [
    "# ISANP (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b84453c-edb8-4824-8a07-21130cd212a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: isanp-isanp-num_latents-8_matern\n",
      "Total number of parameters: 785602\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "matern-seed0.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 200 lr 5.000e-04 [train_loss] tar_ll -0.6898 loss 0.6898 (5.893 secs)\n",
      "isanp:isanp-num_latents-8_matern step 400 lr 5.000e-04 [train_loss] tar_ll -0.6563 loss 0.6563 (5.880 secs)\n",
      "isanp:isanp-num_latents-8_matern step 600 lr 5.000e-04 [train_loss] tar_ll -0.5759 loss 0.5759 (5.845 secs)\n",
      "isanp:isanp-num_latents-8_matern step 800 lr 4.999e-04 [train_loss] tar_ll -0.5361 loss 0.5361 (5.993 secs)\n",
      "isanp:isanp-num_latents-8_matern step 1000 lr 4.999e-04 [train_loss] tar_ll -0.4680 loss 0.4680 (5.762 secs)\n",
      "isanp:isanp-num_latents-8_matern step 1200 lr 4.998e-04 [train_loss] tar_ll -0.4129 loss 0.4129 (5.799 secs)\n",
      "isanp:isanp-num_latents-8_matern step 1400 lr 4.998e-04 [train_loss] tar_ll -0.2526 loss 0.2526 (5.651 secs)\n",
      "isanp:isanp-num_latents-8_matern step 1600 lr 4.997e-04 [train_loss] tar_ll -0.1966 loss 0.1966 (5.735 secs)\n",
      "isanp:isanp-num_latents-8_matern step 1800 lr 4.996e-04 [train_loss] tar_ll -0.1199 loss 0.1199 (5.822 secs)\n",
      "isanp:isanp-num_latents-8_matern step 2000 lr 4.995e-04 [train_loss] tar_ll -0.0780 loss 0.0780 (5.772 secs)\n",
      "isanp:isanp-num_latents-8_matern step 2200 lr 4.994e-04 [train_loss] tar_ll 0.0694 loss -0.0694 (5.778 secs)\n",
      "isanp:isanp-num_latents-8_matern step 2400 lr 4.993e-04 [train_loss] tar_ll 0.0752 loss -0.0752 (5.846 secs)\n",
      "isanp:isanp-num_latents-8_matern step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1191 loss -0.1191 (5.708 secs)\n",
      "isanp:isanp-num_latents-8_matern step 2800 lr 4.990e-04 [train_loss] tar_ll 0.1445 loss -0.1445 (5.767 secs)\n",
      "isanp:isanp-num_latents-8_matern step 3000 lr 4.989e-04 [train_loss] tar_ll 0.2015 loss -0.2015 (5.919 secs)\n",
      "isanp:isanp-num_latents-8_matern step 3200 lr 4.987e-04 [train_loss] tar_ll 0.1419 loss -0.1419 (5.790 secs)\n",
      "isanp:isanp-num_latents-8_matern step 3400 lr 4.986e-04 [train_loss] tar_ll 0.1480 loss -0.1480 (5.788 secs)\n",
      "isanp:isanp-num_latents-8_matern step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2858 loss -0.2858 (5.838 secs)\n",
      "isanp:isanp-num_latents-8_matern step 3800 lr 4.982e-04 [train_loss] tar_ll 0.2213 loss -0.2213 (5.737 secs)\n",
      "isanp:isanp-num_latents-8_matern step 4000 lr 4.980e-04 [train_loss] tar_ll 0.2703 loss -0.2703 (5.850 secs)\n",
      "isanp:isanp-num_latents-8_matern step 4200 lr 4.978e-04 [train_loss] tar_ll 0.2766 loss -0.2766 (5.763 secs)\n",
      "isanp:isanp-num_latents-8_matern step 4400 lr 4.976e-04 [train_loss] tar_ll 0.3117 loss -0.3117 (5.764 secs)\n",
      "isanp:isanp-num_latents-8_matern step 4600 lr 4.974e-04 [train_loss] tar_ll 0.3732 loss -0.3732 (5.813 secs)\n",
      "isanp:isanp-num_latents-8_matern step 4800 lr 4.972e-04 [train_loss] tar_ll 0.3574 loss -0.3574 (5.776 secs)\n",
      "isanp:isanp-num_latents-8_matern step 5000 lr 4.969e-04 [train_loss] tar_ll 0.3947 loss -0.3947 (5.950 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 104.20it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.1981 loss -0.1981 (28.795 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 5200 lr 4.967e-04 [train_loss] tar_ll 0.3682 loss -0.3682 (6.025 secs)\n",
      "isanp:isanp-num_latents-8_matern step 5400 lr 4.964e-04 [train_loss] tar_ll 0.4837 loss -0.4837 (5.999 secs)\n",
      "isanp:isanp-num_latents-8_matern step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4091 loss -0.4091 (6.130 secs)\n",
      "isanp:isanp-num_latents-8_matern step 5800 lr 4.959e-04 [train_loss] tar_ll 0.3694 loss -0.3694 (6.166 secs)\n",
      "isanp:isanp-num_latents-8_matern step 6000 lr 4.956e-04 [train_loss] tar_ll 0.3995 loss -0.3995 (6.231 secs)\n",
      "isanp:isanp-num_latents-8_matern step 6200 lr 4.953e-04 [train_loss] tar_ll 0.4433 loss -0.4433 (6.072 secs)\n",
      "isanp:isanp-num_latents-8_matern step 6400 lr 4.950e-04 [train_loss] tar_ll 0.3873 loss -0.3873 (5.824 secs)\n",
      "isanp:isanp-num_latents-8_matern step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5225 loss -0.5225 (5.940 secs)\n",
      "isanp:isanp-num_latents-8_matern step 6800 lr 4.943e-04 [train_loss] tar_ll 0.5168 loss -0.5168 (6.018 secs)\n",
      "isanp:isanp-num_latents-8_matern step 7000 lr 4.940e-04 [train_loss] tar_ll 0.3852 loss -0.3852 (5.724 secs)\n",
      "isanp:isanp-num_latents-8_matern step 7200 lr 4.936e-04 [train_loss] tar_ll 0.5474 loss -0.5474 (5.722 secs)\n",
      "isanp:isanp-num_latents-8_matern step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5190 loss -0.5190 (5.780 secs)\n",
      "isanp:isanp-num_latents-8_matern step 7600 lr 4.929e-04 [train_loss] tar_ll 0.5080 loss -0.5080 (5.724 secs)\n",
      "isanp:isanp-num_latents-8_matern step 7800 lr 4.925e-04 [train_loss] tar_ll 0.4430 loss -0.4430 (5.743 secs)\n",
      "isanp:isanp-num_latents-8_matern step 8000 lr 4.921e-04 [train_loss] tar_ll 0.4551 loss -0.4551 (5.878 secs)\n",
      "isanp:isanp-num_latents-8_matern step 8200 lr 4.918e-04 [train_loss] tar_ll 0.4235 loss -0.4235 (5.905 secs)\n",
      "isanp:isanp-num_latents-8_matern step 8400 lr 4.913e-04 [train_loss] tar_ll 0.5337 loss -0.5337 (5.863 secs)\n",
      "isanp:isanp-num_latents-8_matern step 8600 lr 4.909e-04 [train_loss] tar_ll 0.4945 loss -0.4945 (5.774 secs)\n",
      "isanp:isanp-num_latents-8_matern step 8800 lr 4.905e-04 [train_loss] tar_ll 0.4528 loss -0.4528 (5.739 secs)\n",
      "isanp:isanp-num_latents-8_matern step 9000 lr 4.901e-04 [train_loss] tar_ll 0.5092 loss -0.5092 (5.806 secs)\n",
      "isanp:isanp-num_latents-8_matern step 9200 lr 4.896e-04 [train_loss] tar_ll 0.5782 loss -0.5782 (5.711 secs)\n",
      "isanp:isanp-num_latents-8_matern step 9400 lr 4.892e-04 [train_loss] tar_ll 0.5703 loss -0.5703 (5.626 secs)\n",
      "isanp:isanp-num_latents-8_matern step 9600 lr 4.887e-04 [train_loss] tar_ll 0.5936 loss -0.5936 (5.741 secs)\n",
      "isanp:isanp-num_latents-8_matern step 9800 lr 4.882e-04 [train_loss] tar_ll 0.5920 loss -0.5920 (5.764 secs)\n",
      "isanp:isanp-num_latents-8_matern step 10000 lr 4.878e-04 [train_loss] tar_ll 0.5087 loss -0.5087 (5.627 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:30<00:00, 99.62it/s] \n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.3743 loss -0.3743 (30.116 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 10200 lr 4.873e-04 [train_loss] tar_ll 0.6366 loss -0.6366 (6.690 secs)\n",
      "isanp:isanp-num_latents-8_matern step 10400 lr 4.868e-04 [train_loss] tar_ll 0.6038 loss -0.6038 (6.136 secs)\n",
      "isanp:isanp-num_latents-8_matern step 10600 lr 4.863e-04 [train_loss] tar_ll 0.6502 loss -0.6502 (6.358 secs)\n",
      "isanp:isanp-num_latents-8_matern step 10800 lr 4.857e-04 [train_loss] tar_ll 0.5620 loss -0.5620 (5.938 secs)\n",
      "isanp:isanp-num_latents-8_matern step 11000 lr 4.852e-04 [train_loss] tar_ll 0.6360 loss -0.6360 (5.674 secs)\n",
      "isanp:isanp-num_latents-8_matern step 11200 lr 4.847e-04 [train_loss] tar_ll 0.6035 loss -0.6035 (6.026 secs)\n",
      "isanp:isanp-num_latents-8_matern step 11400 lr 4.841e-04 [train_loss] tar_ll 0.6527 loss -0.6527 (5.641 secs)\n",
      "isanp:isanp-num_latents-8_matern step 11600 lr 4.836e-04 [train_loss] tar_ll 0.6188 loss -0.6188 (5.664 secs)\n",
      "isanp:isanp-num_latents-8_matern step 11800 lr 4.830e-04 [train_loss] tar_ll 0.6108 loss -0.6108 (5.753 secs)\n",
      "isanp:isanp-num_latents-8_matern step 12000 lr 4.824e-04 [train_loss] tar_ll 0.6565 loss -0.6565 (5.793 secs)\n",
      "isanp:isanp-num_latents-8_matern step 12200 lr 4.819e-04 [train_loss] tar_ll 0.5953 loss -0.5953 (5.791 secs)\n",
      "isanp:isanp-num_latents-8_matern step 12400 lr 4.813e-04 [train_loss] tar_ll 0.5225 loss -0.5225 (5.875 secs)\n",
      "isanp:isanp-num_latents-8_matern step 12600 lr 4.807e-04 [train_loss] tar_ll 0.5335 loss -0.5335 (5.759 secs)\n",
      "isanp:isanp-num_latents-8_matern step 12800 lr 4.801e-04 [train_loss] tar_ll 0.6490 loss -0.6490 (5.843 secs)\n",
      "isanp:isanp-num_latents-8_matern step 13000 lr 4.794e-04 [train_loss] tar_ll 0.6281 loss -0.6281 (5.805 secs)\n",
      "isanp:isanp-num_latents-8_matern step 13200 lr 4.788e-04 [train_loss] tar_ll 0.6273 loss -0.6273 (5.787 secs)\n",
      "isanp:isanp-num_latents-8_matern step 13400 lr 4.782e-04 [train_loss] tar_ll 0.6650 loss -0.6650 (5.960 secs)\n",
      "isanp:isanp-num_latents-8_matern step 13600 lr 4.775e-04 [train_loss] tar_ll 0.5815 loss -0.5815 (5.772 secs)\n",
      "isanp:isanp-num_latents-8_matern step 13800 lr 4.769e-04 [train_loss] tar_ll 0.6449 loss -0.6449 (5.741 secs)\n",
      "isanp:isanp-num_latents-8_matern step 14000 lr 4.762e-04 [train_loss] tar_ll 0.6004 loss -0.6004 (5.860 secs)\n",
      "isanp:isanp-num_latents-8_matern step 14200 lr 4.755e-04 [train_loss] tar_ll 0.4623 loss -0.4623 (5.761 secs)\n",
      "isanp:isanp-num_latents-8_matern step 14400 lr 4.749e-04 [train_loss] tar_ll 0.6887 loss -0.6887 (5.738 secs)\n",
      "isanp:isanp-num_latents-8_matern step 14600 lr 4.742e-04 [train_loss] tar_ll 0.6480 loss -0.6480 (5.882 secs)\n",
      "isanp:isanp-num_latents-8_matern step 14800 lr 4.735e-04 [train_loss] tar_ll 0.6306 loss -0.6306 (5.767 secs)\n",
      "isanp:isanp-num_latents-8_matern step 15000 lr 4.728e-04 [train_loss] tar_ll 0.6739 loss -0.6739 (5.707 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 106.58it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.4473 loss -0.4473 (28.148 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 15200 lr 4.720e-04 [train_loss] tar_ll 0.6697 loss -0.6697 (6.061 secs)\n",
      "isanp:isanp-num_latents-8_matern step 15400 lr 4.713e-04 [train_loss] tar_ll 0.6717 loss -0.6717 (5.943 secs)\n",
      "isanp:isanp-num_latents-8_matern step 15600 lr 4.706e-04 [train_loss] tar_ll 0.7306 loss -0.7306 (5.893 secs)\n",
      "isanp:isanp-num_latents-8_matern step 15800 lr 4.698e-04 [train_loss] tar_ll 0.6442 loss -0.6442 (5.986 secs)\n",
      "isanp:isanp-num_latents-8_matern step 16000 lr 4.691e-04 [train_loss] tar_ll 0.7234 loss -0.7234 (5.909 secs)\n",
      "isanp:isanp-num_latents-8_matern step 16200 lr 4.683e-04 [train_loss] tar_ll 0.6469 loss -0.6469 (6.157 secs)\n",
      "isanp:isanp-num_latents-8_matern step 16400 lr 4.675e-04 [train_loss] tar_ll 0.6672 loss -0.6672 (6.027 secs)\n",
      "isanp:isanp-num_latents-8_matern step 16600 lr 4.668e-04 [train_loss] tar_ll 0.6884 loss -0.6884 (6.050 secs)\n",
      "isanp:isanp-num_latents-8_matern step 16800 lr 4.660e-04 [train_loss] tar_ll 0.7028 loss -0.7028 (6.020 secs)\n",
      "isanp:isanp-num_latents-8_matern step 17000 lr 4.652e-04 [train_loss] tar_ll 0.7338 loss -0.7338 (5.981 secs)\n",
      "isanp:isanp-num_latents-8_matern step 17200 lr 4.644e-04 [train_loss] tar_ll 0.7733 loss -0.7733 (6.137 secs)\n",
      "isanp:isanp-num_latents-8_matern step 17400 lr 4.636e-04 [train_loss] tar_ll 0.7084 loss -0.7084 (6.117 secs)\n",
      "isanp:isanp-num_latents-8_matern step 17600 lr 4.627e-04 [train_loss] tar_ll 0.7947 loss -0.7947 (6.179 secs)\n",
      "isanp:isanp-num_latents-8_matern step 17800 lr 4.619e-04 [train_loss] tar_ll 0.7783 loss -0.7783 (6.276 secs)\n",
      "isanp:isanp-num_latents-8_matern step 18000 lr 4.611e-04 [train_loss] tar_ll 0.7637 loss -0.7637 (6.081 secs)\n",
      "isanp:isanp-num_latents-8_matern step 18200 lr 4.602e-04 [train_loss] tar_ll 0.7076 loss -0.7076 (5.935 secs)\n",
      "isanp:isanp-num_latents-8_matern step 18400 lr 4.594e-04 [train_loss] tar_ll 0.6553 loss -0.6553 (5.809 secs)\n",
      "isanp:isanp-num_latents-8_matern step 18600 lr 4.585e-04 [train_loss] tar_ll 0.6447 loss -0.6447 (5.764 secs)\n",
      "isanp:isanp-num_latents-8_matern step 18800 lr 4.576e-04 [train_loss] tar_ll 0.6854 loss -0.6854 (5.784 secs)\n",
      "isanp:isanp-num_latents-8_matern step 19000 lr 4.568e-04 [train_loss] tar_ll 0.7143 loss -0.7143 (5.885 secs)\n",
      "isanp:isanp-num_latents-8_matern step 19200 lr 4.559e-04 [train_loss] tar_ll 0.6786 loss -0.6786 (5.749 secs)\n",
      "isanp:isanp-num_latents-8_matern step 19400 lr 4.550e-04 [train_loss] tar_ll 0.7691 loss -0.7691 (5.747 secs)\n",
      "isanp:isanp-num_latents-8_matern step 19600 lr 4.541e-04 [train_loss] tar_ll 0.7135 loss -0.7135 (5.774 secs)\n",
      "isanp:isanp-num_latents-8_matern step 19800 lr 4.532e-04 [train_loss] tar_ll 0.7501 loss -0.7501 (5.714 secs)\n",
      "isanp:isanp-num_latents-8_matern step 20000 lr 4.523e-04 [train_loss] tar_ll 0.7552 loss -0.7552 (5.747 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 102.04it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.4394 loss -0.4394 (29.403 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 20200 lr 4.513e-04 [train_loss] tar_ll 0.7354 loss -0.7354 (5.958 secs)\n",
      "isanp:isanp-num_latents-8_matern step 20400 lr 4.504e-04 [train_loss] tar_ll 0.7031 loss -0.7031 (5.782 secs)\n",
      "isanp:isanp-num_latents-8_matern step 20600 lr 4.494e-04 [train_loss] tar_ll 0.6752 loss -0.6752 (5.838 secs)\n",
      "isanp:isanp-num_latents-8_matern step 20800 lr 4.485e-04 [train_loss] tar_ll 0.7880 loss -0.7880 (5.763 secs)\n",
      "isanp:isanp-num_latents-8_matern step 21000 lr 4.475e-04 [train_loss] tar_ll 0.7312 loss -0.7312 (5.761 secs)\n",
      "isanp:isanp-num_latents-8_matern step 21200 lr 4.466e-04 [train_loss] tar_ll 0.6839 loss -0.6839 (5.862 secs)\n",
      "isanp:isanp-num_latents-8_matern step 21400 lr 4.456e-04 [train_loss] tar_ll 0.6829 loss -0.6829 (5.959 secs)\n",
      "isanp:isanp-num_latents-8_matern step 21600 lr 4.446e-04 [train_loss] tar_ll 0.7576 loss -0.7576 (5.682 secs)\n",
      "isanp:isanp-num_latents-8_matern step 21800 lr 4.436e-04 [train_loss] tar_ll 0.7790 loss -0.7790 (5.913 secs)\n",
      "isanp:isanp-num_latents-8_matern step 22000 lr 4.426e-04 [train_loss] tar_ll 0.7312 loss -0.7312 (5.684 secs)\n",
      "isanp:isanp-num_latents-8_matern step 22200 lr 4.416e-04 [train_loss] tar_ll 0.7895 loss -0.7895 (5.773 secs)\n",
      "isanp:isanp-num_latents-8_matern step 22400 lr 4.406e-04 [train_loss] tar_ll 0.6956 loss -0.6956 (5.904 secs)\n",
      "isanp:isanp-num_latents-8_matern step 22600 lr 4.396e-04 [train_loss] tar_ll 0.7590 loss -0.7590 (5.739 secs)\n",
      "isanp:isanp-num_latents-8_matern step 22800 lr 4.386e-04 [train_loss] tar_ll 0.6138 loss -0.6138 (5.983 secs)\n",
      "isanp:isanp-num_latents-8_matern step 23000 lr 4.375e-04 [train_loss] tar_ll 0.6900 loss -0.6900 (5.773 secs)\n",
      "isanp:isanp-num_latents-8_matern step 23200 lr 4.365e-04 [train_loss] tar_ll 0.7753 loss -0.7753 (5.811 secs)\n",
      "isanp:isanp-num_latents-8_matern step 23400 lr 4.354e-04 [train_loss] tar_ll 0.8273 loss -0.8273 (5.901 secs)\n",
      "isanp:isanp-num_latents-8_matern step 23600 lr 4.344e-04 [train_loss] tar_ll 0.7702 loss -0.7702 (5.768 secs)\n",
      "isanp:isanp-num_latents-8_matern step 23800 lr 4.333e-04 [train_loss] tar_ll 0.8122 loss -0.8122 (5.771 secs)\n",
      "isanp:isanp-num_latents-8_matern step 24000 lr 4.322e-04 [train_loss] tar_ll 0.8983 loss -0.8983 (5.864 secs)\n",
      "isanp:isanp-num_latents-8_matern step 24200 lr 4.312e-04 [train_loss] tar_ll 0.7671 loss -0.7671 (5.867 secs)\n",
      "isanp:isanp-num_latents-8_matern step 24400 lr 4.301e-04 [train_loss] tar_ll 0.7827 loss -0.7827 (5.764 secs)\n",
      "isanp:isanp-num_latents-8_matern step 24600 lr 4.290e-04 [train_loss] tar_ll 0.8901 loss -0.8901 (5.820 secs)\n",
      "isanp:isanp-num_latents-8_matern step 24800 lr 4.279e-04 [train_loss] tar_ll 0.7522 loss -0.7522 (5.734 secs)\n",
      "isanp:isanp-num_latents-8_matern step 25000 lr 4.268e-04 [train_loss] tar_ll 0.7600 loss -0.7600 (5.866 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 103.52it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.4403 loss -0.4403 (28.982 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 25200 lr 4.257e-04 [train_loss] tar_ll 0.7587 loss -0.7587 (5.961 secs)\n",
      "isanp:isanp-num_latents-8_matern step 25400 lr 4.245e-04 [train_loss] tar_ll 0.7452 loss -0.7452 (5.979 secs)\n",
      "isanp:isanp-num_latents-8_matern step 25600 lr 4.234e-04 [train_loss] tar_ll 0.8170 loss -0.8170 (5.994 secs)\n",
      "isanp:isanp-num_latents-8_matern step 25800 lr 4.223e-04 [train_loss] tar_ll 0.7674 loss -0.7674 (5.972 secs)\n",
      "isanp:isanp-num_latents-8_matern step 26000 lr 4.211e-04 [train_loss] tar_ll 0.8331 loss -0.8331 (5.986 secs)\n",
      "isanp:isanp-num_latents-8_matern step 26200 lr 4.200e-04 [train_loss] tar_ll 0.7240 loss -0.7240 (6.058 secs)\n",
      "isanp:isanp-num_latents-8_matern step 26400 lr 4.188e-04 [train_loss] tar_ll 0.8259 loss -0.8259 (5.990 secs)\n",
      "isanp:isanp-num_latents-8_matern step 26600 lr 4.177e-04 [train_loss] tar_ll 0.7876 loss -0.7876 (5.965 secs)\n",
      "isanp:isanp-num_latents-8_matern step 26800 lr 4.165e-04 [train_loss] tar_ll 0.6448 loss -0.6448 (5.997 secs)\n",
      "isanp:isanp-num_latents-8_matern step 27000 lr 4.153e-04 [train_loss] tar_ll 0.7475 loss -0.7475 (5.705 secs)\n",
      "isanp:isanp-num_latents-8_matern step 27200 lr 4.141e-04 [train_loss] tar_ll 0.6630 loss -0.6630 (5.772 secs)\n",
      "isanp:isanp-num_latents-8_matern step 27400 lr 4.130e-04 [train_loss] tar_ll 0.7235 loss -0.7235 (5.734 secs)\n",
      "isanp:isanp-num_latents-8_matern step 27600 lr 4.118e-04 [train_loss] tar_ll 0.8364 loss -0.8364 (5.697 secs)\n",
      "isanp:isanp-num_latents-8_matern step 27800 lr 4.106e-04 [train_loss] tar_ll 0.8016 loss -0.8016 (5.766 secs)\n",
      "isanp:isanp-num_latents-8_matern step 28000 lr 4.094e-04 [train_loss] tar_ll 0.8192 loss -0.8192 (5.629 secs)\n",
      "isanp:isanp-num_latents-8_matern step 28200 lr 4.081e-04 [train_loss] tar_ll 0.8698 loss -0.8698 (5.784 secs)\n",
      "isanp:isanp-num_latents-8_matern step 28400 lr 4.069e-04 [train_loss] tar_ll 0.6994 loss -0.6994 (5.837 secs)\n",
      "isanp:isanp-num_latents-8_matern step 28600 lr 4.057e-04 [train_loss] tar_ll 0.7580 loss -0.7580 (5.864 secs)\n",
      "isanp:isanp-num_latents-8_matern step 28800 lr 4.045e-04 [train_loss] tar_ll 0.8653 loss -0.8653 (5.778 secs)\n",
      "isanp:isanp-num_latents-8_matern step 29000 lr 4.032e-04 [train_loss] tar_ll 0.8321 loss -0.8321 (5.798 secs)\n",
      "isanp:isanp-num_latents-8_matern step 29200 lr 4.020e-04 [train_loss] tar_ll 0.8511 loss -0.8511 (5.920 secs)\n",
      "isanp:isanp-num_latents-8_matern step 29400 lr 4.007e-04 [train_loss] tar_ll 0.8420 loss -0.8420 (5.671 secs)\n",
      "isanp:isanp-num_latents-8_matern step 29600 lr 3.995e-04 [train_loss] tar_ll 0.7738 loss -0.7738 (5.735 secs)\n",
      "isanp:isanp-num_latents-8_matern step 29800 lr 3.982e-04 [train_loss] tar_ll 0.8771 loss -0.8771 (5.640 secs)\n",
      "isanp:isanp-num_latents-8_matern step 30000 lr 3.969e-04 [train_loss] tar_ll 0.8806 loss -0.8806 (5.734 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 107.06it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.5600 loss -0.5600 (28.024 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 30200 lr 3.957e-04 [train_loss] tar_ll 0.8731 loss -0.8731 (5.947 secs)\n",
      "isanp:isanp-num_latents-8_matern step 30400 lr 3.944e-04 [train_loss] tar_ll 0.7881 loss -0.7881 (5.855 secs)\n",
      "isanp:isanp-num_latents-8_matern step 30600 lr 3.931e-04 [train_loss] tar_ll 0.9215 loss -0.9215 (5.758 secs)\n",
      "isanp:isanp-num_latents-8_matern step 30800 lr 3.918e-04 [train_loss] tar_ll 0.8542 loss -0.8542 (5.839 secs)\n",
      "isanp:isanp-num_latents-8_matern step 31000 lr 3.905e-04 [train_loss] tar_ll 0.9203 loss -0.9203 (5.801 secs)\n",
      "isanp:isanp-num_latents-8_matern step 31200 lr 3.892e-04 [train_loss] tar_ll 0.9248 loss -0.9248 (5.869 secs)\n",
      "isanp:isanp-num_latents-8_matern step 31400 lr 3.879e-04 [train_loss] tar_ll 0.8595 loss -0.8595 (6.485 secs)\n",
      "isanp:isanp-num_latents-8_matern step 31600 lr 3.866e-04 [train_loss] tar_ll 0.8474 loss -0.8474 (6.130 secs)\n",
      "isanp:isanp-num_latents-8_matern step 31800 lr 3.853e-04 [train_loss] tar_ll 0.8573 loss -0.8573 (6.245 secs)\n",
      "isanp:isanp-num_latents-8_matern step 32000 lr 3.840e-04 [train_loss] tar_ll 0.9011 loss -0.9011 (6.039 secs)\n",
      "isanp:isanp-num_latents-8_matern step 32200 lr 3.826e-04 [train_loss] tar_ll 0.8925 loss -0.8925 (6.216 secs)\n",
      "isanp:isanp-num_latents-8_matern step 32400 lr 3.813e-04 [train_loss] tar_ll 0.8603 loss -0.8603 (6.057 secs)\n",
      "isanp:isanp-num_latents-8_matern step 32600 lr 3.800e-04 [train_loss] tar_ll 0.9277 loss -0.9277 (6.021 secs)\n",
      "isanp:isanp-num_latents-8_matern step 32800 lr 3.786e-04 [train_loss] tar_ll 0.8142 loss -0.8142 (5.837 secs)\n",
      "isanp:isanp-num_latents-8_matern step 33000 lr 3.773e-04 [train_loss] tar_ll 0.8682 loss -0.8682 (5.785 secs)\n",
      "isanp:isanp-num_latents-8_matern step 33200 lr 3.759e-04 [train_loss] tar_ll 0.8489 loss -0.8489 (5.886 secs)\n",
      "isanp:isanp-num_latents-8_matern step 33400 lr 3.745e-04 [train_loss] tar_ll 0.8974 loss -0.8974 (5.841 secs)\n",
      "isanp:isanp-num_latents-8_matern step 33600 lr 3.732e-04 [train_loss] tar_ll 0.8240 loss -0.8240 (5.784 secs)\n",
      "isanp:isanp-num_latents-8_matern step 33800 lr 3.718e-04 [train_loss] tar_ll 0.8580 loss -0.8580 (5.697 secs)\n",
      "isanp:isanp-num_latents-8_matern step 34000 lr 3.704e-04 [train_loss] tar_ll 0.8146 loss -0.8146 (5.831 secs)\n",
      "isanp:isanp-num_latents-8_matern step 34200 lr 3.691e-04 [train_loss] tar_ll 0.8513 loss -0.8513 (5.919 secs)\n",
      "isanp:isanp-num_latents-8_matern step 34400 lr 3.677e-04 [train_loss] tar_ll 0.7478 loss -0.7478 (5.816 secs)\n",
      "isanp:isanp-num_latents-8_matern step 34600 lr 3.663e-04 [train_loss] tar_ll 0.8862 loss -0.8862 (5.913 secs)\n",
      "isanp:isanp-num_latents-8_matern step 34800 lr 3.649e-04 [train_loss] tar_ll 0.8567 loss -0.8567 (5.730 secs)\n",
      "isanp:isanp-num_latents-8_matern step 35000 lr 3.635e-04 [train_loss] tar_ll 0.8315 loss -0.8315 (5.868 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 102.41it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.5810 loss -0.5810 (29.296 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 35200 lr 3.621e-04 [train_loss] tar_ll 0.9003 loss -0.9003 (6.019 secs)\n",
      "isanp:isanp-num_latents-8_matern step 35400 lr 3.607e-04 [train_loss] tar_ll 0.8975 loss -0.8975 (5.731 secs)\n",
      "isanp:isanp-num_latents-8_matern step 35600 lr 3.593e-04 [train_loss] tar_ll 0.9056 loss -0.9056 (5.993 secs)\n",
      "isanp:isanp-num_latents-8_matern step 35800 lr 3.579e-04 [train_loss] tar_ll 0.8162 loss -0.8162 (6.068 secs)\n",
      "isanp:isanp-num_latents-8_matern step 36000 lr 3.564e-04 [train_loss] tar_ll 0.8363 loss -0.8363 (5.908 secs)\n",
      "isanp:isanp-num_latents-8_matern step 36200 lr 3.550e-04 [train_loss] tar_ll 0.8442 loss -0.8442 (6.156 secs)\n",
      "isanp:isanp-num_latents-8_matern step 36400 lr 3.536e-04 [train_loss] tar_ll 0.8138 loss -0.8138 (5.954 secs)\n",
      "isanp:isanp-num_latents-8_matern step 36600 lr 3.522e-04 [train_loss] tar_ll 0.8880 loss -0.8880 (5.989 secs)\n",
      "isanp:isanp-num_latents-8_matern step 36800 lr 3.507e-04 [train_loss] tar_ll 0.6993 loss -0.6993 (6.011 secs)\n",
      "isanp:isanp-num_latents-8_matern step 37000 lr 3.493e-04 [train_loss] tar_ll 0.7651 loss -0.7651 (5.914 secs)\n",
      "isanp:isanp-num_latents-8_matern step 37200 lr 3.478e-04 [train_loss] tar_ll 0.8681 loss -0.8681 (6.023 secs)\n",
      "isanp:isanp-num_latents-8_matern step 37400 lr 3.464e-04 [train_loss] tar_ll 0.9094 loss -0.9094 (5.965 secs)\n",
      "isanp:isanp-num_latents-8_matern step 37600 lr 3.449e-04 [train_loss] tar_ll 0.8999 loss -0.8999 (6.127 secs)\n",
      "isanp:isanp-num_latents-8_matern step 37800 lr 3.435e-04 [train_loss] tar_ll 0.8977 loss -0.8977 (6.183 secs)\n",
      "isanp:isanp-num_latents-8_matern step 38000 lr 3.420e-04 [train_loss] tar_ll 0.8106 loss -0.8106 (6.209 secs)\n",
      "isanp:isanp-num_latents-8_matern step 38200 lr 3.406e-04 [train_loss] tar_ll 0.9595 loss -0.9595 (6.187 secs)\n",
      "isanp:isanp-num_latents-8_matern step 38400 lr 3.391e-04 [train_loss] tar_ll 0.9469 loss -0.9469 (6.157 secs)\n",
      "isanp:isanp-num_latents-8_matern step 38600 lr 3.376e-04 [train_loss] tar_ll 0.9141 loss -0.9141 (5.717 secs)\n",
      "isanp:isanp-num_latents-8_matern step 38800 lr 3.362e-04 [train_loss] tar_ll 0.9840 loss -0.9840 (5.877 secs)\n",
      "isanp:isanp-num_latents-8_matern step 39000 lr 3.347e-04 [train_loss] tar_ll 0.8628 loss -0.8628 (5.684 secs)\n",
      "isanp:isanp-num_latents-8_matern step 39200 lr 3.332e-04 [train_loss] tar_ll 0.8921 loss -0.8921 (5.870 secs)\n",
      "isanp:isanp-num_latents-8_matern step 39400 lr 3.317e-04 [train_loss] tar_ll 0.8328 loss -0.8328 (5.802 secs)\n",
      "isanp:isanp-num_latents-8_matern step 39600 lr 3.302e-04 [train_loss] tar_ll 0.8688 loss -0.8688 (5.793 secs)\n",
      "isanp:isanp-num_latents-8_matern step 39800 lr 3.287e-04 [train_loss] tar_ll 0.8989 loss -0.8989 (5.711 secs)\n",
      "isanp:isanp-num_latents-8_matern step 40000 lr 3.273e-04 [train_loss] tar_ll 0.8853 loss -0.8853 (5.825 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 106.72it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.4733 loss -0.4733 (28.115 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 40200 lr 3.258e-04 [train_loss] tar_ll 0.9556 loss -0.9556 (5.810 secs)\n",
      "isanp:isanp-num_latents-8_matern step 40400 lr 3.243e-04 [train_loss] tar_ll 0.9256 loss -0.9256 (5.760 secs)\n",
      "isanp:isanp-num_latents-8_matern step 40600 lr 3.228e-04 [train_loss] tar_ll 1.0337 loss -1.0337 (5.795 secs)\n",
      "isanp:isanp-num_latents-8_matern step 40800 lr 3.213e-04 [train_loss] tar_ll 0.9050 loss -0.9050 (5.862 secs)\n",
      "isanp:isanp-num_latents-8_matern step 41000 lr 3.197e-04 [train_loss] tar_ll 0.8839 loss -0.8839 (5.703 secs)\n",
      "isanp:isanp-num_latents-8_matern step 41200 lr 3.182e-04 [train_loss] tar_ll 0.9133 loss -0.9133 (5.903 secs)\n",
      "isanp:isanp-num_latents-8_matern step 41400 lr 3.167e-04 [train_loss] tar_ll 0.9331 loss -0.9331 (5.755 secs)\n",
      "isanp:isanp-num_latents-8_matern step 41600 lr 3.152e-04 [train_loss] tar_ll 0.9498 loss -0.9498 (5.731 secs)\n",
      "isanp:isanp-num_latents-8_matern step 41800 lr 3.137e-04 [train_loss] tar_ll 0.9056 loss -0.9056 (5.756 secs)\n",
      "isanp:isanp-num_latents-8_matern step 42000 lr 3.122e-04 [train_loss] tar_ll 0.9338 loss -0.9338 (5.799 secs)\n",
      "isanp:isanp-num_latents-8_matern step 42200 lr 3.106e-04 [train_loss] tar_ll 0.9609 loss -0.9609 (5.894 secs)\n",
      "isanp:isanp-num_latents-8_matern step 42400 lr 3.091e-04 [train_loss] tar_ll 0.9761 loss -0.9761 (5.719 secs)\n",
      "isanp:isanp-num_latents-8_matern step 42600 lr 3.076e-04 [train_loss] tar_ll 1.0022 loss -1.0022 (5.710 secs)\n",
      "isanp:isanp-num_latents-8_matern step 42800 lr 3.061e-04 [train_loss] tar_ll 0.9448 loss -0.9448 (5.899 secs)\n",
      "isanp:isanp-num_latents-8_matern step 43000 lr 3.045e-04 [train_loss] tar_ll 1.0236 loss -1.0236 (5.818 secs)\n",
      "isanp:isanp-num_latents-8_matern step 43200 lr 3.030e-04 [train_loss] tar_ll 0.9247 loss -0.9247 (5.863 secs)\n",
      "isanp:isanp-num_latents-8_matern step 43400 lr 3.015e-04 [train_loss] tar_ll 0.9147 loss -0.9147 (5.895 secs)\n",
      "isanp:isanp-num_latents-8_matern step 43600 lr 2.999e-04 [train_loss] tar_ll 0.9971 loss -0.9971 (5.826 secs)\n",
      "isanp:isanp-num_latents-8_matern step 43800 lr 2.984e-04 [train_loss] tar_ll 1.0043 loss -1.0043 (5.810 secs)\n",
      "isanp:isanp-num_latents-8_matern step 44000 lr 2.968e-04 [train_loss] tar_ll 0.8931 loss -0.8931 (5.883 secs)\n",
      "isanp:isanp-num_latents-8_matern step 44200 lr 2.953e-04 [train_loss] tar_ll 1.0150 loss -1.0150 (5.838 secs)\n",
      "isanp:isanp-num_latents-8_matern step 44400 lr 2.938e-04 [train_loss] tar_ll 0.9984 loss -0.9984 (6.110 secs)\n",
      "isanp:isanp-num_latents-8_matern step 44600 lr 2.922e-04 [train_loss] tar_ll 0.9476 loss -0.9476 (5.751 secs)\n",
      "isanp:isanp-num_latents-8_matern step 44800 lr 2.907e-04 [train_loss] tar_ll 0.9557 loss -0.9557 (5.779 secs)\n",
      "isanp:isanp-num_latents-8_matern step 45000 lr 2.891e-04 [train_loss] tar_ll 0.9148 loss -0.9148 (6.073 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 105.97it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.6129 loss -0.6129 (28.314 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 45200 lr 2.876e-04 [train_loss] tar_ll 1.0568 loss -1.0568 (6.201 secs)\n",
      "isanp:isanp-num_latents-8_matern step 45400 lr 2.860e-04 [train_loss] tar_ll 1.0075 loss -1.0075 (6.323 secs)\n",
      "isanp:isanp-num_latents-8_matern step 45600 lr 2.844e-04 [train_loss] tar_ll 1.0033 loss -1.0033 (6.063 secs)\n",
      "isanp:isanp-num_latents-8_matern step 45800 lr 2.829e-04 [train_loss] tar_ll 1.0419 loss -1.0419 (5.916 secs)\n",
      "isanp:isanp-num_latents-8_matern step 46000 lr 2.813e-04 [train_loss] tar_ll 0.9070 loss -0.9070 (6.019 secs)\n",
      "isanp:isanp-num_latents-8_matern step 46200 lr 2.798e-04 [train_loss] tar_ll 0.9940 loss -0.9940 (6.138 secs)\n",
      "isanp:isanp-num_latents-8_matern step 46400 lr 2.782e-04 [train_loss] tar_ll 0.9909 loss -0.9909 (5.980 secs)\n",
      "isanp:isanp-num_latents-8_matern step 46600 lr 2.767e-04 [train_loss] tar_ll 0.9910 loss -0.9910 (6.152 secs)\n",
      "isanp:isanp-num_latents-8_matern step 46800 lr 2.751e-04 [train_loss] tar_ll 0.9474 loss -0.9474 (5.925 secs)\n",
      "isanp:isanp-num_latents-8_matern step 47000 lr 2.735e-04 [train_loss] tar_ll 0.9522 loss -0.9522 (6.095 secs)\n",
      "isanp:isanp-num_latents-8_matern step 47200 lr 2.720e-04 [train_loss] tar_ll 0.9418 loss -0.9418 (6.097 secs)\n",
      "isanp:isanp-num_latents-8_matern step 47400 lr 2.704e-04 [train_loss] tar_ll 0.9501 loss -0.9501 (6.019 secs)\n",
      "isanp:isanp-num_latents-8_matern step 47600 lr 2.688e-04 [train_loss] tar_ll 0.9815 loss -0.9815 (5.966 secs)\n",
      "isanp:isanp-num_latents-8_matern step 47800 lr 2.673e-04 [train_loss] tar_ll 1.0477 loss -1.0477 (6.098 secs)\n",
      "isanp:isanp-num_latents-8_matern step 48000 lr 2.657e-04 [train_loss] tar_ll 1.0480 loss -1.0480 (6.300 secs)\n",
      "isanp:isanp-num_latents-8_matern step 48200 lr 2.641e-04 [train_loss] tar_ll 0.9490 loss -0.9490 (6.219 secs)\n",
      "isanp:isanp-num_latents-8_matern step 48400 lr 2.626e-04 [train_loss] tar_ll 0.9712 loss -0.9712 (6.114 secs)\n",
      "isanp:isanp-num_latents-8_matern step 48600 lr 2.610e-04 [train_loss] tar_ll 1.0191 loss -1.0191 (6.078 secs)\n",
      "isanp:isanp-num_latents-8_matern step 48800 lr 2.594e-04 [train_loss] tar_ll 1.0573 loss -1.0573 (5.933 secs)\n",
      "isanp:isanp-num_latents-8_matern step 49000 lr 2.579e-04 [train_loss] tar_ll 1.0031 loss -1.0031 (5.959 secs)\n",
      "isanp:isanp-num_latents-8_matern step 49200 lr 2.563e-04 [train_loss] tar_ll 1.0037 loss -1.0037 (5.887 secs)\n",
      "isanp:isanp-num_latents-8_matern step 49400 lr 2.547e-04 [train_loss] tar_ll 1.0200 loss -1.0200 (5.941 secs)\n",
      "isanp:isanp-num_latents-8_matern step 49600 lr 2.531e-04 [train_loss] tar_ll 0.9555 loss -0.9555 (5.761 secs)\n",
      "isanp:isanp-num_latents-8_matern step 49800 lr 2.516e-04 [train_loss] tar_ll 1.0700 loss -1.0700 (5.801 secs)\n",
      "isanp:isanp-num_latents-8_matern step 50000 lr 2.500e-04 [train_loss] tar_ll 0.9469 loss -0.9469 (5.782 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 107.34it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.5785 loss -0.5785 (27.951 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 50200 lr 2.484e-04 [train_loss] tar_ll 1.0119 loss -1.0119 (5.820 secs)\n",
      "isanp:isanp-num_latents-8_matern step 50400 lr 2.469e-04 [train_loss] tar_ll 1.0654 loss -1.0654 (5.728 secs)\n",
      "isanp:isanp-num_latents-8_matern step 50600 lr 2.453e-04 [train_loss] tar_ll 1.0109 loss -1.0109 (6.049 secs)\n",
      "isanp:isanp-num_latents-8_matern step 50800 lr 2.437e-04 [train_loss] tar_ll 1.0191 loss -1.0191 (5.813 secs)\n",
      "isanp:isanp-num_latents-8_matern step 51000 lr 2.421e-04 [train_loss] tar_ll 0.9706 loss -0.9706 (5.769 secs)\n",
      "isanp:isanp-num_latents-8_matern step 51200 lr 2.406e-04 [train_loss] tar_ll 1.0963 loss -1.0963 (5.869 secs)\n",
      "isanp:isanp-num_latents-8_matern step 51400 lr 2.390e-04 [train_loss] tar_ll 1.0347 loss -1.0347 (5.780 secs)\n",
      "isanp:isanp-num_latents-8_matern step 51600 lr 2.374e-04 [train_loss] tar_ll 1.0221 loss -1.0221 (5.771 secs)\n",
      "isanp:isanp-num_latents-8_matern step 51800 lr 2.359e-04 [train_loss] tar_ll 1.0503 loss -1.0503 (5.656 secs)\n",
      "isanp:isanp-num_latents-8_matern step 52000 lr 2.343e-04 [train_loss] tar_ll 1.0665 loss -1.0665 (5.611 secs)\n",
      "isanp:isanp-num_latents-8_matern step 52200 lr 2.327e-04 [train_loss] tar_ll 0.9794 loss -0.9794 (6.069 secs)\n",
      "isanp:isanp-num_latents-8_matern step 52400 lr 2.312e-04 [train_loss] tar_ll 1.0003 loss -1.0003 (5.666 secs)\n",
      "isanp:isanp-num_latents-8_matern step 52600 lr 2.296e-04 [train_loss] tar_ll 1.0156 loss -1.0156 (5.668 secs)\n",
      "isanp:isanp-num_latents-8_matern step 52800 lr 2.280e-04 [train_loss] tar_ll 1.0464 loss -1.0464 (5.986 secs)\n",
      "isanp:isanp-num_latents-8_matern step 53000 lr 2.265e-04 [train_loss] tar_ll 1.0137 loss -1.0137 (5.927 secs)\n",
      "isanp:isanp-num_latents-8_matern step 53200 lr 2.249e-04 [train_loss] tar_ll 1.0053 loss -1.0053 (6.164 secs)\n",
      "isanp:isanp-num_latents-8_matern step 53400 lr 2.233e-04 [train_loss] tar_ll 1.0050 loss -1.0050 (5.859 secs)\n",
      "isanp:isanp-num_latents-8_matern step 53600 lr 2.218e-04 [train_loss] tar_ll 0.9426 loss -0.9426 (5.846 secs)\n",
      "isanp:isanp-num_latents-8_matern step 53800 lr 2.202e-04 [train_loss] tar_ll 1.0679 loss -1.0679 (5.882 secs)\n",
      "isanp:isanp-num_latents-8_matern step 54000 lr 2.187e-04 [train_loss] tar_ll 1.0349 loss -1.0349 (5.908 secs)\n",
      "isanp:isanp-num_latents-8_matern step 54200 lr 2.171e-04 [train_loss] tar_ll 1.1918 loss -1.1918 (5.822 secs)\n",
      "isanp:isanp-num_latents-8_matern step 54400 lr 2.156e-04 [train_loss] tar_ll 0.9972 loss -0.9972 (5.895 secs)\n",
      "isanp:isanp-num_latents-8_matern step 54600 lr 2.140e-04 [train_loss] tar_ll 1.0464 loss -1.0464 (5.812 secs)\n",
      "isanp:isanp-num_latents-8_matern step 54800 lr 2.124e-04 [train_loss] tar_ll 1.1313 loss -1.1313 (5.748 secs)\n",
      "isanp:isanp-num_latents-8_matern step 55000 lr 2.109e-04 [train_loss] tar_ll 1.0601 loss -1.0601 (5.882 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 103.47it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.6603 loss -0.6603 (28.996 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 55200 lr 2.093e-04 [train_loss] tar_ll 0.9955 loss -0.9955 (6.005 secs)\n",
      "isanp:isanp-num_latents-8_matern step 55400 lr 2.078e-04 [train_loss] tar_ll 1.1115 loss -1.1115 (5.727 secs)\n",
      "isanp:isanp-num_latents-8_matern step 55600 lr 2.062e-04 [train_loss] tar_ll 1.1194 loss -1.1194 (6.037 secs)\n",
      "isanp:isanp-num_latents-8_matern step 55800 lr 2.047e-04 [train_loss] tar_ll 0.9362 loss -0.9362 (5.825 secs)\n",
      "isanp:isanp-num_latents-8_matern step 56000 lr 2.032e-04 [train_loss] tar_ll 1.0708 loss -1.0708 (6.034 secs)\n",
      "isanp:isanp-num_latents-8_matern step 56200 lr 2.016e-04 [train_loss] tar_ll 1.1319 loss -1.1319 (6.000 secs)\n",
      "isanp:isanp-num_latents-8_matern step 56400 lr 2.001e-04 [train_loss] tar_ll 1.0447 loss -1.0447 (5.990 secs)\n",
      "isanp:isanp-num_latents-8_matern step 56600 lr 1.985e-04 [train_loss] tar_ll 1.0556 loss -1.0556 (6.172 secs)\n",
      "isanp:isanp-num_latents-8_matern step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0312 loss -1.0312 (5.833 secs)\n",
      "isanp:isanp-num_latents-8_matern step 57000 lr 1.955e-04 [train_loss] tar_ll 1.0741 loss -1.0741 (6.008 secs)\n",
      "isanp:isanp-num_latents-8_matern step 57200 lr 1.939e-04 [train_loss] tar_ll 1.0651 loss -1.0651 (6.138 secs)\n",
      "isanp:isanp-num_latents-8_matern step 57400 lr 1.924e-04 [train_loss] tar_ll 1.0220 loss -1.0220 (5.924 secs)\n",
      "isanp:isanp-num_latents-8_matern step 57600 lr 1.909e-04 [train_loss] tar_ll 1.1250 loss -1.1250 (6.006 secs)\n",
      "isanp:isanp-num_latents-8_matern step 57800 lr 1.894e-04 [train_loss] tar_ll 1.0481 loss -1.0481 (6.023 secs)\n",
      "isanp:isanp-num_latents-8_matern step 58000 lr 1.878e-04 [train_loss] tar_ll 1.1374 loss -1.1374 (5.958 secs)\n",
      "isanp:isanp-num_latents-8_matern step 58200 lr 1.863e-04 [train_loss] tar_ll 1.1279 loss -1.1279 (6.180 secs)\n",
      "isanp:isanp-num_latents-8_matern step 58400 lr 1.848e-04 [train_loss] tar_ll 1.0972 loss -1.0972 (5.760 secs)\n",
      "isanp:isanp-num_latents-8_matern step 58600 lr 1.833e-04 [train_loss] tar_ll 1.1111 loss -1.1111 (5.724 secs)\n",
      "isanp:isanp-num_latents-8_matern step 58800 lr 1.818e-04 [train_loss] tar_ll 1.0407 loss -1.0407 (5.802 secs)\n",
      "isanp:isanp-num_latents-8_matern step 59000 lr 1.803e-04 [train_loss] tar_ll 1.0573 loss -1.0573 (5.784 secs)\n",
      "isanp:isanp-num_latents-8_matern step 59200 lr 1.787e-04 [train_loss] tar_ll 1.1359 loss -1.1359 (5.851 secs)\n",
      "isanp:isanp-num_latents-8_matern step 59400 lr 1.772e-04 [train_loss] tar_ll 1.0881 loss -1.0881 (6.007 secs)\n",
      "isanp:isanp-num_latents-8_matern step 59600 lr 1.757e-04 [train_loss] tar_ll 1.0831 loss -1.0831 (5.749 secs)\n",
      "isanp:isanp-num_latents-8_matern step 59800 lr 1.742e-04 [train_loss] tar_ll 1.1553 loss -1.1553 (5.884 secs)\n",
      "isanp:isanp-num_latents-8_matern step 60000 lr 1.727e-04 [train_loss] tar_ll 1.0925 loss -1.0925 (5.656 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 106.64it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.7163 loss -0.7163 (28.132 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 60200 lr 1.713e-04 [train_loss] tar_ll 1.0011 loss -1.0011 (5.766 secs)\n",
      "isanp:isanp-num_latents-8_matern step 60400 lr 1.698e-04 [train_loss] tar_ll 1.0106 loss -1.0106 (5.753 secs)\n",
      "isanp:isanp-num_latents-8_matern step 60600 lr 1.683e-04 [train_loss] tar_ll 1.0003 loss -1.0003 (5.888 secs)\n",
      "isanp:isanp-num_latents-8_matern step 60800 lr 1.668e-04 [train_loss] tar_ll 0.9859 loss -0.9859 (5.746 secs)\n",
      "isanp:isanp-num_latents-8_matern step 61000 lr 1.653e-04 [train_loss] tar_ll 1.0683 loss -1.0683 (5.913 secs)\n",
      "isanp:isanp-num_latents-8_matern step 61200 lr 1.638e-04 [train_loss] tar_ll 1.1249 loss -1.1249 (5.823 secs)\n",
      "isanp:isanp-num_latents-8_matern step 61400 lr 1.624e-04 [train_loss] tar_ll 1.1208 loss -1.1208 (5.769 secs)\n",
      "isanp:isanp-num_latents-8_matern step 61600 lr 1.609e-04 [train_loss] tar_ll 1.0691 loss -1.0691 (5.891 secs)\n",
      "isanp:isanp-num_latents-8_matern step 61800 lr 1.594e-04 [train_loss] tar_ll 1.0674 loss -1.0674 (6.337 secs)\n",
      "isanp:isanp-num_latents-8_matern step 62000 lr 1.580e-04 [train_loss] tar_ll 1.0965 loss -1.0965 (6.279 secs)\n",
      "isanp:isanp-num_latents-8_matern step 62200 lr 1.565e-04 [train_loss] tar_ll 1.0717 loss -1.0717 (5.990 secs)\n",
      "isanp:isanp-num_latents-8_matern step 62400 lr 1.551e-04 [train_loss] tar_ll 1.1520 loss -1.1520 (5.851 secs)\n",
      "isanp:isanp-num_latents-8_matern step 62600 lr 1.536e-04 [train_loss] tar_ll 1.1247 loss -1.1247 (5.760 secs)\n",
      "isanp:isanp-num_latents-8_matern step 62800 lr 1.522e-04 [train_loss] tar_ll 1.0767 loss -1.0767 (5.728 secs)\n",
      "isanp:isanp-num_latents-8_matern step 63000 lr 1.507e-04 [train_loss] tar_ll 1.1449 loss -1.1449 (5.708 secs)\n",
      "isanp:isanp-num_latents-8_matern step 63200 lr 1.493e-04 [train_loss] tar_ll 1.0342 loss -1.0342 (6.657 secs)\n",
      "isanp:isanp-num_latents-8_matern step 63400 lr 1.478e-04 [train_loss] tar_ll 1.1920 loss -1.1920 (6.444 secs)\n",
      "isanp:isanp-num_latents-8_matern step 63600 lr 1.464e-04 [train_loss] tar_ll 1.1546 loss -1.1546 (6.908 secs)\n",
      "isanp:isanp-num_latents-8_matern step 63800 lr 1.450e-04 [train_loss] tar_ll 1.1669 loss -1.1669 (5.978 secs)\n",
      "isanp:isanp-num_latents-8_matern step 64000 lr 1.436e-04 [train_loss] tar_ll 1.1170 loss -1.1170 (5.776 secs)\n",
      "isanp:isanp-num_latents-8_matern step 64200 lr 1.421e-04 [train_loss] tar_ll 1.1868 loss -1.1868 (5.829 secs)\n",
      "isanp:isanp-num_latents-8_matern step 64400 lr 1.407e-04 [train_loss] tar_ll 1.1086 loss -1.1086 (5.685 secs)\n",
      "isanp:isanp-num_latents-8_matern step 64600 lr 1.393e-04 [train_loss] tar_ll 1.1763 loss -1.1763 (5.826 secs)\n",
      "isanp:isanp-num_latents-8_matern step 64800 lr 1.379e-04 [train_loss] tar_ll 1.1466 loss -1.1466 (5.782 secs)\n",
      "isanp:isanp-num_latents-8_matern step 65000 lr 1.365e-04 [train_loss] tar_ll 1.1782 loss -1.1782 (5.699 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 102.04it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.7369 loss -0.7369 (29.400 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 65200 lr 1.351e-04 [train_loss] tar_ll 1.1623 loss -1.1623 (6.076 secs)\n",
      "isanp:isanp-num_latents-8_matern step 65400 lr 1.337e-04 [train_loss] tar_ll 1.1277 loss -1.1277 (6.092 secs)\n",
      "isanp:isanp-num_latents-8_matern step 65600 lr 1.323e-04 [train_loss] tar_ll 1.0785 loss -1.0785 (5.754 secs)\n",
      "isanp:isanp-num_latents-8_matern step 65800 lr 1.309e-04 [train_loss] tar_ll 1.1918 loss -1.1918 (5.711 secs)\n",
      "isanp:isanp-num_latents-8_matern step 66000 lr 1.296e-04 [train_loss] tar_ll 1.1183 loss -1.1183 (5.826 secs)\n",
      "isanp:isanp-num_latents-8_matern step 66200 lr 1.282e-04 [train_loss] tar_ll 1.1366 loss -1.1366 (5.973 secs)\n",
      "isanp:isanp-num_latents-8_matern step 66400 lr 1.268e-04 [train_loss] tar_ll 1.1290 loss -1.1290 (6.032 secs)\n",
      "isanp:isanp-num_latents-8_matern step 66600 lr 1.255e-04 [train_loss] tar_ll 1.1486 loss -1.1486 (5.996 secs)\n",
      "isanp:isanp-num_latents-8_matern step 66800 lr 1.241e-04 [train_loss] tar_ll 1.2123 loss -1.2123 (5.974 secs)\n",
      "isanp:isanp-num_latents-8_matern step 67000 lr 1.227e-04 [train_loss] tar_ll 1.0841 loss -1.0841 (6.047 secs)\n",
      "isanp:isanp-num_latents-8_matern step 67200 lr 1.214e-04 [train_loss] tar_ll 1.1293 loss -1.1293 (6.143 secs)\n",
      "isanp:isanp-num_latents-8_matern step 67400 lr 1.200e-04 [train_loss] tar_ll 1.1741 loss -1.1741 (5.925 secs)\n",
      "isanp:isanp-num_latents-8_matern step 67600 lr 1.187e-04 [train_loss] tar_ll 1.2254 loss -1.2254 (5.988 secs)\n",
      "isanp:isanp-num_latents-8_matern step 67800 lr 1.174e-04 [train_loss] tar_ll 1.1353 loss -1.1353 (5.955 secs)\n",
      "isanp:isanp-num_latents-8_matern step 68000 lr 1.160e-04 [train_loss] tar_ll 1.2580 loss -1.2580 (5.901 secs)\n",
      "isanp:isanp-num_latents-8_matern step 68200 lr 1.147e-04 [train_loss] tar_ll 1.2019 loss -1.2019 (6.272 secs)\n",
      "isanp:isanp-num_latents-8_matern step 68400 lr 1.134e-04 [train_loss] tar_ll 1.0812 loss -1.0812 (6.273 secs)\n",
      "isanp:isanp-num_latents-8_matern step 68600 lr 1.121e-04 [train_loss] tar_ll 1.1432 loss -1.1432 (6.257 secs)\n",
      "isanp:isanp-num_latents-8_matern step 68800 lr 1.108e-04 [train_loss] tar_ll 1.1632 loss -1.1632 (6.202 secs)\n",
      "isanp:isanp-num_latents-8_matern step 69000 lr 1.095e-04 [train_loss] tar_ll 1.2191 loss -1.2191 (6.003 secs)\n",
      "isanp:isanp-num_latents-8_matern step 69200 lr 1.082e-04 [train_loss] tar_ll 1.2144 loss -1.2144 (5.977 secs)\n",
      "isanp:isanp-num_latents-8_matern step 69400 lr 1.069e-04 [train_loss] tar_ll 1.1536 loss -1.1536 (5.904 secs)\n",
      "isanp:isanp-num_latents-8_matern step 69600 lr 1.056e-04 [train_loss] tar_ll 1.2184 loss -1.2184 (5.886 secs)\n",
      "isanp:isanp-num_latents-8_matern step 69800 lr 1.043e-04 [train_loss] tar_ll 1.0338 loss -1.0338 (5.740 secs)\n",
      "isanp:isanp-num_latents-8_matern step 70000 lr 1.031e-04 [train_loss] tar_ll 1.2069 loss -1.2069 (5.700 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 104.28it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.7552 loss -0.7552 (28.771 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 70200 lr 1.018e-04 [train_loss] tar_ll 1.2332 loss -1.2332 (6.607 secs)\n",
      "isanp:isanp-num_latents-8_matern step 70400 lr 1.005e-04 [train_loss] tar_ll 1.1949 loss -1.1949 (6.605 secs)\n",
      "isanp:isanp-num_latents-8_matern step 70600 lr 9.927e-05 [train_loss] tar_ll 1.1331 loss -1.1331 (6.436 secs)\n",
      "isanp:isanp-num_latents-8_matern step 70800 lr 9.802e-05 [train_loss] tar_ll 1.1727 loss -1.1727 (6.496 secs)\n",
      "isanp:isanp-num_latents-8_matern step 71000 lr 9.677e-05 [train_loss] tar_ll 1.1144 loss -1.1144 (6.588 secs)\n",
      "isanp:isanp-num_latents-8_matern step 71200 lr 9.554e-05 [train_loss] tar_ll 1.1398 loss -1.1398 (6.471 secs)\n",
      "isanp:isanp-num_latents-8_matern step 71400 lr 9.430e-05 [train_loss] tar_ll 1.1896 loss -1.1896 (6.402 secs)\n",
      "isanp:isanp-num_latents-8_matern step 71600 lr 9.308e-05 [train_loss] tar_ll 1.1947 loss -1.1947 (6.840 secs)\n",
      "isanp:isanp-num_latents-8_matern step 71800 lr 9.186e-05 [train_loss] tar_ll 1.1197 loss -1.1197 (6.148 secs)\n",
      "isanp:isanp-num_latents-8_matern step 72000 lr 9.064e-05 [train_loss] tar_ll 1.1950 loss -1.1950 (5.640 secs)\n",
      "isanp:isanp-num_latents-8_matern step 72200 lr 8.944e-05 [train_loss] tar_ll 1.2165 loss -1.2165 (5.838 secs)\n",
      "isanp:isanp-num_latents-8_matern step 72400 lr 8.824e-05 [train_loss] tar_ll 1.1105 loss -1.1105 (5.791 secs)\n",
      "isanp:isanp-num_latents-8_matern step 72600 lr 8.704e-05 [train_loss] tar_ll 1.1675 loss -1.1675 (5.654 secs)\n",
      "isanp:isanp-num_latents-8_matern step 72800 lr 8.585e-05 [train_loss] tar_ll 1.2494 loss -1.2494 (5.766 secs)\n",
      "isanp:isanp-num_latents-8_matern step 73000 lr 8.467e-05 [train_loss] tar_ll 1.1495 loss -1.1495 (5.720 secs)\n",
      "isanp:isanp-num_latents-8_matern step 73200 lr 8.350e-05 [train_loss] tar_ll 1.2208 loss -1.2208 (5.878 secs)\n",
      "isanp:isanp-num_latents-8_matern step 73400 lr 8.233e-05 [train_loss] tar_ll 1.2265 loss -1.2265 (5.855 secs)\n",
      "isanp:isanp-num_latents-8_matern step 73600 lr 8.117e-05 [train_loss] tar_ll 1.1907 loss -1.1907 (5.828 secs)\n",
      "isanp:isanp-num_latents-8_matern step 73800 lr 8.001e-05 [train_loss] tar_ll 1.2605 loss -1.2605 (5.750 secs)\n",
      "isanp:isanp-num_latents-8_matern step 74000 lr 7.886e-05 [train_loss] tar_ll 1.1688 loss -1.1688 (5.835 secs)\n",
      "isanp:isanp-num_latents-8_matern step 74200 lr 7.772e-05 [train_loss] tar_ll 1.1365 loss -1.1365 (5.836 secs)\n",
      "isanp:isanp-num_latents-8_matern step 74400 lr 7.659e-05 [train_loss] tar_ll 1.2176 loss -1.2176 (5.794 secs)\n",
      "isanp:isanp-num_latents-8_matern step 74600 lr 7.546e-05 [train_loss] tar_ll 1.1684 loss -1.1684 (5.914 secs)\n",
      "isanp:isanp-num_latents-8_matern step 74800 lr 7.434e-05 [train_loss] tar_ll 1.2251 loss -1.2251 (5.821 secs)\n",
      "isanp:isanp-num_latents-8_matern step 75000 lr 7.322e-05 [train_loss] tar_ll 1.2122 loss -1.2122 (5.882 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 100.69it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.7645 loss -0.7645 (29.795 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 75200 lr 7.212e-05 [train_loss] tar_ll 1.2883 loss -1.2883 (5.882 secs)\n",
      "isanp:isanp-num_latents-8_matern step 75400 lr 7.102e-05 [train_loss] tar_ll 1.1748 loss -1.1748 (5.872 secs)\n",
      "isanp:isanp-num_latents-8_matern step 75600 lr 6.992e-05 [train_loss] tar_ll 1.1375 loss -1.1375 (5.773 secs)\n",
      "isanp:isanp-num_latents-8_matern step 75800 lr 6.884e-05 [train_loss] tar_ll 1.1596 loss -1.1596 (5.700 secs)\n",
      "isanp:isanp-num_latents-8_matern step 76000 lr 6.776e-05 [train_loss] tar_ll 1.1993 loss -1.1993 (5.744 secs)\n",
      "isanp:isanp-num_latents-8_matern step 76200 lr 6.669e-05 [train_loss] tar_ll 1.1355 loss -1.1355 (6.126 secs)\n",
      "isanp:isanp-num_latents-8_matern step 76400 lr 6.562e-05 [train_loss] tar_ll 1.2358 loss -1.2358 (5.847 secs)\n",
      "isanp:isanp-num_latents-8_matern step 76600 lr 6.456e-05 [train_loss] tar_ll 1.2315 loss -1.2315 (5.992 secs)\n",
      "isanp:isanp-num_latents-8_matern step 76800 lr 6.351e-05 [train_loss] tar_ll 1.2150 loss -1.2150 (6.044 secs)\n",
      "isanp:isanp-num_latents-8_matern step 77000 lr 6.247e-05 [train_loss] tar_ll 1.2545 loss -1.2545 (6.014 secs)\n",
      "isanp:isanp-num_latents-8_matern step 77200 lr 6.144e-05 [train_loss] tar_ll 1.1896 loss -1.1896 (5.728 secs)\n",
      "isanp:isanp-num_latents-8_matern step 77400 lr 6.041e-05 [train_loss] tar_ll 1.1970 loss -1.1970 (5.764 secs)\n",
      "isanp:isanp-num_latents-8_matern step 77600 lr 5.939e-05 [train_loss] tar_ll 1.1627 loss -1.1627 (5.710 secs)\n",
      "isanp:isanp-num_latents-8_matern step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2713 loss -1.2713 (5.808 secs)\n",
      "isanp:isanp-num_latents-8_matern step 78000 lr 5.737e-05 [train_loss] tar_ll 1.1237 loss -1.1237 (5.731 secs)\n",
      "isanp:isanp-num_latents-8_matern step 78200 lr 5.637e-05 [train_loss] tar_ll 1.2417 loss -1.2417 (5.718 secs)\n",
      "isanp:isanp-num_latents-8_matern step 78400 lr 5.538e-05 [train_loss] tar_ll 1.2194 loss -1.2194 (5.890 secs)\n",
      "isanp:isanp-num_latents-8_matern step 78600 lr 5.440e-05 [train_loss] tar_ll 1.1768 loss -1.1768 (5.707 secs)\n",
      "isanp:isanp-num_latents-8_matern step 78800 lr 5.343e-05 [train_loss] tar_ll 1.2543 loss -1.2543 (5.674 secs)\n",
      "isanp:isanp-num_latents-8_matern step 79000 lr 5.246e-05 [train_loss] tar_ll 1.1928 loss -1.1928 (5.743 secs)\n",
      "isanp:isanp-num_latents-8_matern step 79200 lr 5.150e-05 [train_loss] tar_ll 1.2525 loss -1.2525 (5.738 secs)\n",
      "isanp:isanp-num_latents-8_matern step 79400 lr 5.055e-05 [train_loss] tar_ll 1.1744 loss -1.1744 (5.812 secs)\n",
      "isanp:isanp-num_latents-8_matern step 79600 lr 4.961e-05 [train_loss] tar_ll 1.1668 loss -1.1668 (5.919 secs)\n",
      "isanp:isanp-num_latents-8_matern step 79800 lr 4.867e-05 [train_loss] tar_ll 1.2189 loss -1.2189 (5.794 secs)\n",
      "isanp:isanp-num_latents-8_matern step 80000 lr 4.775e-05 [train_loss] tar_ll 1.2198 loss -1.2198 (5.835 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 108.59it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.7966 loss -0.7966 (27.628 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 80200 lr 4.683e-05 [train_loss] tar_ll 1.2704 loss -1.2704 (5.866 secs)\n",
      "isanp:isanp-num_latents-8_matern step 80400 lr 4.592e-05 [train_loss] tar_ll 1.1908 loss -1.1908 (5.700 secs)\n",
      "isanp:isanp-num_latents-8_matern step 80600 lr 4.501e-05 [train_loss] tar_ll 1.2112 loss -1.2112 (5.693 secs)\n",
      "isanp:isanp-num_latents-8_matern step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2277 loss -1.2277 (5.795 secs)\n",
      "isanp:isanp-num_latents-8_matern step 81000 lr 4.323e-05 [train_loss] tar_ll 1.2080 loss -1.2080 (5.739 secs)\n",
      "isanp:isanp-num_latents-8_matern step 81200 lr 4.235e-05 [train_loss] tar_ll 1.1951 loss -1.1951 (6.357 secs)\n",
      "isanp:isanp-num_latents-8_matern step 81400 lr 4.148e-05 [train_loss] tar_ll 1.2502 loss -1.2502 (6.365 secs)\n",
      "isanp:isanp-num_latents-8_matern step 81600 lr 4.062e-05 [train_loss] tar_ll 1.2442 loss -1.2442 (5.858 secs)\n",
      "isanp:isanp-num_latents-8_matern step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2361 loss -1.2361 (5.977 secs)\n",
      "isanp:isanp-num_latents-8_matern step 82000 lr 3.892e-05 [train_loss] tar_ll 1.2207 loss -1.2207 (5.777 secs)\n",
      "isanp:isanp-num_latents-8_matern step 82200 lr 3.808e-05 [train_loss] tar_ll 1.1438 loss -1.1438 (5.864 secs)\n",
      "isanp:isanp-num_latents-8_matern step 82400 lr 3.725e-05 [train_loss] tar_ll 1.2648 loss -1.2648 (5.783 secs)\n",
      "isanp:isanp-num_latents-8_matern step 82600 lr 3.643e-05 [train_loss] tar_ll 1.2753 loss -1.2753 (5.878 secs)\n",
      "isanp:isanp-num_latents-8_matern step 82800 lr 3.562e-05 [train_loss] tar_ll 1.2557 loss -1.2557 (5.731 secs)\n",
      "isanp:isanp-num_latents-8_matern step 83000 lr 3.481e-05 [train_loss] tar_ll 1.2336 loss -1.2336 (5.849 secs)\n",
      "isanp:isanp-num_latents-8_matern step 83200 lr 3.402e-05 [train_loss] tar_ll 1.1808 loss -1.1808 (5.775 secs)\n",
      "isanp:isanp-num_latents-8_matern step 83400 lr 3.323e-05 [train_loss] tar_ll 1.2552 loss -1.2552 (5.891 secs)\n",
      "isanp:isanp-num_latents-8_matern step 83600 lr 3.245e-05 [train_loss] tar_ll 1.2675 loss -1.2675 (5.796 secs)\n",
      "isanp:isanp-num_latents-8_matern step 83800 lr 3.168e-05 [train_loss] tar_ll 1.2343 loss -1.2343 (5.826 secs)\n",
      "isanp:isanp-num_latents-8_matern step 84000 lr 3.092e-05 [train_loss] tar_ll 1.2585 loss -1.2585 (5.931 secs)\n",
      "isanp:isanp-num_latents-8_matern step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2560 loss -1.2560 (5.806 secs)\n",
      "isanp:isanp-num_latents-8_matern step 84400 lr 2.943e-05 [train_loss] tar_ll 1.2654 loss -1.2654 (5.871 secs)\n",
      "isanp:isanp-num_latents-8_matern step 84600 lr 2.869e-05 [train_loss] tar_ll 1.1481 loss -1.1481 (5.951 secs)\n",
      "isanp:isanp-num_latents-8_matern step 84800 lr 2.797e-05 [train_loss] tar_ll 1.1687 loss -1.1687 (5.844 secs)\n",
      "isanp:isanp-num_latents-8_matern step 85000 lr 2.725e-05 [train_loss] tar_ll 1.1725 loss -1.1725 (5.807 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 107.59it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.8070 loss -0.8070 (27.884 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 85200 lr 2.654e-05 [train_loss] tar_ll 1.2712 loss -1.2712 (5.937 secs)\n",
      "isanp:isanp-num_latents-8_matern step 85400 lr 2.584e-05 [train_loss] tar_ll 1.2349 loss -1.2349 (5.795 secs)\n",
      "isanp:isanp-num_latents-8_matern step 85600 lr 2.515e-05 [train_loss] tar_ll 1.2353 loss -1.2353 (5.827 secs)\n",
      "isanp:isanp-num_latents-8_matern step 85800 lr 2.447e-05 [train_loss] tar_ll 1.2356 loss -1.2356 (5.909 secs)\n",
      "isanp:isanp-num_latents-8_matern step 86000 lr 2.379e-05 [train_loss] tar_ll 1.2361 loss -1.2361 (5.762 secs)\n",
      "isanp:isanp-num_latents-8_matern step 86200 lr 2.313e-05 [train_loss] tar_ll 1.2326 loss -1.2326 (5.817 secs)\n",
      "isanp:isanp-num_latents-8_matern step 86400 lr 2.247e-05 [train_loss] tar_ll 1.2023 loss -1.2023 (5.855 secs)\n",
      "isanp:isanp-num_latents-8_matern step 86600 lr 2.183e-05 [train_loss] tar_ll 1.2011 loss -1.2011 (5.953 secs)\n",
      "isanp:isanp-num_latents-8_matern step 86800 lr 2.119e-05 [train_loss] tar_ll 1.2574 loss -1.2574 (6.118 secs)\n",
      "isanp:isanp-num_latents-8_matern step 87000 lr 2.056e-05 [train_loss] tar_ll 1.2097 loss -1.2097 (5.950 secs)\n",
      "isanp:isanp-num_latents-8_matern step 87200 lr 1.994e-05 [train_loss] tar_ll 1.2243 loss -1.2243 (5.929 secs)\n",
      "isanp:isanp-num_latents-8_matern step 87400 lr 1.933e-05 [train_loss] tar_ll 1.2541 loss -1.2541 (5.926 secs)\n",
      "isanp:isanp-num_latents-8_matern step 87600 lr 1.873e-05 [train_loss] tar_ll 1.2762 loss -1.2762 (6.033 secs)\n",
      "isanp:isanp-num_latents-8_matern step 87800 lr 1.814e-05 [train_loss] tar_ll 1.1950 loss -1.1950 (5.678 secs)\n",
      "isanp:isanp-num_latents-8_matern step 88000 lr 1.756e-05 [train_loss] tar_ll 1.2672 loss -1.2672 (5.819 secs)\n",
      "isanp:isanp-num_latents-8_matern step 88200 lr 1.698e-05 [train_loss] tar_ll 1.2810 loss -1.2810 (5.723 secs)\n",
      "isanp:isanp-num_latents-8_matern step 88400 lr 1.642e-05 [train_loss] tar_ll 1.2723 loss -1.2723 (5.719 secs)\n",
      "isanp:isanp-num_latents-8_matern step 88600 lr 1.586e-05 [train_loss] tar_ll 1.2019 loss -1.2019 (5.698 secs)\n",
      "isanp:isanp-num_latents-8_matern step 88800 lr 1.532e-05 [train_loss] tar_ll 1.2315 loss -1.2315 (5.649 secs)\n",
      "isanp:isanp-num_latents-8_matern step 89000 lr 1.478e-05 [train_loss] tar_ll 1.2131 loss -1.2131 (5.767 secs)\n",
      "isanp:isanp-num_latents-8_matern step 89200 lr 1.425e-05 [train_loss] tar_ll 1.3519 loss -1.3519 (5.756 secs)\n",
      "isanp:isanp-num_latents-8_matern step 89400 lr 1.373e-05 [train_loss] tar_ll 1.2124 loss -1.2124 (5.671 secs)\n",
      "isanp:isanp-num_latents-8_matern step 89600 lr 1.323e-05 [train_loss] tar_ll 1.2727 loss -1.2727 (5.792 secs)\n",
      "isanp:isanp-num_latents-8_matern step 89800 lr 1.273e-05 [train_loss] tar_ll 1.2926 loss -1.2926 (5.758 secs)\n",
      "isanp:isanp-num_latents-8_matern step 90000 lr 1.224e-05 [train_loss] tar_ll 1.2207 loss -1.2207 (5.812 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 108.53it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.8118 loss -0.8118 (27.644 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 90200 lr 1.176e-05 [train_loss] tar_ll 1.2793 loss -1.2793 (5.756 secs)\n",
      "isanp:isanp-num_latents-8_matern step 90400 lr 1.128e-05 [train_loss] tar_ll 1.2583 loss -1.2583 (5.782 secs)\n",
      "isanp:isanp-num_latents-8_matern step 90600 lr 1.082e-05 [train_loss] tar_ll 1.2948 loss -1.2948 (5.870 secs)\n",
      "isanp:isanp-num_latents-8_matern step 90800 lr 1.037e-05 [train_loss] tar_ll 1.2580 loss -1.2580 (5.853 secs)\n",
      "isanp:isanp-num_latents-8_matern step 91000 lr 9.927e-06 [train_loss] tar_ll 1.2041 loss -1.2041 (5.776 secs)\n",
      "isanp:isanp-num_latents-8_matern step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2721 loss -1.2721 (5.802 secs)\n",
      "isanp:isanp-num_latents-8_matern step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2490 loss -1.2490 (5.809 secs)\n",
      "isanp:isanp-num_latents-8_matern step 91600 lr 8.655e-06 [train_loss] tar_ll 1.2954 loss -1.2954 (5.756 secs)\n",
      "isanp:isanp-num_latents-8_matern step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2567 loss -1.2567 (5.802 secs)\n",
      "isanp:isanp-num_latents-8_matern step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2389 loss -1.2389 (5.864 secs)\n",
      "isanp:isanp-num_latents-8_matern step 92200 lr 7.468e-06 [train_loss] tar_ll 1.2374 loss -1.2374 (6.421 secs)\n",
      "isanp:isanp-num_latents-8_matern step 92400 lr 7.092e-06 [train_loss] tar_ll 1.2979 loss -1.2979 (6.434 secs)\n",
      "isanp:isanp-num_latents-8_matern step 92600 lr 6.725e-06 [train_loss] tar_ll 1.2594 loss -1.2594 (6.322 secs)\n",
      "isanp:isanp-num_latents-8_matern step 92800 lr 6.368e-06 [train_loss] tar_ll 1.2307 loss -1.2307 (5.929 secs)\n",
      "isanp:isanp-num_latents-8_matern step 93000 lr 6.021e-06 [train_loss] tar_ll 1.3429 loss -1.3429 (6.182 secs)\n",
      "isanp:isanp-num_latents-8_matern step 93200 lr 5.683e-06 [train_loss] tar_ll 1.2923 loss -1.2923 (5.837 secs)\n",
      "isanp:isanp-num_latents-8_matern step 93400 lr 5.355e-06 [train_loss] tar_ll 1.2159 loss -1.2159 (5.734 secs)\n",
      "isanp:isanp-num_latents-8_matern step 93600 lr 5.036e-06 [train_loss] tar_ll 1.2813 loss -1.2813 (5.938 secs)\n",
      "isanp:isanp-num_latents-8_matern step 93800 lr 4.727e-06 [train_loss] tar_ll 1.2374 loss -1.2374 (5.782 secs)\n",
      "isanp:isanp-num_latents-8_matern step 94000 lr 4.428e-06 [train_loss] tar_ll 1.2314 loss -1.2314 (5.834 secs)\n",
      "isanp:isanp-num_latents-8_matern step 94200 lr 4.139e-06 [train_loss] tar_ll 1.1848 loss -1.1848 (5.972 secs)\n",
      "isanp:isanp-num_latents-8_matern step 94400 lr 3.859e-06 [train_loss] tar_ll 1.2534 loss -1.2534 (5.786 secs)\n",
      "isanp:isanp-num_latents-8_matern step 94600 lr 3.589e-06 [train_loss] tar_ll 1.2554 loss -1.2554 (5.858 secs)\n",
      "isanp:isanp-num_latents-8_matern step 94800 lr 3.329e-06 [train_loss] tar_ll 1.3274 loss -1.3274 (5.794 secs)\n",
      "isanp:isanp-num_latents-8_matern step 95000 lr 3.078e-06 [train_loss] tar_ll 1.2657 loss -1.2657 (5.781 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 103.63it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.8175 loss -0.8175 (28.952 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_matern step 95200 lr 2.837e-06 [train_loss] tar_ll 1.2033 loss -1.2033 (6.157 secs)\n",
      "isanp:isanp-num_latents-8_matern step 95400 lr 2.606e-06 [train_loss] tar_ll 1.3057 loss -1.3057 (5.772 secs)\n",
      "isanp:isanp-num_latents-8_matern step 95600 lr 2.385e-06 [train_loss] tar_ll 1.1427 loss -1.1427 (5.719 secs)\n",
      "isanp:isanp-num_latents-8_matern step 95800 lr 2.173e-06 [train_loss] tar_ll 1.1956 loss -1.1956 (5.872 secs)\n",
      "isanp:isanp-num_latents-8_matern step 96000 lr 1.971e-06 [train_loss] tar_ll 1.3127 loss -1.3127 (5.813 secs)\n",
      "isanp:isanp-num_latents-8_matern step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2478 loss -1.2478 (5.798 secs)\n",
      "isanp:isanp-num_latents-8_matern step 96400 lr 1.597e-06 [train_loss] tar_ll 1.2803 loss -1.2803 (5.815 secs)\n",
      "isanp:isanp-num_latents-8_matern step 96600 lr 1.425e-06 [train_loss] tar_ll 1.1919 loss -1.1919 (5.740 secs)\n",
      "isanp:isanp-num_latents-8_matern step 96800 lr 1.262e-06 [train_loss] tar_ll 1.2114 loss -1.2114 (5.878 secs)\n",
      "isanp:isanp-num_latents-8_matern step 97000 lr 1.110e-06 [train_loss] tar_ll 1.3036 loss -1.3036 (5.976 secs)\n",
      "isanp:isanp-num_latents-8_matern step 97200 lr 9.666e-07 [train_loss] tar_ll 1.2862 loss -1.2862 (6.066 secs)\n",
      "isanp:isanp-num_latents-8_matern step 97400 lr 8.335e-07 [train_loss] tar_ll 1.2979 loss -1.2979 (5.966 secs)\n",
      "isanp:isanp-num_latents-8_matern step 97600 lr 7.103e-07 [train_loss] tar_ll 1.2106 loss -1.2106 (5.877 secs)\n",
      "isanp:isanp-num_latents-8_matern step 97800 lr 5.969e-07 [train_loss] tar_ll 1.2903 loss -1.2903 (5.821 secs)\n",
      "isanp:isanp-num_latents-8_matern step 98000 lr 4.933e-07 [train_loss] tar_ll 1.2817 loss -1.2817 (6.008 secs)\n",
      "isanp:isanp-num_latents-8_matern step 98200 lr 3.996e-07 [train_loss] tar_ll 1.2931 loss -1.2931 (6.003 secs)\n",
      "isanp:isanp-num_latents-8_matern step 98400 lr 3.158e-07 [train_loss] tar_ll 1.4161 loss -1.4161 (6.016 secs)\n",
      "isanp:isanp-num_latents-8_matern step 98600 lr 2.418e-07 [train_loss] tar_ll 1.2742 loss -1.2742 (5.929 secs)\n",
      "isanp:isanp-num_latents-8_matern step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2921 loss -1.2921 (6.047 secs)\n",
      "isanp:isanp-num_latents-8_matern step 99000 lr 1.234e-07 [train_loss] tar_ll 1.2775 loss -1.2775 (6.225 secs)\n",
      "isanp:isanp-num_latents-8_matern step 99200 lr 7.895e-08 [train_loss] tar_ll 1.2634 loss -1.2634 (6.244 secs)\n",
      "isanp:isanp-num_latents-8_matern step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2650 loss -1.2650 (6.151 secs)\n",
      "isanp:isanp-num_latents-8_matern step 99600 lr 1.974e-08 [train_loss] tar_ll 1.2997 loss -1.2997 (6.211 secs)\n",
      "isanp:isanp-num_latents-8_matern step 99800 lr 4.935e-09 [train_loss] tar_ll 1.2297 loss -1.2297 (5.834 secs)\n",
      "isanp:isanp-num_latents-8_matern step 100000 lr 0.000e+00 [train_loss] tar_ll 1.2366 loss -1.2366 (5.967 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 107.69it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.8181 loss -0.8181 (27.859 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:27<00:00, 107.53it/s]\n",
      "isanp:isanp-num_latents-8_matern matern tar_ll 0.8181 loss -0.8181 (27.900 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "Execution time: 3564548.0 miliseconds\n",
      "Execution time: 3564.548 seconds\n",
      "Initial Memory Usage: 39.69580078125 MB\n",
      "Final Memory Usage: 69.0849609375 MB\n",
      "Memory Usage Change: 29.38916015625 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='isanp-num_latents-8_matern',val_seed=0, val_l=8,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1365a1-dc07-4616-9db0-be0c94016ff9",
   "metadata": {},
   "source": [
    "## ISANP (128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "594e589d-7de5-4d66-94bd-cdd922512910",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: isanp-isanp-num_latents-128_matern\n",
      "Total number of parameters: 793282\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "matern-seed0.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 200 lr 5.000e-04 [train_loss] tar_ll -0.7026 loss 0.7026 (5.829 secs)\n",
      "isanp:isanp-num_latents-128_matern step 400 lr 5.000e-04 [train_loss] tar_ll -0.6599 loss 0.6599 (5.945 secs)\n",
      "isanp:isanp-num_latents-128_matern step 600 lr 5.000e-04 [train_loss] tar_ll -0.5840 loss 0.5840 (5.834 secs)\n",
      "isanp:isanp-num_latents-128_matern step 800 lr 4.999e-04 [train_loss] tar_ll -0.5492 loss 0.5492 (5.934 secs)\n",
      "isanp:isanp-num_latents-128_matern step 1000 lr 4.999e-04 [train_loss] tar_ll -0.5144 loss 0.5144 (5.989 secs)\n",
      "isanp:isanp-num_latents-128_matern step 1200 lr 4.998e-04 [train_loss] tar_ll -0.4125 loss 0.4125 (5.968 secs)\n",
      "isanp:isanp-num_latents-128_matern step 1400 lr 4.998e-04 [train_loss] tar_ll -0.2347 loss 0.2347 (6.104 secs)\n",
      "isanp:isanp-num_latents-128_matern step 1600 lr 4.997e-04 [train_loss] tar_ll -0.1561 loss 0.1561 (5.870 secs)\n",
      "isanp:isanp-num_latents-128_matern step 1800 lr 4.996e-04 [train_loss] tar_ll -0.0436 loss 0.0436 (5.876 secs)\n",
      "isanp:isanp-num_latents-128_matern step 2000 lr 4.995e-04 [train_loss] tar_ll 0.0322 loss -0.0322 (5.934 secs)\n",
      "isanp:isanp-num_latents-128_matern step 2200 lr 4.994e-04 [train_loss] tar_ll 0.1501 loss -0.1501 (6.062 secs)\n",
      "isanp:isanp-num_latents-128_matern step 2400 lr 4.993e-04 [train_loss] tar_ll 0.1637 loss -0.1637 (5.952 secs)\n",
      "isanp:isanp-num_latents-128_matern step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1842 loss -0.1842 (5.879 secs)\n",
      "isanp:isanp-num_latents-128_matern step 2800 lr 4.990e-04 [train_loss] tar_ll 0.2161 loss -0.2161 (5.934 secs)\n",
      "isanp:isanp-num_latents-128_matern step 3000 lr 4.989e-04 [train_loss] tar_ll 0.2822 loss -0.2822 (6.090 secs)\n",
      "isanp:isanp-num_latents-128_matern step 3200 lr 4.987e-04 [train_loss] tar_ll 0.2276 loss -0.2276 (6.053 secs)\n",
      "isanp:isanp-num_latents-128_matern step 3400 lr 4.986e-04 [train_loss] tar_ll 0.3453 loss -0.3453 (5.915 secs)\n",
      "isanp:isanp-num_latents-128_matern step 3600 lr 4.984e-04 [train_loss] tar_ll 0.3373 loss -0.3373 (6.072 secs)\n",
      "isanp:isanp-num_latents-128_matern step 3800 lr 4.982e-04 [train_loss] tar_ll 0.3457 loss -0.3457 (5.909 secs)\n",
      "isanp:isanp-num_latents-128_matern step 4000 lr 4.980e-04 [train_loss] tar_ll 0.3580 loss -0.3580 (6.223 secs)\n",
      "isanp:isanp-num_latents-128_matern step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3895 loss -0.3895 (6.127 secs)\n",
      "isanp:isanp-num_latents-128_matern step 4400 lr 4.976e-04 [train_loss] tar_ll 0.2653 loss -0.2653 (6.014 secs)\n",
      "isanp:isanp-num_latents-128_matern step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4609 loss -0.4609 (6.079 secs)\n",
      "isanp:isanp-num_latents-128_matern step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4440 loss -0.4440 (5.995 secs)\n",
      "isanp:isanp-num_latents-128_matern step 5000 lr 4.969e-04 [train_loss] tar_ll 0.4125 loss -0.4125 (5.916 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 104.87it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.2713 loss -0.2713 (28.608 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 5200 lr 4.967e-04 [train_loss] tar_ll 0.4863 loss -0.4863 (6.147 secs)\n",
      "isanp:isanp-num_latents-128_matern step 5400 lr 4.964e-04 [train_loss] tar_ll 0.5010 loss -0.5010 (6.367 secs)\n",
      "isanp:isanp-num_latents-128_matern step 5600 lr 4.961e-04 [train_loss] tar_ll 0.3645 loss -0.3645 (6.475 secs)\n",
      "isanp:isanp-num_latents-128_matern step 5800 lr 4.959e-04 [train_loss] tar_ll 0.4209 loss -0.4209 (5.987 secs)\n",
      "isanp:isanp-num_latents-128_matern step 6000 lr 4.956e-04 [train_loss] tar_ll 0.5204 loss -0.5204 (6.171 secs)\n",
      "isanp:isanp-num_latents-128_matern step 6200 lr 4.953e-04 [train_loss] tar_ll 0.5375 loss -0.5375 (6.427 secs)\n",
      "isanp:isanp-num_latents-128_matern step 6400 lr 4.950e-04 [train_loss] tar_ll 0.4836 loss -0.4836 (6.187 secs)\n",
      "isanp:isanp-num_latents-128_matern step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5083 loss -0.5083 (6.106 secs)\n",
      "isanp:isanp-num_latents-128_matern step 6800 lr 4.943e-04 [train_loss] tar_ll 0.5731 loss -0.5731 (6.270 secs)\n",
      "isanp:isanp-num_latents-128_matern step 7000 lr 4.940e-04 [train_loss] tar_ll 0.5622 loss -0.5622 (6.169 secs)\n",
      "isanp:isanp-num_latents-128_matern step 7200 lr 4.936e-04 [train_loss] tar_ll 0.5573 loss -0.5573 (6.272 secs)\n",
      "isanp:isanp-num_latents-128_matern step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5825 loss -0.5825 (6.196 secs)\n",
      "isanp:isanp-num_latents-128_matern step 7600 lr 4.929e-04 [train_loss] tar_ll 0.5382 loss -0.5382 (6.120 secs)\n",
      "isanp:isanp-num_latents-128_matern step 7800 lr 4.925e-04 [train_loss] tar_ll 0.5936 loss -0.5936 (6.344 secs)\n",
      "isanp:isanp-num_latents-128_matern step 8000 lr 4.921e-04 [train_loss] tar_ll 0.6063 loss -0.6063 (6.283 secs)\n",
      "isanp:isanp-num_latents-128_matern step 8200 lr 4.918e-04 [train_loss] tar_ll 0.6646 loss -0.6646 (6.319 secs)\n",
      "isanp:isanp-num_latents-128_matern step 8400 lr 4.913e-04 [train_loss] tar_ll 0.3807 loss -0.3807 (6.271 secs)\n",
      "isanp:isanp-num_latents-128_matern step 8600 lr 4.909e-04 [train_loss] tar_ll 0.6413 loss -0.6413 (6.298 secs)\n",
      "isanp:isanp-num_latents-128_matern step 8800 lr 4.905e-04 [train_loss] tar_ll 0.6251 loss -0.6251 (6.294 secs)\n",
      "isanp:isanp-num_latents-128_matern step 9000 lr 4.901e-04 [train_loss] tar_ll 0.6450 loss -0.6450 (5.986 secs)\n",
      "isanp:isanp-num_latents-128_matern step 9200 lr 4.896e-04 [train_loss] tar_ll 0.6445 loss -0.6445 (5.997 secs)\n",
      "isanp:isanp-num_latents-128_matern step 9400 lr 4.892e-04 [train_loss] tar_ll 0.7197 loss -0.7197 (6.060 secs)\n",
      "isanp:isanp-num_latents-128_matern step 9600 lr 4.887e-04 [train_loss] tar_ll 0.6635 loss -0.6635 (5.987 secs)\n",
      "isanp:isanp-num_latents-128_matern step 9800 lr 4.882e-04 [train_loss] tar_ll 0.7241 loss -0.7241 (6.182 secs)\n",
      "isanp:isanp-num_latents-128_matern step 10000 lr 4.878e-04 [train_loss] tar_ll 0.7458 loss -0.7458 (6.002 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 103.75it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.3571 loss -0.3571 (28.916 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 10200 lr 4.873e-04 [train_loss] tar_ll 0.5872 loss -0.5872 (6.076 secs)\n",
      "isanp:isanp-num_latents-128_matern step 10400 lr 4.868e-04 [train_loss] tar_ll 0.2073 loss -0.2073 (6.052 secs)\n",
      "isanp:isanp-num_latents-128_matern step 10600 lr 4.863e-04 [train_loss] tar_ll 0.4875 loss -0.4875 (5.968 secs)\n",
      "isanp:isanp-num_latents-128_matern step 10800 lr 4.857e-04 [train_loss] tar_ll 0.5396 loss -0.5396 (5.940 secs)\n",
      "isanp:isanp-num_latents-128_matern step 11000 lr 4.852e-04 [train_loss] tar_ll 0.6651 loss -0.6651 (6.345 secs)\n",
      "isanp:isanp-num_latents-128_matern step 11200 lr 4.847e-04 [train_loss] tar_ll 0.7212 loss -0.7212 (6.639 secs)\n",
      "isanp:isanp-num_latents-128_matern step 11400 lr 4.841e-04 [train_loss] tar_ll 0.6101 loss -0.6101 (6.531 secs)\n",
      "isanp:isanp-num_latents-128_matern step 11600 lr 4.836e-04 [train_loss] tar_ll 0.5245 loss -0.5245 (6.610 secs)\n",
      "isanp:isanp-num_latents-128_matern step 11800 lr 4.830e-04 [train_loss] tar_ll 0.5126 loss -0.5126 (6.164 secs)\n",
      "isanp:isanp-num_latents-128_matern step 12000 lr 4.824e-04 [train_loss] tar_ll 0.5822 loss -0.5822 (6.456 secs)\n",
      "isanp:isanp-num_latents-128_matern step 12200 lr 4.819e-04 [train_loss] tar_ll 0.7232 loss -0.7232 (6.268 secs)\n",
      "isanp:isanp-num_latents-128_matern step 12400 lr 4.813e-04 [train_loss] tar_ll 0.7259 loss -0.7259 (6.367 secs)\n",
      "isanp:isanp-num_latents-128_matern step 12600 lr 4.807e-04 [train_loss] tar_ll 0.6286 loss -0.6286 (6.278 secs)\n",
      "isanp:isanp-num_latents-128_matern step 12800 lr 4.801e-04 [train_loss] tar_ll 0.6658 loss -0.6658 (6.205 secs)\n",
      "isanp:isanp-num_latents-128_matern step 13000 lr 4.794e-04 [train_loss] tar_ll 0.7261 loss -0.7261 (6.240 secs)\n",
      "isanp:isanp-num_latents-128_matern step 13200 lr 4.788e-04 [train_loss] tar_ll 0.6731 loss -0.6731 (6.031 secs)\n",
      "isanp:isanp-num_latents-128_matern step 13400 lr 4.782e-04 [train_loss] tar_ll 0.7399 loss -0.7399 (5.987 secs)\n",
      "isanp:isanp-num_latents-128_matern step 13600 lr 4.775e-04 [train_loss] tar_ll 0.7559 loss -0.7559 (6.128 secs)\n",
      "isanp:isanp-num_latents-128_matern step 13800 lr 4.769e-04 [train_loss] tar_ll 0.7292 loss -0.7292 (6.010 secs)\n",
      "isanp:isanp-num_latents-128_matern step 14000 lr 4.762e-04 [train_loss] tar_ll 0.7971 loss -0.7971 (6.078 secs)\n",
      "isanp:isanp-num_latents-128_matern step 14200 lr 4.755e-04 [train_loss] tar_ll 0.7447 loss -0.7447 (6.001 secs)\n",
      "isanp:isanp-num_latents-128_matern step 14400 lr 4.749e-04 [train_loss] tar_ll 0.7664 loss -0.7664 (6.004 secs)\n",
      "isanp:isanp-num_latents-128_matern step 14600 lr 4.742e-04 [train_loss] tar_ll 0.6581 loss -0.6581 (6.275 secs)\n",
      "isanp:isanp-num_latents-128_matern step 14800 lr 4.735e-04 [train_loss] tar_ll 0.7244 loss -0.7244 (6.000 secs)\n",
      "isanp:isanp-num_latents-128_matern step 15000 lr 4.728e-04 [train_loss] tar_ll 0.7302 loss -0.7302 (6.012 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 103.43it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.5581 loss -0.5581 (29.007 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 15200 lr 4.720e-04 [train_loss] tar_ll 0.7739 loss -0.7739 (6.132 secs)\n",
      "isanp:isanp-num_latents-128_matern step 15400 lr 4.713e-04 [train_loss] tar_ll 0.7515 loss -0.7515 (5.905 secs)\n",
      "isanp:isanp-num_latents-128_matern step 15600 lr 4.706e-04 [train_loss] tar_ll 0.8036 loss -0.8036 (6.041 secs)\n",
      "isanp:isanp-num_latents-128_matern step 15800 lr 4.698e-04 [train_loss] tar_ll 0.7693 loss -0.7693 (6.257 secs)\n",
      "isanp:isanp-num_latents-128_matern step 16000 lr 4.691e-04 [train_loss] tar_ll 0.7237 loss -0.7237 (6.029 secs)\n",
      "isanp:isanp-num_latents-128_matern step 16200 lr 4.683e-04 [train_loss] tar_ll 0.8332 loss -0.8332 (6.357 secs)\n",
      "isanp:isanp-num_latents-128_matern step 16400 lr 4.675e-04 [train_loss] tar_ll 0.7249 loss -0.7249 (6.104 secs)\n",
      "isanp:isanp-num_latents-128_matern step 16600 lr 4.668e-04 [train_loss] tar_ll 0.7591 loss -0.7591 (6.205 secs)\n",
      "isanp:isanp-num_latents-128_matern step 16800 lr 4.660e-04 [train_loss] tar_ll 0.8389 loss -0.8389 (6.262 secs)\n",
      "isanp:isanp-num_latents-128_matern step 17000 lr 4.652e-04 [train_loss] tar_ll 0.7564 loss -0.7564 (6.166 secs)\n",
      "isanp:isanp-num_latents-128_matern step 17200 lr 4.644e-04 [train_loss] tar_ll 0.7835 loss -0.7835 (6.221 secs)\n",
      "isanp:isanp-num_latents-128_matern step 17400 lr 4.636e-04 [train_loss] tar_ll 0.7567 loss -0.7567 (6.112 secs)\n",
      "isanp:isanp-num_latents-128_matern step 17600 lr 4.627e-04 [train_loss] tar_ll 0.7803 loss -0.7803 (6.358 secs)\n",
      "isanp:isanp-num_latents-128_matern step 17800 lr 4.619e-04 [train_loss] tar_ll 0.7312 loss -0.7312 (6.307 secs)\n",
      "isanp:isanp-num_latents-128_matern step 18000 lr 4.611e-04 [train_loss] tar_ll 0.7806 loss -0.7806 (6.260 secs)\n",
      "isanp:isanp-num_latents-128_matern step 18200 lr 4.602e-04 [train_loss] tar_ll 0.8234 loss -0.8234 (6.364 secs)\n",
      "isanp:isanp-num_latents-128_matern step 18400 lr 4.594e-04 [train_loss] tar_ll 0.8549 loss -0.8549 (6.274 secs)\n",
      "isanp:isanp-num_latents-128_matern step 18600 lr 4.585e-04 [train_loss] tar_ll 0.8663 loss -0.8663 (5.881 secs)\n",
      "isanp:isanp-num_latents-128_matern step 18800 lr 4.576e-04 [train_loss] tar_ll 0.8411 loss -0.8411 (6.064 secs)\n",
      "isanp:isanp-num_latents-128_matern step 19000 lr 4.568e-04 [train_loss] tar_ll 0.7878 loss -0.7878 (5.977 secs)\n",
      "isanp:isanp-num_latents-128_matern step 19200 lr 4.559e-04 [train_loss] tar_ll 0.5803 loss -0.5803 (6.054 secs)\n",
      "isanp:isanp-num_latents-128_matern step 19400 lr 4.550e-04 [train_loss] tar_ll 0.5820 loss -0.5820 (6.075 secs)\n",
      "isanp:isanp-num_latents-128_matern step 19600 lr 4.541e-04 [train_loss] tar_ll 0.7883 loss -0.7883 (5.830 secs)\n",
      "isanp:isanp-num_latents-128_matern step 19800 lr 4.532e-04 [train_loss] tar_ll 0.8218 loss -0.8218 (5.947 secs)\n",
      "isanp:isanp-num_latents-128_matern step 20000 lr 4.523e-04 [train_loss] tar_ll 0.7800 loss -0.7800 (5.909 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 104.65it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.5550 loss -0.5550 (28.670 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 20200 lr 4.513e-04 [train_loss] tar_ll 0.7563 loss -0.7563 (5.999 secs)\n",
      "isanp:isanp-num_latents-128_matern step 20400 lr 4.504e-04 [train_loss] tar_ll 0.8728 loss -0.8728 (5.885 secs)\n",
      "isanp:isanp-num_latents-128_matern step 20600 lr 4.494e-04 [train_loss] tar_ll 0.8029 loss -0.8029 (6.029 secs)\n",
      "isanp:isanp-num_latents-128_matern step 20800 lr 4.485e-04 [train_loss] tar_ll 0.8400 loss -0.8400 (6.016 secs)\n",
      "isanp:isanp-num_latents-128_matern step 21000 lr 4.475e-04 [train_loss] tar_ll 0.7691 loss -0.7691 (6.008 secs)\n",
      "isanp:isanp-num_latents-128_matern step 21200 lr 4.466e-04 [train_loss] tar_ll 0.8463 loss -0.8463 (6.004 secs)\n",
      "isanp:isanp-num_latents-128_matern step 21400 lr 4.456e-04 [train_loss] tar_ll 0.6897 loss -0.6897 (5.876 secs)\n",
      "isanp:isanp-num_latents-128_matern step 21600 lr 4.446e-04 [train_loss] tar_ll 0.8686 loss -0.8686 (6.131 secs)\n",
      "isanp:isanp-num_latents-128_matern step 21800 lr 4.436e-04 [train_loss] tar_ll 0.7783 loss -0.7783 (5.991 secs)\n",
      "isanp:isanp-num_latents-128_matern step 22000 lr 4.426e-04 [train_loss] tar_ll 0.7457 loss -0.7457 (5.915 secs)\n",
      "isanp:isanp-num_latents-128_matern step 22200 lr 4.416e-04 [train_loss] tar_ll 0.8409 loss -0.8409 (6.194 secs)\n",
      "isanp:isanp-num_latents-128_matern step 22400 lr 4.406e-04 [train_loss] tar_ll 0.7766 loss -0.7766 (5.806 secs)\n",
      "isanp:isanp-num_latents-128_matern step 22600 lr 4.396e-04 [train_loss] tar_ll 0.8443 loss -0.8443 (6.071 secs)\n",
      "isanp:isanp-num_latents-128_matern step 22800 lr 4.386e-04 [train_loss] tar_ll 0.8125 loss -0.8125 (5.971 secs)\n",
      "isanp:isanp-num_latents-128_matern step 23000 lr 4.375e-04 [train_loss] tar_ll 0.9063 loss -0.9063 (6.009 secs)\n",
      "isanp:isanp-num_latents-128_matern step 23200 lr 4.365e-04 [train_loss] tar_ll 0.8363 loss -0.8363 (6.287 secs)\n",
      "isanp:isanp-num_latents-128_matern step 23400 lr 4.354e-04 [train_loss] tar_ll 0.8346 loss -0.8346 (5.963 secs)\n",
      "isanp:isanp-num_latents-128_matern step 23600 lr 4.344e-04 [train_loss] tar_ll 0.8759 loss -0.8759 (6.056 secs)\n",
      "isanp:isanp-num_latents-128_matern step 23800 lr 4.333e-04 [train_loss] tar_ll 0.8393 loss -0.8393 (6.058 secs)\n",
      "isanp:isanp-num_latents-128_matern step 24000 lr 4.322e-04 [train_loss] tar_ll 0.5396 loss -0.5396 (5.932 secs)\n",
      "isanp:isanp-num_latents-128_matern step 24200 lr 4.312e-04 [train_loss] tar_ll 0.5735 loss -0.5735 (6.073 secs)\n",
      "isanp:isanp-num_latents-128_matern step 24400 lr 4.301e-04 [train_loss] tar_ll 0.7527 loss -0.7527 (5.945 secs)\n",
      "isanp:isanp-num_latents-128_matern step 24600 lr 4.290e-04 [train_loss] tar_ll 0.8228 loss -0.8228 (5.895 secs)\n",
      "isanp:isanp-num_latents-128_matern step 24800 lr 4.279e-04 [train_loss] tar_ll 0.7532 loss -0.7532 (6.063 secs)\n",
      "isanp:isanp-num_latents-128_matern step 25000 lr 4.268e-04 [train_loss] tar_ll 0.8791 loss -0.8791 (5.923 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 101.20it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.5840 loss -0.5840 (29.646 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 25200 lr 4.257e-04 [train_loss] tar_ll 0.8462 loss -0.8462 (6.028 secs)\n",
      "isanp:isanp-num_latents-128_matern step 25400 lr 4.245e-04 [train_loss] tar_ll 0.8146 loss -0.8146 (6.060 secs)\n",
      "isanp:isanp-num_latents-128_matern step 25600 lr 4.234e-04 [train_loss] tar_ll 0.8564 loss -0.8564 (6.071 secs)\n",
      "isanp:isanp-num_latents-128_matern step 25800 lr 4.223e-04 [train_loss] tar_ll 0.8707 loss -0.8707 (6.190 secs)\n",
      "isanp:isanp-num_latents-128_matern step 26000 lr 4.211e-04 [train_loss] tar_ll 0.8707 loss -0.8707 (6.182 secs)\n",
      "isanp:isanp-num_latents-128_matern step 26200 lr 4.200e-04 [train_loss] tar_ll 0.8716 loss -0.8716 (6.137 secs)\n",
      "isanp:isanp-num_latents-128_matern step 26400 lr 4.188e-04 [train_loss] tar_ll 0.8734 loss -0.8734 (6.217 secs)\n",
      "isanp:isanp-num_latents-128_matern step 26600 lr 4.177e-04 [train_loss] tar_ll 0.8088 loss -0.8088 (6.185 secs)\n",
      "isanp:isanp-num_latents-128_matern step 26800 lr 4.165e-04 [train_loss] tar_ll 0.7273 loss -0.7273 (6.052 secs)\n",
      "isanp:isanp-num_latents-128_matern step 27000 lr 4.153e-04 [train_loss] tar_ll 0.5418 loss -0.5418 (6.282 secs)\n",
      "isanp:isanp-num_latents-128_matern step 27200 lr 4.141e-04 [train_loss] tar_ll 0.7488 loss -0.7488 (6.129 secs)\n",
      "isanp:isanp-num_latents-128_matern step 27400 lr 4.130e-04 [train_loss] tar_ll 0.7481 loss -0.7481 (6.220 secs)\n",
      "isanp:isanp-num_latents-128_matern step 27600 lr 4.118e-04 [train_loss] tar_ll 0.6883 loss -0.6883 (6.400 secs)\n",
      "isanp:isanp-num_latents-128_matern step 27800 lr 4.106e-04 [train_loss] tar_ll 0.9067 loss -0.9067 (6.355 secs)\n",
      "isanp:isanp-num_latents-128_matern step 28000 lr 4.094e-04 [train_loss] tar_ll 0.9226 loss -0.9226 (6.543 secs)\n",
      "isanp:isanp-num_latents-128_matern step 28200 lr 4.081e-04 [train_loss] tar_ll 0.7260 loss -0.7260 (6.329 secs)\n",
      "isanp:isanp-num_latents-128_matern step 28400 lr 4.069e-04 [train_loss] tar_ll 0.7819 loss -0.7819 (6.140 secs)\n",
      "isanp:isanp-num_latents-128_matern step 28600 lr 4.057e-04 [train_loss] tar_ll 0.7723 loss -0.7723 (5.816 secs)\n",
      "isanp:isanp-num_latents-128_matern step 28800 lr 4.045e-04 [train_loss] tar_ll 0.8312 loss -0.8312 (5.989 secs)\n",
      "isanp:isanp-num_latents-128_matern step 29000 lr 4.032e-04 [train_loss] tar_ll 0.6584 loss -0.6584 (6.007 secs)\n",
      "isanp:isanp-num_latents-128_matern step 29200 lr 4.020e-04 [train_loss] tar_ll 0.7630 loss -0.7630 (5.955 secs)\n",
      "isanp:isanp-num_latents-128_matern step 29400 lr 4.007e-04 [train_loss] tar_ll 0.8207 loss -0.8207 (5.881 secs)\n",
      "isanp:isanp-num_latents-128_matern step 29600 lr 3.995e-04 [train_loss] tar_ll 0.8573 loss -0.8573 (5.944 secs)\n",
      "isanp:isanp-num_latents-128_matern step 29800 lr 3.982e-04 [train_loss] tar_ll 0.8645 loss -0.8645 (5.811 secs)\n",
      "isanp:isanp-num_latents-128_matern step 30000 lr 3.969e-04 [train_loss] tar_ll 0.8184 loss -0.8184 (6.023 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 103.92it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.5762 loss -0.5762 (28.870 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 30200 lr 3.957e-04 [train_loss] tar_ll 0.8149 loss -0.8149 (6.133 secs)\n",
      "isanp:isanp-num_latents-128_matern step 30400 lr 3.944e-04 [train_loss] tar_ll 0.9730 loss -0.9730 (5.923 secs)\n",
      "isanp:isanp-num_latents-128_matern step 30600 lr 3.931e-04 [train_loss] tar_ll 0.8971 loss -0.8971 (5.990 secs)\n",
      "isanp:isanp-num_latents-128_matern step 30800 lr 3.918e-04 [train_loss] tar_ll 0.7621 loss -0.7621 (5.945 secs)\n",
      "isanp:isanp-num_latents-128_matern step 31000 lr 3.905e-04 [train_loss] tar_ll 0.8970 loss -0.8970 (6.168 secs)\n",
      "isanp:isanp-num_latents-128_matern step 31200 lr 3.892e-04 [train_loss] tar_ll 0.9084 loss -0.9084 (6.668 secs)\n",
      "isanp:isanp-num_latents-128_matern step 31400 lr 3.879e-04 [train_loss] tar_ll 0.8811 loss -0.8811 (6.321 secs)\n",
      "isanp:isanp-num_latents-128_matern step 31600 lr 3.866e-04 [train_loss] tar_ll 0.9500 loss -0.9500 (6.080 secs)\n",
      "isanp:isanp-num_latents-128_matern step 31800 lr 3.853e-04 [train_loss] tar_ll 0.8397 loss -0.8397 (6.188 secs)\n",
      "isanp:isanp-num_latents-128_matern step 32000 lr 3.840e-04 [train_loss] tar_ll 0.8874 loss -0.8874 (5.871 secs)\n",
      "isanp:isanp-num_latents-128_matern step 32200 lr 3.826e-04 [train_loss] tar_ll 0.8723 loss -0.8723 (6.002 secs)\n",
      "isanp:isanp-num_latents-128_matern step 32400 lr 3.813e-04 [train_loss] tar_ll 0.8373 loss -0.8373 (6.034 secs)\n",
      "isanp:isanp-num_latents-128_matern step 32600 lr 3.800e-04 [train_loss] tar_ll 0.9647 loss -0.9647 (6.012 secs)\n",
      "isanp:isanp-num_latents-128_matern step 32800 lr 3.786e-04 [train_loss] tar_ll 0.8931 loss -0.8931 (6.043 secs)\n",
      "isanp:isanp-num_latents-128_matern step 33000 lr 3.773e-04 [train_loss] tar_ll 0.9689 loss -0.9689 (5.960 secs)\n",
      "isanp:isanp-num_latents-128_matern step 33200 lr 3.759e-04 [train_loss] tar_ll 0.8992 loss -0.8992 (6.085 secs)\n",
      "isanp:isanp-num_latents-128_matern step 33400 lr 3.745e-04 [train_loss] tar_ll 0.9908 loss -0.9908 (6.019 secs)\n",
      "isanp:isanp-num_latents-128_matern step 33600 lr 3.732e-04 [train_loss] tar_ll 1.0118 loss -1.0118 (5.900 secs)\n",
      "isanp:isanp-num_latents-128_matern step 33800 lr 3.718e-04 [train_loss] tar_ll 0.8153 loss -0.8153 (6.175 secs)\n",
      "isanp:isanp-num_latents-128_matern step 34000 lr 3.704e-04 [train_loss] tar_ll 0.9534 loss -0.9534 (6.039 secs)\n",
      "isanp:isanp-num_latents-128_matern step 34200 lr 3.691e-04 [train_loss] tar_ll 0.9797 loss -0.9797 (6.022 secs)\n",
      "isanp:isanp-num_latents-128_matern step 34400 lr 3.677e-04 [train_loss] tar_ll 1.0102 loss -1.0102 (6.032 secs)\n",
      "isanp:isanp-num_latents-128_matern step 34600 lr 3.663e-04 [train_loss] tar_ll 0.9936 loss -0.9936 (5.965 secs)\n",
      "isanp:isanp-num_latents-128_matern step 34800 lr 3.649e-04 [train_loss] tar_ll 0.9696 loss -0.9696 (5.998 secs)\n",
      "isanp:isanp-num_latents-128_matern step 35000 lr 3.635e-04 [train_loss] tar_ll 0.9449 loss -0.9449 (6.098 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 100.31it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.5698 loss -0.5698 (29.908 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 35200 lr 3.621e-04 [train_loss] tar_ll 0.8847 loss -0.8847 (5.955 secs)\n",
      "isanp:isanp-num_latents-128_matern step 35400 lr 3.607e-04 [train_loss] tar_ll 0.9002 loss -0.9002 (6.199 secs)\n",
      "isanp:isanp-num_latents-128_matern step 35600 lr 3.593e-04 [train_loss] tar_ll 1.0065 loss -1.0065 (6.307 secs)\n",
      "isanp:isanp-num_latents-128_matern step 35800 lr 3.579e-04 [train_loss] tar_ll 1.0004 loss -1.0004 (6.096 secs)\n",
      "isanp:isanp-num_latents-128_matern step 36000 lr 3.564e-04 [train_loss] tar_ll 0.7745 loss -0.7745 (6.223 secs)\n",
      "isanp:isanp-num_latents-128_matern step 36200 lr 3.550e-04 [train_loss] tar_ll 0.8407 loss -0.8407 (6.107 secs)\n",
      "isanp:isanp-num_latents-128_matern step 36400 lr 3.536e-04 [train_loss] tar_ll 0.9581 loss -0.9581 (6.167 secs)\n",
      "isanp:isanp-num_latents-128_matern step 36600 lr 3.522e-04 [train_loss] tar_ll 1.0237 loss -1.0237 (6.211 secs)\n",
      "isanp:isanp-num_latents-128_matern step 36800 lr 3.507e-04 [train_loss] tar_ll 0.8873 loss -0.8873 (6.121 secs)\n",
      "isanp:isanp-num_latents-128_matern step 37000 lr 3.493e-04 [train_loss] tar_ll 0.9385 loss -0.9385 (6.254 secs)\n",
      "isanp:isanp-num_latents-128_matern step 37200 lr 3.478e-04 [train_loss] tar_ll 1.0107 loss -1.0107 (6.249 secs)\n",
      "isanp:isanp-num_latents-128_matern step 37400 lr 3.464e-04 [train_loss] tar_ll 0.9798 loss -0.9798 (6.306 secs)\n",
      "isanp:isanp-num_latents-128_matern step 37600 lr 3.449e-04 [train_loss] tar_ll 0.8874 loss -0.8874 (6.250 secs)\n",
      "isanp:isanp-num_latents-128_matern step 37800 lr 3.435e-04 [train_loss] tar_ll 0.8962 loss -0.8962 (6.224 secs)\n",
      "isanp:isanp-num_latents-128_matern step 38000 lr 3.420e-04 [train_loss] tar_ll 0.9444 loss -0.9444 (6.337 secs)\n",
      "isanp:isanp-num_latents-128_matern step 38200 lr 3.406e-04 [train_loss] tar_ll 1.0056 loss -1.0056 (6.241 secs)\n",
      "isanp:isanp-num_latents-128_matern step 38400 lr 3.391e-04 [train_loss] tar_ll 1.0500 loss -1.0500 (5.953 secs)\n",
      "isanp:isanp-num_latents-128_matern step 38600 lr 3.376e-04 [train_loss] tar_ll 1.0327 loss -1.0327 (6.144 secs)\n",
      "isanp:isanp-num_latents-128_matern step 38800 lr 3.362e-04 [train_loss] tar_ll 0.9855 loss -0.9855 (5.920 secs)\n",
      "isanp:isanp-num_latents-128_matern step 39000 lr 3.347e-04 [train_loss] tar_ll 0.9606 loss -0.9606 (6.008 secs)\n",
      "isanp:isanp-num_latents-128_matern step 39200 lr 3.332e-04 [train_loss] tar_ll 0.8828 loss -0.8828 (5.980 secs)\n",
      "isanp:isanp-num_latents-128_matern step 39400 lr 3.317e-04 [train_loss] tar_ll 1.0804 loss -1.0804 (5.940 secs)\n",
      "isanp:isanp-num_latents-128_matern step 39600 lr 3.302e-04 [train_loss] tar_ll 0.9390 loss -0.9390 (5.996 secs)\n",
      "isanp:isanp-num_latents-128_matern step 39800 lr 3.287e-04 [train_loss] tar_ll 0.9933 loss -0.9933 (5.925 secs)\n",
      "isanp:isanp-num_latents-128_matern step 40000 lr 3.273e-04 [train_loss] tar_ll 1.0258 loss -1.0258 (5.951 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 103.94it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.6350 loss -0.6350 (28.864 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 40200 lr 3.258e-04 [train_loss] tar_ll 1.0730 loss -1.0730 (6.298 secs)\n",
      "isanp:isanp-num_latents-128_matern step 40400 lr 3.243e-04 [train_loss] tar_ll 1.0737 loss -1.0737 (5.972 secs)\n",
      "isanp:isanp-num_latents-128_matern step 40600 lr 3.228e-04 [train_loss] tar_ll 0.9283 loss -0.9283 (5.917 secs)\n",
      "isanp:isanp-num_latents-128_matern step 40800 lr 3.213e-04 [train_loss] tar_ll 1.1186 loss -1.1186 (6.028 secs)\n",
      "isanp:isanp-num_latents-128_matern step 41000 lr 3.197e-04 [train_loss] tar_ll 1.0571 loss -1.0571 (5.917 secs)\n",
      "isanp:isanp-num_latents-128_matern step 41200 lr 3.182e-04 [train_loss] tar_ll 1.0315 loss -1.0315 (5.951 secs)\n",
      "isanp:isanp-num_latents-128_matern step 41400 lr 3.167e-04 [train_loss] tar_ll 0.9083 loss -0.9083 (5.977 secs)\n",
      "isanp:isanp-num_latents-128_matern step 41600 lr 3.152e-04 [train_loss] tar_ll 0.8084 loss -0.8084 (6.003 secs)\n",
      "isanp:isanp-num_latents-128_matern step 41800 lr 3.137e-04 [train_loss] tar_ll 0.9492 loss -0.9492 (6.126 secs)\n",
      "isanp:isanp-num_latents-128_matern step 42000 lr 3.122e-04 [train_loss] tar_ll 1.0299 loss -1.0299 (5.932 secs)\n",
      "isanp:isanp-num_latents-128_matern step 42200 lr 3.106e-04 [train_loss] tar_ll 0.9495 loss -0.9495 (5.975 secs)\n",
      "isanp:isanp-num_latents-128_matern step 42400 lr 3.091e-04 [train_loss] tar_ll 0.9315 loss -0.9315 (6.101 secs)\n",
      "isanp:isanp-num_latents-128_matern step 42600 lr 3.076e-04 [train_loss] tar_ll 1.0286 loss -1.0286 (5.990 secs)\n",
      "isanp:isanp-num_latents-128_matern step 42800 lr 3.061e-04 [train_loss] tar_ll 0.9155 loss -0.9155 (6.024 secs)\n",
      "isanp:isanp-num_latents-128_matern step 43000 lr 3.045e-04 [train_loss] tar_ll 0.9787 loss -0.9787 (6.127 secs)\n",
      "isanp:isanp-num_latents-128_matern step 43200 lr 3.030e-04 [train_loss] tar_ll 1.0601 loss -1.0601 (6.422 secs)\n",
      "isanp:isanp-num_latents-128_matern step 43400 lr 3.015e-04 [train_loss] tar_ll 1.0491 loss -1.0491 (6.628 secs)\n",
      "isanp:isanp-num_latents-128_matern step 43600 lr 2.999e-04 [train_loss] tar_ll 1.0539 loss -1.0539 (5.958 secs)\n",
      "isanp:isanp-num_latents-128_matern step 43800 lr 2.984e-04 [train_loss] tar_ll 1.0075 loss -1.0075 (5.971 secs)\n",
      "isanp:isanp-num_latents-128_matern step 44000 lr 2.968e-04 [train_loss] tar_ll 1.0404 loss -1.0404 (6.074 secs)\n",
      "isanp:isanp-num_latents-128_matern step 44200 lr 2.953e-04 [train_loss] tar_ll 1.0755 loss -1.0755 (5.986 secs)\n",
      "isanp:isanp-num_latents-128_matern step 44400 lr 2.938e-04 [train_loss] tar_ll 1.0394 loss -1.0394 (6.029 secs)\n",
      "isanp:isanp-num_latents-128_matern step 44600 lr 2.922e-04 [train_loss] tar_ll 1.0252 loss -1.0252 (6.102 secs)\n",
      "isanp:isanp-num_latents-128_matern step 44800 lr 2.907e-04 [train_loss] tar_ll 1.1180 loss -1.1180 (5.973 secs)\n",
      "isanp:isanp-num_latents-128_matern step 45000 lr 2.891e-04 [train_loss] tar_ll 0.9813 loss -0.9813 (6.056 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 101.64it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.6784 loss -0.6784 (29.519 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 45200 lr 2.876e-04 [train_loss] tar_ll 1.0589 loss -1.0589 (6.135 secs)\n",
      "isanp:isanp-num_latents-128_matern step 45400 lr 2.860e-04 [train_loss] tar_ll 1.0599 loss -1.0599 (6.065 secs)\n",
      "isanp:isanp-num_latents-128_matern step 45600 lr 2.844e-04 [train_loss] tar_ll 0.9656 loss -0.9656 (6.123 secs)\n",
      "isanp:isanp-num_latents-128_matern step 45800 lr 2.829e-04 [train_loss] tar_ll 1.0367 loss -1.0367 (6.107 secs)\n",
      "isanp:isanp-num_latents-128_matern step 46000 lr 2.813e-04 [train_loss] tar_ll 1.0195 loss -1.0195 (6.065 secs)\n",
      "isanp:isanp-num_latents-128_matern step 46200 lr 2.798e-04 [train_loss] tar_ll 1.0591 loss -1.0591 (6.274 secs)\n",
      "isanp:isanp-num_latents-128_matern step 46400 lr 2.782e-04 [train_loss] tar_ll 1.0339 loss -1.0339 (6.207 secs)\n",
      "isanp:isanp-num_latents-128_matern step 46600 lr 2.767e-04 [train_loss] tar_ll 1.0116 loss -1.0116 (6.174 secs)\n",
      "isanp:isanp-num_latents-128_matern step 46800 lr 2.751e-04 [train_loss] tar_ll 1.0781 loss -1.0781 (6.195 secs)\n",
      "isanp:isanp-num_latents-128_matern step 47000 lr 2.735e-04 [train_loss] tar_ll 1.0309 loss -1.0309 (6.135 secs)\n",
      "isanp:isanp-num_latents-128_matern step 47200 lr 2.720e-04 [train_loss] tar_ll 1.0385 loss -1.0385 (6.403 secs)\n",
      "isanp:isanp-num_latents-128_matern step 47400 lr 2.704e-04 [train_loss] tar_ll 1.0683 loss -1.0683 (6.366 secs)\n",
      "isanp:isanp-num_latents-128_matern step 47600 lr 2.688e-04 [train_loss] tar_ll 1.0036 loss -1.0036 (6.604 secs)\n",
      "isanp:isanp-num_latents-128_matern step 47800 lr 2.673e-04 [train_loss] tar_ll 1.0816 loss -1.0816 (6.618 secs)\n",
      "isanp:isanp-num_latents-128_matern step 48000 lr 2.657e-04 [train_loss] tar_ll 1.0480 loss -1.0480 (6.319 secs)\n",
      "isanp:isanp-num_latents-128_matern step 48200 lr 2.641e-04 [train_loss] tar_ll 1.0947 loss -1.0947 (6.078 secs)\n",
      "isanp:isanp-num_latents-128_matern step 48400 lr 2.626e-04 [train_loss] tar_ll 1.1495 loss -1.1495 (6.074 secs)\n",
      "isanp:isanp-num_latents-128_matern step 48600 lr 2.610e-04 [train_loss] tar_ll 1.0727 loss -1.0727 (6.188 secs)\n",
      "isanp:isanp-num_latents-128_matern step 48800 lr 2.594e-04 [train_loss] tar_ll 1.1218 loss -1.1218 (5.926 secs)\n",
      "isanp:isanp-num_latents-128_matern step 49000 lr 2.579e-04 [train_loss] tar_ll 1.0287 loss -1.0287 (5.883 secs)\n",
      "isanp:isanp-num_latents-128_matern step 49200 lr 2.563e-04 [train_loss] tar_ll 1.1161 loss -1.1161 (6.135 secs)\n",
      "isanp:isanp-num_latents-128_matern step 49400 lr 2.547e-04 [train_loss] tar_ll 1.0996 loss -1.0996 (6.028 secs)\n",
      "isanp:isanp-num_latents-128_matern step 49600 lr 2.531e-04 [train_loss] tar_ll 1.0604 loss -1.0604 (5.948 secs)\n",
      "isanp:isanp-num_latents-128_matern step 49800 lr 2.516e-04 [train_loss] tar_ll 1.0709 loss -1.0709 (6.105 secs)\n",
      "isanp:isanp-num_latents-128_matern step 50000 lr 2.500e-04 [train_loss] tar_ll 1.1289 loss -1.1289 (5.939 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:30<00:00, 99.40it/s] \n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.7227 loss -0.7227 (30.185 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 50200 lr 2.484e-04 [train_loss] tar_ll 1.0782 loss -1.0782 (6.561 secs)\n",
      "isanp:isanp-num_latents-128_matern step 50400 lr 2.469e-04 [train_loss] tar_ll 1.0659 loss -1.0659 (6.870 secs)\n",
      "isanp:isanp-num_latents-128_matern step 50600 lr 2.453e-04 [train_loss] tar_ll 1.1150 loss -1.1150 (6.642 secs)\n",
      "isanp:isanp-num_latents-128_matern step 50800 lr 2.437e-04 [train_loss] tar_ll 1.1056 loss -1.1056 (6.769 secs)\n",
      "isanp:isanp-num_latents-128_matern step 51000 lr 2.421e-04 [train_loss] tar_ll 1.0926 loss -1.0926 (6.183 secs)\n",
      "isanp:isanp-num_latents-128_matern step 51200 lr 2.406e-04 [train_loss] tar_ll 1.1480 loss -1.1480 (6.402 secs)\n",
      "isanp:isanp-num_latents-128_matern step 51400 lr 2.390e-04 [train_loss] tar_ll 1.1578 loss -1.1578 (6.137 secs)\n",
      "isanp:isanp-num_latents-128_matern step 51600 lr 2.374e-04 [train_loss] tar_ll 1.1572 loss -1.1572 (5.982 secs)\n",
      "isanp:isanp-num_latents-128_matern step 51800 lr 2.359e-04 [train_loss] tar_ll 1.1700 loss -1.1700 (6.050 secs)\n",
      "isanp:isanp-num_latents-128_matern step 52000 lr 2.343e-04 [train_loss] tar_ll 1.1294 loss -1.1294 (6.381 secs)\n",
      "isanp:isanp-num_latents-128_matern step 52200 lr 2.327e-04 [train_loss] tar_ll 1.0260 loss -1.0260 (6.332 secs)\n",
      "isanp:isanp-num_latents-128_matern step 52400 lr 2.312e-04 [train_loss] tar_ll 1.1180 loss -1.1180 (6.016 secs)\n",
      "isanp:isanp-num_latents-128_matern step 52600 lr 2.296e-04 [train_loss] tar_ll 1.1124 loss -1.1124 (5.930 secs)\n",
      "isanp:isanp-num_latents-128_matern step 52800 lr 2.280e-04 [train_loss] tar_ll 1.1560 loss -1.1560 (6.020 secs)\n",
      "isanp:isanp-num_latents-128_matern step 53000 lr 2.265e-04 [train_loss] tar_ll 1.0927 loss -1.0927 (5.984 secs)\n",
      "isanp:isanp-num_latents-128_matern step 53200 lr 2.249e-04 [train_loss] tar_ll 1.0323 loss -1.0323 (6.022 secs)\n",
      "isanp:isanp-num_latents-128_matern step 53400 lr 2.233e-04 [train_loss] tar_ll 1.1148 loss -1.1148 (6.062 secs)\n",
      "isanp:isanp-num_latents-128_matern step 53600 lr 2.218e-04 [train_loss] tar_ll 1.0489 loss -1.0489 (5.976 secs)\n",
      "isanp:isanp-num_latents-128_matern step 53800 lr 2.202e-04 [train_loss] tar_ll 1.1658 loss -1.1658 (6.026 secs)\n",
      "isanp:isanp-num_latents-128_matern step 54000 lr 2.187e-04 [train_loss] tar_ll 1.0589 loss -1.0589 (6.048 secs)\n",
      "isanp:isanp-num_latents-128_matern step 54200 lr 2.171e-04 [train_loss] tar_ll 1.1326 loss -1.1326 (6.044 secs)\n",
      "isanp:isanp-num_latents-128_matern step 54400 lr 2.156e-04 [train_loss] tar_ll 1.1204 loss -1.1204 (6.036 secs)\n",
      "isanp:isanp-num_latents-128_matern step 54600 lr 2.140e-04 [train_loss] tar_ll 1.1586 loss -1.1586 (5.934 secs)\n",
      "isanp:isanp-num_latents-128_matern step 54800 lr 2.124e-04 [train_loss] tar_ll 1.1697 loss -1.1697 (6.118 secs)\n",
      "isanp:isanp-num_latents-128_matern step 55000 lr 2.109e-04 [train_loss] tar_ll 1.0716 loss -1.0716 (6.094 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 102.42it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.8213 loss -0.8213 (29.295 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 55200 lr 2.093e-04 [train_loss] tar_ll 1.1144 loss -1.1144 (6.162 secs)\n",
      "isanp:isanp-num_latents-128_matern step 55400 lr 2.078e-04 [train_loss] tar_ll 1.1928 loss -1.1928 (6.177 secs)\n",
      "isanp:isanp-num_latents-128_matern step 55600 lr 2.062e-04 [train_loss] tar_ll 1.0705 loss -1.0705 (6.224 secs)\n",
      "isanp:isanp-num_latents-128_matern step 55800 lr 2.047e-04 [train_loss] tar_ll 1.1213 loss -1.1213 (6.110 secs)\n",
      "isanp:isanp-num_latents-128_matern step 56000 lr 2.032e-04 [train_loss] tar_ll 1.1985 loss -1.1985 (6.203 secs)\n",
      "isanp:isanp-num_latents-128_matern step 56200 lr 2.016e-04 [train_loss] tar_ll 1.1884 loss -1.1884 (6.165 secs)\n",
      "isanp:isanp-num_latents-128_matern step 56400 lr 2.001e-04 [train_loss] tar_ll 1.0479 loss -1.0479 (6.095 secs)\n",
      "isanp:isanp-num_latents-128_matern step 56600 lr 1.985e-04 [train_loss] tar_ll 1.1469 loss -1.1469 (6.411 secs)\n",
      "isanp:isanp-num_latents-128_matern step 56800 lr 1.970e-04 [train_loss] tar_ll 1.1022 loss -1.1022 (6.239 secs)\n",
      "isanp:isanp-num_latents-128_matern step 57000 lr 1.955e-04 [train_loss] tar_ll 1.2008 loss -1.2008 (6.470 secs)\n",
      "isanp:isanp-num_latents-128_matern step 57200 lr 1.939e-04 [train_loss] tar_ll 1.0818 loss -1.0818 (6.356 secs)\n",
      "isanp:isanp-num_latents-128_matern step 57400 lr 1.924e-04 [train_loss] tar_ll 1.1053 loss -1.1053 (6.301 secs)\n",
      "isanp:isanp-num_latents-128_matern step 57600 lr 1.909e-04 [train_loss] tar_ll 1.1844 loss -1.1844 (6.315 secs)\n",
      "isanp:isanp-num_latents-128_matern step 57800 lr 1.894e-04 [train_loss] tar_ll 1.1522 loss -1.1522 (6.192 secs)\n",
      "isanp:isanp-num_latents-128_matern step 58000 lr 1.878e-04 [train_loss] tar_ll 1.1541 loss -1.1541 (6.052 secs)\n",
      "isanp:isanp-num_latents-128_matern step 58200 lr 1.863e-04 [train_loss] tar_ll 1.1660 loss -1.1660 (6.182 secs)\n",
      "isanp:isanp-num_latents-128_matern step 58400 lr 1.848e-04 [train_loss] tar_ll 1.1818 loss -1.1818 (6.081 secs)\n",
      "isanp:isanp-num_latents-128_matern step 58600 lr 1.833e-04 [train_loss] tar_ll 0.9853 loss -0.9853 (5.993 secs)\n",
      "isanp:isanp-num_latents-128_matern step 58800 lr 1.818e-04 [train_loss] tar_ll 1.1533 loss -1.1533 (5.964 secs)\n",
      "isanp:isanp-num_latents-128_matern step 59000 lr 1.803e-04 [train_loss] tar_ll 1.1522 loss -1.1522 (5.848 secs)\n",
      "isanp:isanp-num_latents-128_matern step 59200 lr 1.787e-04 [train_loss] tar_ll 1.2008 loss -1.2008 (5.995 secs)\n",
      "isanp:isanp-num_latents-128_matern step 59400 lr 1.772e-04 [train_loss] tar_ll 1.1406 loss -1.1406 (5.940 secs)\n",
      "isanp:isanp-num_latents-128_matern step 59600 lr 1.757e-04 [train_loss] tar_ll 1.1535 loss -1.1535 (6.023 secs)\n",
      "isanp:isanp-num_latents-128_matern step 59800 lr 1.742e-04 [train_loss] tar_ll 1.1414 loss -1.1414 (5.970 secs)\n",
      "isanp:isanp-num_latents-128_matern step 60000 lr 1.727e-04 [train_loss] tar_ll 1.1992 loss -1.1992 (5.967 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 101.20it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.7709 loss -0.7709 (29.648 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 60200 lr 1.713e-04 [train_loss] tar_ll 1.1553 loss -1.1553 (6.040 secs)\n",
      "isanp:isanp-num_latents-128_matern step 60400 lr 1.698e-04 [train_loss] tar_ll 1.1194 loss -1.1194 (5.987 secs)\n",
      "isanp:isanp-num_latents-128_matern step 60600 lr 1.683e-04 [train_loss] tar_ll 1.2050 loss -1.2050 (5.946 secs)\n",
      "isanp:isanp-num_latents-128_matern step 60800 lr 1.668e-04 [train_loss] tar_ll 1.2175 loss -1.2175 (6.023 secs)\n",
      "isanp:isanp-num_latents-128_matern step 61000 lr 1.653e-04 [train_loss] tar_ll 1.1784 loss -1.1784 (5.947 secs)\n",
      "isanp:isanp-num_latents-128_matern step 61200 lr 1.638e-04 [train_loss] tar_ll 1.0647 loss -1.0647 (6.023 secs)\n",
      "isanp:isanp-num_latents-128_matern step 61400 lr 1.624e-04 [train_loss] tar_ll 1.2533 loss -1.2533 (6.123 secs)\n",
      "isanp:isanp-num_latents-128_matern step 61600 lr 1.609e-04 [train_loss] tar_ll 1.1489 loss -1.1489 (6.131 secs)\n",
      "isanp:isanp-num_latents-128_matern step 61800 lr 1.594e-04 [train_loss] tar_ll 1.1737 loss -1.1737 (6.358 secs)\n",
      "isanp:isanp-num_latents-128_matern step 62000 lr 1.580e-04 [train_loss] tar_ll 1.1316 loss -1.1316 (6.062 secs)\n",
      "isanp:isanp-num_latents-128_matern step 62200 lr 1.565e-04 [train_loss] tar_ll 1.1333 loss -1.1333 (6.186 secs)\n",
      "isanp:isanp-num_latents-128_matern step 62400 lr 1.551e-04 [train_loss] tar_ll 1.2685 loss -1.2685 (6.235 secs)\n",
      "isanp:isanp-num_latents-128_matern step 62600 lr 1.536e-04 [train_loss] tar_ll 1.1633 loss -1.1633 (5.991 secs)\n",
      "isanp:isanp-num_latents-128_matern step 62800 lr 1.522e-04 [train_loss] tar_ll 1.1983 loss -1.1983 (6.068 secs)\n",
      "isanp:isanp-num_latents-128_matern step 63000 lr 1.507e-04 [train_loss] tar_ll 1.1739 loss -1.1739 (6.338 secs)\n",
      "isanp:isanp-num_latents-128_matern step 63200 lr 1.493e-04 [train_loss] tar_ll 1.1804 loss -1.1804 (6.432 secs)\n",
      "isanp:isanp-num_latents-128_matern step 63400 lr 1.478e-04 [train_loss] tar_ll 1.1818 loss -1.1818 (6.397 secs)\n",
      "isanp:isanp-num_latents-128_matern step 63600 lr 1.464e-04 [train_loss] tar_ll 1.2578 loss -1.2578 (6.334 secs)\n",
      "isanp:isanp-num_latents-128_matern step 63800 lr 1.450e-04 [train_loss] tar_ll 1.2470 loss -1.2470 (7.116 secs)\n",
      "isanp:isanp-num_latents-128_matern step 64000 lr 1.436e-04 [train_loss] tar_ll 1.2249 loss -1.2249 (6.064 secs)\n",
      "isanp:isanp-num_latents-128_matern step 64200 lr 1.421e-04 [train_loss] tar_ll 1.1970 loss -1.1970 (6.018 secs)\n",
      "isanp:isanp-num_latents-128_matern step 64400 lr 1.407e-04 [train_loss] tar_ll 1.2363 loss -1.2363 (6.521 secs)\n",
      "isanp:isanp-num_latents-128_matern step 64600 lr 1.393e-04 [train_loss] tar_ll 1.1617 loss -1.1617 (5.966 secs)\n",
      "isanp:isanp-num_latents-128_matern step 64800 lr 1.379e-04 [train_loss] tar_ll 1.2191 loss -1.2191 (5.992 secs)\n",
      "isanp:isanp-num_latents-128_matern step 65000 lr 1.365e-04 [train_loss] tar_ll 1.2065 loss -1.2065 (6.211 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 101.57it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.8181 loss -0.8181 (29.539 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 65200 lr 1.351e-04 [train_loss] tar_ll 1.1993 loss -1.1993 (6.164 secs)\n",
      "isanp:isanp-num_latents-128_matern step 65400 lr 1.337e-04 [train_loss] tar_ll 1.1551 loss -1.1551 (6.006 secs)\n",
      "isanp:isanp-num_latents-128_matern step 65600 lr 1.323e-04 [train_loss] tar_ll 1.1764 loss -1.1764 (6.250 secs)\n",
      "isanp:isanp-num_latents-128_matern step 65800 lr 1.309e-04 [train_loss] tar_ll 1.2205 loss -1.2205 (6.112 secs)\n",
      "isanp:isanp-num_latents-128_matern step 66000 lr 1.296e-04 [train_loss] tar_ll 1.1914 loss -1.1914 (6.211 secs)\n",
      "isanp:isanp-num_latents-128_matern step 66200 lr 1.282e-04 [train_loss] tar_ll 1.2678 loss -1.2678 (6.120 secs)\n",
      "isanp:isanp-num_latents-128_matern step 66400 lr 1.268e-04 [train_loss] tar_ll 1.1522 loss -1.1522 (6.096 secs)\n",
      "isanp:isanp-num_latents-128_matern step 66600 lr 1.255e-04 [train_loss] tar_ll 1.1411 loss -1.1411 (6.344 secs)\n",
      "isanp:isanp-num_latents-128_matern step 66800 lr 1.241e-04 [train_loss] tar_ll 1.0991 loss -1.0991 (6.286 secs)\n",
      "isanp:isanp-num_latents-128_matern step 67000 lr 1.227e-04 [train_loss] tar_ll 1.1715 loss -1.1715 (6.253 secs)\n",
      "isanp:isanp-num_latents-128_matern step 67200 lr 1.214e-04 [train_loss] tar_ll 1.2788 loss -1.2788 (6.302 secs)\n",
      "isanp:isanp-num_latents-128_matern step 67400 lr 1.200e-04 [train_loss] tar_ll 1.1349 loss -1.1349 (6.284 secs)\n",
      "isanp:isanp-num_latents-128_matern step 67600 lr 1.187e-04 [train_loss] tar_ll 1.1686 loss -1.1686 (6.226 secs)\n",
      "isanp:isanp-num_latents-128_matern step 67800 lr 1.174e-04 [train_loss] tar_ll 1.2768 loss -1.2768 (5.991 secs)\n",
      "isanp:isanp-num_latents-128_matern step 68000 lr 1.160e-04 [train_loss] tar_ll 1.2060 loss -1.2060 (6.099 secs)\n",
      "isanp:isanp-num_latents-128_matern step 68200 lr 1.147e-04 [train_loss] tar_ll 1.2310 loss -1.2310 (6.042 secs)\n",
      "isanp:isanp-num_latents-128_matern step 68400 lr 1.134e-04 [train_loss] tar_ll 1.2380 loss -1.2380 (5.966 secs)\n",
      "isanp:isanp-num_latents-128_matern step 68600 lr 1.121e-04 [train_loss] tar_ll 1.2265 loss -1.2265 (5.900 secs)\n",
      "isanp:isanp-num_latents-128_matern step 68800 lr 1.108e-04 [train_loss] tar_ll 1.2624 loss -1.2624 (5.851 secs)\n",
      "isanp:isanp-num_latents-128_matern step 69000 lr 1.095e-04 [train_loss] tar_ll 1.1960 loss -1.1960 (5.889 secs)\n",
      "isanp:isanp-num_latents-128_matern step 69200 lr 1.082e-04 [train_loss] tar_ll 1.1992 loss -1.1992 (6.191 secs)\n",
      "isanp:isanp-num_latents-128_matern step 69400 lr 1.069e-04 [train_loss] tar_ll 1.2298 loss -1.2298 (5.929 secs)\n",
      "isanp:isanp-num_latents-128_matern step 69600 lr 1.056e-04 [train_loss] tar_ll 1.2573 loss -1.2573 (5.904 secs)\n",
      "isanp:isanp-num_latents-128_matern step 69800 lr 1.043e-04 [train_loss] tar_ll 1.2787 loss -1.2787 (6.041 secs)\n",
      "isanp:isanp-num_latents-128_matern step 70000 lr 1.031e-04 [train_loss] tar_ll 1.2277 loss -1.2277 (5.888 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:32<00:00, 93.28it/s] \n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.8509 loss -0.8509 (32.164 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 70200 lr 1.018e-04 [train_loss] tar_ll 1.2354 loss -1.2354 (6.704 secs)\n",
      "isanp:isanp-num_latents-128_matern step 70400 lr 1.005e-04 [train_loss] tar_ll 1.3413 loss -1.3413 (6.524 secs)\n",
      "isanp:isanp-num_latents-128_matern step 70600 lr 9.927e-05 [train_loss] tar_ll 1.2108 loss -1.2108 (6.262 secs)\n",
      "isanp:isanp-num_latents-128_matern step 70800 lr 9.802e-05 [train_loss] tar_ll 1.2525 loss -1.2525 (6.072 secs)\n",
      "isanp:isanp-num_latents-128_matern step 71000 lr 9.677e-05 [train_loss] tar_ll 1.2338 loss -1.2338 (6.068 secs)\n",
      "isanp:isanp-num_latents-128_matern step 71200 lr 9.554e-05 [train_loss] tar_ll 1.2489 loss -1.2489 (6.183 secs)\n",
      "isanp:isanp-num_latents-128_matern step 71400 lr 9.430e-05 [train_loss] tar_ll 1.2797 loss -1.2797 (5.948 secs)\n",
      "isanp:isanp-num_latents-128_matern step 71600 lr 9.308e-05 [train_loss] tar_ll 1.3122 loss -1.3122 (5.886 secs)\n",
      "isanp:isanp-num_latents-128_matern step 71800 lr 9.186e-05 [train_loss] tar_ll 1.2941 loss -1.2941 (6.185 secs)\n",
      "isanp:isanp-num_latents-128_matern step 72000 lr 9.064e-05 [train_loss] tar_ll 1.2472 loss -1.2472 (6.890 secs)\n",
      "isanp:isanp-num_latents-128_matern step 72200 lr 8.944e-05 [train_loss] tar_ll 1.3358 loss -1.3358 (6.967 secs)\n",
      "isanp:isanp-num_latents-128_matern step 72400 lr 8.824e-05 [train_loss] tar_ll 1.2418 loss -1.2418 (5.916 secs)\n",
      "isanp:isanp-num_latents-128_matern step 72600 lr 8.704e-05 [train_loss] tar_ll 1.2424 loss -1.2424 (5.947 secs)\n",
      "isanp:isanp-num_latents-128_matern step 72800 lr 8.585e-05 [train_loss] tar_ll 1.2203 loss -1.2203 (6.008 secs)\n",
      "isanp:isanp-num_latents-128_matern step 73000 lr 8.467e-05 [train_loss] tar_ll 1.2597 loss -1.2597 (5.931 secs)\n",
      "isanp:isanp-num_latents-128_matern step 73200 lr 8.350e-05 [train_loss] tar_ll 1.2759 loss -1.2759 (6.041 secs)\n",
      "isanp:isanp-num_latents-128_matern step 73400 lr 8.233e-05 [train_loss] tar_ll 1.2719 loss -1.2719 (6.128 secs)\n",
      "isanp:isanp-num_latents-128_matern step 73600 lr 8.117e-05 [train_loss] tar_ll 1.2417 loss -1.2417 (6.469 secs)\n",
      "isanp:isanp-num_latents-128_matern step 73800 lr 8.001e-05 [train_loss] tar_ll 1.3123 loss -1.3123 (6.370 secs)\n",
      "isanp:isanp-num_latents-128_matern step 74000 lr 7.886e-05 [train_loss] tar_ll 1.3417 loss -1.3417 (5.940 secs)\n",
      "isanp:isanp-num_latents-128_matern step 74200 lr 7.772e-05 [train_loss] tar_ll 1.3279 loss -1.3279 (6.004 secs)\n",
      "isanp:isanp-num_latents-128_matern step 74400 lr 7.659e-05 [train_loss] tar_ll 1.2946 loss -1.2946 (6.022 secs)\n",
      "isanp:isanp-num_latents-128_matern step 74600 lr 7.546e-05 [train_loss] tar_ll 1.3323 loss -1.3323 (6.113 secs)\n",
      "isanp:isanp-num_latents-128_matern step 74800 lr 7.434e-05 [train_loss] tar_ll 1.2309 loss -1.2309 (6.103 secs)\n",
      "isanp:isanp-num_latents-128_matern step 75000 lr 7.322e-05 [train_loss] tar_ll 1.2581 loss -1.2581 (6.089 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 101.52it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.8786 loss -0.8786 (29.551 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 75200 lr 7.212e-05 [train_loss] tar_ll 1.2304 loss -1.2304 (6.039 secs)\n",
      "isanp:isanp-num_latents-128_matern step 75400 lr 7.102e-05 [train_loss] tar_ll 1.2646 loss -1.2646 (5.879 secs)\n",
      "isanp:isanp-num_latents-128_matern step 75600 lr 6.992e-05 [train_loss] tar_ll 1.3117 loss -1.3117 (5.854 secs)\n",
      "isanp:isanp-num_latents-128_matern step 75800 lr 6.884e-05 [train_loss] tar_ll 1.2442 loss -1.2442 (5.946 secs)\n",
      "isanp:isanp-num_latents-128_matern step 76000 lr 6.776e-05 [train_loss] tar_ll 1.1979 loss -1.1979 (5.969 secs)\n",
      "isanp:isanp-num_latents-128_matern step 76200 lr 6.669e-05 [train_loss] tar_ll 1.3306 loss -1.3306 (5.837 secs)\n",
      "isanp:isanp-num_latents-128_matern step 76400 lr 6.562e-05 [train_loss] tar_ll 1.2639 loss -1.2639 (5.775 secs)\n",
      "isanp:isanp-num_latents-128_matern step 76600 lr 6.456e-05 [train_loss] tar_ll 1.2931 loss -1.2931 (5.970 secs)\n",
      "isanp:isanp-num_latents-128_matern step 76800 lr 6.351e-05 [train_loss] tar_ll 1.3642 loss -1.3642 (5.828 secs)\n",
      "isanp:isanp-num_latents-128_matern step 77000 lr 6.247e-05 [train_loss] tar_ll 1.3053 loss -1.3053 (5.810 secs)\n",
      "isanp:isanp-num_latents-128_matern step 77200 lr 6.144e-05 [train_loss] tar_ll 1.2319 loss -1.2319 (5.871 secs)\n",
      "isanp:isanp-num_latents-128_matern step 77400 lr 6.041e-05 [train_loss] tar_ll 1.2518 loss -1.2518 (5.898 secs)\n",
      "isanp:isanp-num_latents-128_matern step 77600 lr 5.939e-05 [train_loss] tar_ll 1.3319 loss -1.3319 (5.948 secs)\n",
      "isanp:isanp-num_latents-128_matern step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2823 loss -1.2823 (6.095 secs)\n",
      "isanp:isanp-num_latents-128_matern step 78000 lr 5.737e-05 [train_loss] tar_ll 1.1752 loss -1.1752 (5.978 secs)\n",
      "isanp:isanp-num_latents-128_matern step 78200 lr 5.637e-05 [train_loss] tar_ll 1.2784 loss -1.2784 (5.998 secs)\n",
      "isanp:isanp-num_latents-128_matern step 78400 lr 5.538e-05 [train_loss] tar_ll 1.2927 loss -1.2927 (5.856 secs)\n",
      "isanp:isanp-num_latents-128_matern step 78600 lr 5.440e-05 [train_loss] tar_ll 1.3360 loss -1.3360 (5.804 secs)\n",
      "isanp:isanp-num_latents-128_matern step 78800 lr 5.343e-05 [train_loss] tar_ll 1.3038 loss -1.3038 (6.007 secs)\n",
      "isanp:isanp-num_latents-128_matern step 79000 lr 5.246e-05 [train_loss] tar_ll 1.2733 loss -1.2733 (5.886 secs)\n",
      "isanp:isanp-num_latents-128_matern step 79200 lr 5.150e-05 [train_loss] tar_ll 1.3272 loss -1.3272 (6.010 secs)\n",
      "isanp:isanp-num_latents-128_matern step 79400 lr 5.055e-05 [train_loss] tar_ll 1.2321 loss -1.2321 (6.238 secs)\n",
      "isanp:isanp-num_latents-128_matern step 79600 lr 4.961e-05 [train_loss] tar_ll 1.2361 loss -1.2361 (6.092 secs)\n",
      "isanp:isanp-num_latents-128_matern step 79800 lr 4.867e-05 [train_loss] tar_ll 1.2619 loss -1.2619 (6.181 secs)\n",
      "isanp:isanp-num_latents-128_matern step 80000 lr 4.775e-05 [train_loss] tar_ll 1.3786 loss -1.3786 (6.172 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:32<00:00, 92.26it/s] \n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.8987 loss -0.8987 (32.517 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 80200 lr 4.683e-05 [train_loss] tar_ll 1.2395 loss -1.2395 (6.576 secs)\n",
      "isanp:isanp-num_latents-128_matern step 80400 lr 4.592e-05 [train_loss] tar_ll 1.3549 loss -1.3549 (6.140 secs)\n",
      "isanp:isanp-num_latents-128_matern step 80600 lr 4.501e-05 [train_loss] tar_ll 1.3097 loss -1.3097 (6.174 secs)\n",
      "isanp:isanp-num_latents-128_matern step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2560 loss -1.2560 (6.407 secs)\n",
      "isanp:isanp-num_latents-128_matern step 81000 lr 4.323e-05 [train_loss] tar_ll 1.2961 loss -1.2961 (6.298 secs)\n",
      "isanp:isanp-num_latents-128_matern step 81200 lr 4.235e-05 [train_loss] tar_ll 1.3384 loss -1.3384 (6.190 secs)\n",
      "isanp:isanp-num_latents-128_matern step 81400 lr 4.148e-05 [train_loss] tar_ll 1.2570 loss -1.2570 (6.027 secs)\n",
      "isanp:isanp-num_latents-128_matern step 81600 lr 4.062e-05 [train_loss] tar_ll 1.3366 loss -1.3366 (5.933 secs)\n",
      "isanp:isanp-num_latents-128_matern step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2645 loss -1.2645 (6.030 secs)\n",
      "isanp:isanp-num_latents-128_matern step 82000 lr 3.892e-05 [train_loss] tar_ll 1.3073 loss -1.3073 (5.908 secs)\n",
      "isanp:isanp-num_latents-128_matern step 82200 lr 3.808e-05 [train_loss] tar_ll 1.3068 loss -1.3068 (5.999 secs)\n",
      "isanp:isanp-num_latents-128_matern step 82400 lr 3.725e-05 [train_loss] tar_ll 1.3038 loss -1.3038 (6.382 secs)\n",
      "isanp:isanp-num_latents-128_matern step 82600 lr 3.643e-05 [train_loss] tar_ll 1.3270 loss -1.3270 (6.295 secs)\n",
      "isanp:isanp-num_latents-128_matern step 82800 lr 3.562e-05 [train_loss] tar_ll 1.2378 loss -1.2378 (6.241 secs)\n",
      "isanp:isanp-num_latents-128_matern step 83000 lr 3.481e-05 [train_loss] tar_ll 1.3371 loss -1.3371 (5.927 secs)\n",
      "isanp:isanp-num_latents-128_matern step 83200 lr 3.402e-05 [train_loss] tar_ll 1.3371 loss -1.3371 (6.641 secs)\n",
      "isanp:isanp-num_latents-128_matern step 83400 lr 3.323e-05 [train_loss] tar_ll 1.2825 loss -1.2825 (6.330 secs)\n",
      "isanp:isanp-num_latents-128_matern step 83600 lr 3.245e-05 [train_loss] tar_ll 1.3486 loss -1.3486 (5.943 secs)\n",
      "isanp:isanp-num_latents-128_matern step 83800 lr 3.168e-05 [train_loss] tar_ll 1.3829 loss -1.3829 (6.035 secs)\n",
      "isanp:isanp-num_latents-128_matern step 84000 lr 3.092e-05 [train_loss] tar_ll 1.3443 loss -1.3443 (5.957 secs)\n",
      "isanp:isanp-num_latents-128_matern step 84200 lr 3.017e-05 [train_loss] tar_ll 1.3047 loss -1.3047 (5.943 secs)\n",
      "isanp:isanp-num_latents-128_matern step 84400 lr 2.943e-05 [train_loss] tar_ll 1.2561 loss -1.2561 (6.038 secs)\n",
      "isanp:isanp-num_latents-128_matern step 84600 lr 2.869e-05 [train_loss] tar_ll 1.3220 loss -1.3220 (5.999 secs)\n",
      "isanp:isanp-num_latents-128_matern step 84800 lr 2.797e-05 [train_loss] tar_ll 1.3589 loss -1.3589 (5.901 secs)\n",
      "isanp:isanp-num_latents-128_matern step 85000 lr 2.725e-05 [train_loss] tar_ll 1.2818 loss -1.2818 (6.054 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 101.29it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.9081 loss -0.9081 (29.621 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 85200 lr 2.654e-05 [train_loss] tar_ll 1.3839 loss -1.3839 (6.279 secs)\n",
      "isanp:isanp-num_latents-128_matern step 85400 lr 2.584e-05 [train_loss] tar_ll 1.2732 loss -1.2732 (6.371 secs)\n",
      "isanp:isanp-num_latents-128_matern step 85600 lr 2.515e-05 [train_loss] tar_ll 1.3183 loss -1.3183 (6.220 secs)\n",
      "isanp:isanp-num_latents-128_matern step 85800 lr 2.447e-05 [train_loss] tar_ll 1.3258 loss -1.3258 (5.802 secs)\n",
      "isanp:isanp-num_latents-128_matern step 86000 lr 2.379e-05 [train_loss] tar_ll 1.4370 loss -1.4370 (5.950 secs)\n",
      "isanp:isanp-num_latents-128_matern step 86200 lr 2.313e-05 [train_loss] tar_ll 1.3440 loss -1.3440 (5.895 secs)\n",
      "isanp:isanp-num_latents-128_matern step 86400 lr 2.247e-05 [train_loss] tar_ll 1.3495 loss -1.3495 (5.836 secs)\n",
      "isanp:isanp-num_latents-128_matern step 86600 lr 2.183e-05 [train_loss] tar_ll 1.3299 loss -1.3299 (5.946 secs)\n",
      "isanp:isanp-num_latents-128_matern step 86800 lr 2.119e-05 [train_loss] tar_ll 1.3358 loss -1.3358 (5.763 secs)\n",
      "isanp:isanp-num_latents-128_matern step 87000 lr 2.056e-05 [train_loss] tar_ll 1.2791 loss -1.2791 (5.832 secs)\n",
      "isanp:isanp-num_latents-128_matern step 87200 lr 1.994e-05 [train_loss] tar_ll 1.2011 loss -1.2011 (6.062 secs)\n",
      "isanp:isanp-num_latents-128_matern step 87400 lr 1.933e-05 [train_loss] tar_ll 1.3604 loss -1.3604 (5.855 secs)\n",
      "isanp:isanp-num_latents-128_matern step 87600 lr 1.873e-05 [train_loss] tar_ll 1.3734 loss -1.3734 (6.091 secs)\n",
      "isanp:isanp-num_latents-128_matern step 87800 lr 1.814e-05 [train_loss] tar_ll 1.3000 loss -1.3000 (5.928 secs)\n",
      "isanp:isanp-num_latents-128_matern step 88000 lr 1.756e-05 [train_loss] tar_ll 1.3148 loss -1.3148 (5.900 secs)\n",
      "isanp:isanp-num_latents-128_matern step 88200 lr 1.698e-05 [train_loss] tar_ll 1.3839 loss -1.3839 (6.005 secs)\n",
      "isanp:isanp-num_latents-128_matern step 88400 lr 1.642e-05 [train_loss] tar_ll 1.2602 loss -1.2602 (5.864 secs)\n",
      "isanp:isanp-num_latents-128_matern step 88600 lr 1.586e-05 [train_loss] tar_ll 1.3128 loss -1.3128 (5.855 secs)\n",
      "isanp:isanp-num_latents-128_matern step 88800 lr 1.532e-05 [train_loss] tar_ll 1.2865 loss -1.2865 (6.020 secs)\n",
      "isanp:isanp-num_latents-128_matern step 89000 lr 1.478e-05 [train_loss] tar_ll 1.3156 loss -1.3156 (5.818 secs)\n",
      "isanp:isanp-num_latents-128_matern step 89200 lr 1.425e-05 [train_loss] tar_ll 1.3093 loss -1.3093 (5.926 secs)\n",
      "isanp:isanp-num_latents-128_matern step 89400 lr 1.373e-05 [train_loss] tar_ll 1.2856 loss -1.2856 (5.841 secs)\n",
      "isanp:isanp-num_latents-128_matern step 89600 lr 1.323e-05 [train_loss] tar_ll 1.2930 loss -1.2930 (5.806 secs)\n",
      "isanp:isanp-num_latents-128_matern step 89800 lr 1.273e-05 [train_loss] tar_ll 1.3450 loss -1.3450 (5.962 secs)\n",
      "isanp:isanp-num_latents-128_matern step 90000 lr 1.224e-05 [train_loss] tar_ll 1.3734 loss -1.3734 (5.713 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:30<00:00, 99.02it/s] \n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.8928 loss -0.8928 (30.298 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 90200 lr 1.176e-05 [train_loss] tar_ll 1.3345 loss -1.3345 (5.915 secs)\n",
      "isanp:isanp-num_latents-128_matern step 90400 lr 1.128e-05 [train_loss] tar_ll 1.3398 loss -1.3398 (6.009 secs)\n",
      "isanp:isanp-num_latents-128_matern step 90600 lr 1.082e-05 [train_loss] tar_ll 1.3188 loss -1.3188 (5.955 secs)\n",
      "isanp:isanp-num_latents-128_matern step 90800 lr 1.037e-05 [train_loss] tar_ll 1.3258 loss -1.3258 (5.864 secs)\n",
      "isanp:isanp-num_latents-128_matern step 91000 lr 9.927e-06 [train_loss] tar_ll 1.2282 loss -1.2282 (6.049 secs)\n",
      "isanp:isanp-num_latents-128_matern step 91200 lr 9.493e-06 [train_loss] tar_ll 1.3212 loss -1.3212 (6.110 secs)\n",
      "isanp:isanp-num_latents-128_matern step 91400 lr 9.069e-06 [train_loss] tar_ll 1.3318 loss -1.3318 (5.962 secs)\n",
      "isanp:isanp-num_latents-128_matern step 91600 lr 8.655e-06 [train_loss] tar_ll 1.3496 loss -1.3496 (6.055 secs)\n",
      "isanp:isanp-num_latents-128_matern step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2703 loss -1.2703 (5.963 secs)\n",
      "isanp:isanp-num_latents-128_matern step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2991 loss -1.2991 (6.197 secs)\n",
      "isanp:isanp-num_latents-128_matern step 92200 lr 7.468e-06 [train_loss] tar_ll 1.3538 loss -1.3538 (6.037 secs)\n",
      "isanp:isanp-num_latents-128_matern step 92400 lr 7.092e-06 [train_loss] tar_ll 1.3528 loss -1.3528 (6.001 secs)\n",
      "isanp:isanp-num_latents-128_matern step 92600 lr 6.725e-06 [train_loss] tar_ll 1.3496 loss -1.3496 (6.052 secs)\n",
      "isanp:isanp-num_latents-128_matern step 92800 lr 6.368e-06 [train_loss] tar_ll 1.3144 loss -1.3144 (5.910 secs)\n",
      "isanp:isanp-num_latents-128_matern step 93000 lr 6.021e-06 [train_loss] tar_ll 1.3365 loss -1.3365 (6.008 secs)\n",
      "isanp:isanp-num_latents-128_matern step 93200 lr 5.683e-06 [train_loss] tar_ll 1.3213 loss -1.3213 (6.085 secs)\n",
      "isanp:isanp-num_latents-128_matern step 93400 lr 5.355e-06 [train_loss] tar_ll 1.2530 loss -1.2530 (5.995 secs)\n",
      "isanp:isanp-num_latents-128_matern step 93600 lr 5.036e-06 [train_loss] tar_ll 1.3164 loss -1.3164 (6.015 secs)\n",
      "isanp:isanp-num_latents-128_matern step 93800 lr 4.727e-06 [train_loss] tar_ll 1.2658 loss -1.2658 (5.909 secs)\n",
      "isanp:isanp-num_latents-128_matern step 94000 lr 4.428e-06 [train_loss] tar_ll 1.3153 loss -1.3153 (5.938 secs)\n",
      "isanp:isanp-num_latents-128_matern step 94200 lr 4.139e-06 [train_loss] tar_ll 1.2390 loss -1.2390 (6.160 secs)\n",
      "isanp:isanp-num_latents-128_matern step 94400 lr 3.859e-06 [train_loss] tar_ll 1.2674 loss -1.2674 (5.983 secs)\n",
      "isanp:isanp-num_latents-128_matern step 94600 lr 3.589e-06 [train_loss] tar_ll 1.3970 loss -1.3970 (6.467 secs)\n",
      "isanp:isanp-num_latents-128_matern step 94800 lr 3.329e-06 [train_loss] tar_ll 1.4503 loss -1.4503 (5.900 secs)\n",
      "isanp:isanp-num_latents-128_matern step 95000 lr 3.078e-06 [train_loss] tar_ll 1.3291 loss -1.3291 (5.936 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:29<00:00, 101.85it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.9174 loss -0.9174 (29.456 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-128_matern step 95200 lr 2.837e-06 [train_loss] tar_ll 1.3511 loss -1.3511 (6.243 secs)\n",
      "isanp:isanp-num_latents-128_matern step 95400 lr 2.606e-06 [train_loss] tar_ll 1.3739 loss -1.3739 (6.084 secs)\n",
      "isanp:isanp-num_latents-128_matern step 95600 lr 2.385e-06 [train_loss] tar_ll 1.2952 loss -1.2952 (6.194 secs)\n",
      "isanp:isanp-num_latents-128_matern step 95800 lr 2.173e-06 [train_loss] tar_ll 1.3874 loss -1.3874 (6.182 secs)\n",
      "isanp:isanp-num_latents-128_matern step 96000 lr 1.971e-06 [train_loss] tar_ll 1.3489 loss -1.3489 (6.410 secs)\n",
      "isanp:isanp-num_latents-128_matern step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2638 loss -1.2638 (6.452 secs)\n",
      "isanp:isanp-num_latents-128_matern step 96400 lr 1.597e-06 [train_loss] tar_ll 1.3465 loss -1.3465 (6.344 secs)\n",
      "isanp:isanp-num_latents-128_matern step 96600 lr 1.425e-06 [train_loss] tar_ll 1.4126 loss -1.4126 (6.249 secs)\n",
      "isanp:isanp-num_latents-128_matern step 96800 lr 1.262e-06 [train_loss] tar_ll 1.3037 loss -1.3037 (6.324 secs)\n",
      "isanp:isanp-num_latents-128_matern step 97000 lr 1.110e-06 [train_loss] tar_ll 1.4485 loss -1.4485 (6.108 secs)\n",
      "isanp:isanp-num_latents-128_matern step 97200 lr 9.666e-07 [train_loss] tar_ll 1.3371 loss -1.3371 (5.917 secs)\n",
      "isanp:isanp-num_latents-128_matern step 97400 lr 8.335e-07 [train_loss] tar_ll 1.3153 loss -1.3153 (6.134 secs)\n",
      "isanp:isanp-num_latents-128_matern step 97600 lr 7.103e-07 [train_loss] tar_ll 1.3101 loss -1.3101 (5.884 secs)\n",
      "isanp:isanp-num_latents-128_matern step 97800 lr 5.969e-07 [train_loss] tar_ll 1.3231 loss -1.3231 (5.944 secs)\n",
      "isanp:isanp-num_latents-128_matern step 98000 lr 4.933e-07 [train_loss] tar_ll 1.3530 loss -1.3530 (5.820 secs)\n",
      "isanp:isanp-num_latents-128_matern step 98200 lr 3.996e-07 [train_loss] tar_ll 1.3166 loss -1.3166 (5.840 secs)\n",
      "isanp:isanp-num_latents-128_matern step 98400 lr 3.158e-07 [train_loss] tar_ll 1.3574 loss -1.3574 (5.917 secs)\n",
      "isanp:isanp-num_latents-128_matern step 98600 lr 2.418e-07 [train_loss] tar_ll 1.3117 loss -1.3117 (5.781 secs)\n",
      "isanp:isanp-num_latents-128_matern step 98800 lr 1.776e-07 [train_loss] tar_ll 1.3551 loss -1.3551 (5.872 secs)\n",
      "isanp:isanp-num_latents-128_matern step 99000 lr 1.234e-07 [train_loss] tar_ll 1.3021 loss -1.3021 (6.076 secs)\n",
      "isanp:isanp-num_latents-128_matern step 99200 lr 7.895e-08 [train_loss] tar_ll 1.3294 loss -1.3294 (5.952 secs)\n",
      "isanp:isanp-num_latents-128_matern step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2525 loss -1.2525 (5.942 secs)\n",
      "isanp:isanp-num_latents-128_matern step 99600 lr 1.974e-08 [train_loss] tar_ll 1.4284 loss -1.4284 (5.953 secs)\n",
      "isanp:isanp-num_latents-128_matern step 99800 lr 4.935e-09 [train_loss] tar_ll 1.3874 loss -1.3874 (5.918 secs)\n",
      "isanp:isanp-num_latents-128_matern step 100000 lr 0.000e+00 [train_loss] tar_ll 1.3559 loss -1.3559 (6.087 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:30<00:00, 97.11it/s] \n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.9146 loss -0.9146 (30.894 secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "False\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:28<00:00, 104.98it/s]\n",
      "isanp:isanp-num_latents-128_matern matern tar_ll 0.9146 loss -0.9146 (28.579 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVal has finished\n",
      "Execution time: 3685762.5 miliseconds\n",
      "Execution time: 3685.7625 seconds\n",
      "Initial Memory Usage: 28.0322265625 MB\n",
      "Final Memory Usage: 131.6259765625 MB\n",
      "Memory Usage Change: 103.59375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='isanp-num_latents-128_matern',val_seed=0, val_l=128,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249063f0-7c0a-43e8-b9a8-f4f3448dd5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c15f2-09ae-4b4d-8fcf-f17205f00da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bc4bd25-c0f0-4f3b-9d97-2c64a8db4592",
   "metadata": {},
   "source": [
    "# PERIODIC Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee43263-d786-4213-ba00-61674fed47d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ee3715-8d3a-4acb-a459-c78024a18f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating evaluation sets...\n",
      "Generating Evaluation Sets with periodic kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:07<00:00, 413.13it/s]\n",
      "Experiment: cnp-cnp_periodic\n",
      "Total number of parameters: 215682\n",
      "\n",
      "cnp:cnp_periodic step 200 lr 5.000e-04 [train_loss] loss 0.6688 (2.202 secs)\n",
      "cnp:cnp_periodic step 400 lr 5.000e-04 [train_loss] loss 0.6109 (2.246 secs)\n",
      "cnp:cnp_periodic step 600 lr 5.000e-04 [train_loss] loss 0.4749 (2.349 secs)\n",
      "cnp:cnp_periodic step 800 lr 4.999e-04 [train_loss] loss 0.4338 (2.383 secs)\n",
      "cnp:cnp_periodic step 1000 lr 4.999e-04 [train_loss] loss 0.4343 (2.459 secs)\n",
      "cnp:cnp_periodic step 1200 lr 4.998e-04 [train_loss] loss 0.4580 (2.180 secs)\n",
      "cnp:cnp_periodic step 1400 lr 4.998e-04 [train_loss] loss 0.4410 (2.233 secs)\n",
      "cnp:cnp_periodic step 1600 lr 4.997e-04 [train_loss] loss 0.4310 (2.216 secs)\n",
      "cnp:cnp_periodic step 1800 lr 4.996e-04 [train_loss] loss 0.4104 (2.254 secs)\n",
      "cnp:cnp_periodic step 2000 lr 4.995e-04 [train_loss] loss 0.4515 (2.352 secs)\n",
      "cnp:cnp_periodic step 2200 lr 4.994e-04 [train_loss] loss 0.3933 (2.389 secs)\n",
      "cnp:cnp_periodic step 2400 lr 4.993e-04 [train_loss] loss 0.3564 (2.260 secs)\n",
      "cnp:cnp_periodic step 2600 lr 4.992e-04 [train_loss] loss 0.3878 (2.286 secs)\n",
      "cnp:cnp_periodic step 2800 lr 4.990e-04 [train_loss] loss 0.3588 (2.388 secs)\n",
      "cnp:cnp_periodic step 3000 lr 4.989e-04 [train_loss] loss 0.3139 (2.249 secs)\n",
      "cnp:cnp_periodic step 3200 lr 4.987e-04 [train_loss] loss 0.3620 (2.309 secs)\n",
      "cnp:cnp_periodic step 3400 lr 4.986e-04 [train_loss] loss 0.3374 (2.231 secs)\n",
      "cnp:cnp_periodic step 3600 lr 4.984e-04 [train_loss] loss 0.3412 (2.287 secs)\n",
      "cnp:cnp_periodic step 3800 lr 4.982e-04 [train_loss] loss 0.3173 (2.305 secs)\n",
      "cnp:cnp_periodic step 4000 lr 4.980e-04 [train_loss] loss 0.3143 (2.278 secs)\n",
      "cnp:cnp_periodic step 4200 lr 4.978e-04 [train_loss] loss 0.3204 (2.320 secs)\n",
      "cnp:cnp_periodic step 4400 lr 4.976e-04 [train_loss] loss 0.2866 (2.191 secs)\n",
      "cnp:cnp_periodic step 4600 lr 4.974e-04 [train_loss] loss 0.2784 (2.206 secs)\n",
      "cnp:cnp_periodic step 4800 lr 4.972e-04 [train_loss] loss 0.2494 (2.219 secs)\n",
      "cnp:cnp_periodic step 5000 lr 4.969e-04 [train_loss] loss 0.2429 (2.462 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 395.26it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.4891 tar_ll -0.9303 (7.592 secs)\n",
      "\n",
      "cnp:cnp_periodic step 5200 lr 4.967e-04 [train_loss] loss 0.2196 (2.193 secs)\n",
      "cnp:cnp_periodic step 5400 lr 4.964e-04 [train_loss] loss 0.2510 (2.224 secs)\n",
      "cnp:cnp_periodic step 5600 lr 4.961e-04 [train_loss] loss 0.2112 (2.224 secs)\n",
      "cnp:cnp_periodic step 5800 lr 4.959e-04 [train_loss] loss 0.1969 (2.170 secs)\n",
      "cnp:cnp_periodic step 6000 lr 4.956e-04 [train_loss] loss 0.1756 (2.232 secs)\n",
      "cnp:cnp_periodic step 6200 lr 4.953e-04 [train_loss] loss 0.2121 (2.276 secs)\n",
      "cnp:cnp_periodic step 6400 lr 4.950e-04 [train_loss] loss 0.1958 (2.323 secs)\n",
      "cnp:cnp_periodic step 6600 lr 4.946e-04 [train_loss] loss 0.1810 (2.174 secs)\n",
      "cnp:cnp_periodic step 6800 lr 4.943e-04 [train_loss] loss 0.1692 (2.227 secs)\n",
      "cnp:cnp_periodic step 7000 lr 4.940e-04 [train_loss] loss 0.1533 (2.270 secs)\n",
      "cnp:cnp_periodic step 7200 lr 4.936e-04 [train_loss] loss 0.1932 (2.236 secs)\n",
      "cnp:cnp_periodic step 7400 lr 4.933e-04 [train_loss] loss 0.1863 (2.232 secs)\n",
      "cnp:cnp_periodic step 7600 lr 4.929e-04 [train_loss] loss 0.1610 (2.225 secs)\n",
      "cnp:cnp_periodic step 7800 lr 4.925e-04 [train_loss] loss 0.1743 (2.505 secs)\n",
      "cnp:cnp_periodic step 8000 lr 4.921e-04 [train_loss] loss 0.1783 (2.400 secs)\n",
      "cnp:cnp_periodic step 8200 lr 4.918e-04 [train_loss] loss 0.1245 (2.301 secs)\n",
      "cnp:cnp_periodic step 8400 lr 4.913e-04 [train_loss] loss 0.1299 (2.378 secs)\n",
      "cnp:cnp_periodic step 8600 lr 4.909e-04 [train_loss] loss 0.1084 (2.363 secs)\n",
      "cnp:cnp_periodic step 8800 lr 4.905e-04 [train_loss] loss 0.1173 (2.195 secs)\n",
      "cnp:cnp_periodic step 9000 lr 4.901e-04 [train_loss] loss 0.1291 (2.224 secs)\n",
      "cnp:cnp_periodic step 9200 lr 4.896e-04 [train_loss] loss 0.0848 (2.294 secs)\n",
      "cnp:cnp_periodic step 9400 lr 4.892e-04 [train_loss] loss 0.1011 (2.314 secs)\n",
      "cnp:cnp_periodic step 9600 lr 4.887e-04 [train_loss] loss 0.1118 (2.263 secs)\n",
      "cnp:cnp_periodic step 9800 lr 4.882e-04 [train_loss] loss 0.0705 (2.390 secs)\n",
      "cnp:cnp_periodic step 10000 lr 4.878e-04 [train_loss] loss 0.1136 (2.321 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 395.80it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.3953 tar_ll -0.9969 (7.583 secs)\n",
      "\n",
      "cnp:cnp_periodic step 10200 lr 4.873e-04 [train_loss] loss 0.0701 (2.311 secs)\n",
      "cnp:cnp_periodic step 10400 lr 4.868e-04 [train_loss] loss 0.0719 (2.482 secs)\n",
      "cnp:cnp_periodic step 10600 lr 4.863e-04 [train_loss] loss 0.0792 (2.385 secs)\n",
      "cnp:cnp_periodic step 10800 lr 4.857e-04 [train_loss] loss 0.0137 (2.218 secs)\n",
      "cnp:cnp_periodic step 11000 lr 4.852e-04 [train_loss] loss 0.0271 (2.357 secs)\n",
      "cnp:cnp_periodic step 11200 lr 4.847e-04 [train_loss] loss 0.0693 (2.313 secs)\n",
      "cnp:cnp_periodic step 11400 lr 4.841e-04 [train_loss] loss -0.0170 (2.464 secs)\n",
      "cnp:cnp_periodic step 11600 lr 4.836e-04 [train_loss] loss 0.0446 (2.231 secs)\n",
      "cnp:cnp_periodic step 11800 lr 4.830e-04 [train_loss] loss 0.0392 (2.591 secs)\n",
      "cnp:cnp_periodic step 12000 lr 4.824e-04 [train_loss] loss 0.0612 (2.265 secs)\n",
      "cnp:cnp_periodic step 12200 lr 4.819e-04 [train_loss] loss -0.0157 (2.377 secs)\n",
      "cnp:cnp_periodic step 12400 lr 4.813e-04 [train_loss] loss 0.0718 (2.269 secs)\n",
      "cnp:cnp_periodic step 12600 lr 4.807e-04 [train_loss] loss 0.0534 (2.378 secs)\n",
      "cnp:cnp_periodic step 12800 lr 4.801e-04 [train_loss] loss 0.0377 (2.330 secs)\n",
      "cnp:cnp_periodic step 13000 lr 4.794e-04 [train_loss] loss -0.0145 (2.258 secs)\n",
      "cnp:cnp_periodic step 13200 lr 4.788e-04 [train_loss] loss 0.0232 (2.445 secs)\n",
      "cnp:cnp_periodic step 13400 lr 4.782e-04 [train_loss] loss 0.0003 (2.325 secs)\n",
      "cnp:cnp_periodic step 13600 lr 4.775e-04 [train_loss] loss 0.0437 (2.315 secs)\n",
      "cnp:cnp_periodic step 13800 lr 4.769e-04 [train_loss] loss 0.0036 (2.271 secs)\n",
      "cnp:cnp_periodic step 14000 lr 4.762e-04 [train_loss] loss 0.0008 (2.219 secs)\n",
      "cnp:cnp_periodic step 14200 lr 4.755e-04 [train_loss] loss -0.0116 (2.325 secs)\n",
      "cnp:cnp_periodic step 14400 lr 4.749e-04 [train_loss] loss 0.0054 (2.309 secs)\n",
      "cnp:cnp_periodic step 14600 lr 4.742e-04 [train_loss] loss -0.0222 (2.488 secs)\n",
      "cnp:cnp_periodic step 14800 lr 4.735e-04 [train_loss] loss -0.0463 (2.265 secs)\n",
      "cnp:cnp_periodic step 15000 lr 4.728e-04 [train_loss] loss 0.0247 (2.350 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 387.17it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.3876 tar_ll -1.0443 (7.751 secs)\n",
      "\n",
      "cnp:cnp_periodic step 15200 lr 4.720e-04 [train_loss] loss -0.0162 (2.378 secs)\n",
      "cnp:cnp_periodic step 15400 lr 4.713e-04 [train_loss] loss -0.0279 (2.212 secs)\n",
      "cnp:cnp_periodic step 15600 lr 4.706e-04 [train_loss] loss -0.0322 (2.255 secs)\n",
      "cnp:cnp_periodic step 15800 lr 4.698e-04 [train_loss] loss -0.0575 (2.264 secs)\n",
      "cnp:cnp_periodic step 16000 lr 4.691e-04 [train_loss] loss -0.0294 (2.383 secs)\n",
      "cnp:cnp_periodic step 16200 lr 4.683e-04 [train_loss] loss 0.0171 (2.276 secs)\n",
      "cnp:cnp_periodic step 16400 lr 4.675e-04 [train_loss] loss -0.0366 (2.322 secs)\n",
      "cnp:cnp_periodic step 16600 lr 4.668e-04 [train_loss] loss -0.0511 (2.462 secs)\n",
      "cnp:cnp_periodic step 16800 lr 4.660e-04 [train_loss] loss -0.0360 (2.270 secs)\n",
      "cnp:cnp_periodic step 17000 lr 4.652e-04 [train_loss] loss -0.1113 (2.298 secs)\n",
      "cnp:cnp_periodic step 17200 lr 4.644e-04 [train_loss] loss -0.0683 (2.232 secs)\n",
      "cnp:cnp_periodic step 17400 lr 4.636e-04 [train_loss] loss -0.0690 (2.267 secs)\n",
      "cnp:cnp_periodic step 17600 lr 4.627e-04 [train_loss] loss -0.0382 (2.282 secs)\n",
      "cnp:cnp_periodic step 17800 lr 4.619e-04 [train_loss] loss -0.0719 (2.304 secs)\n",
      "cnp:cnp_periodic step 18000 lr 4.611e-04 [train_loss] loss -0.0646 (2.316 secs)\n",
      "cnp:cnp_periodic step 18200 lr 4.602e-04 [train_loss] loss -0.1137 (2.369 secs)\n",
      "cnp:cnp_periodic step 18400 lr 4.594e-04 [train_loss] loss -0.1013 (2.196 secs)\n",
      "cnp:cnp_periodic step 18600 lr 4.585e-04 [train_loss] loss -0.0396 (2.306 secs)\n",
      "cnp:cnp_periodic step 18800 lr 4.576e-04 [train_loss] loss -0.0801 (2.172 secs)\n",
      "cnp:cnp_periodic step 19000 lr 4.568e-04 [train_loss] loss -0.0042 (2.282 secs)\n",
      "cnp:cnp_periodic step 19200 lr 4.559e-04 [train_loss] loss -0.0018 (2.313 secs)\n",
      "cnp:cnp_periodic step 19400 lr 4.550e-04 [train_loss] loss -0.1113 (2.414 secs)\n",
      "cnp:cnp_periodic step 19600 lr 4.541e-04 [train_loss] loss -0.1049 (2.173 secs)\n",
      "cnp:cnp_periodic step 19800 lr 4.532e-04 [train_loss] loss -0.1016 (2.243 secs)\n",
      "cnp:cnp_periodic step 20000 lr 4.523e-04 [train_loss] loss -0.0944 (2.256 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 383.45it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.3643 tar_ll -0.9255 (7.827 secs)\n",
      "\n",
      "cnp:cnp_periodic step 20200 lr 4.513e-04 [train_loss] loss -0.0847 (2.320 secs)\n",
      "cnp:cnp_periodic step 20400 lr 4.504e-04 [train_loss] loss -0.0569 (2.243 secs)\n",
      "cnp:cnp_periodic step 20600 lr 4.494e-04 [train_loss] loss -0.1452 (2.210 secs)\n",
      "cnp:cnp_periodic step 20800 lr 4.485e-04 [train_loss] loss -0.1000 (2.309 secs)\n",
      "cnp:cnp_periodic step 21000 lr 4.475e-04 [train_loss] loss -0.0961 (2.270 secs)\n",
      "cnp:cnp_periodic step 21200 lr 4.466e-04 [train_loss] loss -0.0916 (2.321 secs)\n",
      "cnp:cnp_periodic step 21400 lr 4.456e-04 [train_loss] loss -0.1182 (2.245 secs)\n",
      "cnp:cnp_periodic step 21600 lr 4.446e-04 [train_loss] loss -0.0719 (2.268 secs)\n",
      "cnp:cnp_periodic step 21800 lr 4.436e-04 [train_loss] loss -0.0655 (2.370 secs)\n",
      "cnp:cnp_periodic step 22000 lr 4.426e-04 [train_loss] loss -0.1247 (2.269 secs)\n",
      "cnp:cnp_periodic step 22200 lr 4.416e-04 [train_loss] loss -0.0867 (2.377 secs)\n",
      "cnp:cnp_periodic step 22400 lr 4.406e-04 [train_loss] loss -0.1292 (2.243 secs)\n",
      "cnp:cnp_periodic step 22600 lr 4.396e-04 [train_loss] loss -0.1016 (2.257 secs)\n",
      "cnp:cnp_periodic step 22800 lr 4.386e-04 [train_loss] loss -0.1092 (2.347 secs)\n",
      "cnp:cnp_periodic step 23000 lr 4.375e-04 [train_loss] loss -0.1055 (2.444 secs)\n",
      "cnp:cnp_periodic step 23200 lr 4.365e-04 [train_loss] loss -0.0908 (2.324 secs)\n",
      "cnp:cnp_periodic step 23400 lr 4.354e-04 [train_loss] loss -0.1593 (2.194 secs)\n",
      "cnp:cnp_periodic step 23600 lr 4.344e-04 [train_loss] loss -0.1334 (2.212 secs)\n",
      "cnp:cnp_periodic step 23800 lr 4.333e-04 [train_loss] loss -0.1244 (2.182 secs)\n",
      "cnp:cnp_periodic step 24000 lr 4.322e-04 [train_loss] loss -0.1428 (2.366 secs)\n",
      "cnp:cnp_periodic step 24200 lr 4.312e-04 [train_loss] loss -0.1169 (2.323 secs)\n",
      "cnp:cnp_periodic step 24400 lr 4.301e-04 [train_loss] loss -0.1048 (2.262 secs)\n",
      "cnp:cnp_periodic step 24600 lr 4.290e-04 [train_loss] loss -0.1119 (2.249 secs)\n",
      "cnp:cnp_periodic step 24800 lr 4.279e-04 [train_loss] loss -0.1732 (2.209 secs)\n",
      "cnp:cnp_periodic step 25000 lr 4.268e-04 [train_loss] loss -0.1290 (2.184 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 390.67it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.3825 tar_ll -1.2862 (7.681 secs)\n",
      "\n",
      "cnp:cnp_periodic step 25200 lr 4.257e-04 [train_loss] loss -0.1487 (2.312 secs)\n",
      "cnp:cnp_periodic step 25400 lr 4.245e-04 [train_loss] loss -0.1585 (2.299 secs)\n",
      "cnp:cnp_periodic step 25600 lr 4.234e-04 [train_loss] loss -0.1577 (2.295 secs)\n",
      "cnp:cnp_periodic step 25800 lr 4.223e-04 [train_loss] loss -0.1378 (2.339 secs)\n",
      "cnp:cnp_periodic step 26000 lr 4.211e-04 [train_loss] loss -0.1654 (2.360 secs)\n",
      "cnp:cnp_periodic step 26200 lr 4.200e-04 [train_loss] loss -0.1809 (2.497 secs)\n",
      "cnp:cnp_periodic step 26400 lr 4.188e-04 [train_loss] loss -0.1557 (2.314 secs)\n",
      "cnp:cnp_periodic step 26600 lr 4.177e-04 [train_loss] loss -0.1537 (2.411 secs)\n",
      "cnp:cnp_periodic step 26800 lr 4.165e-04 [train_loss] loss -0.1233 (2.252 secs)\n",
      "cnp:cnp_periodic step 27000 lr 4.153e-04 [train_loss] loss -0.2033 (2.316 secs)\n",
      "cnp:cnp_periodic step 27200 lr 4.141e-04 [train_loss] loss -0.1781 (2.235 secs)\n",
      "cnp:cnp_periodic step 27400 lr 4.130e-04 [train_loss] loss -0.1769 (2.276 secs)\n",
      "cnp:cnp_periodic step 27600 lr 4.118e-04 [train_loss] loss -0.1632 (2.347 secs)\n",
      "cnp:cnp_periodic step 27800 lr 4.106e-04 [train_loss] loss -0.1812 (2.363 secs)\n",
      "cnp:cnp_periodic step 28000 lr 4.094e-04 [train_loss] loss -0.1153 (2.355 secs)\n",
      "cnp:cnp_periodic step 28200 lr 4.081e-04 [train_loss] loss -0.1497 (2.296 secs)\n",
      "cnp:cnp_periodic step 28400 lr 4.069e-04 [train_loss] loss -0.1743 (2.364 secs)\n",
      "cnp:cnp_periodic step 28600 lr 4.057e-04 [train_loss] loss -0.2017 (2.309 secs)\n",
      "cnp:cnp_periodic step 28800 lr 4.045e-04 [train_loss] loss -0.1616 (2.466 secs)\n",
      "cnp:cnp_periodic step 29000 lr 4.032e-04 [train_loss] loss -0.2102 (2.277 secs)\n",
      "cnp:cnp_periodic step 29200 lr 4.020e-04 [train_loss] loss -0.2074 (2.388 secs)\n",
      "cnp:cnp_periodic step 29400 lr 4.007e-04 [train_loss] loss -0.2269 (2.327 secs)\n",
      "cnp:cnp_periodic step 29600 lr 3.995e-04 [train_loss] loss -0.1808 (2.249 secs)\n",
      "cnp:cnp_periodic step 29800 lr 3.982e-04 [train_loss] loss -0.2086 (2.347 secs)\n",
      "cnp:cnp_periodic step 30000 lr 3.969e-04 [train_loss] loss -0.1831 (2.302 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 387.46it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.3608 tar_ll -1.3996 (7.744 secs)\n",
      "\n",
      "cnp:cnp_periodic step 30200 lr 3.957e-04 [train_loss] loss -0.1832 (2.378 secs)\n",
      "cnp:cnp_periodic step 30400 lr 3.944e-04 [train_loss] loss -0.1942 (2.204 secs)\n",
      "cnp:cnp_periodic step 30600 lr 3.931e-04 [train_loss] loss -0.2243 (2.294 secs)\n",
      "cnp:cnp_periodic step 30800 lr 3.918e-04 [train_loss] loss -0.1921 (2.405 secs)\n",
      "cnp:cnp_periodic step 31000 lr 3.905e-04 [train_loss] loss -0.2068 (2.265 secs)\n",
      "cnp:cnp_periodic step 31200 lr 3.892e-04 [train_loss] loss -0.1924 (2.349 secs)\n",
      "cnp:cnp_periodic step 31400 lr 3.879e-04 [train_loss] loss -0.2761 (2.225 secs)\n",
      "cnp:cnp_periodic step 31600 lr 3.866e-04 [train_loss] loss -0.2347 (2.424 secs)\n",
      "cnp:cnp_periodic step 31800 lr 3.853e-04 [train_loss] loss -0.2203 (2.191 secs)\n",
      "cnp:cnp_periodic step 32000 lr 3.840e-04 [train_loss] loss -0.2131 (2.427 secs)\n",
      "cnp:cnp_periodic step 32200 lr 3.826e-04 [train_loss] loss -0.2482 (2.366 secs)\n",
      "cnp:cnp_periodic step 32400 lr 3.813e-04 [train_loss] loss -0.2099 (2.289 secs)\n",
      "cnp:cnp_periodic step 32600 lr 3.800e-04 [train_loss] loss -0.1997 (2.276 secs)\n",
      "cnp:cnp_periodic step 32800 lr 3.786e-04 [train_loss] loss -0.2294 (2.348 secs)\n",
      "cnp:cnp_periodic step 33000 lr 3.773e-04 [train_loss] loss -0.1886 (2.295 secs)\n",
      "cnp:cnp_periodic step 33200 lr 3.759e-04 [train_loss] loss -0.2351 (2.333 secs)\n",
      "cnp:cnp_periodic step 33400 lr 3.745e-04 [train_loss] loss -0.2109 (2.286 secs)\n",
      "cnp:cnp_periodic step 33600 lr 3.732e-04 [train_loss] loss -0.2077 (2.421 secs)\n",
      "cnp:cnp_periodic step 33800 lr 3.718e-04 [train_loss] loss -0.2094 (2.242 secs)\n",
      "cnp:cnp_periodic step 34000 lr 3.704e-04 [train_loss] loss -0.1777 (2.335 secs)\n",
      "cnp:cnp_periodic step 34200 lr 3.691e-04 [train_loss] loss -0.2196 (2.345 secs)\n",
      "cnp:cnp_periodic step 34400 lr 3.677e-04 [train_loss] loss -0.1914 (2.231 secs)\n",
      "cnp:cnp_periodic step 34600 lr 3.663e-04 [train_loss] loss -0.2262 (2.355 secs)\n",
      "cnp:cnp_periodic step 34800 lr 3.649e-04 [train_loss] loss -0.2656 (2.166 secs)\n",
      "cnp:cnp_periodic step 35000 lr 3.635e-04 [train_loss] loss -0.2094 (2.370 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 393.59it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.3610 tar_ll -1.1835 (7.624 secs)\n",
      "\n",
      "cnp:cnp_periodic step 35200 lr 3.621e-04 [train_loss] loss -0.2264 (2.178 secs)\n",
      "cnp:cnp_periodic step 35400 lr 3.607e-04 [train_loss] loss -0.2610 (2.341 secs)\n",
      "cnp:cnp_periodic step 35600 lr 3.593e-04 [train_loss] loss -0.2333 (2.333 secs)\n",
      "cnp:cnp_periodic step 35800 lr 3.579e-04 [train_loss] loss -0.2527 (2.161 secs)\n",
      "cnp:cnp_periodic step 36000 lr 3.564e-04 [train_loss] loss -0.2098 (2.178 secs)\n",
      "cnp:cnp_periodic step 36200 lr 3.550e-04 [train_loss] loss -0.2139 (2.249 secs)\n",
      "cnp:cnp_periodic step 36400 lr 3.536e-04 [train_loss] loss -0.2387 (2.170 secs)\n",
      "cnp:cnp_periodic step 36600 lr 3.522e-04 [train_loss] loss -0.2733 (2.334 secs)\n",
      "cnp:cnp_periodic step 36800 lr 3.507e-04 [train_loss] loss -0.2863 (2.271 secs)\n",
      "cnp:cnp_periodic step 37000 lr 3.493e-04 [train_loss] loss -0.1974 (2.404 secs)\n",
      "cnp:cnp_periodic step 37200 lr 3.478e-04 [train_loss] loss -0.2439 (2.324 secs)\n",
      "cnp:cnp_periodic step 37400 lr 3.464e-04 [train_loss] loss -0.2566 (2.216 secs)\n",
      "cnp:cnp_periodic step 37600 lr 3.449e-04 [train_loss] loss -0.2485 (2.235 secs)\n",
      "cnp:cnp_periodic step 37800 lr 3.435e-04 [train_loss] loss -0.2197 (2.395 secs)\n",
      "cnp:cnp_periodic step 38000 lr 3.420e-04 [train_loss] loss -0.2430 (2.235 secs)\n",
      "cnp:cnp_periodic step 38200 lr 3.406e-04 [train_loss] loss -0.2186 (2.244 secs)\n",
      "cnp:cnp_periodic step 38400 lr 3.391e-04 [train_loss] loss -0.2564 (2.379 secs)\n",
      "cnp:cnp_periodic step 38600 lr 3.376e-04 [train_loss] loss -0.2486 (2.336 secs)\n",
      "cnp:cnp_periodic step 38800 lr 3.362e-04 [train_loss] loss -0.2578 (2.263 secs)\n",
      "cnp:cnp_periodic step 39000 lr 3.347e-04 [train_loss] loss -0.2773 (2.229 secs)\n",
      "cnp:cnp_periodic step 39200 lr 3.332e-04 [train_loss] loss -0.2827 (2.241 secs)\n",
      "cnp:cnp_periodic step 39400 lr 3.317e-04 [train_loss] loss -0.2712 (2.293 secs)\n",
      "cnp:cnp_periodic step 39600 lr 3.302e-04 [train_loss] loss -0.2610 (2.208 secs)\n",
      "cnp:cnp_periodic step 39800 lr 3.287e-04 [train_loss] loss -0.2834 (2.251 secs)\n",
      "cnp:cnp_periodic step 40000 lr 3.273e-04 [train_loss] loss -0.2855 (2.384 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 401.67it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.3185 tar_ll -1.2339 (7.471 secs)\n",
      "\n",
      "cnp:cnp_periodic step 40200 lr 3.258e-04 [train_loss] loss -0.2952 (2.291 secs)\n",
      "cnp:cnp_periodic step 40400 lr 3.243e-04 [train_loss] loss -0.3059 (2.231 secs)\n",
      "cnp:cnp_periodic step 40600 lr 3.228e-04 [train_loss] loss -0.2751 (2.299 secs)\n",
      "cnp:cnp_periodic step 40800 lr 3.213e-04 [train_loss] loss -0.2334 (2.274 secs)\n",
      "cnp:cnp_periodic step 41000 lr 3.197e-04 [train_loss] loss -0.2588 (2.231 secs)\n",
      "cnp:cnp_periodic step 41200 lr 3.182e-04 [train_loss] loss -0.2487 (2.215 secs)\n",
      "cnp:cnp_periodic step 41400 lr 3.167e-04 [train_loss] loss -0.3085 (2.318 secs)\n",
      "cnp:cnp_periodic step 41600 lr 3.152e-04 [train_loss] loss -0.2860 (2.213 secs)\n",
      "cnp:cnp_periodic step 41800 lr 3.137e-04 [train_loss] loss -0.2722 (2.480 secs)\n",
      "cnp:cnp_periodic step 42000 lr 3.122e-04 [train_loss] loss -0.2977 (2.226 secs)\n",
      "cnp:cnp_periodic step 42200 lr 3.106e-04 [train_loss] loss -0.3302 (2.177 secs)\n",
      "cnp:cnp_periodic step 42400 lr 3.091e-04 [train_loss] loss -0.2971 (2.307 secs)\n",
      "cnp:cnp_periodic step 42600 lr 3.076e-04 [train_loss] loss -0.2355 (2.340 secs)\n",
      "cnp:cnp_periodic step 42800 lr 3.061e-04 [train_loss] loss -0.2795 (2.266 secs)\n",
      "cnp:cnp_periodic step 43000 lr 3.045e-04 [train_loss] loss -0.3012 (2.121 secs)\n",
      "cnp:cnp_periodic step 43200 lr 3.030e-04 [train_loss] loss -0.2882 (2.216 secs)\n",
      "cnp:cnp_periodic step 43400 lr 3.015e-04 [train_loss] loss -0.2578 (2.173 secs)\n",
      "cnp:cnp_periodic step 43600 lr 2.999e-04 [train_loss] loss -0.2875 (2.185 secs)\n",
      "cnp:cnp_periodic step 43800 lr 2.984e-04 [train_loss] loss -0.3009 (2.359 secs)\n",
      "cnp:cnp_periodic step 44000 lr 2.968e-04 [train_loss] loss -0.3273 (2.300 secs)\n",
      "cnp:cnp_periodic step 44200 lr 2.953e-04 [train_loss] loss -0.2995 (2.250 secs)\n",
      "cnp:cnp_periodic step 44400 lr 2.938e-04 [train_loss] loss -0.2848 (2.359 secs)\n",
      "cnp:cnp_periodic step 44600 lr 2.922e-04 [train_loss] loss -0.3376 (2.388 secs)\n",
      "cnp:cnp_periodic step 44800 lr 2.907e-04 [train_loss] loss -0.2890 (2.380 secs)\n",
      "cnp:cnp_periodic step 45000 lr 2.891e-04 [train_loss] loss -0.2900 (2.183 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 386.64it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.3213 tar_ll -1.2853 (7.764 secs)\n",
      "\n",
      "cnp:cnp_periodic step 45200 lr 2.876e-04 [train_loss] loss -0.2610 (2.447 secs)\n",
      "cnp:cnp_periodic step 45400 lr 2.860e-04 [train_loss] loss -0.3365 (2.219 secs)\n",
      "cnp:cnp_periodic step 45600 lr 2.844e-04 [train_loss] loss -0.3026 (2.335 secs)\n",
      "cnp:cnp_periodic step 45800 lr 2.829e-04 [train_loss] loss -0.2917 (2.252 secs)\n",
      "cnp:cnp_periodic step 46000 lr 2.813e-04 [train_loss] loss -0.3019 (2.357 secs)\n",
      "cnp:cnp_periodic step 46200 lr 2.798e-04 [train_loss] loss -0.2962 (2.225 secs)\n",
      "cnp:cnp_periodic step 46400 lr 2.782e-04 [train_loss] loss -0.3575 (2.326 secs)\n",
      "cnp:cnp_periodic step 46600 lr 2.767e-04 [train_loss] loss -0.3618 (2.455 secs)\n",
      "cnp:cnp_periodic step 46800 lr 2.751e-04 [train_loss] loss -0.2733 (2.292 secs)\n",
      "cnp:cnp_periodic step 47000 lr 2.735e-04 [train_loss] loss -0.3354 (2.275 secs)\n",
      "cnp:cnp_periodic step 47200 lr 2.720e-04 [train_loss] loss -0.2846 (2.309 secs)\n",
      "cnp:cnp_periodic step 47400 lr 2.704e-04 [train_loss] loss -0.2823 (2.323 secs)\n",
      "cnp:cnp_periodic step 47600 lr 2.688e-04 [train_loss] loss -0.2577 (2.281 secs)\n",
      "cnp:cnp_periodic step 47800 lr 2.673e-04 [train_loss] loss -0.2973 (2.331 secs)\n",
      "cnp:cnp_periodic step 48000 lr 2.657e-04 [train_loss] loss -0.3191 (2.504 secs)\n",
      "cnp:cnp_periodic step 48200 lr 2.641e-04 [train_loss] loss -0.3188 (2.284 secs)\n",
      "cnp:cnp_periodic step 48400 lr 2.626e-04 [train_loss] loss -0.3082 (2.304 secs)\n",
      "cnp:cnp_periodic step 48600 lr 2.610e-04 [train_loss] loss -0.2969 (2.309 secs)\n",
      "cnp:cnp_periodic step 48800 lr 2.594e-04 [train_loss] loss -0.3222 (2.220 secs)\n",
      "cnp:cnp_periodic step 49000 lr 2.579e-04 [train_loss] loss -0.3276 (2.370 secs)\n",
      "cnp:cnp_periodic step 49200 lr 2.563e-04 [train_loss] loss -0.3249 (2.320 secs)\n",
      "cnp:cnp_periodic step 49400 lr 2.547e-04 [train_loss] loss -0.3493 (2.409 secs)\n",
      "cnp:cnp_periodic step 49600 lr 2.531e-04 [train_loss] loss -0.3120 (2.253 secs)\n",
      "cnp:cnp_periodic step 49800 lr 2.516e-04 [train_loss] loss -0.3349 (2.290 secs)\n",
      "cnp:cnp_periodic step 50000 lr 2.500e-04 [train_loss] loss -0.3526 (2.392 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 388.09it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.3154 tar_ll -1.3823 (7.732 secs)\n",
      "\n",
      "cnp:cnp_periodic step 50200 lr 2.484e-04 [train_loss] loss -0.3719 (2.309 secs)\n",
      "cnp:cnp_periodic step 50400 lr 2.469e-04 [train_loss] loss -0.3527 (2.215 secs)\n",
      "cnp:cnp_periodic step 50600 lr 2.453e-04 [train_loss] loss -0.3420 (2.228 secs)\n",
      "cnp:cnp_periodic step 50800 lr 2.437e-04 [train_loss] loss -0.3534 (2.312 secs)\n",
      "cnp:cnp_periodic step 51000 lr 2.421e-04 [train_loss] loss -0.3303 (2.386 secs)\n",
      "cnp:cnp_periodic step 51200 lr 2.406e-04 [train_loss] loss -0.3583 (2.271 secs)\n",
      "cnp:cnp_periodic step 51400 lr 2.390e-04 [train_loss] loss -0.3468 (2.398 secs)\n",
      "cnp:cnp_periodic step 51600 lr 2.374e-04 [train_loss] loss -0.3131 (2.288 secs)\n",
      "cnp:cnp_periodic step 51800 lr 2.359e-04 [train_loss] loss -0.3709 (2.344 secs)\n",
      "cnp:cnp_periodic step 52000 lr 2.343e-04 [train_loss] loss -0.3391 (2.250 secs)\n",
      "cnp:cnp_periodic step 52200 lr 2.327e-04 [train_loss] loss -0.2805 (2.312 secs)\n",
      "cnp:cnp_periodic step 52400 lr 2.312e-04 [train_loss] loss -0.3369 (2.226 secs)\n",
      "cnp:cnp_periodic step 52600 lr 2.296e-04 [train_loss] loss -0.3341 (2.216 secs)\n",
      "cnp:cnp_periodic step 52800 lr 2.280e-04 [train_loss] loss -0.3300 (2.384 secs)\n",
      "cnp:cnp_periodic step 53000 lr 2.265e-04 [train_loss] loss -0.3380 (2.295 secs)\n",
      "cnp:cnp_periodic step 53200 lr 2.249e-04 [train_loss] loss -0.3367 (2.354 secs)\n",
      "cnp:cnp_periodic step 53400 lr 2.233e-04 [train_loss] loss -0.3259 (2.263 secs)\n",
      "cnp:cnp_periodic step 53600 lr 2.218e-04 [train_loss] loss -0.3416 (2.304 secs)\n",
      "cnp:cnp_periodic step 53800 lr 2.202e-04 [train_loss] loss -0.3127 (2.181 secs)\n",
      "cnp:cnp_periodic step 54000 lr 2.187e-04 [train_loss] loss -0.3520 (2.363 secs)\n",
      "cnp:cnp_periodic step 54200 lr 2.171e-04 [train_loss] loss -0.3967 (2.309 secs)\n",
      "cnp:cnp_periodic step 54400 lr 2.156e-04 [train_loss] loss -0.3659 (2.271 secs)\n",
      "cnp:cnp_periodic step 54600 lr 2.140e-04 [train_loss] loss -0.3481 (2.187 secs)\n",
      "cnp:cnp_periodic step 54800 lr 2.124e-04 [train_loss] loss -0.3400 (2.202 secs)\n",
      "cnp:cnp_periodic step 55000 lr 2.109e-04 [train_loss] loss -0.3388 (2.192 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 382.38it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.3195 tar_ll -1.4722 (7.848 secs)\n",
      "\n",
      "cnp:cnp_periodic step 55200 lr 2.093e-04 [train_loss] loss -0.3557 (2.235 secs)\n",
      "cnp:cnp_periodic step 55400 lr 2.078e-04 [train_loss] loss -0.3989 (2.256 secs)\n",
      "cnp:cnp_periodic step 55600 lr 2.062e-04 [train_loss] loss -0.3706 (2.356 secs)\n",
      "cnp:cnp_periodic step 55800 lr 2.047e-04 [train_loss] loss -0.3651 (2.346 secs)\n",
      "cnp:cnp_periodic step 56000 lr 2.032e-04 [train_loss] loss -0.3612 (2.269 secs)\n",
      "cnp:cnp_periodic step 56200 lr 2.016e-04 [train_loss] loss -0.3781 (2.347 secs)\n",
      "cnp:cnp_periodic step 56400 lr 2.001e-04 [train_loss] loss -0.3707 (2.431 secs)\n",
      "cnp:cnp_periodic step 56600 lr 1.985e-04 [train_loss] loss -0.4237 (2.268 secs)\n",
      "cnp:cnp_periodic step 56800 lr 1.970e-04 [train_loss] loss -0.3640 (2.377 secs)\n",
      "cnp:cnp_periodic step 57000 lr 1.955e-04 [train_loss] loss -0.3420 (2.201 secs)\n",
      "cnp:cnp_periodic step 57200 lr 1.939e-04 [train_loss] loss -0.3913 (2.333 secs)\n",
      "cnp:cnp_periodic step 57400 lr 1.924e-04 [train_loss] loss -0.3816 (2.275 secs)\n",
      "cnp:cnp_periodic step 57600 lr 1.909e-04 [train_loss] loss -0.3482 (2.370 secs)\n",
      "cnp:cnp_periodic step 57800 lr 1.894e-04 [train_loss] loss -0.3861 (2.236 secs)\n",
      "cnp:cnp_periodic step 58000 lr 1.878e-04 [train_loss] loss -0.3416 (2.235 secs)\n",
      "cnp:cnp_periodic step 58200 lr 1.863e-04 [train_loss] loss -0.4073 (2.250 secs)\n",
      "cnp:cnp_periodic step 58400 lr 1.848e-04 [train_loss] loss -0.3695 (2.284 secs)\n",
      "cnp:cnp_periodic step 58600 lr 1.833e-04 [train_loss] loss -0.3685 (2.352 secs)\n",
      "cnp:cnp_periodic step 58800 lr 1.818e-04 [train_loss] loss -0.3962 (2.189 secs)\n",
      "cnp:cnp_periodic step 59000 lr 1.803e-04 [train_loss] loss -0.3818 (2.347 secs)\n",
      "cnp:cnp_periodic step 59200 lr 1.787e-04 [train_loss] loss -0.3726 (2.215 secs)\n",
      "cnp:cnp_periodic step 59400 lr 1.772e-04 [train_loss] loss -0.3835 (2.360 secs)\n",
      "cnp:cnp_periodic step 59600 lr 1.757e-04 [train_loss] loss -0.3846 (2.158 secs)\n",
      "cnp:cnp_periodic step 59800 lr 1.742e-04 [train_loss] loss -0.3725 (2.360 secs)\n",
      "cnp:cnp_periodic step 60000 lr 1.727e-04 [train_loss] loss -0.4270 (2.386 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 388.05it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.2786 tar_ll -1.1946 (7.735 secs)\n",
      "\n",
      "cnp:cnp_periodic step 60200 lr 1.713e-04 [train_loss] loss -0.3386 (2.305 secs)\n",
      "cnp:cnp_periodic step 60400 lr 1.698e-04 [train_loss] loss -0.4130 (2.240 secs)\n",
      "cnp:cnp_periodic step 60600 lr 1.683e-04 [train_loss] loss -0.3695 (2.255 secs)\n",
      "cnp:cnp_periodic step 60800 lr 1.668e-04 [train_loss] loss -0.3276 (2.458 secs)\n",
      "cnp:cnp_periodic step 61000 lr 1.653e-04 [train_loss] loss -0.4181 (2.295 secs)\n",
      "cnp:cnp_periodic step 61200 lr 1.638e-04 [train_loss] loss -0.4104 (2.390 secs)\n",
      "cnp:cnp_periodic step 61400 lr 1.624e-04 [train_loss] loss -0.3917 (2.269 secs)\n",
      "cnp:cnp_periodic step 61600 lr 1.609e-04 [train_loss] loss -0.3819 (2.202 secs)\n",
      "cnp:cnp_periodic step 61800 lr 1.594e-04 [train_loss] loss -0.3599 (2.309 secs)\n",
      "cnp:cnp_periodic step 62000 lr 1.580e-04 [train_loss] loss -0.4071 (2.225 secs)\n",
      "cnp:cnp_periodic step 62200 lr 1.565e-04 [train_loss] loss -0.3657 (2.459 secs)\n",
      "cnp:cnp_periodic step 62400 lr 1.551e-04 [train_loss] loss -0.3983 (2.495 secs)\n",
      "cnp:cnp_periodic step 62600 lr 1.536e-04 [train_loss] loss -0.3949 (2.295 secs)\n",
      "cnp:cnp_periodic step 62800 lr 1.522e-04 [train_loss] loss -0.3447 (2.208 secs)\n",
      "cnp:cnp_periodic step 63000 lr 1.507e-04 [train_loss] loss -0.4105 (2.377 secs)\n",
      "cnp:cnp_periodic step 63200 lr 1.493e-04 [train_loss] loss -0.4569 (2.364 secs)\n",
      "cnp:cnp_periodic step 63400 lr 1.478e-04 [train_loss] loss -0.4336 (2.365 secs)\n",
      "cnp:cnp_periodic step 63600 lr 1.464e-04 [train_loss] loss -0.4094 (2.174 secs)\n",
      "cnp:cnp_periodic step 63800 lr 1.450e-04 [train_loss] loss -0.3989 (2.454 secs)\n",
      "cnp:cnp_periodic step 64000 lr 1.436e-04 [train_loss] loss -0.3903 (2.344 secs)\n",
      "cnp:cnp_periodic step 64200 lr 1.421e-04 [train_loss] loss -0.4058 (2.324 secs)\n",
      "cnp:cnp_periodic step 64400 lr 1.407e-04 [train_loss] loss -0.3898 (2.368 secs)\n",
      "cnp:cnp_periodic step 64600 lr 1.393e-04 [train_loss] loss -0.3964 (2.266 secs)\n",
      "cnp:cnp_periodic step 64800 lr 1.379e-04 [train_loss] loss -0.3850 (2.287 secs)\n",
      "cnp:cnp_periodic step 65000 lr 1.365e-04 [train_loss] loss -0.4423 (2.308 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 386.75it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.2874 tar_ll -1.4717 (7.760 secs)\n",
      "\n",
      "cnp:cnp_periodic step 65200 lr 1.351e-04 [train_loss] loss -0.3941 (2.257 secs)\n",
      "cnp:cnp_periodic step 65400 lr 1.337e-04 [train_loss] loss -0.3930 (2.215 secs)\n",
      "cnp:cnp_periodic step 65600 lr 1.323e-04 [train_loss] loss -0.4197 (2.328 secs)\n",
      "cnp:cnp_periodic step 65800 lr 1.309e-04 [train_loss] loss -0.3745 (2.462 secs)\n",
      "cnp:cnp_periodic step 66000 lr 1.296e-04 [train_loss] loss -0.3793 (2.267 secs)\n",
      "cnp:cnp_periodic step 66200 lr 1.282e-04 [train_loss] loss -0.3836 (2.332 secs)\n",
      "cnp:cnp_periodic step 66400 lr 1.268e-04 [train_loss] loss -0.4070 (2.267 secs)\n",
      "cnp:cnp_periodic step 66600 lr 1.255e-04 [train_loss] loss -0.4047 (2.422 secs)\n",
      "cnp:cnp_periodic step 66800 lr 1.241e-04 [train_loss] loss -0.4117 (2.299 secs)\n",
      "cnp:cnp_periodic step 67000 lr 1.227e-04 [train_loss] loss -0.4526 (2.516 secs)\n",
      "cnp:cnp_periodic step 67200 lr 1.214e-04 [train_loss] loss -0.4062 (2.311 secs)\n",
      "cnp:cnp_periodic step 67400 lr 1.200e-04 [train_loss] loss -0.3684 (2.314 secs)\n",
      "cnp:cnp_periodic step 67600 lr 1.187e-04 [train_loss] loss -0.4409 (2.266 secs)\n",
      "cnp:cnp_periodic step 67800 lr 1.174e-04 [train_loss] loss -0.4162 (2.377 secs)\n",
      "cnp:cnp_periodic step 68000 lr 1.160e-04 [train_loss] loss -0.4162 (2.184 secs)\n",
      "cnp:cnp_periodic step 68200 lr 1.147e-04 [train_loss] loss -0.4523 (2.246 secs)\n",
      "cnp:cnp_periodic step 68400 lr 1.134e-04 [train_loss] loss -0.4349 (2.459 secs)\n",
      "cnp:cnp_periodic step 68600 lr 1.121e-04 [train_loss] loss -0.3986 (2.328 secs)\n",
      "cnp:cnp_periodic step 68800 lr 1.108e-04 [train_loss] loss -0.4323 (2.436 secs)\n",
      "cnp:cnp_periodic step 69000 lr 1.095e-04 [train_loss] loss -0.4328 (2.341 secs)\n",
      "cnp:cnp_periodic step 69200 lr 1.082e-04 [train_loss] loss -0.4032 (2.297 secs)\n",
      "cnp:cnp_periodic step 69400 lr 1.069e-04 [train_loss] loss -0.4045 (2.289 secs)\n",
      "cnp:cnp_periodic step 69600 lr 1.056e-04 [train_loss] loss -0.3971 (2.307 secs)\n",
      "cnp:cnp_periodic step 69800 lr 1.043e-04 [train_loss] loss -0.4385 (2.506 secs)\n",
      "cnp:cnp_periodic step 70000 lr 1.031e-04 [train_loss] loss -0.4385 (2.447 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 384.29it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.2860 tar_ll -1.4480 (7.810 secs)\n",
      "\n",
      "cnp:cnp_periodic step 70200 lr 1.018e-04 [train_loss] loss -0.3980 (2.207 secs)\n",
      "cnp:cnp_periodic step 70400 lr 1.005e-04 [train_loss] loss -0.4080 (2.235 secs)\n",
      "cnp:cnp_periodic step 70600 lr 9.927e-05 [train_loss] loss -0.4403 (2.299 secs)\n",
      "cnp:cnp_periodic step 70800 lr 9.802e-05 [train_loss] loss -0.4340 (2.233 secs)\n",
      "cnp:cnp_periodic step 71000 lr 9.677e-05 [train_loss] loss -0.4751 (2.331 secs)\n",
      "cnp:cnp_periodic step 71200 lr 9.554e-05 [train_loss] loss -0.4039 (2.507 secs)\n",
      "cnp:cnp_periodic step 71400 lr 9.430e-05 [train_loss] loss -0.4483 (2.213 secs)\n",
      "cnp:cnp_periodic step 71600 lr 9.308e-05 [train_loss] loss -0.4560 (2.368 secs)\n",
      "cnp:cnp_periodic step 71800 lr 9.186e-05 [train_loss] loss -0.4131 (2.274 secs)\n",
      "cnp:cnp_periodic step 72000 lr 9.064e-05 [train_loss] loss -0.4217 (2.262 secs)\n",
      "cnp:cnp_periodic step 72200 lr 8.944e-05 [train_loss] loss -0.4461 (2.308 secs)\n",
      "cnp:cnp_periodic step 72400 lr 8.824e-05 [train_loss] loss -0.4316 (2.310 secs)\n",
      "cnp:cnp_periodic step 72600 lr 8.704e-05 [train_loss] loss -0.4481 (2.260 secs)\n",
      "cnp:cnp_periodic step 72800 lr 8.585e-05 [train_loss] loss -0.3787 (2.367 secs)\n",
      "cnp:cnp_periodic step 73000 lr 8.467e-05 [train_loss] loss -0.4459 (2.343 secs)\n",
      "cnp:cnp_periodic step 73200 lr 8.350e-05 [train_loss] loss -0.4609 (2.320 secs)\n",
      "cnp:cnp_periodic step 73400 lr 8.233e-05 [train_loss] loss -0.4548 (2.301 secs)\n",
      "cnp:cnp_periodic step 73600 lr 8.117e-05 [train_loss] loss -0.4205 (2.284 secs)\n",
      "cnp:cnp_periodic step 73800 lr 8.001e-05 [train_loss] loss -0.4269 (2.279 secs)\n",
      "cnp:cnp_periodic step 74000 lr 7.886e-05 [train_loss] loss -0.4598 (2.223 secs)\n",
      "cnp:cnp_periodic step 74200 lr 7.772e-05 [train_loss] loss -0.4348 (2.285 secs)\n",
      "cnp:cnp_periodic step 74400 lr 7.659e-05 [train_loss] loss -0.4743 (2.281 secs)\n",
      "cnp:cnp_periodic step 74600 lr 7.546e-05 [train_loss] loss -0.4599 (2.198 secs)\n",
      "cnp:cnp_periodic step 74800 lr 7.434e-05 [train_loss] loss -0.4451 (2.206 secs)\n",
      "cnp:cnp_periodic step 75000 lr 7.322e-05 [train_loss] loss -0.4375 (2.350 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 396.89it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.2719 tar_ll -1.4226 (7.563 secs)\n",
      "\n",
      "cnp:cnp_periodic step 75200 lr 7.212e-05 [train_loss] loss -0.4523 (2.365 secs)\n",
      "cnp:cnp_periodic step 75400 lr 7.102e-05 [train_loss] loss -0.4208 (2.177 secs)\n",
      "cnp:cnp_periodic step 75600 lr 6.992e-05 [train_loss] loss -0.4246 (2.410 secs)\n",
      "cnp:cnp_periodic step 75800 lr 6.884e-05 [train_loss] loss -0.4513 (2.204 secs)\n",
      "cnp:cnp_periodic step 76000 lr 6.776e-05 [train_loss] loss -0.4123 (2.269 secs)\n",
      "cnp:cnp_periodic step 76200 lr 6.669e-05 [train_loss] loss -0.4454 (2.330 secs)\n",
      "cnp:cnp_periodic step 76400 lr 6.562e-05 [train_loss] loss -0.4495 (2.243 secs)\n",
      "cnp:cnp_periodic step 76600 lr 6.456e-05 [train_loss] loss -0.4861 (2.307 secs)\n",
      "cnp:cnp_periodic step 76800 lr 6.351e-05 [train_loss] loss -0.4661 (2.285 secs)\n",
      "cnp:cnp_periodic step 77000 lr 6.247e-05 [train_loss] loss -0.4646 (2.364 secs)\n",
      "cnp:cnp_periodic step 77200 lr 6.144e-05 [train_loss] loss -0.4689 (2.296 secs)\n",
      "cnp:cnp_periodic step 77400 lr 6.041e-05 [train_loss] loss -0.4367 (2.328 secs)\n",
      "cnp:cnp_periodic step 77600 lr 5.939e-05 [train_loss] loss -0.4397 (2.183 secs)\n",
      "cnp:cnp_periodic step 77800 lr 5.838e-05 [train_loss] loss -0.4221 (2.279 secs)\n",
      "cnp:cnp_periodic step 78000 lr 5.737e-05 [train_loss] loss -0.4233 (2.215 secs)\n",
      "cnp:cnp_periodic step 78200 lr 5.637e-05 [train_loss] loss -0.4038 (2.225 secs)\n",
      "cnp:cnp_periodic step 78400 lr 5.538e-05 [train_loss] loss -0.4913 (2.267 secs)\n",
      "cnp:cnp_periodic step 78600 lr 5.440e-05 [train_loss] loss -0.4108 (2.260 secs)\n",
      "cnp:cnp_periodic step 78800 lr 5.343e-05 [train_loss] loss -0.4544 (2.188 secs)\n",
      "cnp:cnp_periodic step 79000 lr 5.246e-05 [train_loss] loss -0.4805 (2.219 secs)\n",
      "cnp:cnp_periodic step 79200 lr 5.150e-05 [train_loss] loss -0.4395 (2.212 secs)\n",
      "cnp:cnp_periodic step 79400 lr 5.055e-05 [train_loss] loss -0.4605 (2.293 secs)\n",
      "cnp:cnp_periodic step 79600 lr 4.961e-05 [train_loss] loss -0.4473 (2.365 secs)\n",
      "cnp:cnp_periodic step 79800 lr 4.867e-05 [train_loss] loss -0.4639 (2.187 secs)\n",
      "cnp:cnp_periodic step 80000 lr 4.775e-05 [train_loss] loss -0.4466 (2.319 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 387.29it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.2754 tar_ll -1.4690 (7.749 secs)\n",
      "\n",
      "cnp:cnp_periodic step 80200 lr 4.683e-05 [train_loss] loss -0.3982 (2.340 secs)\n",
      "cnp:cnp_periodic step 80400 lr 4.592e-05 [train_loss] loss -0.4678 (2.335 secs)\n",
      "cnp:cnp_periodic step 80600 lr 4.501e-05 [train_loss] loss -0.4698 (2.251 secs)\n",
      "cnp:cnp_periodic step 80800 lr 4.412e-05 [train_loss] loss -0.4510 (2.255 secs)\n",
      "cnp:cnp_periodic step 81000 lr 4.323e-05 [train_loss] loss -0.4757 (2.384 secs)\n",
      "cnp:cnp_periodic step 81200 lr 4.235e-05 [train_loss] loss -0.4429 (2.286 secs)\n",
      "cnp:cnp_periodic step 81400 lr 4.148e-05 [train_loss] loss -0.4419 (2.348 secs)\n",
      "cnp:cnp_periodic step 81600 lr 4.062e-05 [train_loss] loss -0.4608 (2.234 secs)\n",
      "cnp:cnp_periodic step 81800 lr 3.976e-05 [train_loss] loss -0.4375 (2.402 secs)\n",
      "cnp:cnp_periodic step 82000 lr 3.892e-05 [train_loss] loss -0.4282 (2.300 secs)\n",
      "cnp:cnp_periodic step 82200 lr 3.808e-05 [train_loss] loss -0.4340 (2.456 secs)\n",
      "cnp:cnp_periodic step 82400 lr 3.725e-05 [train_loss] loss -0.4283 (2.335 secs)\n",
      "cnp:cnp_periodic step 82600 lr 3.643e-05 [train_loss] loss -0.4521 (2.320 secs)\n",
      "cnp:cnp_periodic step 82800 lr 3.562e-05 [train_loss] loss -0.4929 (2.472 secs)\n",
      "cnp:cnp_periodic step 83000 lr 3.481e-05 [train_loss] loss -0.4377 (2.335 secs)\n",
      "cnp:cnp_periodic step 83200 lr 3.402e-05 [train_loss] loss -0.4769 (2.307 secs)\n",
      "cnp:cnp_periodic step 83400 lr 3.323e-05 [train_loss] loss -0.4678 (2.229 secs)\n",
      "cnp:cnp_periodic step 83600 lr 3.245e-05 [train_loss] loss -0.4183 (2.321 secs)\n",
      "cnp:cnp_periodic step 83800 lr 3.168e-05 [train_loss] loss -0.4569 (2.384 secs)\n",
      "cnp:cnp_periodic step 84000 lr 3.092e-05 [train_loss] loss -0.4324 (2.401 secs)\n",
      "cnp:cnp_periodic step 84200 lr 3.017e-05 [train_loss] loss -0.4406 (2.479 secs)\n",
      "cnp:cnp_periodic step 84400 lr 2.943e-05 [train_loss] loss -0.4427 (2.313 secs)\n",
      "cnp:cnp_periodic step 84600 lr 2.869e-05 [train_loss] loss -0.4690 (2.300 secs)\n",
      "cnp:cnp_periodic step 84800 lr 2.797e-05 [train_loss] loss -0.4374 (2.244 secs)\n",
      "cnp:cnp_periodic step 85000 lr 2.725e-05 [train_loss] loss -0.4526 (2.327 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 387.81it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.2680 tar_ll -1.4056 (7.739 secs)\n",
      "\n",
      "cnp:cnp_periodic step 85200 lr 2.654e-05 [train_loss] loss -0.4892 (2.232 secs)\n",
      "cnp:cnp_periodic step 85400 lr 2.584e-05 [train_loss] loss -0.4834 (2.327 secs)\n",
      "cnp:cnp_periodic step 85600 lr 2.515e-05 [train_loss] loss -0.4489 (2.255 secs)\n",
      "cnp:cnp_periodic step 85800 lr 2.447e-05 [train_loss] loss -0.5169 (2.269 secs)\n",
      "cnp:cnp_periodic step 86000 lr 2.379e-05 [train_loss] loss -0.4583 (2.235 secs)\n",
      "cnp:cnp_periodic step 86200 lr 2.313e-05 [train_loss] loss -0.4619 (2.416 secs)\n",
      "cnp:cnp_periodic step 86400 lr 2.247e-05 [train_loss] loss -0.4786 (2.279 secs)\n",
      "cnp:cnp_periodic step 86600 lr 2.183e-05 [train_loss] loss -0.4304 (2.262 secs)\n",
      "cnp:cnp_periodic step 86800 lr 2.119e-05 [train_loss] loss -0.5165 (2.231 secs)\n",
      "cnp:cnp_periodic step 87000 lr 2.056e-05 [train_loss] loss -0.4792 (2.281 secs)\n",
      "cnp:cnp_periodic step 87200 lr 1.994e-05 [train_loss] loss -0.4677 (2.390 secs)\n",
      "cnp:cnp_periodic step 87400 lr 1.933e-05 [train_loss] loss -0.4752 (2.306 secs)\n",
      "cnp:cnp_periodic step 87600 lr 1.873e-05 [train_loss] loss -0.5186 (2.387 secs)\n",
      "cnp:cnp_periodic step 87800 lr 1.814e-05 [train_loss] loss -0.4592 (2.230 secs)\n",
      "cnp:cnp_periodic step 88000 lr 1.756e-05 [train_loss] loss -0.4788 (2.343 secs)\n",
      "cnp:cnp_periodic step 88200 lr 1.698e-05 [train_loss] loss -0.4485 (2.212 secs)\n",
      "cnp:cnp_periodic step 88400 lr 1.642e-05 [train_loss] loss -0.4857 (2.297 secs)\n",
      "cnp:cnp_periodic step 88600 lr 1.586e-05 [train_loss] loss -0.4812 (2.305 secs)\n",
      "cnp:cnp_periodic step 88800 lr 1.532e-05 [train_loss] loss -0.4149 (2.272 secs)\n",
      "cnp:cnp_periodic step 89000 lr 1.478e-05 [train_loss] loss -0.4438 (2.370 secs)\n",
      "cnp:cnp_periodic step 89200 lr 1.425e-05 [train_loss] loss -0.4809 (2.200 secs)\n",
      "cnp:cnp_periodic step 89400 lr 1.373e-05 [train_loss] loss -0.4686 (2.294 secs)\n",
      "cnp:cnp_periodic step 89600 lr 1.323e-05 [train_loss] loss -0.4806 (2.206 secs)\n",
      "cnp:cnp_periodic step 89800 lr 1.273e-05 [train_loss] loss -0.4624 (2.316 secs)\n",
      "cnp:cnp_periodic step 90000 lr 1.224e-05 [train_loss] loss -0.4967 (2.283 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 394.79it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.2649 tar_ll -1.4227 (7.603 secs)\n",
      "\n",
      "cnp:cnp_periodic step 90200 lr 1.176e-05 [train_loss] loss -0.4855 (2.192 secs)\n",
      "cnp:cnp_periodic step 90400 lr 1.128e-05 [train_loss] loss -0.4439 (2.246 secs)\n",
      "cnp:cnp_periodic step 90600 lr 1.082e-05 [train_loss] loss -0.4627 (2.180 secs)\n",
      "cnp:cnp_periodic step 90800 lr 1.037e-05 [train_loss] loss -0.4632 (2.321 secs)\n",
      "cnp:cnp_periodic step 91000 lr 9.927e-06 [train_loss] loss -0.4440 (2.332 secs)\n",
      "cnp:cnp_periodic step 91200 lr 9.493e-06 [train_loss] loss -0.4633 (2.285 secs)\n",
      "cnp:cnp_periodic step 91400 lr 9.069e-06 [train_loss] loss -0.4630 (2.246 secs)\n",
      "cnp:cnp_periodic step 91600 lr 8.655e-06 [train_loss] loss -0.4587 (2.289 secs)\n",
      "cnp:cnp_periodic step 91800 lr 8.250e-06 [train_loss] loss -0.4575 (2.263 secs)\n",
      "cnp:cnp_periodic step 92000 lr 7.854e-06 [train_loss] loss -0.4723 (2.279 secs)\n",
      "cnp:cnp_periodic step 92200 lr 7.468e-06 [train_loss] loss -0.4669 (2.335 secs)\n",
      "cnp:cnp_periodic step 92400 lr 7.092e-06 [train_loss] loss -0.4733 (2.314 secs)\n",
      "cnp:cnp_periodic step 92600 lr 6.725e-06 [train_loss] loss -0.5129 (2.280 secs)\n",
      "cnp:cnp_periodic step 92800 lr 6.368e-06 [train_loss] loss -0.4526 (2.326 secs)\n",
      "cnp:cnp_periodic step 93000 lr 6.021e-06 [train_loss] loss -0.5161 (2.315 secs)\n",
      "cnp:cnp_periodic step 93200 lr 5.683e-06 [train_loss] loss -0.4508 (2.186 secs)\n",
      "cnp:cnp_periodic step 93400 lr 5.355e-06 [train_loss] loss -0.4716 (2.267 secs)\n",
      "cnp:cnp_periodic step 93600 lr 5.036e-06 [train_loss] loss -0.4499 (2.318 secs)\n",
      "cnp:cnp_periodic step 93800 lr 4.727e-06 [train_loss] loss -0.4639 (2.307 secs)\n",
      "cnp:cnp_periodic step 94000 lr 4.428e-06 [train_loss] loss -0.4755 (2.235 secs)\n",
      "cnp:cnp_periodic step 94200 lr 4.139e-06 [train_loss] loss -0.5082 (2.335 secs)\n",
      "cnp:cnp_periodic step 94400 lr 3.859e-06 [train_loss] loss -0.4699 (2.376 secs)\n",
      "cnp:cnp_periodic step 94600 lr 3.589e-06 [train_loss] loss -0.4552 (2.252 secs)\n",
      "cnp:cnp_periodic step 94800 lr 3.329e-06 [train_loss] loss -0.4679 (2.221 secs)\n",
      "cnp:cnp_periodic step 95000 lr 3.078e-06 [train_loss] loss -0.4662 (2.217 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 391.03it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.2661 tar_ll -1.4277 (7.675 secs)\n",
      "\n",
      "cnp:cnp_periodic step 95200 lr 2.837e-06 [train_loss] loss -0.4812 (2.322 secs)\n",
      "cnp:cnp_periodic step 95400 lr 2.606e-06 [train_loss] loss -0.4761 (2.229 secs)\n",
      "cnp:cnp_periodic step 95600 lr 2.385e-06 [train_loss] loss -0.4680 (2.356 secs)\n",
      "cnp:cnp_periodic step 95800 lr 2.173e-06 [train_loss] loss -0.4318 (2.334 secs)\n",
      "cnp:cnp_periodic step 96000 lr 1.971e-06 [train_loss] loss -0.4822 (2.161 secs)\n",
      "cnp:cnp_periodic step 96200 lr 1.779e-06 [train_loss] loss -0.4858 (2.181 secs)\n",
      "cnp:cnp_periodic step 96400 lr 1.597e-06 [train_loss] loss -0.4895 (2.244 secs)\n",
      "cnp:cnp_periodic step 96600 lr 1.425e-06 [train_loss] loss -0.4194 (2.240 secs)\n",
      "cnp:cnp_periodic step 96800 lr 1.262e-06 [train_loss] loss -0.4282 (2.316 secs)\n",
      "cnp:cnp_periodic step 97000 lr 1.110e-06 [train_loss] loss -0.5216 (2.240 secs)\n",
      "cnp:cnp_periodic step 97200 lr 9.666e-07 [train_loss] loss -0.4282 (2.420 secs)\n",
      "cnp:cnp_periodic step 97400 lr 8.335e-07 [train_loss] loss -0.4516 (2.217 secs)\n",
      "cnp:cnp_periodic step 97600 lr 7.103e-07 [train_loss] loss -0.5186 (2.186 secs)\n",
      "cnp:cnp_periodic step 97800 lr 5.969e-07 [train_loss] loss -0.4916 (2.235 secs)\n",
      "cnp:cnp_periodic step 98000 lr 4.933e-07 [train_loss] loss -0.4767 (2.296 secs)\n",
      "cnp:cnp_periodic step 98200 lr 3.996e-07 [train_loss] loss -0.4904 (2.322 secs)\n",
      "cnp:cnp_periodic step 98400 lr 3.158e-07 [train_loss] loss -0.4438 (2.203 secs)\n",
      "cnp:cnp_periodic step 98600 lr 2.418e-07 [train_loss] loss -0.4945 (2.355 secs)\n",
      "cnp:cnp_periodic step 98800 lr 1.776e-07 [train_loss] loss -0.4444 (2.325 secs)\n",
      "cnp:cnp_periodic step 99000 lr 1.234e-07 [train_loss] loss -0.4506 (2.447 secs)\n",
      "cnp:cnp_periodic step 99200 lr 7.895e-08 [train_loss] loss -0.4761 (2.270 secs)\n",
      "cnp:cnp_periodic step 99400 lr 4.441e-08 [train_loss] loss -0.4536 (2.314 secs)\n",
      "cnp:cnp_periodic step 99600 lr 1.974e-08 [train_loss] loss -0.4794 (2.173 secs)\n",
      "cnp:cnp_periodic step 99800 lr 4.935e-09 [train_loss] loss -0.4431 (2.255 secs)\n",
      "cnp:cnp_periodic step 100000 lr 0.000e+00 [train_loss] loss -0.4512 (2.351 secs)\n",
      "100%|##########| 3000/3000 [00:07<00:00, 389.45it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.2635 tar_ll -1.4109 (7.706 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:07<00:00, 380.07it/s]\n",
      "cnp:cnp_periodic periodic ctx_ll -0.2635 tar_ll -1.4109 (7.896 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1348956.375 miliseconds\n",
      "Execution time: 1348.956375 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 39.53662109375 MB\n",
      "Memory Usage Change: 23.28662109375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='cnp', name='cnp_periodic',val_seed=100, val_l=None,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75d2d1-0943-443e-b122-2c93201bbaa8",
   "metadata": {},
   "source": [
    "## CANP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47dfe55d-9157-40a6-b734-91a1dc17ff6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: canp-canp_periodic\n",
      "Total number of parameters: 331906\n",
      "\n",
      "canp:canp_periodic step 200 lr 5.000e-04 [train_loss] loss 0.5080 (3.730 secs)\n",
      "canp:canp_periodic step 400 lr 5.000e-04 [train_loss] loss 0.3571 (3.638 secs)\n",
      "canp:canp_periodic step 600 lr 5.000e-04 [train_loss] loss -0.0670 (3.676 secs)\n",
      "canp:canp_periodic step 800 lr 4.999e-04 [train_loss] loss -0.2409 (3.726 secs)\n",
      "canp:canp_periodic step 1000 lr 4.999e-04 [train_loss] loss -0.4116 (3.623 secs)\n",
      "canp:canp_periodic step 1200 lr 4.998e-04 [train_loss] loss -0.3826 (3.645 secs)\n",
      "canp:canp_periodic step 1400 lr 4.998e-04 [train_loss] loss -0.4413 (3.700 secs)\n",
      "canp:canp_periodic step 1600 lr 4.997e-04 [train_loss] loss -0.5477 (3.780 secs)\n",
      "canp:canp_periodic step 1800 lr 4.996e-04 [train_loss] loss -0.3489 (3.680 secs)\n",
      "canp:canp_periodic step 2000 lr 4.995e-04 [train_loss] loss -0.4218 (3.676 secs)\n",
      "canp:canp_periodic step 2200 lr 4.994e-04 [train_loss] loss -0.6230 (3.618 secs)\n",
      "canp:canp_periodic step 2400 lr 4.993e-04 [train_loss] loss -0.7372 (3.540 secs)\n",
      "canp:canp_periodic step 2600 lr 4.992e-04 [train_loss] loss -0.7404 (3.700 secs)\n",
      "canp:canp_periodic step 2800 lr 4.990e-04 [train_loss] loss -0.6908 (3.635 secs)\n",
      "canp:canp_periodic step 3000 lr 4.989e-04 [train_loss] loss -0.8512 (3.500 secs)\n",
      "canp:canp_periodic step 3200 lr 4.987e-04 [train_loss] loss -0.8283 (3.619 secs)\n",
      "canp:canp_periodic step 3400 lr 4.986e-04 [train_loss] loss -0.8041 (3.944 secs)\n",
      "canp:canp_periodic step 3600 lr 4.984e-04 [train_loss] loss -0.7320 (3.660 secs)\n",
      "canp:canp_periodic step 3800 lr 4.982e-04 [train_loss] loss -0.8438 (3.725 secs)\n",
      "canp:canp_periodic step 4000 lr 4.980e-04 [train_loss] loss -0.8309 (3.610 secs)\n",
      "canp:canp_periodic step 4200 lr 4.978e-04 [train_loss] loss -0.8927 (3.966 secs)\n",
      "canp:canp_periodic step 4400 lr 4.976e-04 [train_loss] loss -0.8260 (3.682 secs)\n",
      "canp:canp_periodic step 4600 lr 4.974e-04 [train_loss] loss -0.8057 (3.722 secs)\n",
      "canp:canp_periodic step 4800 lr 4.972e-04 [train_loss] loss -0.9216 (3.610 secs)\n",
      "canp:canp_periodic step 5000 lr 4.969e-04 [train_loss] loss -0.8946 (3.789 secs)\n",
      "100%|##########| 3000/3000 [00:15<00:00, 195.41it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.9102 tar_ll -10.6097 (15.356 secs)\n",
      "\n",
      "canp:canp_periodic step 5200 lr 4.967e-04 [train_loss] loss -0.8846 (3.789 secs)\n",
      "canp:canp_periodic step 5400 lr 4.964e-04 [train_loss] loss -0.8511 (3.544 secs)\n",
      "canp:canp_periodic step 5600 lr 4.961e-04 [train_loss] loss -0.8544 (3.613 secs)\n",
      "canp:canp_periodic step 5800 lr 4.959e-04 [train_loss] loss -0.7084 (3.734 secs)\n",
      "canp:canp_periodic step 6000 lr 4.956e-04 [train_loss] loss -0.8200 (3.861 secs)\n",
      "canp:canp_periodic step 6200 lr 4.953e-04 [train_loss] loss -0.8531 (3.718 secs)\n",
      "canp:canp_periodic step 6400 lr 4.950e-04 [train_loss] loss -0.9349 (3.539 secs)\n",
      "canp:canp_periodic step 6600 lr 4.946e-04 [train_loss] loss -0.7581 (3.602 secs)\n",
      "canp:canp_periodic step 6800 lr 4.943e-04 [train_loss] loss -0.6414 (3.552 secs)\n",
      "canp:canp_periodic step 7000 lr 4.940e-04 [train_loss] loss -0.7649 (3.667 secs)\n",
      "canp:canp_periodic step 7200 lr 4.936e-04 [train_loss] loss -0.9175 (3.595 secs)\n",
      "canp:canp_periodic step 7400 lr 4.933e-04 [train_loss] loss -0.9328 (3.673 secs)\n",
      "canp:canp_periodic step 7600 lr 4.929e-04 [train_loss] loss -0.6550 (3.644 secs)\n",
      "canp:canp_periodic step 7800 lr 4.925e-04 [train_loss] loss -0.8410 (3.646 secs)\n",
      "canp:canp_periodic step 8000 lr 4.921e-04 [train_loss] loss -0.8540 (3.477 secs)\n",
      "canp:canp_periodic step 8200 lr 4.918e-04 [train_loss] loss -0.9097 (3.684 secs)\n",
      "canp:canp_periodic step 8400 lr 4.913e-04 [train_loss] loss -0.9367 (3.570 secs)\n",
      "canp:canp_periodic step 8600 lr 4.909e-04 [train_loss] loss -0.9186 (3.690 secs)\n",
      "canp:canp_periodic step 8800 lr 4.905e-04 [train_loss] loss -0.9550 (3.574 secs)\n",
      "canp:canp_periodic step 9000 lr 4.901e-04 [train_loss] loss -0.8808 (3.675 secs)\n",
      "canp:canp_periodic step 9200 lr 4.896e-04 [train_loss] loss -0.9674 (3.668 secs)\n",
      "canp:canp_periodic step 9400 lr 4.892e-04 [train_loss] loss -0.7733 (3.642 secs)\n",
      "canp:canp_periodic step 9600 lr 4.887e-04 [train_loss] loss -0.9223 (3.642 secs)\n",
      "canp:canp_periodic step 9800 lr 4.882e-04 [train_loss] loss -0.9208 (3.791 secs)\n",
      "canp:canp_periodic step 10000 lr 4.878e-04 [train_loss] loss -0.8805 (3.630 secs)\n",
      "100%|##########| 3000/3000 [00:14<00:00, 201.28it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.8093 tar_ll -8.7877 (14.909 secs)\n",
      "\n",
      "canp:canp_periodic step 10200 lr 4.873e-04 [train_loss] loss -0.9305 (3.730 secs)\n",
      "canp:canp_periodic step 10400 lr 4.868e-04 [train_loss] loss -0.6449 (3.854 secs)\n",
      "canp:canp_periodic step 10600 lr 4.863e-04 [train_loss] loss -0.8177 (3.553 secs)\n",
      "canp:canp_periodic step 10800 lr 4.857e-04 [train_loss] loss -0.8931 (3.664 secs)\n",
      "canp:canp_periodic step 11000 lr 4.852e-04 [train_loss] loss -0.8885 (3.684 secs)\n",
      "canp:canp_periodic step 11200 lr 4.847e-04 [train_loss] loss -0.9386 (3.830 secs)\n",
      "canp:canp_periodic step 11400 lr 4.841e-04 [train_loss] loss -0.8845 (3.689 secs)\n",
      "canp:canp_periodic step 11600 lr 4.836e-04 [train_loss] loss -0.8893 (3.630 secs)\n",
      "canp:canp_periodic step 11800 lr 4.830e-04 [train_loss] loss -0.8363 (3.714 secs)\n",
      "canp:canp_periodic step 12000 lr 4.824e-04 [train_loss] loss -0.9440 (3.758 secs)\n",
      "canp:canp_periodic step 12200 lr 4.819e-04 [train_loss] loss -0.9167 (3.757 secs)\n",
      "canp:canp_periodic step 12400 lr 4.813e-04 [train_loss] loss -0.9845 (3.657 secs)\n",
      "canp:canp_periodic step 12600 lr 4.807e-04 [train_loss] loss -0.9654 (3.548 secs)\n",
      "canp:canp_periodic step 12800 lr 4.801e-04 [train_loss] loss -0.8506 (3.816 secs)\n",
      "canp:canp_periodic step 13000 lr 4.794e-04 [train_loss] loss -0.9739 (3.822 secs)\n",
      "canp:canp_periodic step 13200 lr 4.788e-04 [train_loss] loss -1.0623 (3.657 secs)\n",
      "canp:canp_periodic step 13400 lr 4.782e-04 [train_loss] loss -0.8716 (3.598 secs)\n",
      "canp:canp_periodic step 13600 lr 4.775e-04 [train_loss] loss -0.7969 (3.766 secs)\n",
      "canp:canp_periodic step 13800 lr 4.769e-04 [train_loss] loss -0.9676 (3.869 secs)\n",
      "canp:canp_periodic step 14000 lr 4.762e-04 [train_loss] loss -0.8740 (3.619 secs)\n",
      "canp:canp_periodic step 14200 lr 4.755e-04 [train_loss] loss -0.9324 (3.678 secs)\n",
      "canp:canp_periodic step 14400 lr 4.749e-04 [train_loss] loss -0.9656 (3.691 secs)\n",
      "canp:canp_periodic step 14600 lr 4.742e-04 [train_loss] loss -0.9242 (3.881 secs)\n",
      "canp:canp_periodic step 14800 lr 4.735e-04 [train_loss] loss -1.0006 (3.600 secs)\n",
      "canp:canp_periodic step 15000 lr 4.728e-04 [train_loss] loss -0.9842 (3.767 secs)\n",
      "100%|##########| 3000/3000 [00:14<00:00, 204.65it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.6934 tar_ll -10.3849 (14.661 secs)\n",
      "\n",
      "canp:canp_periodic step 15200 lr 4.720e-04 [train_loss] loss -0.9674 (3.667 secs)\n",
      "canp:canp_periodic step 15400 lr 4.713e-04 [train_loss] loss -0.9882 (3.828 secs)\n",
      "canp:canp_periodic step 15600 lr 4.706e-04 [train_loss] loss -0.9110 (3.673 secs)\n",
      "canp:canp_periodic step 15800 lr 4.698e-04 [train_loss] loss -1.0212 (3.576 secs)\n",
      "canp:canp_periodic step 16000 lr 4.691e-04 [train_loss] loss -0.9629 (3.555 secs)\n",
      "canp:canp_periodic step 16200 lr 4.683e-04 [train_loss] loss -0.8961 (3.680 secs)\n",
      "canp:canp_periodic step 16400 lr 4.675e-04 [train_loss] loss -0.9880 (3.742 secs)\n",
      "canp:canp_periodic step 16600 lr 4.668e-04 [train_loss] loss -0.9608 (3.625 secs)\n",
      "canp:canp_periodic step 16800 lr 4.660e-04 [train_loss] loss -0.9704 (3.599 secs)\n",
      "canp:canp_periodic step 17000 lr 4.652e-04 [train_loss] loss -0.8370 (3.507 secs)\n",
      "canp:canp_periodic step 17200 lr 4.644e-04 [train_loss] loss -0.8493 (3.673 secs)\n",
      "canp:canp_periodic step 17400 lr 4.636e-04 [train_loss] loss -0.8795 (3.720 secs)\n",
      "canp:canp_periodic step 17600 lr 4.627e-04 [train_loss] loss -0.9251 (3.698 secs)\n",
      "canp:canp_periodic step 17800 lr 4.619e-04 [train_loss] loss -0.9512 (3.687 secs)\n",
      "canp:canp_periodic step 18000 lr 4.611e-04 [train_loss] loss -0.8590 (3.719 secs)\n",
      "canp:canp_periodic step 18200 lr 4.602e-04 [train_loss] loss -0.9749 (3.756 secs)\n",
      "canp:canp_periodic step 18400 lr 4.594e-04 [train_loss] loss -0.9120 (3.609 secs)\n",
      "canp:canp_periodic step 18600 lr 4.585e-04 [train_loss] loss -0.9401 (3.652 secs)\n",
      "canp:canp_periodic step 18800 lr 4.576e-04 [train_loss] loss -0.9561 (3.717 secs)\n",
      "canp:canp_periodic step 19000 lr 4.568e-04 [train_loss] loss -0.8973 (3.742 secs)\n",
      "canp:canp_periodic step 19200 lr 4.559e-04 [train_loss] loss -0.8904 (3.610 secs)\n",
      "canp:canp_periodic step 19400 lr 4.550e-04 [train_loss] loss -0.9239 (3.557 secs)\n",
      "canp:canp_periodic step 19600 lr 4.541e-04 [train_loss] loss -0.9925 (3.735 secs)\n",
      "canp:canp_periodic step 19800 lr 4.532e-04 [train_loss] loss -0.9359 (3.699 secs)\n",
      "canp:canp_periodic step 20000 lr 4.523e-04 [train_loss] loss -0.9570 (3.518 secs)\n",
      "100%|##########| 3000/3000 [00:14<00:00, 204.65it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.5457 tar_ll -10.3213 (14.661 secs)\n",
      "\n",
      "canp:canp_periodic step 20200 lr 4.513e-04 [train_loss] loss -0.9792 (3.807 secs)\n",
      "canp:canp_periodic step 20400 lr 4.504e-04 [train_loss] loss -0.9743 (3.685 secs)\n",
      "canp:canp_periodic step 20600 lr 4.494e-04 [train_loss] loss -0.9292 (3.681 secs)\n",
      "canp:canp_periodic step 20800 lr 4.485e-04 [train_loss] loss -1.0412 (3.775 secs)\n",
      "canp:canp_periodic step 21000 lr 4.475e-04 [train_loss] loss -0.9087 (3.824 secs)\n",
      "canp:canp_periodic step 21200 lr 4.466e-04 [train_loss] loss -0.7004 (3.774 secs)\n",
      "canp:canp_periodic step 21400 lr 4.456e-04 [train_loss] loss -0.8142 (3.651 secs)\n",
      "canp:canp_periodic step 21600 lr 4.446e-04 [train_loss] loss -0.6218 (3.786 secs)\n",
      "canp:canp_periodic step 21800 lr 4.436e-04 [train_loss] loss -0.8329 (3.729 secs)\n",
      "canp:canp_periodic step 22000 lr 4.426e-04 [train_loss] loss -0.8965 (3.766 secs)\n",
      "canp:canp_periodic step 22200 lr 4.416e-04 [train_loss] loss -0.8719 (3.782 secs)\n",
      "canp:canp_periodic step 22400 lr 4.406e-04 [train_loss] loss -0.9502 (3.833 secs)\n",
      "canp:canp_periodic step 22600 lr 4.396e-04 [train_loss] loss -0.9812 (3.672 secs)\n",
      "canp:canp_periodic step 22800 lr 4.386e-04 [train_loss] loss -1.0172 (3.775 secs)\n",
      "canp:canp_periodic step 23000 lr 4.375e-04 [train_loss] loss -0.9074 (3.659 secs)\n",
      "canp:canp_periodic step 23200 lr 4.365e-04 [train_loss] loss -0.9341 (3.900 secs)\n",
      "canp:canp_periodic step 23400 lr 4.354e-04 [train_loss] loss -0.9729 (3.687 secs)\n",
      "canp:canp_periodic step 23600 lr 4.344e-04 [train_loss] loss -0.9721 (3.713 secs)\n",
      "canp:canp_periodic step 23800 lr 4.333e-04 [train_loss] loss -0.9705 (3.597 secs)\n",
      "canp:canp_periodic step 24000 lr 4.322e-04 [train_loss] loss -0.9737 (3.649 secs)\n",
      "canp:canp_periodic step 24200 lr 4.312e-04 [train_loss] loss -0.9629 (3.831 secs)\n",
      "canp:canp_periodic step 24400 lr 4.301e-04 [train_loss] loss -0.9752 (3.579 secs)\n",
      "canp:canp_periodic step 24600 lr 4.290e-04 [train_loss] loss -0.9670 (3.611 secs)\n",
      "canp:canp_periodic step 24800 lr 4.279e-04 [train_loss] loss -1.0015 (3.733 secs)\n",
      "canp:canp_periodic step 25000 lr 4.268e-04 [train_loss] loss -1.0169 (3.831 secs)\n",
      "100%|##########| 3000/3000 [00:15<00:00, 197.30it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.3398 tar_ll -9.7789 (15.209 secs)\n",
      "\n",
      "canp:canp_periodic step 25200 lr 4.257e-04 [train_loss] loss -0.9824 (3.729 secs)\n",
      "canp:canp_periodic step 25400 lr 4.245e-04 [train_loss] loss -0.9786 (3.797 secs)\n",
      "canp:canp_periodic step 25600 lr 4.234e-04 [train_loss] loss -0.9636 (3.614 secs)\n",
      "canp:canp_periodic step 25800 lr 4.223e-04 [train_loss] loss -0.9962 (4.025 secs)\n",
      "canp:canp_periodic step 26000 lr 4.211e-04 [train_loss] loss -1.0136 (3.658 secs)\n",
      "canp:canp_periodic step 26200 lr 4.200e-04 [train_loss] loss -1.0163 (3.562 secs)\n",
      "canp:canp_periodic step 26400 lr 4.188e-04 [train_loss] loss -0.9671 (3.613 secs)\n",
      "canp:canp_periodic step 26600 lr 4.177e-04 [train_loss] loss -1.0070 (3.663 secs)\n",
      "canp:canp_periodic step 26800 lr 4.165e-04 [train_loss] loss -1.0055 (3.645 secs)\n",
      "canp:canp_periodic step 27000 lr 4.153e-04 [train_loss] loss -0.9436 (3.523 secs)\n",
      "canp:canp_periodic step 27200 lr 4.141e-04 [train_loss] loss -0.9864 (3.533 secs)\n",
      "canp:canp_periodic step 27400 lr 4.130e-04 [train_loss] loss -0.9441 (3.519 secs)\n",
      "canp:canp_periodic step 27600 lr 4.118e-04 [train_loss] loss -0.9248 (3.748 secs)\n",
      "canp:canp_periodic step 27800 lr 4.106e-04 [train_loss] loss -0.9761 (3.586 secs)\n",
      "canp:canp_periodic step 28000 lr 4.094e-04 [train_loss] loss -0.9506 (3.710 secs)\n",
      "canp:canp_periodic step 28200 lr 4.081e-04 [train_loss] loss -0.9766 (3.581 secs)\n",
      "canp:canp_periodic step 28400 lr 4.069e-04 [train_loss] loss -0.9727 (3.679 secs)\n",
      "canp:canp_periodic step 28600 lr 4.057e-04 [train_loss] loss -0.9803 (3.663 secs)\n",
      "canp:canp_periodic step 28800 lr 4.045e-04 [train_loss] loss -0.9478 (3.560 secs)\n",
      "canp:canp_periodic step 29000 lr 4.032e-04 [train_loss] loss -0.9495 (3.512 secs)\n",
      "canp:canp_periodic step 29200 lr 4.020e-04 [train_loss] loss -0.9774 (3.567 secs)\n",
      "canp:canp_periodic step 29400 lr 4.007e-04 [train_loss] loss -0.9941 (3.705 secs)\n",
      "canp:canp_periodic step 29600 lr 3.995e-04 [train_loss] loss -0.9484 (3.516 secs)\n",
      "canp:canp_periodic step 29800 lr 3.982e-04 [train_loss] loss -0.9455 (3.542 secs)\n",
      "canp:canp_periodic step 30000 lr 3.969e-04 [train_loss] loss -0.8874 (3.692 secs)\n",
      "100%|##########| 3000/3000 [00:14<00:00, 208.06it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.7875 tar_ll -9.8358 (14.423 secs)\n",
      "\n",
      "canp:canp_periodic step 30200 lr 3.957e-04 [train_loss] loss -0.9580 (3.651 secs)\n",
      "canp:canp_periodic step 30400 lr 3.944e-04 [train_loss] loss -1.0208 (3.624 secs)\n",
      "canp:canp_periodic step 30600 lr 3.931e-04 [train_loss] loss -1.0425 (3.546 secs)\n",
      "canp:canp_periodic step 30800 lr 3.918e-04 [train_loss] loss -1.0061 (3.620 secs)\n",
      "canp:canp_periodic step 31000 lr 3.905e-04 [train_loss] loss -0.9437 (3.650 secs)\n",
      "canp:canp_periodic step 31200 lr 3.892e-04 [train_loss] loss -0.8753 (3.788 secs)\n",
      "canp:canp_periodic step 31400 lr 3.879e-04 [train_loss] loss -0.9518 (3.623 secs)\n",
      "canp:canp_periodic step 31600 lr 3.866e-04 [train_loss] loss -0.9964 (3.794 secs)\n",
      "canp:canp_periodic step 31800 lr 3.853e-04 [train_loss] loss -0.9462 (3.614 secs)\n",
      "canp:canp_periodic step 32000 lr 3.840e-04 [train_loss] loss -0.9906 (3.720 secs)\n",
      "canp:canp_periodic step 32200 lr 3.826e-04 [train_loss] loss -1.0199 (3.676 secs)\n",
      "canp:canp_periodic step 32400 lr 3.813e-04 [train_loss] loss -0.9499 (3.714 secs)\n",
      "canp:canp_periodic step 32600 lr 3.800e-04 [train_loss] loss -0.9950 (3.881 secs)\n",
      "canp:canp_periodic step 32800 lr 3.786e-04 [train_loss] loss -1.0035 (3.732 secs)\n",
      "canp:canp_periodic step 33000 lr 3.773e-04 [train_loss] loss -1.0476 (3.688 secs)\n",
      "canp:canp_periodic step 33200 lr 3.759e-04 [train_loss] loss -0.9072 (3.703 secs)\n",
      "canp:canp_periodic step 33400 lr 3.745e-04 [train_loss] loss -0.9894 (3.632 secs)\n",
      "canp:canp_periodic step 33600 lr 3.732e-04 [train_loss] loss -0.9748 (3.597 secs)\n",
      "canp:canp_periodic step 33800 lr 3.718e-04 [train_loss] loss -1.0258 (3.643 secs)\n",
      "canp:canp_periodic step 34000 lr 3.704e-04 [train_loss] loss -0.9516 (3.751 secs)\n",
      "canp:canp_periodic step 34200 lr 3.691e-04 [train_loss] loss -0.9743 (3.590 secs)\n",
      "canp:canp_periodic step 34400 lr 3.677e-04 [train_loss] loss -0.9989 (3.608 secs)\n",
      "canp:canp_periodic step 34600 lr 3.663e-04 [train_loss] loss -0.9918 (3.869 secs)\n",
      "canp:canp_periodic step 34800 lr 3.649e-04 [train_loss] loss -0.9747 (3.574 secs)\n",
      "canp:canp_periodic step 35000 lr 3.635e-04 [train_loss] loss -1.0168 (3.602 secs)\n",
      "100%|##########| 3000/3000 [00:14<00:00, 206.92it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.3497 tar_ll -9.0389 (14.502 secs)\n",
      "\n",
      "canp:canp_periodic step 35200 lr 3.621e-04 [train_loss] loss -0.9196 (3.849 secs)\n",
      "canp:canp_periodic step 35400 lr 3.607e-04 [train_loss] loss -1.0169 (3.766 secs)\n",
      "canp:canp_periodic step 35600 lr 3.593e-04 [train_loss] loss -1.0099 (3.728 secs)\n",
      "canp:canp_periodic step 35800 lr 3.579e-04 [train_loss] loss -0.9578 (3.566 secs)\n",
      "canp:canp_periodic step 36000 lr 3.564e-04 [train_loss] loss -1.0094 (3.677 secs)\n",
      "canp:canp_periodic step 36200 lr 3.550e-04 [train_loss] loss -1.0249 (3.853 secs)\n",
      "canp:canp_periodic step 36400 lr 3.536e-04 [train_loss] loss -1.0035 (3.742 secs)\n",
      "canp:canp_periodic step 36600 lr 3.522e-04 [train_loss] loss -0.9717 (3.600 secs)\n",
      "canp:canp_periodic step 36800 lr 3.507e-04 [train_loss] loss -0.8897 (3.597 secs)\n",
      "canp:canp_periodic step 37000 lr 3.493e-04 [train_loss] loss -0.9531 (3.670 secs)\n",
      "canp:canp_periodic step 37200 lr 3.478e-04 [train_loss] loss -1.0037 (3.676 secs)\n",
      "canp:canp_periodic step 37400 lr 3.464e-04 [train_loss] loss -1.0476 (3.611 secs)\n",
      "canp:canp_periodic step 37600 lr 3.449e-04 [train_loss] loss -0.9038 (3.648 secs)\n",
      "canp:canp_periodic step 37800 lr 3.435e-04 [train_loss] loss -0.9671 (3.571 secs)\n",
      "canp:canp_periodic step 38000 lr 3.420e-04 [train_loss] loss -0.9619 (3.766 secs)\n",
      "canp:canp_periodic step 38200 lr 3.406e-04 [train_loss] loss -0.9717 (3.801 secs)\n",
      "canp:canp_periodic step 38400 lr 3.391e-04 [train_loss] loss -1.0226 (3.732 secs)\n",
      "canp:canp_periodic step 38600 lr 3.376e-04 [train_loss] loss -0.9955 (3.626 secs)\n",
      "canp:canp_periodic step 38800 lr 3.362e-04 [train_loss] loss -0.9969 (3.620 secs)\n",
      "canp:canp_periodic step 39000 lr 3.347e-04 [train_loss] loss -1.0039 (3.700 secs)\n",
      "canp:canp_periodic step 39200 lr 3.332e-04 [train_loss] loss -0.9669 (3.773 secs)\n",
      "canp:canp_periodic step 39400 lr 3.317e-04 [train_loss] loss -0.9897 (3.557 secs)\n",
      "canp:canp_periodic step 39600 lr 3.302e-04 [train_loss] loss -1.0200 (3.511 secs)\n",
      "canp:canp_periodic step 39800 lr 3.287e-04 [train_loss] loss -0.9908 (3.684 secs)\n",
      "canp:canp_periodic step 40000 lr 3.273e-04 [train_loss] loss -0.9559 (3.651 secs)\n",
      "100%|##########| 3000/3000 [00:14<00:00, 203.32it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.3326 tar_ll -9.2894 (14.758 secs)\n",
      "\n",
      "canp:canp_periodic step 40200 lr 3.258e-04 [train_loss] loss -1.0118 (3.767 secs)\n",
      "canp:canp_periodic step 40400 lr 3.243e-04 [train_loss] loss -0.9591 (3.720 secs)\n",
      "canp:canp_periodic step 40600 lr 3.228e-04 [train_loss] loss -0.9769 (3.887 secs)\n",
      "canp:canp_periodic step 40800 lr 3.213e-04 [train_loss] loss -0.9284 (3.610 secs)\n",
      "canp:canp_periodic step 41000 lr 3.197e-04 [train_loss] loss -0.9843 (3.878 secs)\n",
      "canp:canp_periodic step 41200 lr 3.182e-04 [train_loss] loss -1.0464 (3.786 secs)\n",
      "canp:canp_periodic step 41400 lr 3.167e-04 [train_loss] loss -1.0575 (3.696 secs)\n",
      "canp:canp_periodic step 41600 lr 3.152e-04 [train_loss] loss -1.0132 (3.721 secs)\n",
      "canp:canp_periodic step 41800 lr 3.137e-04 [train_loss] loss -1.0021 (3.653 secs)\n",
      "canp:canp_periodic step 42000 lr 3.122e-04 [train_loss] loss -0.9657 (3.619 secs)\n",
      "canp:canp_periodic step 42200 lr 3.106e-04 [train_loss] loss -1.0446 (3.736 secs)\n",
      "canp:canp_periodic step 42400 lr 3.091e-04 [train_loss] loss -1.0428 (3.816 secs)\n",
      "canp:canp_periodic step 42600 lr 3.076e-04 [train_loss] loss -1.0134 (3.617 secs)\n",
      "canp:canp_periodic step 42800 lr 3.061e-04 [train_loss] loss -1.0268 (3.667 secs)\n",
      "canp:canp_periodic step 43000 lr 3.045e-04 [train_loss] loss -0.9965 (3.683 secs)\n",
      "canp:canp_periodic step 43200 lr 3.030e-04 [train_loss] loss -0.9907 (4.015 secs)\n",
      "canp:canp_periodic step 43400 lr 3.015e-04 [train_loss] loss -0.9795 (3.746 secs)\n",
      "canp:canp_periodic step 43600 lr 2.999e-04 [train_loss] loss -1.0143 (3.662 secs)\n",
      "canp:canp_periodic step 43800 lr 2.984e-04 [train_loss] loss -0.9732 (3.800 secs)\n",
      "canp:canp_periodic step 44000 lr 2.968e-04 [train_loss] loss -1.0294 (3.836 secs)\n",
      "canp:canp_periodic step 44200 lr 2.953e-04 [train_loss] loss -0.9554 (3.761 secs)\n",
      "canp:canp_periodic step 44400 lr 2.938e-04 [train_loss] loss -1.0076 (3.670 secs)\n",
      "canp:canp_periodic step 44600 lr 2.922e-04 [train_loss] loss -1.0110 (3.753 secs)\n",
      "canp:canp_periodic step 44800 lr 2.907e-04 [train_loss] loss -1.0661 (3.637 secs)\n",
      "canp:canp_periodic step 45000 lr 2.891e-04 [train_loss] loss -1.0358 (3.767 secs)\n",
      "100%|##########| 3000/3000 [00:15<00:00, 194.88it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.0887 tar_ll -8.4800 (15.397 secs)\n",
      "\n",
      "canp:canp_periodic step 45200 lr 2.876e-04 [train_loss] loss -0.9962 (3.832 secs)\n",
      "canp:canp_periodic step 45400 lr 2.860e-04 [train_loss] loss -0.9749 (4.043 secs)\n",
      "canp:canp_periodic step 45600 lr 2.844e-04 [train_loss] loss -0.9583 (3.913 secs)\n",
      "canp:canp_periodic step 45800 lr 2.829e-04 [train_loss] loss -0.9916 (3.708 secs)\n",
      "canp:canp_periodic step 46000 lr 2.813e-04 [train_loss] loss -1.0771 (3.811 secs)\n",
      "canp:canp_periodic step 46200 lr 2.798e-04 [train_loss] loss -1.0065 (3.637 secs)\n",
      "canp:canp_periodic step 46400 lr 2.782e-04 [train_loss] loss -1.0160 (3.751 secs)\n",
      "canp:canp_periodic step 46600 lr 2.767e-04 [train_loss] loss -1.0017 (3.868 secs)\n",
      "canp:canp_periodic step 46800 lr 2.751e-04 [train_loss] loss -1.0030 (3.816 secs)\n",
      "canp:canp_periodic step 47000 lr 2.735e-04 [train_loss] loss -0.9966 (3.792 secs)\n",
      "canp:canp_periodic step 47200 lr 2.720e-04 [train_loss] loss -0.9279 (3.778 secs)\n",
      "canp:canp_periodic step 47400 lr 2.704e-04 [train_loss] loss -1.0340 (3.801 secs)\n",
      "canp:canp_periodic step 47600 lr 2.688e-04 [train_loss] loss -1.0117 (3.495 secs)\n",
      "canp:canp_periodic step 47800 lr 2.673e-04 [train_loss] loss -1.0583 (3.641 secs)\n",
      "canp:canp_periodic step 48000 lr 2.657e-04 [train_loss] loss -1.0164 (3.646 secs)\n",
      "canp:canp_periodic step 48200 lr 2.641e-04 [train_loss] loss -1.0588 (3.681 secs)\n",
      "canp:canp_periodic step 48400 lr 2.626e-04 [train_loss] loss -1.0253 (3.599 secs)\n",
      "canp:canp_periodic step 48600 lr 2.610e-04 [train_loss] loss -1.0070 (3.740 secs)\n",
      "canp:canp_periodic step 48800 lr 2.594e-04 [train_loss] loss -1.0523 (3.604 secs)\n",
      "canp:canp_periodic step 49000 lr 2.579e-04 [train_loss] loss -0.9851 (3.581 secs)\n",
      "canp:canp_periodic step 49200 lr 2.563e-04 [train_loss] loss -1.0312 (3.806 secs)\n",
      "canp:canp_periodic step 49400 lr 2.547e-04 [train_loss] loss -1.0196 (3.605 secs)\n",
      "canp:canp_periodic step 49600 lr 2.531e-04 [train_loss] loss -1.0135 (3.536 secs)\n",
      "canp:canp_periodic step 49800 lr 2.516e-04 [train_loss] loss -1.0645 (3.515 secs)\n",
      "canp:canp_periodic step 50000 lr 2.500e-04 [train_loss] loss -1.0092 (3.827 secs)\n",
      "100%|##########| 3000/3000 [00:14<00:00, 207.72it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.0873 tar_ll -8.8186 (14.446 secs)\n",
      "\n",
      "canp:canp_periodic step 50200 lr 2.484e-04 [train_loss] loss -1.0594 (3.597 secs)\n",
      "canp:canp_periodic step 50400 lr 2.469e-04 [train_loss] loss -0.9848 (3.467 secs)\n",
      "canp:canp_periodic step 50600 lr 2.453e-04 [train_loss] loss -1.0017 (3.521 secs)\n",
      "canp:canp_periodic step 50800 lr 2.437e-04 [train_loss] loss -0.9975 (3.434 secs)\n",
      "canp:canp_periodic step 51000 lr 2.421e-04 [train_loss] loss -0.9846 (3.639 secs)\n",
      "canp:canp_periodic step 51200 lr 2.406e-04 [train_loss] loss -1.0409 (3.526 secs)\n",
      "canp:canp_periodic step 51400 lr 2.390e-04 [train_loss] loss -1.0291 (3.612 secs)\n",
      "canp:canp_periodic step 51600 lr 2.374e-04 [train_loss] loss -1.0517 (3.700 secs)\n",
      "canp:canp_periodic step 51800 lr 2.359e-04 [train_loss] loss -0.9793 (3.694 secs)\n",
      "canp:canp_periodic step 52000 lr 2.343e-04 [train_loss] loss -1.0158 (3.565 secs)\n",
      "canp:canp_periodic step 52200 lr 2.327e-04 [train_loss] loss -1.0300 (3.580 secs)\n",
      "canp:canp_periodic step 52400 lr 2.312e-04 [train_loss] loss -1.0257 (3.500 secs)\n",
      "canp:canp_periodic step 52600 lr 2.296e-04 [train_loss] loss -1.0395 (3.516 secs)\n",
      "canp:canp_periodic step 52800 lr 2.280e-04 [train_loss] loss -1.0488 (3.577 secs)\n",
      "canp:canp_periodic step 53000 lr 2.265e-04 [train_loss] loss -1.0069 (3.448 secs)\n",
      "canp:canp_periodic step 53200 lr 2.249e-04 [train_loss] loss -1.0154 (3.432 secs)\n",
      "canp:canp_periodic step 53400 lr 2.233e-04 [train_loss] loss -1.0288 (3.618 secs)\n",
      "canp:canp_periodic step 53600 lr 2.218e-04 [train_loss] loss -0.9873 (3.745 secs)\n",
      "canp:canp_periodic step 53800 lr 2.202e-04 [train_loss] loss -1.0475 (3.425 secs)\n",
      "canp:canp_periodic step 54000 lr 2.187e-04 [train_loss] loss -1.0288 (3.659 secs)\n",
      "canp:canp_periodic step 54200 lr 2.171e-04 [train_loss] loss -1.0187 (3.721 secs)\n",
      "canp:canp_periodic step 54400 lr 2.156e-04 [train_loss] loss -1.0238 (3.522 secs)\n",
      "canp:canp_periodic step 54600 lr 2.140e-04 [train_loss] loss -1.0599 (3.832 secs)\n",
      "canp:canp_periodic step 54800 lr 2.124e-04 [train_loss] loss -1.0304 (3.727 secs)\n",
      "canp:canp_periodic step 55000 lr 2.109e-04 [train_loss] loss -0.9973 (3.714 secs)\n",
      "100%|##########| 3000/3000 [00:14<00:00, 206.18it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.1476 tar_ll -9.3061 (14.552 secs)\n",
      "\n",
      "canp:canp_periodic step 55200 lr 2.093e-04 [train_loss] loss -1.0272 (3.673 secs)\n",
      "canp:canp_periodic step 55400 lr 2.078e-04 [train_loss] loss -1.0649 (3.887 secs)\n",
      "canp:canp_periodic step 55600 lr 2.062e-04 [train_loss] loss -1.0739 (3.896 secs)\n",
      "canp:canp_periodic step 55800 lr 2.047e-04 [train_loss] loss -0.9962 (3.728 secs)\n",
      "canp:canp_periodic step 56000 lr 2.032e-04 [train_loss] loss -1.0303 (3.545 secs)\n",
      "canp:canp_periodic step 56200 lr 2.016e-04 [train_loss] loss -1.0582 (3.829 secs)\n",
      "canp:canp_periodic step 56400 lr 2.001e-04 [train_loss] loss -1.0347 (3.466 secs)\n",
      "canp:canp_periodic step 56600 lr 1.985e-04 [train_loss] loss -1.0272 (3.581 secs)\n",
      "canp:canp_periodic step 56800 lr 1.970e-04 [train_loss] loss -1.0263 (3.597 secs)\n",
      "canp:canp_periodic step 57000 lr 1.955e-04 [train_loss] loss -1.0685 (3.632 secs)\n",
      "canp:canp_periodic step 57200 lr 1.939e-04 [train_loss] loss -1.0072 (3.667 secs)\n",
      "canp:canp_periodic step 57400 lr 1.924e-04 [train_loss] loss -0.9447 (3.620 secs)\n",
      "canp:canp_periodic step 57600 lr 1.909e-04 [train_loss] loss -0.9579 (3.701 secs)\n",
      "canp:canp_periodic step 57800 lr 1.894e-04 [train_loss] loss -1.0733 (3.513 secs)\n",
      "canp:canp_periodic step 58000 lr 1.878e-04 [train_loss] loss -1.0041 (3.717 secs)\n",
      "canp:canp_periodic step 58200 lr 1.863e-04 [train_loss] loss -1.0023 (3.791 secs)\n",
      "canp:canp_periodic step 58400 lr 1.848e-04 [train_loss] loss -1.0109 (3.537 secs)\n",
      "canp:canp_periodic step 58600 lr 1.833e-04 [train_loss] loss -1.0220 (3.891 secs)\n",
      "canp:canp_periodic step 58800 lr 1.818e-04 [train_loss] loss -1.0095 (3.616 secs)\n",
      "canp:canp_periodic step 59000 lr 1.803e-04 [train_loss] loss -1.0242 (3.638 secs)\n",
      "canp:canp_periodic step 59200 lr 1.787e-04 [train_loss] loss -1.0668 (3.717 secs)\n",
      "canp:canp_periodic step 59400 lr 1.772e-04 [train_loss] loss -1.0426 (4.004 secs)\n",
      "canp:canp_periodic step 59600 lr 1.757e-04 [train_loss] loss -0.9777 (3.633 secs)\n",
      "canp:canp_periodic step 59800 lr 1.742e-04 [train_loss] loss -0.9564 (3.639 secs)\n",
      "canp:canp_periodic step 60000 lr 1.727e-04 [train_loss] loss -1.0360 (3.615 secs)\n",
      "100%|##########| 3000/3000 [00:14<00:00, 209.68it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.4299 tar_ll -8.1046 (14.311 secs)\n",
      "\n",
      "canp:canp_periodic step 60200 lr 1.713e-04 [train_loss] loss -1.0098 (3.614 secs)\n",
      "canp:canp_periodic step 60400 lr 1.698e-04 [train_loss] loss -1.0037 (3.530 secs)\n",
      "canp:canp_periodic step 60600 lr 1.683e-04 [train_loss] loss -0.9691 (3.577 secs)\n",
      "canp:canp_periodic step 60800 lr 1.668e-04 [train_loss] loss -1.0434 (3.223 secs)\n",
      "canp:canp_periodic step 61000 lr 1.653e-04 [train_loss] loss -1.0148 (3.237 secs)\n",
      "canp:canp_periodic step 61200 lr 1.638e-04 [train_loss] loss -1.0453 (3.198 secs)\n",
      "canp:canp_periodic step 61400 lr 1.624e-04 [train_loss] loss -1.0817 (3.284 secs)\n",
      "canp:canp_periodic step 61600 lr 1.609e-04 [train_loss] loss -1.0678 (3.408 secs)\n",
      "canp:canp_periodic step 61800 lr 1.594e-04 [train_loss] loss -0.9957 (3.772 secs)\n",
      "canp:canp_periodic step 62000 lr 1.580e-04 [train_loss] loss -0.9744 (4.100 secs)\n",
      "canp:canp_periodic step 62200 lr 1.565e-04 [train_loss] loss -0.9689 (4.075 secs)\n",
      "canp:canp_periodic step 62400 lr 1.551e-04 [train_loss] loss -1.0271 (9.998 secs)\n",
      "canp:canp_periodic step 62600 lr 1.536e-04 [train_loss] loss -1.0172 (9.784 secs)\n",
      "canp:canp_periodic step 62800 lr 1.522e-04 [train_loss] loss -1.0772 (11.674 secs)\n",
      "canp:canp_periodic step 63000 lr 1.507e-04 [train_loss] loss -1.0260 (11.632 secs)\n",
      "canp:canp_periodic step 63200 lr 1.493e-04 [train_loss] loss -1.0251 (9.363 secs)\n",
      "canp:canp_periodic step 63400 lr 1.478e-04 [train_loss] loss -1.0346 (10.174 secs)\n",
      "canp:canp_periodic step 63600 lr 1.464e-04 [train_loss] loss -1.0709 (9.994 secs)\n",
      "canp:canp_periodic step 63800 lr 1.450e-04 [train_loss] loss -0.9743 (3.668 secs)\n",
      "canp:canp_periodic step 64000 lr 1.436e-04 [train_loss] loss -1.0039 (3.914 secs)\n",
      "canp:canp_periodic step 64200 lr 1.421e-04 [train_loss] loss -1.0412 (3.974 secs)\n",
      "canp:canp_periodic step 64400 lr 1.407e-04 [train_loss] loss -1.0300 (3.725 secs)\n",
      "canp:canp_periodic step 64600 lr 1.393e-04 [train_loss] loss -1.0175 (3.674 secs)\n",
      "canp:canp_periodic step 64800 lr 1.379e-04 [train_loss] loss -1.0259 (3.701 secs)\n",
      "canp:canp_periodic step 65000 lr 1.365e-04 [train_loss] loss -1.0727 (3.749 secs)\n",
      "100%|##########| 3000/3000 [00:15<00:00, 198.97it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.1458 tar_ll -7.8177 (15.080 secs)\n",
      "\n",
      "canp:canp_periodic step 65200 lr 1.351e-04 [train_loss] loss -1.0520 (3.944 secs)\n",
      "canp:canp_periodic step 65400 lr 1.337e-04 [train_loss] loss -1.0146 (3.986 secs)\n",
      "canp:canp_periodic step 65600 lr 1.323e-04 [train_loss] loss -1.0182 (4.069 secs)\n",
      "canp:canp_periodic step 65800 lr 1.309e-04 [train_loss] loss -1.0025 (4.121 secs)\n",
      "canp:canp_periodic step 66000 lr 1.296e-04 [train_loss] loss -1.0818 (3.711 secs)\n",
      "canp:canp_periodic step 66200 lr 1.282e-04 [train_loss] loss -1.0197 (3.734 secs)\n",
      "canp:canp_periodic step 66400 lr 1.268e-04 [train_loss] loss -1.0385 (3.841 secs)\n",
      "canp:canp_periodic step 66600 lr 1.255e-04 [train_loss] loss -1.1053 (3.969 secs)\n",
      "canp:canp_periodic step 66800 lr 1.241e-04 [train_loss] loss -1.0495 (3.886 secs)\n",
      "canp:canp_periodic step 67000 lr 1.227e-04 [train_loss] loss -0.9973 (3.996 secs)\n",
      "canp:canp_periodic step 67200 lr 1.214e-04 [train_loss] loss -1.0042 (3.751 secs)\n",
      "canp:canp_periodic step 67400 lr 1.200e-04 [train_loss] loss -1.0268 (3.784 secs)\n",
      "canp:canp_periodic step 67600 lr 1.187e-04 [train_loss] loss -1.0418 (3.811 secs)\n",
      "canp:canp_periodic step 67800 lr 1.174e-04 [train_loss] loss -1.0550 (3.621 secs)\n",
      "canp:canp_periodic step 68000 lr 1.160e-04 [train_loss] loss -1.0145 (3.531 secs)\n",
      "canp:canp_periodic step 68200 lr 1.147e-04 [train_loss] loss -1.0549 (3.628 secs)\n",
      "canp:canp_periodic step 68400 lr 1.134e-04 [train_loss] loss -1.0697 (3.948 secs)\n",
      "canp:canp_periodic step 68600 lr 1.121e-04 [train_loss] loss -1.0049 (4.182 secs)\n",
      "canp:canp_periodic step 68800 lr 1.108e-04 [train_loss] loss -1.0779 (3.852 secs)\n",
      "canp:canp_periodic step 69000 lr 1.095e-04 [train_loss] loss -1.0171 (3.785 secs)\n",
      "canp:canp_periodic step 69200 lr 1.082e-04 [train_loss] loss -1.0364 (4.004 secs)\n",
      "canp:canp_periodic step 69400 lr 1.069e-04 [train_loss] loss -1.0156 (3.822 secs)\n",
      "canp:canp_periodic step 69600 lr 1.056e-04 [train_loss] loss -0.9741 (3.781 secs)\n",
      "canp:canp_periodic step 69800 lr 1.043e-04 [train_loss] loss -1.0004 (3.822 secs)\n",
      "canp:canp_periodic step 70000 lr 1.031e-04 [train_loss] loss -1.0369 (4.072 secs)\n",
      "100%|##########| 3000/3000 [00:15<00:00, 189.23it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.0240 tar_ll -9.3689 (15.857 secs)\n",
      "\n",
      "canp:canp_periodic step 70200 lr 1.018e-04 [train_loss] loss -0.9950 (4.000 secs)\n",
      "canp:canp_periodic step 70400 lr 1.005e-04 [train_loss] loss -0.9945 (4.345 secs)\n",
      "canp:canp_periodic step 70600 lr 9.927e-05 [train_loss] loss -1.0180 (3.625 secs)\n",
      "canp:canp_periodic step 70800 lr 9.802e-05 [train_loss] loss -1.0111 (3.762 secs)\n",
      "canp:canp_periodic step 71000 lr 9.677e-05 [train_loss] loss -0.9765 (3.534 secs)\n",
      "canp:canp_periodic step 71200 lr 9.554e-05 [train_loss] loss -0.9778 (3.536 secs)\n",
      "canp:canp_periodic step 71400 lr 9.430e-05 [train_loss] loss -1.0443 (3.490 secs)\n",
      "canp:canp_periodic step 71600 lr 9.308e-05 [train_loss] loss -0.9978 (3.665 secs)\n",
      "canp:canp_periodic step 71800 lr 9.186e-05 [train_loss] loss -1.0257 (3.611 secs)\n",
      "canp:canp_periodic step 72000 lr 9.064e-05 [train_loss] loss -1.0182 (3.686 secs)\n",
      "canp:canp_periodic step 72200 lr 8.944e-05 [train_loss] loss -1.0620 (3.592 secs)\n",
      "canp:canp_periodic step 72400 lr 8.824e-05 [train_loss] loss -1.0641 (3.768 secs)\n",
      "canp:canp_periodic step 72600 lr 8.704e-05 [train_loss] loss -1.0277 (4.009 secs)\n",
      "canp:canp_periodic step 72800 lr 8.585e-05 [train_loss] loss -1.0517 (3.713 secs)\n",
      "canp:canp_periodic step 73000 lr 8.467e-05 [train_loss] loss -1.0546 (3.611 secs)\n",
      "canp:canp_periodic step 73200 lr 8.350e-05 [train_loss] loss -1.0587 (4.067 secs)\n",
      "canp:canp_periodic step 73400 lr 8.233e-05 [train_loss] loss -1.0505 (3.665 secs)\n",
      "canp:canp_periodic step 73600 lr 8.117e-05 [train_loss] loss -1.0218 (3.627 secs)\n",
      "canp:canp_periodic step 73800 lr 8.001e-05 [train_loss] loss -1.0420 (3.920 secs)\n",
      "canp:canp_periodic step 74000 lr 7.886e-05 [train_loss] loss -1.0032 (3.766 secs)\n",
      "canp:canp_periodic step 74200 lr 7.772e-05 [train_loss] loss -1.0578 (3.875 secs)\n",
      "canp:canp_periodic step 74400 lr 7.659e-05 [train_loss] loss -1.0574 (3.759 secs)\n",
      "canp:canp_periodic step 74600 lr 7.546e-05 [train_loss] loss -1.0377 (3.812 secs)\n",
      "canp:canp_periodic step 74800 lr 7.434e-05 [train_loss] loss -1.0701 (3.709 secs)\n",
      "canp:canp_periodic step 75000 lr 7.322e-05 [train_loss] loss -1.0314 (3.851 secs)\n",
      "100%|##########| 3000/3000 [00:14<00:00, 204.21it/s]\n",
      "canp:canp_periodic periodic ctx_ll 0.0182 tar_ll -8.1448 (14.693 secs)\n",
      "\n",
      "canp:canp_periodic step 75200 lr 7.212e-05 [train_loss] loss -1.0241 (3.903 secs)\n",
      "canp:canp_periodic step 75400 lr 7.102e-05 [train_loss] loss -1.0287 (3.750 secs)\n",
      "canp:canp_periodic step 75600 lr 6.992e-05 [train_loss] loss -1.0357 (3.844 secs)\n",
      "canp:canp_periodic step 75800 lr 6.884e-05 [train_loss] loss -1.0414 (3.747 secs)\n",
      "canp:canp_periodic step 76000 lr 6.776e-05 [train_loss] loss -1.0654 (3.737 secs)\n",
      "canp:canp_periodic step 76200 lr 6.669e-05 [train_loss] loss -1.0623 (3.721 secs)\n",
      "canp:canp_periodic step 76400 lr 6.562e-05 [train_loss] loss -1.0497 (3.742 secs)\n",
      "canp:canp_periodic step 76600 lr 6.456e-05 [train_loss] loss -1.0350 (3.825 secs)\n",
      "canp:canp_periodic step 76800 lr 6.351e-05 [train_loss] loss -1.0462 (4.017 secs)\n",
      "canp:canp_periodic step 77000 lr 6.247e-05 [train_loss] loss -1.0564 (3.778 secs)\n",
      "canp:canp_periodic step 77200 lr 6.144e-05 [train_loss] loss -1.0719 (3.737 secs)\n",
      "canp:canp_periodic step 77400 lr 6.041e-05 [train_loss] loss -1.0217 (3.727 secs)\n",
      "canp:canp_periodic step 77600 lr 5.939e-05 [train_loss] loss -1.0492 (3.729 secs)\n",
      "canp:canp_periodic step 77800 lr 5.838e-05 [train_loss] loss -1.0863 (3.824 secs)\n",
      "canp:canp_periodic step 78000 lr 5.737e-05 [train_loss] loss -1.0832 (3.592 secs)\n",
      "canp:canp_periodic step 78200 lr 5.637e-05 [train_loss] loss -1.0192 (3.740 secs)\n",
      "canp:canp_periodic step 78400 lr 5.538e-05 [train_loss] loss -1.0485 (3.562 secs)\n",
      "canp:canp_periodic step 78600 lr 5.440e-05 [train_loss] loss -1.0551 (3.323 secs)\n",
      "canp:canp_periodic step 78800 lr 5.343e-05 [train_loss] loss -1.0757 (3.597 secs)\n",
      "canp:canp_periodic step 79000 lr 5.246e-05 [train_loss] loss -0.9987 (3.650 secs)\n",
      "canp:canp_periodic step 79200 lr 5.150e-05 [train_loss] loss -0.9565 (3.675 secs)\n",
      "canp:canp_periodic step 79400 lr 5.055e-05 [train_loss] loss -1.0568 (3.677 secs)\n",
      "canp:canp_periodic step 79600 lr 4.961e-05 [train_loss] loss -1.0642 (3.628 secs)\n",
      "canp:canp_periodic step 79800 lr 4.867e-05 [train_loss] loss -1.0443 (3.502 secs)\n",
      "canp:canp_periodic step 80000 lr 4.775e-05 [train_loss] loss -1.0408 (3.660 secs)\n",
      "100%|##########| 3000/3000 [00:14<00:00, 210.66it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.1953 tar_ll -8.0330 (14.244 secs)\n",
      "\n",
      "canp:canp_periodic step 80200 lr 4.683e-05 [train_loss] loss -1.0495 (3.795 secs)\n",
      "canp:canp_periodic step 80400 lr 4.592e-05 [train_loss] loss -1.0972 (3.389 secs)\n",
      "canp:canp_periodic step 80600 lr 4.501e-05 [train_loss] loss -1.0385 (3.457 secs)\n",
      "canp:canp_periodic step 80800 lr 4.412e-05 [train_loss] loss -1.0729 (3.458 secs)\n",
      "canp:canp_periodic step 81000 lr 4.323e-05 [train_loss] loss -1.0279 (3.469 secs)\n",
      "canp:canp_periodic step 81200 lr 4.235e-05 [train_loss] loss -1.0337 (3.640 secs)\n",
      "canp:canp_periodic step 81400 lr 4.148e-05 [train_loss] loss -1.0483 (3.422 secs)\n",
      "canp:canp_periodic step 81600 lr 4.062e-05 [train_loss] loss -1.0213 (3.427 secs)\n",
      "canp:canp_periodic step 81800 lr 3.976e-05 [train_loss] loss -1.0714 (3.661 secs)\n",
      "canp:canp_periodic step 82000 lr 3.892e-05 [train_loss] loss -1.0214 (3.665 secs)\n",
      "canp:canp_periodic step 82200 lr 3.808e-05 [train_loss] loss -1.0172 (3.561 secs)\n",
      "canp:canp_periodic step 82400 lr 3.725e-05 [train_loss] loss -1.0295 (3.652 secs)\n",
      "canp:canp_periodic step 82600 lr 3.643e-05 [train_loss] loss -1.0362 (3.827 secs)\n",
      "canp:canp_periodic step 82800 lr 3.562e-05 [train_loss] loss -1.0329 (4.257 secs)\n",
      "canp:canp_periodic step 83000 lr 3.481e-05 [train_loss] loss -1.0331 (3.821 secs)\n",
      "canp:canp_periodic step 83200 lr 3.402e-05 [train_loss] loss -1.0523 (3.614 secs)\n",
      "canp:canp_periodic step 83400 lr 3.323e-05 [train_loss] loss -1.0430 (3.412 secs)\n",
      "canp:canp_periodic step 83600 lr 3.245e-05 [train_loss] loss -1.0587 (3.521 secs)\n",
      "canp:canp_periodic step 83800 lr 3.168e-05 [train_loss] loss -1.0752 (3.756 secs)\n",
      "canp:canp_periodic step 84000 lr 3.092e-05 [train_loss] loss -1.0422 (3.607 secs)\n",
      "canp:canp_periodic step 84200 lr 3.017e-05 [train_loss] loss -1.0199 (3.653 secs)\n",
      "canp:canp_periodic step 84400 lr 2.943e-05 [train_loss] loss -1.0898 (3.716 secs)\n",
      "canp:canp_periodic step 84600 lr 2.869e-05 [train_loss] loss -1.0028 (3.590 secs)\n",
      "canp:canp_periodic step 84800 lr 2.797e-05 [train_loss] loss -1.1150 (3.789 secs)\n",
      "canp:canp_periodic step 85000 lr 2.725e-05 [train_loss] loss -1.0884 (3.711 secs)\n",
      "100%|##########| 3000/3000 [00:14<00:00, 201.35it/s]\n",
      "canp:canp_periodic periodic ctx_ll -0.0756 tar_ll -8.1418 (14.902 secs)\n",
      "\n",
      "canp:canp_periodic step 85200 lr 2.654e-05 [train_loss] loss -1.1222 (3.847 secs)\n",
      "canp:canp_periodic step 85400 lr 2.584e-05 [train_loss] loss -1.0477 (3.738 secs)\n",
      "canp:canp_periodic step 85600 lr 2.515e-05 [train_loss] loss -1.0290 (3.811 secs)\n",
      "canp:canp_periodic step 85800 lr 2.447e-05 [train_loss] loss -0.9990 (3.823 secs)\n",
      "canp:canp_periodic step 86000 lr 2.379e-05 [train_loss] loss -1.0712 (3.793 secs)\n",
      "canp:canp_periodic step 86200 lr 2.313e-05 [train_loss] loss -1.0739 (3.763 secs)\n",
      "canp:canp_periodic step 86400 lr 2.247e-05 [train_loss] loss -1.0067 (3.831 secs)\n",
      "canp:canp_periodic step 86600 lr 2.183e-05 [train_loss] loss -1.0077 (3.689 secs)\n",
      "canp:canp_periodic step 86800 lr 2.119e-05 [train_loss] loss -1.0834 (3.798 secs)\n",
      "canp:canp_periodic step 87000 lr 2.056e-05 [train_loss] loss -1.0539 (3.684 secs)\n",
      "canp:canp_periodic step 87200 lr 1.994e-05 [train_loss] loss -1.0628 (3.874 secs)\n",
      "canp:canp_periodic step 87400 lr 1.933e-05 [train_loss] loss -1.0550 (3.590 secs)\n",
      "canp:canp_periodic step 87600 lr 1.873e-05 [train_loss] loss -1.0829 (3.614 secs)\n",
      "canp:canp_periodic step 87800 lr 1.814e-05 [train_loss] loss -1.0517 (3.655 secs)\n",
      "canp:canp_periodic step 88000 lr 1.756e-05 [train_loss] loss -1.0641 (3.830 secs)\n",
      "canp:canp_periodic step 88200 lr 1.698e-05 [train_loss] loss -1.0650 (3.598 secs)\n",
      "canp:canp_periodic step 88400 lr 1.642e-05 [train_loss] loss -1.0126 (3.578 secs)\n",
      "canp:canp_periodic step 88600 lr 1.586e-05 [train_loss] loss -1.0796 (3.741 secs)\n",
      "canp:canp_periodic step 88800 lr 1.532e-05 [train_loss] loss -1.0562 (3.674 secs)\n",
      "canp:canp_periodic step 89000 lr 1.478e-05 [train_loss] loss -1.0647 (3.729 secs)\n",
      "canp:canp_periodic step 89200 lr 1.425e-05 [train_loss] loss -1.0230 (3.789 secs)\n",
      "canp:canp_periodic step 89400 lr 1.373e-05 [train_loss] loss -1.0520 (3.765 secs)\n",
      "canp:canp_periodic step 89600 lr 1.323e-05 [train_loss] loss -1.0575 (3.680 secs)\n",
      "canp:canp_periodic step 89800 lr 1.273e-05 [train_loss] loss -1.0313 (3.781 secs)\n",
      "canp:canp_periodic step 90000 lr 1.224e-05 [train_loss] loss -1.0081 (3.764 secs)\n",
      "100%|##########| 3000/3000 [00:14<00:00, 200.80it/s]\n",
      "canp:canp_periodic periodic ctx_ll 0.0772 tar_ll -8.1441 (14.944 secs)\n",
      "\n",
      "canp:canp_periodic step 90200 lr 1.176e-05 [train_loss] loss -1.0208 (3.522 secs)\n",
      "canp:canp_periodic step 90400 lr 1.128e-05 [train_loss] loss -1.0905 (3.651 secs)\n",
      "canp:canp_periodic step 90600 lr 1.082e-05 [train_loss] loss -1.0732 (3.717 secs)\n",
      "canp:canp_periodic step 90800 lr 1.037e-05 [train_loss] loss -1.1175 (3.570 secs)\n",
      "canp:canp_periodic step 91000 lr 9.927e-06 [train_loss] loss -1.0354 (3.655 secs)\n",
      "canp:canp_periodic step 91200 lr 9.493e-06 [train_loss] loss -1.0559 (3.553 secs)\n",
      "canp:canp_periodic step 91400 lr 9.069e-06 [train_loss] loss -1.0563 (3.659 secs)\n",
      "canp:canp_periodic step 91600 lr 8.655e-06 [train_loss] loss -1.0328 (3.713 secs)\n",
      "canp:canp_periodic step 91800 lr 8.250e-06 [train_loss] loss -1.1068 (3.643 secs)\n",
      "canp:canp_periodic step 92000 lr 7.854e-06 [train_loss] loss -1.0115 (3.625 secs)\n",
      "canp:canp_periodic step 92200 lr 7.468e-06 [train_loss] loss -1.0404 (3.533 secs)\n",
      "canp:canp_periodic step 92400 lr 7.092e-06 [train_loss] loss -1.0540 (3.689 secs)\n",
      "canp:canp_periodic step 92600 lr 6.725e-06 [train_loss] loss -1.0370 (3.679 secs)\n",
      "canp:canp_periodic step 92800 lr 6.368e-06 [train_loss] loss -1.0411 (3.611 secs)\n",
      "canp:canp_periodic step 93000 lr 6.021e-06 [train_loss] loss -1.0123 (3.541 secs)\n",
      "canp:canp_periodic step 93200 lr 5.683e-06 [train_loss] loss -1.0773 (3.666 secs)\n",
      "canp:canp_periodic step 93400 lr 5.355e-06 [train_loss] loss -1.0804 (3.554 secs)\n",
      "canp:canp_periodic step 93600 lr 5.036e-06 [train_loss] loss -1.0602 (3.634 secs)\n",
      "canp:canp_periodic step 93800 lr 4.727e-06 [train_loss] loss -1.0209 (3.583 secs)\n",
      "canp:canp_periodic step 94000 lr 4.428e-06 [train_loss] loss -1.0473 (3.528 secs)\n",
      "canp:canp_periodic step 94200 lr 4.139e-06 [train_loss] loss -1.0625 (4.026 secs)\n",
      "canp:canp_periodic step 94400 lr 3.859e-06 [train_loss] loss -1.0650 (3.685 secs)\n",
      "canp:canp_periodic step 94600 lr 3.589e-06 [train_loss] loss -1.0135 (3.545 secs)\n",
      "canp:canp_periodic step 94800 lr 3.329e-06 [train_loss] loss -1.0659 (3.562 secs)\n",
      "canp:canp_periodic step 95000 lr 3.078e-06 [train_loss] loss -1.0975 (3.686 secs)\n",
      "100%|##########| 3000/3000 [00:14<00:00, 207.53it/s]\n",
      "canp:canp_periodic periodic ctx_ll 0.0954 tar_ll -8.3225 (14.460 secs)\n",
      "\n",
      "canp:canp_periodic step 95200 lr 2.837e-06 [train_loss] loss -1.0160 (3.618 secs)\n",
      "canp:canp_periodic step 95400 lr 2.606e-06 [train_loss] loss -1.0240 (3.810 secs)\n",
      "canp:canp_periodic step 95600 lr 2.385e-06 [train_loss] loss -1.0223 (3.716 secs)\n",
      "canp:canp_periodic step 95800 lr 2.173e-06 [train_loss] loss -1.0617 (3.806 secs)\n",
      "canp:canp_periodic step 96000 lr 1.971e-06 [train_loss] loss -1.0728 (3.793 secs)\n",
      "canp:canp_periodic step 96200 lr 1.779e-06 [train_loss] loss -1.0390 (3.776 secs)\n",
      "canp:canp_periodic step 96400 lr 1.597e-06 [train_loss] loss -0.9958 (3.606 secs)\n",
      "canp:canp_periodic step 96600 lr 1.425e-06 [train_loss] loss -1.0811 (3.631 secs)\n",
      "canp:canp_periodic step 96800 lr 1.262e-06 [train_loss] loss -1.0474 (3.824 secs)\n",
      "canp:canp_periodic step 97000 lr 1.110e-06 [train_loss] loss -1.0538 (3.759 secs)\n",
      "canp:canp_periodic step 97200 lr 9.666e-07 [train_loss] loss -1.0425 (3.650 secs)\n",
      "canp:canp_periodic step 97400 lr 8.335e-07 [train_loss] loss -1.0667 (3.663 secs)\n",
      "canp:canp_periodic step 97600 lr 7.103e-07 [train_loss] loss -1.0638 (4.004 secs)\n",
      "canp:canp_periodic step 97800 lr 5.969e-07 [train_loss] loss -1.0081 (3.766 secs)\n",
      "canp:canp_periodic step 98000 lr 4.933e-07 [train_loss] loss -1.1192 (3.756 secs)\n",
      "canp:canp_periodic step 98200 lr 3.996e-07 [train_loss] loss -1.0395 (3.814 secs)\n",
      "canp:canp_periodic step 98400 lr 3.158e-07 [train_loss] loss -1.0361 (4.158 secs)\n",
      "canp:canp_periodic step 98600 lr 2.418e-07 [train_loss] loss -1.0895 (3.686 secs)\n",
      "canp:canp_periodic step 98800 lr 1.776e-07 [train_loss] loss -1.1058 (3.699 secs)\n",
      "canp:canp_periodic step 99000 lr 1.234e-07 [train_loss] loss -1.0431 (3.915 secs)\n",
      "canp:canp_periodic step 99200 lr 7.895e-08 [train_loss] loss -1.0595 (3.776 secs)\n",
      "canp:canp_periodic step 99400 lr 4.441e-08 [train_loss] loss -1.0881 (3.674 secs)\n",
      "canp:canp_periodic step 99600 lr 1.974e-08 [train_loss] loss -1.0893 (3.666 secs)\n",
      "canp:canp_periodic step 99800 lr 4.935e-09 [train_loss] loss -1.0556 (3.721 secs)\n",
      "canp:canp_periodic step 100000 lr 0.000e+00 [train_loss] loss -1.0631 (3.742 secs)\n",
      "100%|##########| 3000/3000 [00:14<00:00, 203.87it/s]\n",
      "canp:canp_periodic periodic ctx_ll 0.0987 tar_ll -8.2872 (14.717 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:14<00:00, 204.78it/s]\n",
      "canp:canp_periodic periodic ctx_ll 0.0987 tar_ll -8.2872 (14.653 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 2235803.75 miliseconds\n",
      "Execution time: 2235.80375 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 46.615234375 MB\n",
      "Memory Usage Change: 30.365234375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='canp', name='canp_periodic',val_seed=100, val_l=None,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca1edd-8f9f-48ea-b179-f9204a816269",
   "metadata": {},
   "source": [
    "## NP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7630e59a-22bf-4982-913b-5c485e56c1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: np-np_periodic\n",
      "Total number of parameters: 232194\n",
      "\n",
      "np:np_periodic step 200 lr 5.000e-04 [train_loss] loss 0.6644 (3.629 secs)\n",
      "np:np_periodic step 400 lr 5.000e-04 [train_loss] loss 0.5286 (3.493 secs)\n",
      "np:np_periodic step 600 lr 5.000e-04 [train_loss] loss 0.5268 (3.585 secs)\n",
      "np:np_periodic step 800 lr 4.999e-04 [train_loss] loss 0.4715 (3.537 secs)\n",
      "np:np_periodic step 1000 lr 4.999e-04 [train_loss] loss 0.4208 (3.603 secs)\n",
      "np:np_periodic step 1200 lr 4.998e-04 [train_loss] loss 0.4625 (3.432 secs)\n",
      "np:np_periodic step 1400 lr 4.998e-04 [train_loss] loss 0.4454 (3.510 secs)\n",
      "np:np_periodic step 1600 lr 4.997e-04 [train_loss] loss 0.4166 (3.477 secs)\n",
      "np:np_periodic step 1800 lr 4.996e-04 [train_loss] loss 0.4171 (3.472 secs)\n",
      "np:np_periodic step 2000 lr 4.995e-04 [train_loss] loss 0.4265 (3.474 secs)\n",
      "np:np_periodic step 2200 lr 4.994e-04 [train_loss] loss 0.4230 (3.449 secs)\n",
      "np:np_periodic step 2400 lr 4.993e-04 [train_loss] loss 0.3807 (3.489 secs)\n",
      "np:np_periodic step 2600 lr 4.992e-04 [train_loss] loss 0.3542 (3.436 secs)\n",
      "np:np_periodic step 2800 lr 4.990e-04 [train_loss] loss 0.3421 (3.533 secs)\n",
      "np:np_periodic step 3000 lr 4.989e-04 [train_loss] loss 0.3169 (3.478 secs)\n",
      "np:np_periodic step 3200 lr 4.987e-04 [train_loss] loss 0.3313 (3.485 secs)\n",
      "np:np_periodic step 3400 lr 4.986e-04 [train_loss] loss 0.2936 (3.315 secs)\n",
      "np:np_periodic step 3600 lr 4.984e-04 [train_loss] loss 0.2888 (3.474 secs)\n",
      "np:np_periodic step 3800 lr 4.982e-04 [train_loss] loss 0.2809 (3.430 secs)\n",
      "np:np_periodic step 4000 lr 4.980e-04 [train_loss] loss 0.2822 (3.354 secs)\n",
      "np:np_periodic step 4200 lr 4.978e-04 [train_loss] loss 0.2668 (3.484 secs)\n",
      "np:np_periodic step 4400 lr 4.976e-04 [train_loss] loss 0.2631 (3.378 secs)\n",
      "np:np_periodic step 4600 lr 4.974e-04 [train_loss] loss 0.2607 (3.421 secs)\n",
      "np:np_periodic step 4800 lr 4.972e-04 [train_loss] loss 0.2488 (3.690 secs)\n",
      "np:np_periodic step 5000 lr 4.969e-04 [train_loss] loss 0.2356 (3.522 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 245.78it/s]\n",
      "np:np_periodic periodic ctx_ll -0.4551 tar_ll -0.8274 (12.209 secs)\n",
      "\n",
      "np:np_periodic step 5200 lr 4.967e-04 [train_loss] loss 0.2303 (3.479 secs)\n",
      "np:np_periodic step 5400 lr 4.964e-04 [train_loss] loss 0.2269 (3.356 secs)\n",
      "np:np_periodic step 5600 lr 4.961e-04 [train_loss] loss 0.2088 (3.495 secs)\n",
      "np:np_periodic step 5800 lr 4.959e-04 [train_loss] loss 0.2089 (3.599 secs)\n",
      "np:np_periodic step 6000 lr 4.956e-04 [train_loss] loss 0.2104 (3.430 secs)\n",
      "np:np_periodic step 6200 lr 4.953e-04 [train_loss] loss 0.2060 (3.474 secs)\n",
      "np:np_periodic step 6400 lr 4.950e-04 [train_loss] loss 0.1821 (3.563 secs)\n",
      "np:np_periodic step 6600 lr 4.946e-04 [train_loss] loss 0.1533 (3.524 secs)\n",
      "np:np_periodic step 6800 lr 4.943e-04 [train_loss] loss 0.1493 (3.627 secs)\n",
      "np:np_periodic step 7000 lr 4.940e-04 [train_loss] loss 0.1585 (3.603 secs)\n",
      "np:np_periodic step 7200 lr 4.936e-04 [train_loss] loss 0.1397 (3.659 secs)\n",
      "np:np_periodic step 7400 lr 4.933e-04 [train_loss] loss 0.1487 (3.582 secs)\n",
      "np:np_periodic step 7600 lr 4.929e-04 [train_loss] loss 0.1329 (3.548 secs)\n",
      "np:np_periodic step 7800 lr 4.925e-04 [train_loss] loss 0.1374 (3.504 secs)\n",
      "np:np_periodic step 8000 lr 4.921e-04 [train_loss] loss 0.1273 (3.397 secs)\n",
      "np:np_periodic step 8200 lr 4.918e-04 [train_loss] loss 0.1303 (3.427 secs)\n",
      "np:np_periodic step 8400 lr 4.913e-04 [train_loss] loss 0.1425 (3.703 secs)\n",
      "np:np_periodic step 8600 lr 4.909e-04 [train_loss] loss 0.1512 (3.592 secs)\n",
      "np:np_periodic step 8800 lr 4.905e-04 [train_loss] loss 0.1334 (3.641 secs)\n",
      "np:np_periodic step 9000 lr 4.901e-04 [train_loss] loss 0.1356 (3.488 secs)\n",
      "np:np_periodic step 9200 lr 4.896e-04 [train_loss] loss 0.0990 (3.484 secs)\n",
      "np:np_periodic step 9400 lr 4.892e-04 [train_loss] loss 0.1423 (3.652 secs)\n",
      "np:np_periodic step 9600 lr 4.887e-04 [train_loss] loss 0.1013 (3.585 secs)\n",
      "np:np_periodic step 9800 lr 4.882e-04 [train_loss] loss 0.0753 (3.614 secs)\n",
      "np:np_periodic step 10000 lr 4.878e-04 [train_loss] loss 0.0851 (3.426 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 235.81it/s]\n",
      "np:np_periodic periodic ctx_ll -0.4066 tar_ll -0.7806 (12.725 secs)\n",
      "\n",
      "np:np_periodic step 10200 lr 4.873e-04 [train_loss] loss 0.0650 (3.460 secs)\n",
      "np:np_periodic step 10400 lr 4.868e-04 [train_loss] loss 0.0914 (3.717 secs)\n",
      "np:np_periodic step 10600 lr 4.863e-04 [train_loss] loss 0.0470 (3.528 secs)\n",
      "np:np_periodic step 10800 lr 4.857e-04 [train_loss] loss 0.1111 (3.378 secs)\n",
      "np:np_periodic step 11000 lr 4.852e-04 [train_loss] loss 0.0762 (3.415 secs)\n",
      "np:np_periodic step 11200 lr 4.847e-04 [train_loss] loss 0.0650 (3.668 secs)\n",
      "np:np_periodic step 11400 lr 4.841e-04 [train_loss] loss 0.0373 (3.503 secs)\n",
      "np:np_periodic step 11600 lr 4.836e-04 [train_loss] loss 0.0085 (3.427 secs)\n",
      "np:np_periodic step 11800 lr 4.830e-04 [train_loss] loss 0.0271 (3.495 secs)\n",
      "np:np_periodic step 12000 lr 4.824e-04 [train_loss] loss 0.0398 (3.369 secs)\n",
      "np:np_periodic step 12200 lr 4.819e-04 [train_loss] loss 0.0374 (3.690 secs)\n",
      "np:np_periodic step 12400 lr 4.813e-04 [train_loss] loss 0.0041 (3.402 secs)\n",
      "np:np_periodic step 12600 lr 4.807e-04 [train_loss] loss -0.0005 (3.368 secs)\n",
      "np:np_periodic step 12800 lr 4.801e-04 [train_loss] loss -0.0242 (3.482 secs)\n",
      "np:np_periodic step 13000 lr 4.794e-04 [train_loss] loss 0.0068 (3.419 secs)\n",
      "np:np_periodic step 13200 lr 4.788e-04 [train_loss] loss -0.0002 (3.588 secs)\n",
      "np:np_periodic step 13400 lr 4.782e-04 [train_loss] loss 0.0272 (3.453 secs)\n",
      "np:np_periodic step 13600 lr 4.775e-04 [train_loss] loss 0.0057 (3.573 secs)\n",
      "np:np_periodic step 13800 lr 4.769e-04 [train_loss] loss 0.0039 (3.417 secs)\n",
      "np:np_periodic step 14000 lr 4.762e-04 [train_loss] loss -0.0279 (3.421 secs)\n",
      "np:np_periodic step 14200 lr 4.755e-04 [train_loss] loss -0.0303 (3.598 secs)\n",
      "np:np_periodic step 14400 lr 4.749e-04 [train_loss] loss -0.0093 (3.393 secs)\n",
      "np:np_periodic step 14600 lr 4.742e-04 [train_loss] loss -0.0332 (3.454 secs)\n",
      "np:np_periodic step 14800 lr 4.735e-04 [train_loss] loss -0.0121 (3.538 secs)\n",
      "np:np_periodic step 15000 lr 4.728e-04 [train_loss] loss -0.0318 (3.606 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 246.55it/s]\n",
      "np:np_periodic periodic ctx_ll -0.3865 tar_ll -1.0040 (12.173 secs)\n",
      "\n",
      "np:np_periodic step 15200 lr 4.720e-04 [train_loss] loss -0.0521 (3.447 secs)\n",
      "np:np_periodic step 15400 lr 4.713e-04 [train_loss] loss -0.0622 (3.425 secs)\n",
      "np:np_periodic step 15600 lr 4.706e-04 [train_loss] loss -0.0658 (3.415 secs)\n",
      "np:np_periodic step 15800 lr 4.698e-04 [train_loss] loss -0.0159 (3.482 secs)\n",
      "np:np_periodic step 16000 lr 4.691e-04 [train_loss] loss -0.0486 (3.531 secs)\n",
      "np:np_periodic step 16200 lr 4.683e-04 [train_loss] loss -0.0353 (3.524 secs)\n",
      "np:np_periodic step 16400 lr 4.675e-04 [train_loss] loss -0.0535 (3.569 secs)\n",
      "np:np_periodic step 16600 lr 4.668e-04 [train_loss] loss -0.0694 (3.352 secs)\n",
      "np:np_periodic step 16800 lr 4.660e-04 [train_loss] loss -0.0806 (3.433 secs)\n",
      "np:np_periodic step 17000 lr 4.652e-04 [train_loss] loss -0.0478 (3.425 secs)\n",
      "np:np_periodic step 17200 lr 4.644e-04 [train_loss] loss -0.0830 (3.385 secs)\n",
      "np:np_periodic step 17400 lr 4.636e-04 [train_loss] loss -0.1112 (3.394 secs)\n",
      "np:np_periodic step 17600 lr 4.627e-04 [train_loss] loss -0.0975 (3.409 secs)\n",
      "np:np_periodic step 17800 lr 4.619e-04 [train_loss] loss -0.0928 (3.381 secs)\n",
      "np:np_periodic step 18000 lr 4.611e-04 [train_loss] loss -0.0471 (3.401 secs)\n",
      "np:np_periodic step 18200 lr 4.602e-04 [train_loss] loss -0.0920 (3.429 secs)\n",
      "np:np_periodic step 18400 lr 4.594e-04 [train_loss] loss -0.1076 (3.578 secs)\n",
      "np:np_periodic step 18600 lr 4.585e-04 [train_loss] loss -0.0964 (3.496 secs)\n",
      "np:np_periodic step 18800 lr 4.576e-04 [train_loss] loss -0.1085 (3.626 secs)\n",
      "np:np_periodic step 19000 lr 4.568e-04 [train_loss] loss -0.0955 (3.577 secs)\n",
      "np:np_periodic step 19200 lr 4.559e-04 [train_loss] loss -0.0877 (3.526 secs)\n",
      "np:np_periodic step 19400 lr 4.550e-04 [train_loss] loss -0.0938 (3.500 secs)\n",
      "np:np_periodic step 19600 lr 4.541e-04 [train_loss] loss -0.1021 (3.512 secs)\n",
      "np:np_periodic step 19800 lr 4.532e-04 [train_loss] loss -0.1227 (3.590 secs)\n",
      "np:np_periodic step 20000 lr 4.523e-04 [train_loss] loss -0.1022 (3.477 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 238.46it/s]\n",
      "np:np_periodic periodic ctx_ll -0.3610 tar_ll -0.8996 (12.583 secs)\n",
      "\n",
      "np:np_periodic step 20200 lr 4.513e-04 [train_loss] loss -0.1032 (3.561 secs)\n",
      "np:np_periodic step 20400 lr 4.504e-04 [train_loss] loss -0.1290 (3.518 secs)\n",
      "np:np_periodic step 20600 lr 4.494e-04 [train_loss] loss -0.1181 (3.415 secs)\n",
      "np:np_periodic step 20800 lr 4.485e-04 [train_loss] loss -0.1149 (3.777 secs)\n",
      "np:np_periodic step 21000 lr 4.475e-04 [train_loss] loss -0.1554 (3.451 secs)\n",
      "np:np_periodic step 21200 lr 4.466e-04 [train_loss] loss -0.1115 (3.460 secs)\n",
      "np:np_periodic step 21400 lr 4.456e-04 [train_loss] loss -0.1241 (3.433 secs)\n",
      "np:np_periodic step 21600 lr 4.446e-04 [train_loss] loss -0.1097 (3.596 secs)\n",
      "np:np_periodic step 21800 lr 4.436e-04 [train_loss] loss -0.1431 (3.566 secs)\n",
      "np:np_periodic step 22000 lr 4.426e-04 [train_loss] loss -0.1060 (3.386 secs)\n",
      "np:np_periodic step 22200 lr 4.416e-04 [train_loss] loss -0.1705 (3.450 secs)\n",
      "np:np_periodic step 22400 lr 4.406e-04 [train_loss] loss -0.1357 (3.572 secs)\n",
      "np:np_periodic step 22600 lr 4.396e-04 [train_loss] loss -0.1337 (3.610 secs)\n",
      "np:np_periodic step 22800 lr 4.386e-04 [train_loss] loss -0.1470 (3.462 secs)\n",
      "np:np_periodic step 23000 lr 4.375e-04 [train_loss] loss -0.1481 (3.546 secs)\n",
      "np:np_periodic step 23200 lr 4.365e-04 [train_loss] loss -0.1138 (3.504 secs)\n",
      "np:np_periodic step 23400 lr 4.354e-04 [train_loss] loss -0.1932 (3.688 secs)\n",
      "np:np_periodic step 23600 lr 4.344e-04 [train_loss] loss -0.1870 (3.492 secs)\n",
      "np:np_periodic step 23800 lr 4.333e-04 [train_loss] loss -0.1541 (3.412 secs)\n",
      "np:np_periodic step 24000 lr 4.322e-04 [train_loss] loss -0.1630 (3.451 secs)\n",
      "np:np_periodic step 24200 lr 4.312e-04 [train_loss] loss -0.1204 (3.454 secs)\n",
      "np:np_periodic step 24400 lr 4.301e-04 [train_loss] loss -0.1659 (3.668 secs)\n",
      "np:np_periodic step 24600 lr 4.290e-04 [train_loss] loss -0.1814 (3.496 secs)\n",
      "np:np_periodic step 24800 lr 4.279e-04 [train_loss] loss -0.1742 (3.469 secs)\n",
      "np:np_periodic step 25000 lr 4.268e-04 [train_loss] loss -0.1904 (3.458 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 247.19it/s]\n",
      "np:np_periodic periodic ctx_ll -0.3320 tar_ll -0.9477 (12.139 secs)\n",
      "\n",
      "np:np_periodic step 25200 lr 4.257e-04 [train_loss] loss -0.1395 (3.523 secs)\n",
      "np:np_periodic step 25400 lr 4.245e-04 [train_loss] loss -0.1729 (3.475 secs)\n",
      "np:np_periodic step 25600 lr 4.234e-04 [train_loss] loss -0.1947 (3.495 secs)\n",
      "np:np_periodic step 25800 lr 4.223e-04 [train_loss] loss -0.1958 (3.438 secs)\n",
      "np:np_periodic step 26000 lr 4.211e-04 [train_loss] loss -0.2079 (3.416 secs)\n",
      "np:np_periodic step 26200 lr 4.200e-04 [train_loss] loss -0.1965 (3.479 secs)\n",
      "np:np_periodic step 26400 lr 4.188e-04 [train_loss] loss -0.1977 (3.442 secs)\n",
      "np:np_periodic step 26600 lr 4.177e-04 [train_loss] loss -0.2461 (3.541 secs)\n",
      "np:np_periodic step 26800 lr 4.165e-04 [train_loss] loss -0.1932 (3.504 secs)\n",
      "np:np_periodic step 27000 lr 4.153e-04 [train_loss] loss -0.2026 (3.419 secs)\n",
      "np:np_periodic step 27200 lr 4.141e-04 [train_loss] loss -0.2274 (3.541 secs)\n",
      "np:np_periodic step 27400 lr 4.130e-04 [train_loss] loss -0.2105 (3.526 secs)\n",
      "np:np_periodic step 27600 lr 4.118e-04 [train_loss] loss -0.2063 (3.437 secs)\n",
      "np:np_periodic step 27800 lr 4.106e-04 [train_loss] loss -0.1844 (3.384 secs)\n",
      "np:np_periodic step 28000 lr 4.094e-04 [train_loss] loss -0.1989 (3.349 secs)\n",
      "np:np_periodic step 28200 lr 4.081e-04 [train_loss] loss -0.1998 (3.528 secs)\n",
      "np:np_periodic step 28400 lr 4.069e-04 [train_loss] loss -0.2035 (3.457 secs)\n",
      "np:np_periodic step 28600 lr 4.057e-04 [train_loss] loss -0.2184 (3.461 secs)\n",
      "np:np_periodic step 28800 lr 4.045e-04 [train_loss] loss -0.2032 (3.529 secs)\n",
      "np:np_periodic step 29000 lr 4.032e-04 [train_loss] loss -0.2167 (3.413 secs)\n",
      "np:np_periodic step 29200 lr 4.020e-04 [train_loss] loss -0.2378 (3.699 secs)\n",
      "np:np_periodic step 29400 lr 4.007e-04 [train_loss] loss -0.2328 (3.492 secs)\n",
      "np:np_periodic step 29600 lr 3.995e-04 [train_loss] loss -0.2419 (3.454 secs)\n",
      "np:np_periodic step 29800 lr 3.982e-04 [train_loss] loss -0.2228 (3.346 secs)\n",
      "np:np_periodic step 30000 lr 3.969e-04 [train_loss] loss -0.2135 (3.493 secs)\n",
      "100%|##########| 3000/3000 [00:11<00:00, 251.91it/s]\n",
      "np:np_periodic periodic ctx_ll -0.3327 tar_ll -1.0333 (11.911 secs)\n",
      "\n",
      "np:np_periodic step 30200 lr 3.957e-04 [train_loss] loss -0.2546 (3.605 secs)\n",
      "np:np_periodic step 30400 lr 3.944e-04 [train_loss] loss -0.1919 (3.495 secs)\n",
      "np:np_periodic step 30600 lr 3.931e-04 [train_loss] loss -0.2228 (3.494 secs)\n",
      "np:np_periodic step 30800 lr 3.918e-04 [train_loss] loss -0.2123 (3.380 secs)\n",
      "np:np_periodic step 31000 lr 3.905e-04 [train_loss] loss -0.2692 (3.557 secs)\n",
      "np:np_periodic step 31200 lr 3.892e-04 [train_loss] loss -0.2669 (3.660 secs)\n",
      "np:np_periodic step 31400 lr 3.879e-04 [train_loss] loss -0.3033 (3.585 secs)\n",
      "np:np_periodic step 31600 lr 3.866e-04 [train_loss] loss -0.2631 (3.513 secs)\n",
      "np:np_periodic step 31800 lr 3.853e-04 [train_loss] loss -0.2722 (3.563 secs)\n",
      "np:np_periodic step 32000 lr 3.840e-04 [train_loss] loss -0.2810 (3.705 secs)\n",
      "np:np_periodic step 32200 lr 3.826e-04 [train_loss] loss -0.2820 (3.463 secs)\n",
      "np:np_periodic step 32400 lr 3.813e-04 [train_loss] loss -0.2505 (3.561 secs)\n",
      "np:np_periodic step 32600 lr 3.800e-04 [train_loss] loss -0.2710 (3.548 secs)\n",
      "np:np_periodic step 32800 lr 3.786e-04 [train_loss] loss -0.2444 (3.530 secs)\n",
      "np:np_periodic step 33000 lr 3.773e-04 [train_loss] loss -0.2598 (3.643 secs)\n",
      "np:np_periodic step 33200 lr 3.759e-04 [train_loss] loss -0.2807 (3.371 secs)\n",
      "np:np_periodic step 33400 lr 3.745e-04 [train_loss] loss -0.2686 (3.498 secs)\n",
      "np:np_periodic step 33600 lr 3.732e-04 [train_loss] loss -0.2699 (3.454 secs)\n",
      "np:np_periodic step 33800 lr 3.718e-04 [train_loss] loss -0.2516 (3.635 secs)\n",
      "np:np_periodic step 34000 lr 3.704e-04 [train_loss] loss -0.3034 (3.536 secs)\n",
      "np:np_periodic step 34200 lr 3.691e-04 [train_loss] loss -0.2804 (3.504 secs)\n",
      "np:np_periodic step 34400 lr 3.677e-04 [train_loss] loss -0.2553 (3.462 secs)\n",
      "np:np_periodic step 34600 lr 3.663e-04 [train_loss] loss -0.2880 (3.444 secs)\n",
      "np:np_periodic step 34800 lr 3.649e-04 [train_loss] loss -0.2813 (3.630 secs)\n",
      "np:np_periodic step 35000 lr 3.635e-04 [train_loss] loss -0.2584 (3.473 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 239.35it/s]\n",
      "np:np_periodic periodic ctx_ll -0.3170 tar_ll -1.1121 (12.536 secs)\n",
      "\n",
      "np:np_periodic step 35200 lr 3.621e-04 [train_loss] loss -0.2859 (3.456 secs)\n",
      "np:np_periodic step 35400 lr 3.607e-04 [train_loss] loss -0.2761 (3.580 secs)\n",
      "np:np_periodic step 35600 lr 3.593e-04 [train_loss] loss -0.2698 (3.389 secs)\n",
      "np:np_periodic step 35800 lr 3.579e-04 [train_loss] loss -0.2618 (3.445 secs)\n",
      "np:np_periodic step 36000 lr 3.564e-04 [train_loss] loss -0.2753 (3.533 secs)\n",
      "np:np_periodic step 36200 lr 3.550e-04 [train_loss] loss -0.2703 (3.416 secs)\n",
      "np:np_periodic step 36400 lr 3.536e-04 [train_loss] loss -0.2805 (3.259 secs)\n",
      "np:np_periodic step 36600 lr 3.522e-04 [train_loss] loss -0.2360 (3.635 secs)\n",
      "np:np_periodic step 36800 lr 3.507e-04 [train_loss] loss -0.2714 (3.588 secs)\n",
      "np:np_periodic step 37000 lr 3.493e-04 [train_loss] loss -0.2939 (3.309 secs)\n",
      "np:np_periodic step 37200 lr 3.478e-04 [train_loss] loss -0.2802 (3.432 secs)\n",
      "np:np_periodic step 37400 lr 3.464e-04 [train_loss] loss -0.2996 (3.359 secs)\n",
      "np:np_periodic step 37600 lr 3.449e-04 [train_loss] loss -0.2715 (3.561 secs)\n",
      "np:np_periodic step 37800 lr 3.435e-04 [train_loss] loss -0.3152 (3.457 secs)\n",
      "np:np_periodic step 38000 lr 3.420e-04 [train_loss] loss -0.3190 (3.476 secs)\n",
      "np:np_periodic step 38200 lr 3.406e-04 [train_loss] loss -0.2939 (3.314 secs)\n",
      "np:np_periodic step 38400 lr 3.391e-04 [train_loss] loss -0.3195 (3.395 secs)\n",
      "np:np_periodic step 38600 lr 3.376e-04 [train_loss] loss -0.2870 (3.586 secs)\n",
      "np:np_periodic step 38800 lr 3.362e-04 [train_loss] loss -0.2835 (3.340 secs)\n",
      "np:np_periodic step 39000 lr 3.347e-04 [train_loss] loss -0.2934 (3.361 secs)\n",
      "np:np_periodic step 39200 lr 3.332e-04 [train_loss] loss -0.3292 (3.284 secs)\n",
      "np:np_periodic step 39400 lr 3.317e-04 [train_loss] loss -0.3441 (3.474 secs)\n",
      "np:np_periodic step 39600 lr 3.302e-04 [train_loss] loss -0.2896 (3.561 secs)\n",
      "np:np_periodic step 39800 lr 3.287e-04 [train_loss] loss -0.2975 (3.358 secs)\n",
      "np:np_periodic step 40000 lr 3.273e-04 [train_loss] loss -0.3242 (3.446 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 248.21it/s]\n",
      "np:np_periodic periodic ctx_ll -0.3186 tar_ll -1.1579 (12.094 secs)\n",
      "\n",
      "np:np_periodic step 40200 lr 3.258e-04 [train_loss] loss -0.3254 (3.526 secs)\n",
      "np:np_periodic step 40400 lr 3.243e-04 [train_loss] loss -0.3189 (3.390 secs)\n",
      "np:np_periodic step 40600 lr 3.228e-04 [train_loss] loss -0.3476 (3.621 secs)\n",
      "np:np_periodic step 40800 lr 3.213e-04 [train_loss] loss -0.2652 (3.421 secs)\n",
      "np:np_periodic step 41000 lr 3.197e-04 [train_loss] loss -0.2900 (3.489 secs)\n",
      "np:np_periodic step 41200 lr 3.182e-04 [train_loss] loss -0.3488 (3.340 secs)\n",
      "np:np_periodic step 41400 lr 3.167e-04 [train_loss] loss -0.3434 (3.467 secs)\n",
      "np:np_periodic step 41600 lr 3.152e-04 [train_loss] loss -0.3408 (3.509 secs)\n",
      "np:np_periodic step 41800 lr 3.137e-04 [train_loss] loss -0.3279 (3.506 secs)\n",
      "np:np_periodic step 42000 lr 3.122e-04 [train_loss] loss -0.2896 (3.429 secs)\n",
      "np:np_periodic step 42200 lr 3.106e-04 [train_loss] loss -0.3342 (3.679 secs)\n",
      "np:np_periodic step 42400 lr 3.091e-04 [train_loss] loss -0.3442 (3.771 secs)\n",
      "np:np_periodic step 42600 lr 3.076e-04 [train_loss] loss -0.3331 (3.480 secs)\n",
      "np:np_periodic step 42800 lr 3.061e-04 [train_loss] loss -0.2658 (3.576 secs)\n",
      "np:np_periodic step 43000 lr 3.045e-04 [train_loss] loss -0.3487 (3.617 secs)\n",
      "np:np_periodic step 43200 lr 3.030e-04 [train_loss] loss -0.3238 (3.637 secs)\n",
      "np:np_periodic step 43400 lr 3.015e-04 [train_loss] loss -0.3339 (3.454 secs)\n",
      "np:np_periodic step 43600 lr 2.999e-04 [train_loss] loss -0.3417 (3.422 secs)\n",
      "np:np_periodic step 43800 lr 2.984e-04 [train_loss] loss -0.3290 (3.686 secs)\n",
      "np:np_periodic step 44000 lr 2.968e-04 [train_loss] loss -0.3332 (3.495 secs)\n",
      "np:np_periodic step 44200 lr 2.953e-04 [train_loss] loss -0.2925 (3.743 secs)\n",
      "np:np_periodic step 44400 lr 2.938e-04 [train_loss] loss -0.3376 (3.394 secs)\n",
      "np:np_periodic step 44600 lr 2.922e-04 [train_loss] loss -0.3464 (3.439 secs)\n",
      "np:np_periodic step 44800 lr 2.907e-04 [train_loss] loss -0.3589 (3.499 secs)\n",
      "np:np_periodic step 45000 lr 2.891e-04 [train_loss] loss -0.3375 (3.519 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 247.61it/s]\n",
      "np:np_periodic periodic ctx_ll -0.3025 tar_ll -1.1103 (12.120 secs)\n",
      "\n",
      "np:np_periodic step 45200 lr 2.876e-04 [train_loss] loss -0.3425 (3.655 secs)\n",
      "np:np_periodic step 45400 lr 2.860e-04 [train_loss] loss -0.3621 (3.538 secs)\n",
      "np:np_periodic step 45600 lr 2.844e-04 [train_loss] loss -0.3630 (3.438 secs)\n",
      "np:np_periodic step 45800 lr 2.829e-04 [train_loss] loss -0.3088 (3.567 secs)\n",
      "np:np_periodic step 46000 lr 2.813e-04 [train_loss] loss -0.3542 (3.406 secs)\n",
      "np:np_periodic step 46200 lr 2.798e-04 [train_loss] loss -0.3208 (3.606 secs)\n",
      "np:np_periodic step 46400 lr 2.782e-04 [train_loss] loss -0.3006 (3.517 secs)\n",
      "np:np_periodic step 46600 lr 2.767e-04 [train_loss] loss -0.3603 (3.534 secs)\n",
      "np:np_periodic step 46800 lr 2.751e-04 [train_loss] loss -0.3658 (3.450 secs)\n",
      "np:np_periodic step 47000 lr 2.735e-04 [train_loss] loss -0.3646 (3.544 secs)\n",
      "np:np_periodic step 47200 lr 2.720e-04 [train_loss] loss -0.3799 (3.535 secs)\n",
      "np:np_periodic step 47400 lr 2.704e-04 [train_loss] loss -0.3426 (3.446 secs)\n",
      "np:np_periodic step 47600 lr 2.688e-04 [train_loss] loss -0.3331 (3.405 secs)\n",
      "np:np_periodic step 47800 lr 2.673e-04 [train_loss] loss -0.3521 (3.418 secs)\n",
      "np:np_periodic step 48000 lr 2.657e-04 [train_loss] loss -0.3438 (3.551 secs)\n",
      "np:np_periodic step 48200 lr 2.641e-04 [train_loss] loss -0.3722 (3.423 secs)\n",
      "np:np_periodic step 48400 lr 2.626e-04 [train_loss] loss -0.3394 (3.410 secs)\n",
      "np:np_periodic step 48600 lr 2.610e-04 [train_loss] loss -0.3606 (3.443 secs)\n",
      "np:np_periodic step 48800 lr 2.594e-04 [train_loss] loss -0.3917 (3.423 secs)\n",
      "np:np_periodic step 49000 lr 2.579e-04 [train_loss] loss -0.3518 (3.379 secs)\n",
      "np:np_periodic step 49200 lr 2.563e-04 [train_loss] loss -0.3428 (3.648 secs)\n",
      "np:np_periodic step 49400 lr 2.547e-04 [train_loss] loss -0.3764 (3.454 secs)\n",
      "np:np_periodic step 49600 lr 2.531e-04 [train_loss] loss -0.3412 (3.595 secs)\n",
      "np:np_periodic step 49800 lr 2.516e-04 [train_loss] loss -0.3669 (3.716 secs)\n",
      "np:np_periodic step 50000 lr 2.500e-04 [train_loss] loss -0.3830 (3.520 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 246.08it/s]\n",
      "np:np_periodic periodic ctx_ll -0.2998 tar_ll -1.2609 (12.194 secs)\n",
      "\n",
      "np:np_periodic step 50200 lr 2.484e-04 [train_loss] loss -0.3400 (3.527 secs)\n",
      "np:np_periodic step 50400 lr 2.469e-04 [train_loss] loss -0.3361 (3.462 secs)\n",
      "np:np_periodic step 50600 lr 2.453e-04 [train_loss] loss -0.3349 (3.514 secs)\n",
      "np:np_periodic step 50800 lr 2.437e-04 [train_loss] loss -0.3776 (3.693 secs)\n",
      "np:np_periodic step 51000 lr 2.421e-04 [train_loss] loss -0.3564 (3.430 secs)\n",
      "np:np_periodic step 51200 lr 2.406e-04 [train_loss] loss -0.3369 (3.386 secs)\n",
      "np:np_periodic step 51400 lr 2.390e-04 [train_loss] loss -0.3688 (3.498 secs)\n",
      "np:np_periodic step 51600 lr 2.374e-04 [train_loss] loss -0.4043 (3.538 secs)\n",
      "np:np_periodic step 51800 lr 2.359e-04 [train_loss] loss -0.3369 (3.602 secs)\n",
      "np:np_periodic step 52000 lr 2.343e-04 [train_loss] loss -0.3660 (3.508 secs)\n",
      "np:np_periodic step 52200 lr 2.327e-04 [train_loss] loss -0.3721 (3.584 secs)\n",
      "np:np_periodic step 52400 lr 2.312e-04 [train_loss] loss -0.3840 (3.448 secs)\n",
      "np:np_periodic step 52600 lr 2.296e-04 [train_loss] loss -0.3623 (3.653 secs)\n",
      "np:np_periodic step 52800 lr 2.280e-04 [train_loss] loss -0.3369 (3.501 secs)\n",
      "np:np_periodic step 53000 lr 2.265e-04 [train_loss] loss -0.3749 (3.411 secs)\n",
      "np:np_periodic step 53200 lr 2.249e-04 [train_loss] loss -0.3502 (3.397 secs)\n",
      "np:np_periodic step 53400 lr 2.233e-04 [train_loss] loss -0.3863 (3.400 secs)\n",
      "np:np_periodic step 53600 lr 2.218e-04 [train_loss] loss -0.3950 (3.642 secs)\n",
      "np:np_periodic step 53800 lr 2.202e-04 [train_loss] loss -0.3800 (3.417 secs)\n",
      "np:np_periodic step 54000 lr 2.187e-04 [train_loss] loss -0.3968 (3.542 secs)\n",
      "np:np_periodic step 54200 lr 2.171e-04 [train_loss] loss -0.3766 (3.592 secs)\n",
      "np:np_periodic step 54400 lr 2.156e-04 [train_loss] loss -0.3784 (3.602 secs)\n",
      "np:np_periodic step 54600 lr 2.140e-04 [train_loss] loss -0.3915 (3.693 secs)\n",
      "np:np_periodic step 54800 lr 2.124e-04 [train_loss] loss -0.3935 (3.498 secs)\n",
      "np:np_periodic step 55000 lr 2.109e-04 [train_loss] loss -0.3745 (3.634 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 235.74it/s]\n",
      "np:np_periodic periodic ctx_ll -0.2953 tar_ll -1.2573 (12.729 secs)\n",
      "\n",
      "np:np_periodic step 55200 lr 2.093e-04 [train_loss] loss -0.4209 (3.585 secs)\n",
      "np:np_periodic step 55400 lr 2.078e-04 [train_loss] loss -0.3890 (3.633 secs)\n",
      "np:np_periodic step 55600 lr 2.062e-04 [train_loss] loss -0.3810 (3.595 secs)\n",
      "np:np_periodic step 55800 lr 2.047e-04 [train_loss] loss -0.3771 (3.567 secs)\n",
      "np:np_periodic step 56000 lr 2.032e-04 [train_loss] loss -0.4270 (3.553 secs)\n",
      "np:np_periodic step 56200 lr 2.016e-04 [train_loss] loss -0.4047 (3.732 secs)\n",
      "np:np_periodic step 56400 lr 2.001e-04 [train_loss] loss -0.3907 (3.613 secs)\n",
      "np:np_periodic step 56600 lr 1.985e-04 [train_loss] loss -0.4121 (3.556 secs)\n",
      "np:np_periodic step 56800 lr 1.970e-04 [train_loss] loss -0.3563 (3.498 secs)\n",
      "np:np_periodic step 57000 lr 1.955e-04 [train_loss] loss -0.3499 (3.620 secs)\n",
      "np:np_periodic step 57200 lr 1.939e-04 [train_loss] loss -0.4127 (3.554 secs)\n",
      "np:np_periodic step 57400 lr 1.924e-04 [train_loss] loss -0.3990 (3.592 secs)\n",
      "np:np_periodic step 57600 lr 1.909e-04 [train_loss] loss -0.4172 (3.488 secs)\n",
      "np:np_periodic step 57800 lr 1.894e-04 [train_loss] loss -0.4056 (3.533 secs)\n",
      "np:np_periodic step 58000 lr 1.878e-04 [train_loss] loss -0.3967 (3.623 secs)\n",
      "np:np_periodic step 58200 lr 1.863e-04 [train_loss] loss -0.3982 (3.736 secs)\n",
      "np:np_periodic step 58400 lr 1.848e-04 [train_loss] loss -0.3955 (3.487 secs)\n",
      "np:np_periodic step 58600 lr 1.833e-04 [train_loss] loss -0.3920 (3.578 secs)\n",
      "np:np_periodic step 58800 lr 1.818e-04 [train_loss] loss -0.3883 (3.520 secs)\n",
      "np:np_periodic step 59000 lr 1.803e-04 [train_loss] loss -0.4154 (3.788 secs)\n",
      "np:np_periodic step 59200 lr 1.787e-04 [train_loss] loss -0.3865 (3.601 secs)\n",
      "np:np_periodic step 59400 lr 1.772e-04 [train_loss] loss -0.4153 (3.593 secs)\n",
      "np:np_periodic step 59600 lr 1.757e-04 [train_loss] loss -0.3902 (3.503 secs)\n",
      "np:np_periodic step 59800 lr 1.742e-04 [train_loss] loss -0.3682 (3.566 secs)\n",
      "np:np_periodic step 60000 lr 1.727e-04 [train_loss] loss -0.3682 (3.450 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 242.17it/s]\n",
      "np:np_periodic periodic ctx_ll -0.2772 tar_ll -1.0893 (12.390 secs)\n",
      "\n",
      "np:np_periodic step 60200 lr 1.713e-04 [train_loss] loss -0.3825 (3.479 secs)\n",
      "np:np_periodic step 60400 lr 1.698e-04 [train_loss] loss -0.4260 (3.355 secs)\n",
      "np:np_periodic step 60600 lr 1.683e-04 [train_loss] loss -0.3680 (3.470 secs)\n",
      "np:np_periodic step 60800 lr 1.668e-04 [train_loss] loss -0.4314 (3.330 secs)\n",
      "np:np_periodic step 61000 lr 1.653e-04 [train_loss] loss -0.4252 (3.448 secs)\n",
      "np:np_periodic step 61200 lr 1.638e-04 [train_loss] loss -0.3969 (3.430 secs)\n",
      "np:np_periodic step 61400 lr 1.624e-04 [train_loss] loss -0.4442 (3.399 secs)\n",
      "np:np_periodic step 61600 lr 1.609e-04 [train_loss] loss -0.4119 (3.274 secs)\n",
      "np:np_periodic step 61800 lr 1.594e-04 [train_loss] loss -0.3990 (3.384 secs)\n",
      "np:np_periodic step 62000 lr 1.580e-04 [train_loss] loss -0.4478 (3.729 secs)\n",
      "np:np_periodic step 62200 lr 1.565e-04 [train_loss] loss -0.4021 (3.338 secs)\n",
      "np:np_periodic step 62400 lr 1.551e-04 [train_loss] loss -0.4373 (3.426 secs)\n",
      "np:np_periodic step 62600 lr 1.536e-04 [train_loss] loss -0.4405 (3.404 secs)\n",
      "np:np_periodic step 62800 lr 1.522e-04 [train_loss] loss -0.4081 (3.399 secs)\n",
      "np:np_periodic step 63000 lr 1.507e-04 [train_loss] loss -0.4280 (3.444 secs)\n",
      "np:np_periodic step 63200 lr 1.493e-04 [train_loss] loss -0.4078 (3.419 secs)\n",
      "np:np_periodic step 63400 lr 1.478e-04 [train_loss] loss -0.3681 (3.381 secs)\n",
      "np:np_periodic step 63600 lr 1.464e-04 [train_loss] loss -0.4024 (3.382 secs)\n",
      "np:np_periodic step 63800 lr 1.450e-04 [train_loss] loss -0.3908 (3.599 secs)\n",
      "np:np_periodic step 64000 lr 1.436e-04 [train_loss] loss -0.4573 (3.655 secs)\n",
      "np:np_periodic step 64200 lr 1.421e-04 [train_loss] loss -0.4024 (3.458 secs)\n",
      "np:np_periodic step 64400 lr 1.407e-04 [train_loss] loss -0.4182 (3.401 secs)\n",
      "np:np_periodic step 64600 lr 1.393e-04 [train_loss] loss -0.4193 (3.400 secs)\n",
      "np:np_periodic step 64800 lr 1.379e-04 [train_loss] loss -0.3822 (3.383 secs)\n",
      "np:np_periodic step 65000 lr 1.365e-04 [train_loss] loss -0.4264 (3.393 secs)\n",
      "100%|##########| 3000/3000 [00:11<00:00, 251.95it/s]\n",
      "np:np_periodic periodic ctx_ll -0.2695 tar_ll -1.1729 (11.909 secs)\n",
      "\n",
      "np:np_periodic step 65200 lr 1.351e-04 [train_loss] loss -0.4405 (3.485 secs)\n",
      "np:np_periodic step 65400 lr 1.337e-04 [train_loss] loss -0.4396 (3.522 secs)\n",
      "np:np_periodic step 65600 lr 1.323e-04 [train_loss] loss -0.3872 (3.423 secs)\n",
      "np:np_periodic step 65800 lr 1.309e-04 [train_loss] loss -0.4339 (3.568 secs)\n",
      "np:np_periodic step 66000 lr 1.296e-04 [train_loss] loss -0.4294 (3.517 secs)\n",
      "np:np_periodic step 66200 lr 1.282e-04 [train_loss] loss -0.4088 (3.603 secs)\n",
      "np:np_periodic step 66400 lr 1.268e-04 [train_loss] loss -0.4697 (3.429 secs)\n",
      "np:np_periodic step 66600 lr 1.255e-04 [train_loss] loss -0.4264 (3.373 secs)\n",
      "np:np_periodic step 66800 lr 1.241e-04 [train_loss] loss -0.4327 (3.643 secs)\n",
      "np:np_periodic step 67000 lr 1.227e-04 [train_loss] loss -0.4214 (3.590 secs)\n",
      "np:np_periodic step 67200 lr 1.214e-04 [train_loss] loss -0.4413 (3.593 secs)\n",
      "np:np_periodic step 67400 lr 1.200e-04 [train_loss] loss -0.4484 (3.602 secs)\n",
      "np:np_periodic step 67600 lr 1.187e-04 [train_loss] loss -0.3929 (3.593 secs)\n",
      "np:np_periodic step 67800 lr 1.174e-04 [train_loss] loss -0.4155 (3.557 secs)\n",
      "np:np_periodic step 68000 lr 1.160e-04 [train_loss] loss -0.4507 (3.544 secs)\n",
      "np:np_periodic step 68200 lr 1.147e-04 [train_loss] loss -0.4042 (3.635 secs)\n",
      "np:np_periodic step 68400 lr 1.134e-04 [train_loss] loss -0.4430 (3.500 secs)\n",
      "np:np_periodic step 68600 lr 1.121e-04 [train_loss] loss -0.4429 (3.603 secs)\n",
      "np:np_periodic step 68800 lr 1.108e-04 [train_loss] loss -0.4778 (3.662 secs)\n",
      "np:np_periodic step 69000 lr 1.095e-04 [train_loss] loss -0.4387 (3.473 secs)\n",
      "np:np_periodic step 69200 lr 1.082e-04 [train_loss] loss -0.4131 (3.468 secs)\n",
      "np:np_periodic step 69400 lr 1.069e-04 [train_loss] loss -0.4502 (3.689 secs)\n",
      "np:np_periodic step 69600 lr 1.056e-04 [train_loss] loss -0.4513 (3.550 secs)\n",
      "np:np_periodic step 69800 lr 1.043e-04 [train_loss] loss -0.4128 (3.538 secs)\n",
      "np:np_periodic step 70000 lr 1.031e-04 [train_loss] loss -0.4444 (3.496 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 238.37it/s]\n",
      "np:np_periodic periodic ctx_ll -0.2706 tar_ll -1.2526 (12.588 secs)\n",
      "\n",
      "np:np_periodic step 70200 lr 1.018e-04 [train_loss] loss -0.4100 (3.423 secs)\n",
      "np:np_periodic step 70400 lr 1.005e-04 [train_loss] loss -0.3995 (3.672 secs)\n",
      "np:np_periodic step 70600 lr 9.927e-05 [train_loss] loss -0.4367 (3.397 secs)\n",
      "np:np_periodic step 70800 lr 9.802e-05 [train_loss] loss -0.4806 (3.548 secs)\n",
      "np:np_periodic step 71000 lr 9.677e-05 [train_loss] loss -0.4437 (3.531 secs)\n",
      "np:np_periodic step 71200 lr 9.554e-05 [train_loss] loss -0.4574 (3.451 secs)\n",
      "np:np_periodic step 71400 lr 9.430e-05 [train_loss] loss -0.4811 (3.522 secs)\n",
      "np:np_periodic step 71600 lr 9.308e-05 [train_loss] loss -0.4041 (3.500 secs)\n",
      "np:np_periodic step 71800 lr 9.186e-05 [train_loss] loss -0.4335 (3.536 secs)\n",
      "np:np_periodic step 72000 lr 9.064e-05 [train_loss] loss -0.4776 (3.425 secs)\n",
      "np:np_periodic step 72200 lr 8.944e-05 [train_loss] loss -0.4523 (3.485 secs)\n",
      "np:np_periodic step 72400 lr 8.824e-05 [train_loss] loss -0.4175 (3.607 secs)\n",
      "np:np_periodic step 72600 lr 8.704e-05 [train_loss] loss -0.4774 (3.527 secs)\n",
      "np:np_periodic step 72800 lr 8.585e-05 [train_loss] loss -0.4465 (3.461 secs)\n",
      "np:np_periodic step 73000 lr 8.467e-05 [train_loss] loss -0.4395 (3.439 secs)\n",
      "np:np_periodic step 73200 lr 8.350e-05 [train_loss] loss -0.4127 (3.661 secs)\n",
      "np:np_periodic step 73400 lr 8.233e-05 [train_loss] loss -0.4387 (3.558 secs)\n",
      "np:np_periodic step 73600 lr 8.117e-05 [train_loss] loss -0.4870 (3.487 secs)\n",
      "np:np_periodic step 73800 lr 8.001e-05 [train_loss] loss -0.4490 (3.561 secs)\n",
      "np:np_periodic step 74000 lr 7.886e-05 [train_loss] loss -0.4634 (3.580 secs)\n",
      "np:np_periodic step 74200 lr 7.772e-05 [train_loss] loss -0.4465 (3.548 secs)\n",
      "np:np_periodic step 74400 lr 7.659e-05 [train_loss] loss -0.4613 (3.533 secs)\n",
      "np:np_periodic step 74600 lr 7.546e-05 [train_loss] loss -0.4535 (3.356 secs)\n",
      "np:np_periodic step 74800 lr 7.434e-05 [train_loss] loss -0.4434 (3.508 secs)\n",
      "np:np_periodic step 75000 lr 7.322e-05 [train_loss] loss -0.4615 (3.448 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 246.96it/s]\n",
      "np:np_periodic periodic ctx_ll -0.2620 tar_ll -1.0381 (12.150 secs)\n",
      "\n",
      "np:np_periodic step 75200 lr 7.212e-05 [train_loss] loss -0.4439 (3.473 secs)\n",
      "np:np_periodic step 75400 lr 7.102e-05 [train_loss] loss -0.4908 (3.389 secs)\n",
      "np:np_periodic step 75600 lr 6.992e-05 [train_loss] loss -0.4459 (3.477 secs)\n",
      "np:np_periodic step 75800 lr 6.884e-05 [train_loss] loss -0.4472 (3.504 secs)\n",
      "np:np_periodic step 76000 lr 6.776e-05 [train_loss] loss -0.4351 (3.572 secs)\n",
      "np:np_periodic step 76200 lr 6.669e-05 [train_loss] loss -0.4674 (3.434 secs)\n",
      "np:np_periodic step 76400 lr 6.562e-05 [train_loss] loss -0.4708 (3.529 secs)\n",
      "np:np_periodic step 76600 lr 6.456e-05 [train_loss] loss -0.4615 (3.456 secs)\n",
      "np:np_periodic step 76800 lr 6.351e-05 [train_loss] loss -0.4753 (3.533 secs)\n",
      "np:np_periodic step 77000 lr 6.247e-05 [train_loss] loss -0.4151 (3.476 secs)\n",
      "np:np_periodic step 77200 lr 6.144e-05 [train_loss] loss -0.4332 (3.435 secs)\n",
      "np:np_periodic step 77400 lr 6.041e-05 [train_loss] loss -0.4530 (3.465 secs)\n",
      "np:np_periodic step 77600 lr 5.939e-05 [train_loss] loss -0.4650 (3.564 secs)\n",
      "np:np_periodic step 77800 lr 5.838e-05 [train_loss] loss -0.4419 (3.506 secs)\n",
      "np:np_periodic step 78000 lr 5.737e-05 [train_loss] loss -0.4760 (3.694 secs)\n",
      "np:np_periodic step 78200 lr 5.637e-05 [train_loss] loss -0.4914 (3.577 secs)\n",
      "np:np_periodic step 78400 lr 5.538e-05 [train_loss] loss -0.3950 (3.471 secs)\n",
      "np:np_periodic step 78600 lr 5.440e-05 [train_loss] loss -0.4536 (3.517 secs)\n",
      "np:np_periodic step 78800 lr 5.343e-05 [train_loss] loss -0.4434 (3.766 secs)\n",
      "np:np_periodic step 79000 lr 5.246e-05 [train_loss] loss -0.4234 (3.507 secs)\n",
      "np:np_periodic step 79200 lr 5.150e-05 [train_loss] loss -0.4742 (3.381 secs)\n",
      "np:np_periodic step 79400 lr 5.055e-05 [train_loss] loss -0.4649 (3.500 secs)\n",
      "np:np_periodic step 79600 lr 4.961e-05 [train_loss] loss -0.4563 (3.509 secs)\n",
      "np:np_periodic step 79800 lr 4.867e-05 [train_loss] loss -0.4597 (3.579 secs)\n",
      "np:np_periodic step 80000 lr 4.775e-05 [train_loss] loss -0.4762 (3.544 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 237.53it/s]\n",
      "np:np_periodic periodic ctx_ll -0.2570 tar_ll -1.1621 (12.632 secs)\n",
      "\n",
      "np:np_periodic step 80200 lr 4.683e-05 [train_loss] loss -0.4432 (3.446 secs)\n",
      "np:np_periodic step 80400 lr 4.592e-05 [train_loss] loss -0.4550 (3.340 secs)\n",
      "np:np_periodic step 80600 lr 4.501e-05 [train_loss] loss -0.4951 (3.487 secs)\n",
      "np:np_periodic step 80800 lr 4.412e-05 [train_loss] loss -0.4821 (3.668 secs)\n",
      "np:np_periodic step 81000 lr 4.323e-05 [train_loss] loss -0.4345 (3.411 secs)\n",
      "np:np_periodic step 81200 lr 4.235e-05 [train_loss] loss -0.4673 (3.413 secs)\n",
      "np:np_periodic step 81400 lr 4.148e-05 [train_loss] loss -0.4717 (3.521 secs)\n",
      "np:np_periodic step 81600 lr 4.062e-05 [train_loss] loss -0.4870 (3.630 secs)\n",
      "np:np_periodic step 81800 lr 3.976e-05 [train_loss] loss -0.4903 (3.520 secs)\n",
      "np:np_periodic step 82000 lr 3.892e-05 [train_loss] loss -0.4872 (3.559 secs)\n",
      "np:np_periodic step 82200 lr 3.808e-05 [train_loss] loss -0.4553 (3.626 secs)\n",
      "np:np_periodic step 82400 lr 3.725e-05 [train_loss] loss -0.4938 (3.480 secs)\n",
      "np:np_periodic step 82600 lr 3.643e-05 [train_loss] loss -0.4345 (3.705 secs)\n",
      "np:np_periodic step 82800 lr 3.562e-05 [train_loss] loss -0.5127 (3.541 secs)\n",
      "np:np_periodic step 83000 lr 3.481e-05 [train_loss] loss -0.4532 (3.391 secs)\n",
      "np:np_periodic step 83200 lr 3.402e-05 [train_loss] loss -0.4531 (3.467 secs)\n",
      "np:np_periodic step 83400 lr 3.323e-05 [train_loss] loss -0.4518 (3.494 secs)\n",
      "np:np_periodic step 83600 lr 3.245e-05 [train_loss] loss -0.4535 (3.675 secs)\n",
      "np:np_periodic step 83800 lr 3.168e-05 [train_loss] loss -0.4721 (3.478 secs)\n",
      "np:np_periodic step 84000 lr 3.092e-05 [train_loss] loss -0.4587 (3.449 secs)\n",
      "np:np_periodic step 84200 lr 3.017e-05 [train_loss] loss -0.4831 (3.452 secs)\n",
      "np:np_periodic step 84400 lr 2.943e-05 [train_loss] loss -0.5045 (3.525 secs)\n",
      "np:np_periodic step 84600 lr 2.869e-05 [train_loss] loss -0.5003 (3.377 secs)\n",
      "np:np_periodic step 84800 lr 2.797e-05 [train_loss] loss -0.4641 (3.592 secs)\n",
      "np:np_periodic step 85000 lr 2.725e-05 [train_loss] loss -0.4569 (3.606 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 240.53it/s]\n",
      "np:np_periodic periodic ctx_ll -0.2568 tar_ll -1.2060 (12.475 secs)\n",
      "\n",
      "np:np_periodic step 85200 lr 2.654e-05 [train_loss] loss -0.4888 (3.443 secs)\n",
      "np:np_periodic step 85400 lr 2.584e-05 [train_loss] loss -0.4998 (3.546 secs)\n",
      "np:np_periodic step 85600 lr 2.515e-05 [train_loss] loss -0.4773 (3.593 secs)\n",
      "np:np_periodic step 85800 lr 2.447e-05 [train_loss] loss -0.4869 (3.443 secs)\n",
      "np:np_periodic step 86000 lr 2.379e-05 [train_loss] loss -0.4772 (3.590 secs)\n",
      "np:np_periodic step 86200 lr 2.313e-05 [train_loss] loss -0.5466 (3.568 secs)\n",
      "np:np_periodic step 86400 lr 2.247e-05 [train_loss] loss -0.4550 (3.731 secs)\n",
      "np:np_periodic step 86600 lr 2.183e-05 [train_loss] loss -0.4756 (3.414 secs)\n",
      "np:np_periodic step 86800 lr 2.119e-05 [train_loss] loss -0.4689 (3.450 secs)\n",
      "np:np_periodic step 87000 lr 2.056e-05 [train_loss] loss -0.4468 (3.527 secs)\n",
      "np:np_periodic step 87200 lr 1.994e-05 [train_loss] loss -0.4222 (3.638 secs)\n",
      "np:np_periodic step 87400 lr 1.933e-05 [train_loss] loss -0.4846 (3.394 secs)\n",
      "np:np_periodic step 87600 lr 1.873e-05 [train_loss] loss -0.4240 (3.387 secs)\n",
      "np:np_periodic step 87800 lr 1.814e-05 [train_loss] loss -0.4978 (3.422 secs)\n",
      "np:np_periodic step 88000 lr 1.756e-05 [train_loss] loss -0.4766 (3.287 secs)\n",
      "np:np_periodic step 88200 lr 1.698e-05 [train_loss] loss -0.4665 (3.467 secs)\n",
      "np:np_periodic step 88400 lr 1.642e-05 [train_loss] loss -0.4868 (3.498 secs)\n",
      "np:np_periodic step 88600 lr 1.586e-05 [train_loss] loss -0.4968 (3.395 secs)\n",
      "np:np_periodic step 88800 lr 1.532e-05 [train_loss] loss -0.4565 (3.247 secs)\n",
      "np:np_periodic step 89000 lr 1.478e-05 [train_loss] loss -0.5083 (3.432 secs)\n",
      "np:np_periodic step 89200 lr 1.425e-05 [train_loss] loss -0.4853 (3.512 secs)\n",
      "np:np_periodic step 89400 lr 1.373e-05 [train_loss] loss -0.4697 (3.549 secs)\n",
      "np:np_periodic step 89600 lr 1.323e-05 [train_loss] loss -0.4524 (3.453 secs)\n",
      "np:np_periodic step 89800 lr 1.273e-05 [train_loss] loss -0.4471 (3.452 secs)\n",
      "np:np_periodic step 90000 lr 1.224e-05 [train_loss] loss -0.4585 (3.630 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 245.30it/s]\n",
      "np:np_periodic periodic ctx_ll -0.2531 tar_ll -1.1445 (12.232 secs)\n",
      "\n",
      "np:np_periodic step 90200 lr 1.176e-05 [train_loss] loss -0.4941 (3.647 secs)\n",
      "np:np_periodic step 90400 lr 1.128e-05 [train_loss] loss -0.4708 (3.623 secs)\n",
      "np:np_periodic step 90600 lr 1.082e-05 [train_loss] loss -0.4880 (3.617 secs)\n",
      "np:np_periodic step 90800 lr 1.037e-05 [train_loss] loss -0.4692 (3.571 secs)\n",
      "np:np_periodic step 91000 lr 9.927e-06 [train_loss] loss -0.5003 (3.676 secs)\n",
      "np:np_periodic step 91200 lr 9.493e-06 [train_loss] loss -0.4586 (3.573 secs)\n",
      "np:np_periodic step 91400 lr 9.069e-06 [train_loss] loss -0.4965 (3.545 secs)\n",
      "np:np_periodic step 91600 lr 8.655e-06 [train_loss] loss -0.5227 (3.566 secs)\n",
      "np:np_periodic step 91800 lr 8.250e-06 [train_loss] loss -0.5000 (3.567 secs)\n",
      "np:np_periodic step 92000 lr 7.854e-06 [train_loss] loss -0.4562 (3.636 secs)\n",
      "np:np_periodic step 92200 lr 7.468e-06 [train_loss] loss -0.4873 (3.470 secs)\n",
      "np:np_periodic step 92400 lr 7.092e-06 [train_loss] loss -0.5050 (3.432 secs)\n",
      "np:np_periodic step 92600 lr 6.725e-06 [train_loss] loss -0.5080 (3.485 secs)\n",
      "np:np_periodic step 92800 lr 6.368e-06 [train_loss] loss -0.4843 (3.747 secs)\n",
      "np:np_periodic step 93000 lr 6.021e-06 [train_loss] loss -0.4621 (3.385 secs)\n",
      "np:np_periodic step 93200 lr 5.683e-06 [train_loss] loss -0.4754 (3.379 secs)\n",
      "np:np_periodic step 93400 lr 5.355e-06 [train_loss] loss -0.4825 (3.528 secs)\n",
      "np:np_periodic step 93600 lr 5.036e-06 [train_loss] loss -0.5134 (3.598 secs)\n",
      "np:np_periodic step 93800 lr 4.727e-06 [train_loss] loss -0.5145 (3.596 secs)\n",
      "np:np_periodic step 94000 lr 4.428e-06 [train_loss] loss -0.4884 (3.436 secs)\n",
      "np:np_periodic step 94200 lr 4.139e-06 [train_loss] loss -0.4844 (3.494 secs)\n",
      "np:np_periodic step 94400 lr 3.859e-06 [train_loss] loss -0.4667 (3.817 secs)\n",
      "np:np_periodic step 94600 lr 3.589e-06 [train_loss] loss -0.4760 (3.655 secs)\n",
      "np:np_periodic step 94800 lr 3.329e-06 [train_loss] loss -0.4557 (3.706 secs)\n",
      "np:np_periodic step 95000 lr 3.078e-06 [train_loss] loss -0.4959 (3.595 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 234.94it/s]\n",
      "np:np_periodic periodic ctx_ll -0.2527 tar_ll -1.1897 (12.772 secs)\n",
      "\n",
      "np:np_periodic step 95200 lr 2.837e-06 [train_loss] loss -0.4631 (3.358 secs)\n",
      "np:np_periodic step 95400 lr 2.606e-06 [train_loss] loss -0.4548 (3.591 secs)\n",
      "np:np_periodic step 95600 lr 2.385e-06 [train_loss] loss -0.4867 (3.726 secs)\n",
      "np:np_periodic step 95800 lr 2.173e-06 [train_loss] loss -0.5025 (3.445 secs)\n",
      "np:np_periodic step 96000 lr 1.971e-06 [train_loss] loss -0.4588 (3.440 secs)\n",
      "np:np_periodic step 96200 lr 1.779e-06 [train_loss] loss -0.4803 (3.550 secs)\n",
      "np:np_periodic step 96400 lr 1.597e-06 [train_loss] loss -0.4662 (3.330 secs)\n",
      "np:np_periodic step 96600 lr 1.425e-06 [train_loss] loss -0.4904 (3.368 secs)\n",
      "np:np_periodic step 96800 lr 1.262e-06 [train_loss] loss -0.4713 (3.425 secs)\n",
      "np:np_periodic step 97000 lr 1.110e-06 [train_loss] loss -0.5161 (3.427 secs)\n",
      "np:np_periodic step 97200 lr 9.666e-07 [train_loss] loss -0.5023 (3.475 secs)\n",
      "np:np_periodic step 97400 lr 8.335e-07 [train_loss] loss -0.4996 (3.529 secs)\n",
      "np:np_periodic step 97600 lr 7.103e-07 [train_loss] loss -0.4926 (3.449 secs)\n",
      "np:np_periodic step 97800 lr 5.969e-07 [train_loss] loss -0.4645 (3.382 secs)\n",
      "np:np_periodic step 98000 lr 4.933e-07 [train_loss] loss -0.4279 (3.346 secs)\n",
      "np:np_periodic step 98200 lr 3.996e-07 [train_loss] loss -0.5052 (3.493 secs)\n",
      "np:np_periodic step 98400 lr 3.158e-07 [train_loss] loss -0.4700 (3.541 secs)\n",
      "np:np_periodic step 98600 lr 2.418e-07 [train_loss] loss -0.4679 (3.502 secs)\n",
      "np:np_periodic step 98800 lr 1.776e-07 [train_loss] loss -0.4874 (3.346 secs)\n",
      "np:np_periodic step 99000 lr 1.234e-07 [train_loss] loss -0.4503 (3.368 secs)\n",
      "np:np_periodic step 99200 lr 7.895e-08 [train_loss] loss -0.4622 (3.450 secs)\n",
      "np:np_periodic step 99400 lr 4.441e-08 [train_loss] loss -0.4983 (3.396 secs)\n",
      "np:np_periodic step 99600 lr 1.974e-08 [train_loss] loss -0.4905 (3.448 secs)\n",
      "np:np_periodic step 99800 lr 4.935e-09 [train_loss] loss -0.4892 (3.556 secs)\n",
      "np:np_periodic step 100000 lr 0.000e+00 [train_loss] loss -0.4821 (3.561 secs)\n",
      "100%|##########| 3000/3000 [00:12<00:00, 246.22it/s]\n",
      "np:np_periodic periodic ctx_ll -0.2525 tar_ll -1.1791 (12.186 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:12<00:00, 242.67it/s]\n",
      "np:np_periodic periodic ctx_ll -0.2523 tar_ll -1.1796 (12.367 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 2041666.375 miliseconds\n",
      "Execution time: 2041.666375 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 155.63818359375 MB\n",
      "Memory Usage Change: 139.38818359375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='np', name='np_periodic',val_seed=100, val_l=None,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ea8df-0a2e-4a53-b149-aa3f56ead1ef",
   "metadata": {},
   "source": [
    "## ANP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aed3779a-48c8-4a3b-aade-8fe055de27c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: anp-anp_periodic\n",
      "Total number of parameters: 348418\n",
      "\n",
      "anp:anp_periodic step 200 lr 5.000e-04 [train_loss] loss 0.5081 (5.313 secs)\n",
      "anp:anp_periodic step 400 lr 5.000e-04 [train_loss] loss 0.0897 (5.426 secs)\n",
      "anp:anp_periodic step 600 lr 5.000e-04 [train_loss] loss -0.2838 (5.399 secs)\n",
      "anp:anp_periodic step 800 lr 4.999e-04 [train_loss] loss -0.3908 (5.271 secs)\n",
      "anp:anp_periodic step 1000 lr 4.999e-04 [train_loss] loss -0.5695 (5.341 secs)\n",
      "anp:anp_periodic step 1200 lr 4.998e-04 [train_loss] loss -0.5385 (5.395 secs)\n",
      "anp:anp_periodic step 1400 lr 4.998e-04 [train_loss] loss -0.5298 (5.308 secs)\n",
      "anp:anp_periodic step 1600 lr 4.997e-04 [train_loss] loss -0.6256 (5.443 secs)\n",
      "anp:anp_periodic step 1800 lr 4.996e-04 [train_loss] loss -0.6469 (5.350 secs)\n",
      "anp:anp_periodic step 2000 lr 4.995e-04 [train_loss] loss -0.6513 (5.397 secs)\n",
      "anp:anp_periodic step 2200 lr 4.994e-04 [train_loss] loss -0.6638 (5.528 secs)\n",
      "anp:anp_periodic step 2400 lr 4.993e-04 [train_loss] loss -0.7710 (5.197 secs)\n",
      "anp:anp_periodic step 2600 lr 4.992e-04 [train_loss] loss -0.7583 (5.195 secs)\n",
      "anp:anp_periodic step 2800 lr 4.990e-04 [train_loss] loss -0.7796 (5.516 secs)\n",
      "anp:anp_periodic step 3000 lr 4.989e-04 [train_loss] loss -0.7448 (5.153 secs)\n",
      "anp:anp_periodic step 3200 lr 4.987e-04 [train_loss] loss -0.8461 (5.191 secs)\n",
      "anp:anp_periodic step 3400 lr 4.986e-04 [train_loss] loss -0.8377 (5.133 secs)\n",
      "anp:anp_periodic step 3600 lr 4.984e-04 [train_loss] loss -0.7988 (5.180 secs)\n",
      "anp:anp_periodic step 3800 lr 4.982e-04 [train_loss] loss -0.8533 (5.116 secs)\n",
      "anp:anp_periodic step 4000 lr 4.980e-04 [train_loss] loss -0.7481 (5.409 secs)\n",
      "anp:anp_periodic step 4200 lr 4.978e-04 [train_loss] loss -0.8872 (5.342 secs)\n",
      "anp:anp_periodic step 4400 lr 4.976e-04 [train_loss] loss -0.8841 (5.249 secs)\n",
      "anp:anp_periodic step 4600 lr 4.974e-04 [train_loss] loss -0.8685 (5.246 secs)\n",
      "anp:anp_periodic step 4800 lr 4.972e-04 [train_loss] loss -0.7196 (5.086 secs)\n",
      "anp:anp_periodic step 5000 lr 4.969e-04 [train_loss] loss -0.9150 (5.202 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 141.06it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.1118 tar_ll -2.4482 (21.270 secs)\n",
      "\n",
      "anp:anp_periodic step 5200 lr 4.967e-04 [train_loss] loss -0.8554 (5.245 secs)\n",
      "anp:anp_periodic step 5400 lr 4.964e-04 [train_loss] loss -0.8919 (5.217 secs)\n",
      "anp:anp_periodic step 5600 lr 4.961e-04 [train_loss] loss -0.7407 (5.353 secs)\n",
      "anp:anp_periodic step 5800 lr 4.959e-04 [train_loss] loss -0.8814 (5.092 secs)\n",
      "anp:anp_periodic step 6000 lr 4.956e-04 [train_loss] loss -0.9870 (5.020 secs)\n",
      "anp:anp_periodic step 6200 lr 4.953e-04 [train_loss] loss -0.9273 (5.146 secs)\n",
      "anp:anp_periodic step 6400 lr 4.950e-04 [train_loss] loss -0.9205 (5.099 secs)\n",
      "anp:anp_periodic step 6600 lr 4.946e-04 [train_loss] loss -0.9289 (5.088 secs)\n",
      "anp:anp_periodic step 6800 lr 4.943e-04 [train_loss] loss -0.9112 (5.221 secs)\n",
      "anp:anp_periodic step 7000 lr 4.940e-04 [train_loss] loss -0.8821 (5.124 secs)\n",
      "anp:anp_periodic step 7200 lr 4.936e-04 [train_loss] loss -0.9253 (5.003 secs)\n",
      "anp:anp_periodic step 7400 lr 4.933e-04 [train_loss] loss -0.9394 (5.364 secs)\n",
      "anp:anp_periodic step 7600 lr 4.929e-04 [train_loss] loss -0.9480 (5.358 secs)\n",
      "anp:anp_periodic step 7800 lr 4.925e-04 [train_loss] loss -0.9503 (5.227 secs)\n",
      "anp:anp_periodic step 8000 lr 4.921e-04 [train_loss] loss -0.9315 (5.163 secs)\n",
      "anp:anp_periodic step 8200 lr 4.918e-04 [train_loss] loss -0.9259 (5.136 secs)\n",
      "anp:anp_periodic step 8400 lr 4.913e-04 [train_loss] loss -0.8776 (5.324 secs)\n",
      "anp:anp_periodic step 8600 lr 4.909e-04 [train_loss] loss -0.8640 (5.403 secs)\n",
      "anp:anp_periodic step 8800 lr 4.905e-04 [train_loss] loss -0.9924 (5.325 secs)\n",
      "anp:anp_periodic step 9000 lr 4.901e-04 [train_loss] loss -0.9903 (5.465 secs)\n",
      "anp:anp_periodic step 9200 lr 4.896e-04 [train_loss] loss -0.8145 (5.457 secs)\n",
      "anp:anp_periodic step 9400 lr 4.892e-04 [train_loss] loss -0.9042 (5.265 secs)\n",
      "anp:anp_periodic step 9600 lr 4.887e-04 [train_loss] loss -0.9273 (5.344 secs)\n",
      "anp:anp_periodic step 9800 lr 4.882e-04 [train_loss] loss -0.9271 (5.422 secs)\n",
      "anp:anp_periodic step 10000 lr 4.878e-04 [train_loss] loss -0.9964 (5.253 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 144.90it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.2000 tar_ll -3.1827 (20.704 secs)\n",
      "\n",
      "anp:anp_periodic step 10200 lr 4.873e-04 [train_loss] loss -0.8985 (5.386 secs)\n",
      "anp:anp_periodic step 10400 lr 4.868e-04 [train_loss] loss -0.8839 (5.210 secs)\n",
      "anp:anp_periodic step 10600 lr 4.863e-04 [train_loss] loss -0.8963 (5.273 secs)\n",
      "anp:anp_periodic step 10800 lr 4.857e-04 [train_loss] loss -0.9645 (5.412 secs)\n",
      "anp:anp_periodic step 11000 lr 4.852e-04 [train_loss] loss -0.9456 (5.159 secs)\n",
      "anp:anp_periodic step 11200 lr 4.847e-04 [train_loss] loss -0.8903 (5.300 secs)\n",
      "anp:anp_periodic step 11400 lr 4.841e-04 [train_loss] loss -1.0011 (5.669 secs)\n",
      "anp:anp_periodic step 11600 lr 4.836e-04 [train_loss] loss -0.9892 (5.281 secs)\n",
      "anp:anp_periodic step 11800 lr 4.830e-04 [train_loss] loss -0.9551 (5.309 secs)\n",
      "anp:anp_periodic step 12000 lr 4.824e-04 [train_loss] loss -0.9654 (5.161 secs)\n",
      "anp:anp_periodic step 12200 lr 4.819e-04 [train_loss] loss -1.0281 (5.260 secs)\n",
      "anp:anp_periodic step 12400 lr 4.813e-04 [train_loss] loss -0.9190 (5.302 secs)\n",
      "anp:anp_periodic step 12600 lr 4.807e-04 [train_loss] loss -0.9819 (5.278 secs)\n",
      "anp:anp_periodic step 12800 lr 4.801e-04 [train_loss] loss -0.9652 (5.319 secs)\n",
      "anp:anp_periodic step 13000 lr 4.794e-04 [train_loss] loss -1.0370 (5.037 secs)\n",
      "anp:anp_periodic step 13200 lr 4.788e-04 [train_loss] loss -0.9686 (5.467 secs)\n",
      "anp:anp_periodic step 13400 lr 4.782e-04 [train_loss] loss -0.9060 (5.301 secs)\n",
      "anp:anp_periodic step 13600 lr 4.775e-04 [train_loss] loss -0.8544 (5.151 secs)\n",
      "anp:anp_periodic step 13800 lr 4.769e-04 [train_loss] loss -0.9515 (5.306 secs)\n",
      "anp:anp_periodic step 14000 lr 4.762e-04 [train_loss] loss -0.9836 (5.056 secs)\n",
      "anp:anp_periodic step 14200 lr 4.755e-04 [train_loss] loss -1.0072 (5.141 secs)\n",
      "anp:anp_periodic step 14400 lr 4.749e-04 [train_loss] loss -0.9651 (5.436 secs)\n",
      "anp:anp_periodic step 14600 lr 4.742e-04 [train_loss] loss -0.9079 (5.207 secs)\n",
      "anp:anp_periodic step 14800 lr 4.735e-04 [train_loss] loss -0.9821 (5.423 secs)\n",
      "anp:anp_periodic step 15000 lr 4.728e-04 [train_loss] loss -0.9465 (5.184 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 146.09it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.1635 tar_ll -4.2276 (20.538 secs)\n",
      "\n",
      "anp:anp_periodic step 15200 lr 4.720e-04 [train_loss] loss -1.0274 (5.274 secs)\n",
      "anp:anp_periodic step 15400 lr 4.713e-04 [train_loss] loss -0.9264 (5.258 secs)\n",
      "anp:anp_periodic step 15600 lr 4.706e-04 [train_loss] loss -0.9946 (5.236 secs)\n",
      "anp:anp_periodic step 15800 lr 4.698e-04 [train_loss] loss -0.9997 (5.252 secs)\n",
      "anp:anp_periodic step 16000 lr 4.691e-04 [train_loss] loss -0.9395 (5.271 secs)\n",
      "anp:anp_periodic step 16200 lr 4.683e-04 [train_loss] loss -1.0142 (5.221 secs)\n",
      "anp:anp_periodic step 16400 lr 4.675e-04 [train_loss] loss -0.9897 (5.207 secs)\n",
      "anp:anp_periodic step 16600 lr 4.668e-04 [train_loss] loss -0.8809 (5.210 secs)\n",
      "anp:anp_periodic step 16800 lr 4.660e-04 [train_loss] loss -0.9978 (5.238 secs)\n",
      "anp:anp_periodic step 17000 lr 4.652e-04 [train_loss] loss -0.9904 (5.177 secs)\n",
      "anp:anp_periodic step 17200 lr 4.644e-04 [train_loss] loss -0.9756 (5.335 secs)\n",
      "anp:anp_periodic step 17400 lr 4.636e-04 [train_loss] loss -1.0185 (5.199 secs)\n",
      "anp:anp_periodic step 17600 lr 4.627e-04 [train_loss] loss -1.0167 (5.131 secs)\n",
      "anp:anp_periodic step 17800 lr 4.619e-04 [train_loss] loss -1.0185 (5.280 secs)\n",
      "anp:anp_periodic step 18000 lr 4.611e-04 [train_loss] loss -1.0389 (5.171 secs)\n",
      "anp:anp_periodic step 18200 lr 4.602e-04 [train_loss] loss -0.9794 (5.380 secs)\n",
      "anp:anp_periodic step 18400 lr 4.594e-04 [train_loss] loss -0.9522 (5.185 secs)\n",
      "anp:anp_periodic step 18600 lr 4.585e-04 [train_loss] loss -1.0111 (5.301 secs)\n",
      "anp:anp_periodic step 18800 lr 4.576e-04 [train_loss] loss -1.0090 (5.159 secs)\n",
      "anp:anp_periodic step 19000 lr 4.568e-04 [train_loss] loss -1.0007 (5.311 secs)\n",
      "anp:anp_periodic step 19200 lr 4.559e-04 [train_loss] loss -1.0458 (5.228 secs)\n",
      "anp:anp_periodic step 19400 lr 4.550e-04 [train_loss] loss -0.9808 (5.323 secs)\n",
      "anp:anp_periodic step 19600 lr 4.541e-04 [train_loss] loss -0.9618 (5.243 secs)\n",
      "anp:anp_periodic step 19800 lr 4.532e-04 [train_loss] loss -1.0022 (5.405 secs)\n",
      "anp:anp_periodic step 20000 lr 4.523e-04 [train_loss] loss -0.9964 (5.152 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 145.12it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.1189 tar_ll -4.4108 (20.674 secs)\n",
      "\n",
      "anp:anp_periodic step 20200 lr 4.513e-04 [train_loss] loss -1.0332 (5.153 secs)\n",
      "anp:anp_periodic step 20400 lr 4.504e-04 [train_loss] loss -0.9992 (5.175 secs)\n",
      "anp:anp_periodic step 20600 lr 4.494e-04 [train_loss] loss -0.9005 (5.185 secs)\n",
      "anp:anp_periodic step 20800 lr 4.485e-04 [train_loss] loss -1.0115 (5.191 secs)\n",
      "anp:anp_periodic step 21000 lr 4.475e-04 [train_loss] loss -1.0574 (5.264 secs)\n",
      "anp:anp_periodic step 21200 lr 4.466e-04 [train_loss] loss -0.8586 (5.345 secs)\n",
      "anp:anp_periodic step 21400 lr 4.456e-04 [train_loss] loss -0.9656 (5.305 secs)\n",
      "anp:anp_periodic step 21600 lr 4.446e-04 [train_loss] loss -0.9707 (5.098 secs)\n",
      "anp:anp_periodic step 21800 lr 4.436e-04 [train_loss] loss -1.0255 (5.264 secs)\n",
      "anp:anp_periodic step 22000 lr 4.426e-04 [train_loss] loss -1.0051 (5.324 secs)\n",
      "anp:anp_periodic step 22200 lr 4.416e-04 [train_loss] loss -0.9698 (5.155 secs)\n",
      "anp:anp_periodic step 22400 lr 4.406e-04 [train_loss] loss -0.9164 (5.198 secs)\n",
      "anp:anp_periodic step 22600 lr 4.396e-04 [train_loss] loss -0.9755 (5.221 secs)\n",
      "anp:anp_periodic step 22800 lr 4.386e-04 [train_loss] loss -0.9519 (4.993 secs)\n",
      "anp:anp_periodic step 23000 lr 4.375e-04 [train_loss] loss -0.9480 (5.232 secs)\n",
      "anp:anp_periodic step 23200 lr 4.365e-04 [train_loss] loss -0.9993 (5.278 secs)\n",
      "anp:anp_periodic step 23400 lr 4.354e-04 [train_loss] loss -1.0132 (4.972 secs)\n",
      "anp:anp_periodic step 23600 lr 4.344e-04 [train_loss] loss -0.9882 (5.199 secs)\n",
      "anp:anp_periodic step 23800 lr 4.333e-04 [train_loss] loss -1.0062 (5.060 secs)\n",
      "anp:anp_periodic step 24000 lr 4.322e-04 [train_loss] loss -1.0481 (5.566 secs)\n",
      "anp:anp_periodic step 24200 lr 4.312e-04 [train_loss] loss -0.9978 (5.343 secs)\n",
      "anp:anp_periodic step 24400 lr 4.301e-04 [train_loss] loss -1.0215 (5.443 secs)\n",
      "anp:anp_periodic step 24600 lr 4.290e-04 [train_loss] loss -1.0144 (5.327 secs)\n",
      "anp:anp_periodic step 24800 lr 4.279e-04 [train_loss] loss -1.0334 (5.286 secs)\n",
      "anp:anp_periodic step 25000 lr 4.268e-04 [train_loss] loss -1.0167 (5.419 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 142.58it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.0021 tar_ll -5.0346 (21.043 secs)\n",
      "\n",
      "anp:anp_periodic step 25200 lr 4.257e-04 [train_loss] loss -1.0357 (5.377 secs)\n",
      "anp:anp_periodic step 25400 lr 4.245e-04 [train_loss] loss -0.9467 (5.563 secs)\n",
      "anp:anp_periodic step 25600 lr 4.234e-04 [train_loss] loss -1.0067 (5.447 secs)\n",
      "anp:anp_periodic step 25800 lr 4.223e-04 [train_loss] loss -1.0086 (5.420 secs)\n",
      "anp:anp_periodic step 26000 lr 4.211e-04 [train_loss] loss -1.0459 (5.445 secs)\n",
      "anp:anp_periodic step 26200 lr 4.200e-04 [train_loss] loss -0.9483 (5.357 secs)\n",
      "anp:anp_periodic step 26400 lr 4.188e-04 [train_loss] loss -0.9969 (5.611 secs)\n",
      "anp:anp_periodic step 26600 lr 4.177e-04 [train_loss] loss -0.9776 (5.344 secs)\n",
      "anp:anp_periodic step 26800 lr 4.165e-04 [train_loss] loss -0.9615 (5.278 secs)\n",
      "anp:anp_periodic step 27000 lr 4.153e-04 [train_loss] loss -0.9043 (5.471 secs)\n",
      "anp:anp_periodic step 27200 lr 4.141e-04 [train_loss] loss -0.9480 (5.489 secs)\n",
      "anp:anp_periodic step 27400 lr 4.130e-04 [train_loss] loss -0.9433 (5.454 secs)\n",
      "anp:anp_periodic step 27600 lr 4.118e-04 [train_loss] loss -1.0070 (5.290 secs)\n",
      "anp:anp_periodic step 27800 lr 4.106e-04 [train_loss] loss -1.0004 (5.279 secs)\n",
      "anp:anp_periodic step 28000 lr 4.094e-04 [train_loss] loss -0.9615 (5.314 secs)\n",
      "anp:anp_periodic step 28200 lr 4.081e-04 [train_loss] loss -0.9207 (5.187 secs)\n",
      "anp:anp_periodic step 28400 lr 4.069e-04 [train_loss] loss -0.9904 (5.143 secs)\n",
      "anp:anp_periodic step 28600 lr 4.057e-04 [train_loss] loss -0.9984 (5.229 secs)\n",
      "anp:anp_periodic step 28800 lr 4.045e-04 [train_loss] loss -1.0309 (5.543 secs)\n",
      "anp:anp_periodic step 29000 lr 4.032e-04 [train_loss] loss -1.0166 (5.414 secs)\n",
      "anp:anp_periodic step 29200 lr 4.020e-04 [train_loss] loss -1.0322 (5.460 secs)\n",
      "anp:anp_periodic step 29400 lr 4.007e-04 [train_loss] loss -1.0138 (5.452 secs)\n",
      "anp:anp_periodic step 29600 lr 3.995e-04 [train_loss] loss -1.0481 (5.369 secs)\n",
      "anp:anp_periodic step 29800 lr 3.982e-04 [train_loss] loss -1.0435 (5.308 secs)\n",
      "anp:anp_periodic step 30000 lr 3.969e-04 [train_loss] loss -1.0139 (5.246 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 143.19it/s]\n",
      "anp:anp_periodic periodic ctx_ll -0.0238 tar_ll -5.2137 (20.954 secs)\n",
      "\n",
      "anp:anp_periodic step 30200 lr 3.957e-04 [train_loss] loss -1.0197 (5.179 secs)\n",
      "anp:anp_periodic step 30400 lr 3.944e-04 [train_loss] loss -1.0661 (5.406 secs)\n",
      "anp:anp_periodic step 30600 lr 3.931e-04 [train_loss] loss -1.0489 (5.175 secs)\n",
      "anp:anp_periodic step 30800 lr 3.918e-04 [train_loss] loss -0.9952 (5.229 secs)\n",
      "anp:anp_periodic step 31000 lr 3.905e-04 [train_loss] loss -0.9944 (5.429 secs)\n",
      "anp:anp_periodic step 31200 lr 3.892e-04 [train_loss] loss -1.0561 (5.254 secs)\n",
      "anp:anp_periodic step 31400 lr 3.879e-04 [train_loss] loss -1.0855 (5.240 secs)\n",
      "anp:anp_periodic step 31600 lr 3.866e-04 [train_loss] loss -1.0517 (5.469 secs)\n",
      "anp:anp_periodic step 31800 lr 3.853e-04 [train_loss] loss -1.0241 (5.334 secs)\n",
      "anp:anp_periodic step 32000 lr 3.840e-04 [train_loss] loss -0.9601 (5.337 secs)\n",
      "anp:anp_periodic step 32200 lr 3.826e-04 [train_loss] loss -1.0027 (5.352 secs)\n",
      "anp:anp_periodic step 32400 lr 3.813e-04 [train_loss] loss -1.0287 (5.450 secs)\n",
      "anp:anp_periodic step 32600 lr 3.800e-04 [train_loss] loss -1.0333 (5.537 secs)\n",
      "anp:anp_periodic step 32800 lr 3.786e-04 [train_loss] loss -1.0217 (5.675 secs)\n",
      "anp:anp_periodic step 33000 lr 3.773e-04 [train_loss] loss -1.0674 (5.465 secs)\n",
      "anp:anp_periodic step 33200 lr 3.759e-04 [train_loss] loss -1.0083 (5.217 secs)\n",
      "anp:anp_periodic step 33400 lr 3.745e-04 [train_loss] loss -0.9719 (5.464 secs)\n",
      "anp:anp_periodic step 33600 lr 3.732e-04 [train_loss] loss -0.9962 (5.184 secs)\n",
      "anp:anp_periodic step 33800 lr 3.718e-04 [train_loss] loss -0.9200 (5.351 secs)\n",
      "anp:anp_periodic step 34000 lr 3.704e-04 [train_loss] loss -1.0174 (5.476 secs)\n",
      "anp:anp_periodic step 34200 lr 3.691e-04 [train_loss] loss -1.0615 (5.360 secs)\n",
      "anp:anp_periodic step 34400 lr 3.677e-04 [train_loss] loss -0.9981 (5.242 secs)\n",
      "anp:anp_periodic step 34600 lr 3.663e-04 [train_loss] loss -1.0385 (5.542 secs)\n",
      "anp:anp_periodic step 34800 lr 3.649e-04 [train_loss] loss -1.0293 (5.193 secs)\n",
      "anp:anp_periodic step 35000 lr 3.635e-04 [train_loss] loss -1.0374 (5.405 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 140.01it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.0194 tar_ll -4.3305 (21.431 secs)\n",
      "\n",
      "anp:anp_periodic step 35200 lr 3.621e-04 [train_loss] loss -1.0890 (5.192 secs)\n",
      "anp:anp_periodic step 35400 lr 3.607e-04 [train_loss] loss -1.0151 (5.305 secs)\n",
      "anp:anp_periodic step 35600 lr 3.593e-04 [train_loss] loss -1.0420 (5.390 secs)\n",
      "anp:anp_periodic step 35800 lr 3.579e-04 [train_loss] loss -0.9765 (5.249 secs)\n",
      "anp:anp_periodic step 36000 lr 3.564e-04 [train_loss] loss -1.0248 (5.304 secs)\n",
      "anp:anp_periodic step 36200 lr 3.550e-04 [train_loss] loss -1.0359 (5.309 secs)\n",
      "anp:anp_periodic step 36400 lr 3.536e-04 [train_loss] loss -1.0297 (5.389 secs)\n",
      "anp:anp_periodic step 36600 lr 3.522e-04 [train_loss] loss -1.0561 (5.136 secs)\n",
      "anp:anp_periodic step 36800 lr 3.507e-04 [train_loss] loss -1.0305 (5.270 secs)\n",
      "anp:anp_periodic step 37000 lr 3.493e-04 [train_loss] loss -0.9961 (5.190 secs)\n",
      "anp:anp_periodic step 37200 lr 3.478e-04 [train_loss] loss -1.0134 (5.465 secs)\n",
      "anp:anp_periodic step 37400 lr 3.464e-04 [train_loss] loss -1.0456 (5.433 secs)\n",
      "anp:anp_periodic step 37600 lr 3.449e-04 [train_loss] loss -1.0070 (5.319 secs)\n",
      "anp:anp_periodic step 37800 lr 3.435e-04 [train_loss] loss -0.9916 (5.391 secs)\n",
      "anp:anp_periodic step 38000 lr 3.420e-04 [train_loss] loss -1.0340 (5.667 secs)\n",
      "anp:anp_periodic step 38200 lr 3.406e-04 [train_loss] loss -1.0724 (5.365 secs)\n",
      "anp:anp_periodic step 38400 lr 3.391e-04 [train_loss] loss -1.0094 (5.112 secs)\n",
      "anp:anp_periodic step 38600 lr 3.376e-04 [train_loss] loss -1.0301 (5.163 secs)\n",
      "anp:anp_periodic step 38800 lr 3.362e-04 [train_loss] loss -1.0197 (5.170 secs)\n",
      "anp:anp_periodic step 39000 lr 3.347e-04 [train_loss] loss -1.0580 (5.122 secs)\n",
      "anp:anp_periodic step 39200 lr 3.332e-04 [train_loss] loss -1.0572 (5.358 secs)\n",
      "anp:anp_periodic step 39400 lr 3.317e-04 [train_loss] loss -0.9981 (5.199 secs)\n",
      "anp:anp_periodic step 39600 lr 3.302e-04 [train_loss] loss -1.0623 (5.383 secs)\n",
      "anp:anp_periodic step 39800 lr 3.287e-04 [train_loss] loss -1.0303 (5.364 secs)\n",
      "anp:anp_periodic step 40000 lr 3.273e-04 [train_loss] loss -0.8118 (5.376 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 143.31it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.0511 tar_ll -2.9379 (20.938 secs)\n",
      "\n",
      "anp:anp_periodic step 40200 lr 3.258e-04 [train_loss] loss -1.0126 (5.387 secs)\n",
      "anp:anp_periodic step 40400 lr 3.243e-04 [train_loss] loss -0.9759 (5.475 secs)\n",
      "anp:anp_periodic step 40600 lr 3.228e-04 [train_loss] loss -1.0377 (5.487 secs)\n",
      "anp:anp_periodic step 40800 lr 3.213e-04 [train_loss] loss -1.0836 (5.433 secs)\n",
      "anp:anp_periodic step 41000 lr 3.197e-04 [train_loss] loss -1.0123 (5.376 secs)\n",
      "anp:anp_periodic step 41200 lr 3.182e-04 [train_loss] loss -1.0370 (5.471 secs)\n",
      "anp:anp_periodic step 41400 lr 3.167e-04 [train_loss] loss -1.0717 (5.547 secs)\n",
      "anp:anp_periodic step 41600 lr 3.152e-04 [train_loss] loss -1.0129 (5.160 secs)\n",
      "anp:anp_periodic step 41800 lr 3.137e-04 [train_loss] loss -1.0159 (5.189 secs)\n",
      "anp:anp_periodic step 42000 lr 3.122e-04 [train_loss] loss -1.0327 (5.485 secs)\n",
      "anp:anp_periodic step 42200 lr 3.106e-04 [train_loss] loss -1.0659 (5.342 secs)\n",
      "anp:anp_periodic step 42400 lr 3.091e-04 [train_loss] loss -1.0861 (5.212 secs)\n",
      "anp:anp_periodic step 42600 lr 3.076e-04 [train_loss] loss -1.0235 (5.460 secs)\n",
      "anp:anp_periodic step 42800 lr 3.061e-04 [train_loss] loss -1.0775 (5.322 secs)\n",
      "anp:anp_periodic step 43000 lr 3.045e-04 [train_loss] loss -1.0225 (5.403 secs)\n",
      "anp:anp_periodic step 43200 lr 3.030e-04 [train_loss] loss -1.0360 (5.422 secs)\n",
      "anp:anp_periodic step 43400 lr 3.015e-04 [train_loss] loss -1.0623 (5.180 secs)\n",
      "anp:anp_periodic step 43600 lr 2.999e-04 [train_loss] loss -1.0415 (5.188 secs)\n",
      "anp:anp_periodic step 43800 lr 2.984e-04 [train_loss] loss -1.1338 (5.362 secs)\n",
      "anp:anp_periodic step 44000 lr 2.968e-04 [train_loss] loss -1.0512 (5.142 secs)\n",
      "anp:anp_periodic step 44200 lr 2.953e-04 [train_loss] loss -1.0867 (5.278 secs)\n",
      "anp:anp_periodic step 44400 lr 2.938e-04 [train_loss] loss -1.0418 (5.373 secs)\n",
      "anp:anp_periodic step 44600 lr 2.922e-04 [train_loss] loss -1.0390 (5.293 secs)\n",
      "anp:anp_periodic step 44800 lr 2.907e-04 [train_loss] loss -1.0409 (5.084 secs)\n",
      "anp:anp_periodic step 45000 lr 2.891e-04 [train_loss] loss -1.0622 (5.306 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 146.15it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.0086 tar_ll -5.1767 (20.529 secs)\n",
      "\n",
      "anp:anp_periodic step 45200 lr 2.876e-04 [train_loss] loss -1.0671 (5.310 secs)\n",
      "anp:anp_periodic step 45400 lr 2.860e-04 [train_loss] loss -1.0207 (5.440 secs)\n",
      "anp:anp_periodic step 45600 lr 2.844e-04 [train_loss] loss -1.0744 (5.183 secs)\n",
      "anp:anp_periodic step 45800 lr 2.829e-04 [train_loss] loss -1.0394 (5.294 secs)\n",
      "anp:anp_periodic step 46000 lr 2.813e-04 [train_loss] loss -1.0812 (5.293 secs)\n",
      "anp:anp_periodic step 46200 lr 2.798e-04 [train_loss] loss -1.0196 (5.281 secs)\n",
      "anp:anp_periodic step 46400 lr 2.782e-04 [train_loss] loss -1.0935 (5.197 secs)\n",
      "anp:anp_periodic step 46600 lr 2.767e-04 [train_loss] loss -1.0523 (5.315 secs)\n",
      "anp:anp_periodic step 46800 lr 2.751e-04 [train_loss] loss -1.0834 (5.432 secs)\n",
      "anp:anp_periodic step 47000 lr 2.735e-04 [train_loss] loss -1.0683 (5.467 secs)\n",
      "anp:anp_periodic step 47200 lr 2.720e-04 [train_loss] loss -1.0948 (5.517 secs)\n",
      "anp:anp_periodic step 47400 lr 2.704e-04 [train_loss] loss -1.0437 (5.371 secs)\n",
      "anp:anp_periodic step 47600 lr 2.688e-04 [train_loss] loss -1.0514 (5.345 secs)\n",
      "anp:anp_periodic step 47800 lr 2.673e-04 [train_loss] loss -1.0363 (5.532 secs)\n",
      "anp:anp_periodic step 48000 lr 2.657e-04 [train_loss] loss -1.0546 (5.476 secs)\n",
      "anp:anp_periodic step 48200 lr 2.641e-04 [train_loss] loss -1.0738 (5.425 secs)\n",
      "anp:anp_periodic step 48400 lr 2.626e-04 [train_loss] loss -1.0478 (5.497 secs)\n",
      "anp:anp_periodic step 48600 lr 2.610e-04 [train_loss] loss -1.0440 (5.425 secs)\n",
      "anp:anp_periodic step 48800 lr 2.594e-04 [train_loss] loss -1.0149 (5.621 secs)\n",
      "anp:anp_periodic step 49000 lr 2.579e-04 [train_loss] loss -1.0468 (5.315 secs)\n",
      "anp:anp_periodic step 49200 lr 2.563e-04 [train_loss] loss -1.0318 (5.544 secs)\n",
      "anp:anp_periodic step 49400 lr 2.547e-04 [train_loss] loss -1.0217 (5.343 secs)\n",
      "anp:anp_periodic step 49600 lr 2.531e-04 [train_loss] loss -1.0756 (5.320 secs)\n",
      "anp:anp_periodic step 49800 lr 2.516e-04 [train_loss] loss -1.0729 (5.476 secs)\n",
      "anp:anp_periodic step 50000 lr 2.500e-04 [train_loss] loss -1.0558 (5.531 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 142.65it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.1394 tar_ll -4.1418 (21.033 secs)\n",
      "\n",
      "anp:anp_periodic step 50200 lr 2.484e-04 [train_loss] loss -1.0333 (5.293 secs)\n",
      "anp:anp_periodic step 50400 lr 2.469e-04 [train_loss] loss -1.0648 (5.430 secs)\n",
      "anp:anp_periodic step 50600 lr 2.453e-04 [train_loss] loss -1.1064 (5.434 secs)\n",
      "anp:anp_periodic step 50800 lr 2.437e-04 [train_loss] loss -1.0441 (5.434 secs)\n",
      "anp:anp_periodic step 51000 lr 2.421e-04 [train_loss] loss -1.0209 (5.345 secs)\n",
      "anp:anp_periodic step 51200 lr 2.406e-04 [train_loss] loss -1.0529 (5.320 secs)\n",
      "anp:anp_periodic step 51400 lr 2.390e-04 [train_loss] loss -1.0634 (5.342 secs)\n",
      "anp:anp_periodic step 51600 lr 2.374e-04 [train_loss] loss -1.0453 (5.244 secs)\n",
      "anp:anp_periodic step 51800 lr 2.359e-04 [train_loss] loss -1.0967 (5.455 secs)\n",
      "anp:anp_periodic step 52000 lr 2.343e-04 [train_loss] loss -1.0352 (5.234 secs)\n",
      "anp:anp_periodic step 52200 lr 2.327e-04 [train_loss] loss -1.0744 (5.485 secs)\n",
      "anp:anp_periodic step 52400 lr 2.312e-04 [train_loss] loss -1.0453 (5.246 secs)\n",
      "anp:anp_periodic step 52600 lr 2.296e-04 [train_loss] loss -1.0636 (5.108 secs)\n",
      "anp:anp_periodic step 52800 lr 2.280e-04 [train_loss] loss -1.0367 (5.145 secs)\n",
      "anp:anp_periodic step 53000 lr 2.265e-04 [train_loss] loss -1.0608 (5.103 secs)\n",
      "anp:anp_periodic step 53200 lr 2.249e-04 [train_loss] loss -1.0821 (5.193 secs)\n",
      "anp:anp_periodic step 53400 lr 2.233e-04 [train_loss] loss -1.0423 (5.106 secs)\n",
      "anp:anp_periodic step 53600 lr 2.218e-04 [train_loss] loss -1.0203 (5.225 secs)\n",
      "anp:anp_periodic step 53800 lr 2.202e-04 [train_loss] loss -1.0360 (5.383 secs)\n",
      "anp:anp_periodic step 54000 lr 2.187e-04 [train_loss] loss -1.0318 (5.191 secs)\n",
      "anp:anp_periodic step 54200 lr 2.171e-04 [train_loss] loss -1.0465 (4.946 secs)\n",
      "anp:anp_periodic step 54400 lr 2.156e-04 [train_loss] loss -1.0332 (5.140 secs)\n",
      "anp:anp_periodic step 54600 lr 2.140e-04 [train_loss] loss -1.0640 (5.261 secs)\n",
      "anp:anp_periodic step 54800 lr 2.124e-04 [train_loss] loss -1.0769 (5.242 secs)\n",
      "anp:anp_periodic step 55000 lr 2.109e-04 [train_loss] loss -1.0584 (5.452 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 143.08it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.0552 tar_ll -5.0658 (20.971 secs)\n",
      "\n",
      "anp:anp_periodic step 55200 lr 2.093e-04 [train_loss] loss -1.0750 (5.314 secs)\n",
      "anp:anp_periodic step 55400 lr 2.078e-04 [train_loss] loss -1.0531 (5.281 secs)\n",
      "anp:anp_periodic step 55600 lr 2.062e-04 [train_loss] loss -1.1045 (5.311 secs)\n",
      "anp:anp_periodic step 55800 lr 2.047e-04 [train_loss] loss -1.0356 (5.411 secs)\n",
      "anp:anp_periodic step 56000 lr 2.032e-04 [train_loss] loss -1.0487 (5.205 secs)\n",
      "anp:anp_periodic step 56200 lr 2.016e-04 [train_loss] loss -1.0860 (5.439 secs)\n",
      "anp:anp_periodic step 56400 lr 2.001e-04 [train_loss] loss -1.0486 (5.164 secs)\n",
      "anp:anp_periodic step 56600 lr 1.985e-04 [train_loss] loss -1.1059 (5.414 secs)\n",
      "anp:anp_periodic step 56800 lr 1.970e-04 [train_loss] loss -1.0798 (5.350 secs)\n",
      "anp:anp_periodic step 57000 lr 1.955e-04 [train_loss] loss -1.0633 (5.292 secs)\n",
      "anp:anp_periodic step 57200 lr 1.939e-04 [train_loss] loss -1.0534 (5.146 secs)\n",
      "anp:anp_periodic step 57400 lr 1.924e-04 [train_loss] loss -1.0287 (5.507 secs)\n",
      "anp:anp_periodic step 57600 lr 1.909e-04 [train_loss] loss -1.0208 (5.246 secs)\n",
      "anp:anp_periodic step 57800 lr 1.894e-04 [train_loss] loss -1.0624 (5.330 secs)\n",
      "anp:anp_periodic step 58000 lr 1.878e-04 [train_loss] loss -1.0525 (5.630 secs)\n",
      "anp:anp_periodic step 58200 lr 1.863e-04 [train_loss] loss -1.0839 (5.305 secs)\n",
      "anp:anp_periodic step 58400 lr 1.848e-04 [train_loss] loss -1.0400 (5.395 secs)\n",
      "anp:anp_periodic step 58600 lr 1.833e-04 [train_loss] loss -1.0831 (5.416 secs)\n",
      "anp:anp_periodic step 58800 lr 1.818e-04 [train_loss] loss -1.0666 (5.126 secs)\n",
      "anp:anp_periodic step 59000 lr 1.803e-04 [train_loss] loss -1.0529 (5.248 secs)\n",
      "anp:anp_periodic step 59200 lr 1.787e-04 [train_loss] loss -1.0725 (5.237 secs)\n",
      "anp:anp_periodic step 59400 lr 1.772e-04 [train_loss] loss -1.1000 (5.318 secs)\n",
      "anp:anp_periodic step 59600 lr 1.757e-04 [train_loss] loss -1.0680 (5.104 secs)\n",
      "anp:anp_periodic step 59800 lr 1.742e-04 [train_loss] loss -1.0272 (5.394 secs)\n",
      "anp:anp_periodic step 60000 lr 1.727e-04 [train_loss] loss -1.0579 (5.326 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 145.40it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.1507 tar_ll -5.0385 (20.638 secs)\n",
      "\n",
      "anp:anp_periodic step 60200 lr 1.713e-04 [train_loss] loss -1.0759 (5.466 secs)\n",
      "anp:anp_periodic step 60400 lr 1.698e-04 [train_loss] loss -1.0726 (5.336 secs)\n",
      "anp:anp_periodic step 60600 lr 1.683e-04 [train_loss] loss -1.0335 (5.354 secs)\n",
      "anp:anp_periodic step 60800 lr 1.668e-04 [train_loss] loss -1.0743 (5.335 secs)\n",
      "anp:anp_periodic step 61000 lr 1.653e-04 [train_loss] loss -1.0783 (5.311 secs)\n",
      "anp:anp_periodic step 61200 lr 1.638e-04 [train_loss] loss -1.1041 (5.236 secs)\n",
      "anp:anp_periodic step 61400 lr 1.624e-04 [train_loss] loss -1.1041 (5.412 secs)\n",
      "anp:anp_periodic step 61600 lr 1.609e-04 [train_loss] loss -1.0718 (5.229 secs)\n",
      "anp:anp_periodic step 61800 lr 1.594e-04 [train_loss] loss -1.0867 (5.274 secs)\n",
      "anp:anp_periodic step 62000 lr 1.580e-04 [train_loss] loss -1.0940 (5.544 secs)\n",
      "anp:anp_periodic step 62200 lr 1.565e-04 [train_loss] loss -1.0873 (5.475 secs)\n",
      "anp:anp_periodic step 62400 lr 1.551e-04 [train_loss] loss -1.0650 (5.286 secs)\n",
      "anp:anp_periodic step 62600 lr 1.536e-04 [train_loss] loss -1.0746 (5.420 secs)\n",
      "anp:anp_periodic step 62800 lr 1.522e-04 [train_loss] loss -1.1019 (5.424 secs)\n",
      "anp:anp_periodic step 63000 lr 1.507e-04 [train_loss] loss -1.0780 (5.436 secs)\n",
      "anp:anp_periodic step 63200 lr 1.493e-04 [train_loss] loss -1.1114 (5.565 secs)\n",
      "anp:anp_periodic step 63400 lr 1.478e-04 [train_loss] loss -1.0603 (5.437 secs)\n",
      "anp:anp_periodic step 63600 lr 1.464e-04 [train_loss] loss -1.0835 (5.458 secs)\n",
      "anp:anp_periodic step 63800 lr 1.450e-04 [train_loss] loss -1.0737 (5.366 secs)\n",
      "anp:anp_periodic step 64000 lr 1.436e-04 [train_loss] loss -1.1098 (5.477 secs)\n",
      "anp:anp_periodic step 64200 lr 1.421e-04 [train_loss] loss -1.0696 (5.303 secs)\n",
      "anp:anp_periodic step 64400 lr 1.407e-04 [train_loss] loss -1.0734 (5.283 secs)\n",
      "anp:anp_periodic step 64600 lr 1.393e-04 [train_loss] loss -1.0605 (5.326 secs)\n",
      "anp:anp_periodic step 64800 lr 1.379e-04 [train_loss] loss -1.0952 (5.231 secs)\n",
      "anp:anp_periodic step 65000 lr 1.365e-04 [train_loss] loss -1.0566 (5.421 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 139.45it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.0739 tar_ll -4.2517 (21.516 secs)\n",
      "\n",
      "anp:anp_periodic step 65200 lr 1.351e-04 [train_loss] loss -1.0610 (5.441 secs)\n",
      "anp:anp_periodic step 65400 lr 1.337e-04 [train_loss] loss -1.0856 (5.416 secs)\n",
      "anp:anp_periodic step 65600 lr 1.323e-04 [train_loss] loss -1.1285 (5.397 secs)\n",
      "anp:anp_periodic step 65800 lr 1.309e-04 [train_loss] loss -1.0367 (5.293 secs)\n",
      "anp:anp_periodic step 66000 lr 1.296e-04 [train_loss] loss -1.0760 (5.655 secs)\n",
      "anp:anp_periodic step 66200 lr 1.282e-04 [train_loss] loss -1.0614 (5.441 secs)\n",
      "anp:anp_periodic step 66400 lr 1.268e-04 [train_loss] loss -1.0888 (5.148 secs)\n",
      "anp:anp_periodic step 66600 lr 1.255e-04 [train_loss] loss -1.0967 (5.220 secs)\n",
      "anp:anp_periodic step 66800 lr 1.241e-04 [train_loss] loss -1.0765 (5.104 secs)\n",
      "anp:anp_periodic step 67000 lr 1.227e-04 [train_loss] loss -1.0514 (5.100 secs)\n",
      "anp:anp_periodic step 67200 lr 1.214e-04 [train_loss] loss -1.0875 (5.494 secs)\n",
      "anp:anp_periodic step 67400 lr 1.200e-04 [train_loss] loss -1.0726 (5.122 secs)\n",
      "anp:anp_periodic step 67600 lr 1.187e-04 [train_loss] loss -1.0728 (5.131 secs)\n",
      "anp:anp_periodic step 67800 lr 1.174e-04 [train_loss] loss -1.0654 (5.283 secs)\n",
      "anp:anp_periodic step 68000 lr 1.160e-04 [train_loss] loss -1.0527 (5.269 secs)\n",
      "anp:anp_periodic step 68200 lr 1.147e-04 [train_loss] loss -1.0641 (5.274 secs)\n",
      "anp:anp_periodic step 68400 lr 1.134e-04 [train_loss] loss -1.0144 (5.328 secs)\n",
      "anp:anp_periodic step 68600 lr 1.121e-04 [train_loss] loss -1.0742 (5.158 secs)\n",
      "anp:anp_periodic step 68800 lr 1.108e-04 [train_loss] loss -1.0596 (5.277 secs)\n",
      "anp:anp_periodic step 69000 lr 1.095e-04 [train_loss] loss -1.0788 (5.540 secs)\n",
      "anp:anp_periodic step 69200 lr 1.082e-04 [train_loss] loss -1.0924 (5.129 secs)\n",
      "anp:anp_periodic step 69400 lr 1.069e-04 [train_loss] loss -1.0821 (5.187 secs)\n",
      "anp:anp_periodic step 69600 lr 1.056e-04 [train_loss] loss -1.0486 (5.520 secs)\n",
      "anp:anp_periodic step 69800 lr 1.043e-04 [train_loss] loss -1.0714 (5.136 secs)\n",
      "anp:anp_periodic step 70000 lr 1.031e-04 [train_loss] loss -1.0989 (5.122 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 145.25it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.0786 tar_ll -5.1031 (20.655 secs)\n",
      "\n",
      "anp:anp_periodic step 70200 lr 1.018e-04 [train_loss] loss -1.0841 (5.320 secs)\n",
      "anp:anp_periodic step 70400 lr 1.005e-04 [train_loss] loss -1.0572 (5.226 secs)\n",
      "anp:anp_periodic step 70600 lr 9.927e-05 [train_loss] loss -1.1362 (5.515 secs)\n",
      "anp:anp_periodic step 70800 lr 9.802e-05 [train_loss] loss -1.0696 (5.357 secs)\n",
      "anp:anp_periodic step 71000 lr 9.677e-05 [train_loss] loss -1.0778 (6.256 secs)\n",
      "anp:anp_periodic step 71200 lr 9.554e-05 [train_loss] loss -1.1030 (6.743 secs)\n",
      "anp:anp_periodic step 71400 lr 9.430e-05 [train_loss] loss -1.0856 (6.954 secs)\n",
      "anp:anp_periodic step 71600 lr 9.308e-05 [train_loss] loss -1.0232 (7.209 secs)\n",
      "anp:anp_periodic step 71800 lr 9.186e-05 [train_loss] loss -1.1023 (7.327 secs)\n",
      "anp:anp_periodic step 72000 lr 9.064e-05 [train_loss] loss -1.1181 (6.843 secs)\n",
      "anp:anp_periodic step 72200 lr 8.944e-05 [train_loss] loss -1.1070 (6.826 secs)\n",
      "anp:anp_periodic step 72400 lr 8.824e-05 [train_loss] loss -1.0850 (7.155 secs)\n",
      "anp:anp_periodic step 72600 lr 8.704e-05 [train_loss] loss -1.0587 (6.951 secs)\n",
      "anp:anp_periodic step 72800 lr 8.585e-05 [train_loss] loss -1.0820 (6.534 secs)\n",
      "anp:anp_periodic step 73000 lr 8.467e-05 [train_loss] loss -1.0784 (5.421 secs)\n",
      "anp:anp_periodic step 73200 lr 8.350e-05 [train_loss] loss -1.0808 (5.248 secs)\n",
      "anp:anp_periodic step 73400 lr 8.233e-05 [train_loss] loss -1.0481 (5.354 secs)\n",
      "anp:anp_periodic step 73600 lr 8.117e-05 [train_loss] loss -1.0845 (5.251 secs)\n",
      "anp:anp_periodic step 73800 lr 8.001e-05 [train_loss] loss -1.0815 (5.274 secs)\n",
      "anp:anp_periodic step 74000 lr 7.886e-05 [train_loss] loss -1.0908 (5.399 secs)\n",
      "anp:anp_periodic step 74200 lr 7.772e-05 [train_loss] loss -1.0874 (5.451 secs)\n",
      "anp:anp_periodic step 74400 lr 7.659e-05 [train_loss] loss -1.0859 (5.350 secs)\n",
      "anp:anp_periodic step 74600 lr 7.546e-05 [train_loss] loss -1.0370 (5.352 secs)\n",
      "anp:anp_periodic step 74800 lr 7.434e-05 [train_loss] loss -1.0713 (5.660 secs)\n",
      "anp:anp_periodic step 75000 lr 7.322e-05 [train_loss] loss -1.1324 (5.503 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 142.43it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.1730 tar_ll -4.7483 (21.066 secs)\n",
      "\n",
      "anp:anp_periodic step 75200 lr 7.212e-05 [train_loss] loss -1.1042 (5.268 secs)\n",
      "anp:anp_periodic step 75400 lr 7.102e-05 [train_loss] loss -1.0845 (5.158 secs)\n",
      "anp:anp_periodic step 75600 lr 6.992e-05 [train_loss] loss -1.1150 (5.219 secs)\n",
      "anp:anp_periodic step 75800 lr 6.884e-05 [train_loss] loss -1.0819 (5.243 secs)\n",
      "anp:anp_periodic step 76000 lr 6.776e-05 [train_loss] loss -1.0831 (5.246 secs)\n",
      "anp:anp_periodic step 76200 lr 6.669e-05 [train_loss] loss -1.0851 (5.323 secs)\n",
      "anp:anp_periodic step 76400 lr 6.562e-05 [train_loss] loss -1.0963 (5.378 secs)\n",
      "anp:anp_periodic step 76600 lr 6.456e-05 [train_loss] loss -1.1010 (5.146 secs)\n",
      "anp:anp_periodic step 76800 lr 6.351e-05 [train_loss] loss -1.0778 (5.310 secs)\n",
      "anp:anp_periodic step 77000 lr 6.247e-05 [train_loss] loss -1.0551 (5.268 secs)\n",
      "anp:anp_periodic step 77200 lr 6.144e-05 [train_loss] loss -1.1276 (5.316 secs)\n",
      "anp:anp_periodic step 77400 lr 6.041e-05 [train_loss] loss -1.0610 (5.321 secs)\n",
      "anp:anp_periodic step 77600 lr 5.939e-05 [train_loss] loss -1.1245 (5.497 secs)\n",
      "anp:anp_periodic step 77800 lr 5.838e-05 [train_loss] loss -1.1044 (5.373 secs)\n",
      "anp:anp_periodic step 78000 lr 5.737e-05 [train_loss] loss -1.0899 (5.329 secs)\n",
      "anp:anp_periodic step 78200 lr 5.637e-05 [train_loss] loss -1.0397 (5.374 secs)\n",
      "anp:anp_periodic step 78400 lr 5.538e-05 [train_loss] loss -1.0708 (5.375 secs)\n",
      "anp:anp_periodic step 78600 lr 5.440e-05 [train_loss] loss -1.0561 (5.400 secs)\n",
      "anp:anp_periodic step 78800 lr 5.343e-05 [train_loss] loss -1.0869 (5.443 secs)\n",
      "anp:anp_periodic step 79000 lr 5.246e-05 [train_loss] loss -1.1546 (5.312 secs)\n",
      "anp:anp_periodic step 79200 lr 5.150e-05 [train_loss] loss -1.0806 (5.446 secs)\n",
      "anp:anp_periodic step 79400 lr 5.055e-05 [train_loss] loss -1.0729 (5.445 secs)\n",
      "anp:anp_periodic step 79600 lr 4.961e-05 [train_loss] loss -1.1052 (5.320 secs)\n",
      "anp:anp_periodic step 79800 lr 4.867e-05 [train_loss] loss -1.0837 (5.456 secs)\n",
      "anp:anp_periodic step 80000 lr 4.775e-05 [train_loss] loss -1.0650 (5.372 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 141.66it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.1315 tar_ll -5.2221 (21.181 secs)\n",
      "\n",
      "anp:anp_periodic step 80200 lr 4.683e-05 [train_loss] loss -1.1035 (5.455 secs)\n",
      "anp:anp_periodic step 80400 lr 4.592e-05 [train_loss] loss -1.1041 (5.498 secs)\n",
      "anp:anp_periodic step 80600 lr 4.501e-05 [train_loss] loss -1.1048 (5.232 secs)\n",
      "anp:anp_periodic step 80800 lr 4.412e-05 [train_loss] loss -1.0640 (5.301 secs)\n",
      "anp:anp_periodic step 81000 lr 4.323e-05 [train_loss] loss -1.0755 (5.252 secs)\n",
      "anp:anp_periodic step 81200 lr 4.235e-05 [train_loss] loss -1.1415 (5.262 secs)\n",
      "anp:anp_periodic step 81400 lr 4.148e-05 [train_loss] loss -1.0635 (5.346 secs)\n",
      "anp:anp_periodic step 81600 lr 4.062e-05 [train_loss] loss -1.1265 (5.296 secs)\n",
      "anp:anp_periodic step 81800 lr 3.976e-05 [train_loss] loss -1.0397 (5.348 secs)\n",
      "anp:anp_periodic step 82000 lr 3.892e-05 [train_loss] loss -1.0498 (5.197 secs)\n",
      "anp:anp_periodic step 82200 lr 3.808e-05 [train_loss] loss -1.0738 (5.392 secs)\n",
      "anp:anp_periodic step 82400 lr 3.725e-05 [train_loss] loss -1.0770 (5.221 secs)\n",
      "anp:anp_periodic step 82600 lr 3.643e-05 [train_loss] loss -1.0787 (5.233 secs)\n",
      "anp:anp_periodic step 82800 lr 3.562e-05 [train_loss] loss -1.0627 (5.281 secs)\n",
      "anp:anp_periodic step 83000 lr 3.481e-05 [train_loss] loss -1.1298 (5.207 secs)\n",
      "anp:anp_periodic step 83200 lr 3.402e-05 [train_loss] loss -1.1327 (5.319 secs)\n",
      "anp:anp_periodic step 83400 lr 3.323e-05 [train_loss] loss -1.1266 (5.166 secs)\n",
      "anp:anp_periodic step 83600 lr 3.245e-05 [train_loss] loss -1.1256 (5.303 secs)\n",
      "anp:anp_periodic step 83800 lr 3.168e-05 [train_loss] loss -1.0660 (5.308 secs)\n",
      "anp:anp_periodic step 84000 lr 3.092e-05 [train_loss] loss -1.1029 (5.035 secs)\n",
      "anp:anp_periodic step 84200 lr 3.017e-05 [train_loss] loss -1.0653 (5.265 secs)\n",
      "anp:anp_periodic step 84400 lr 2.943e-05 [train_loss] loss -1.0785 (5.100 secs)\n",
      "anp:anp_periodic step 84600 lr 2.869e-05 [train_loss] loss -1.0951 (5.328 secs)\n",
      "anp:anp_periodic step 84800 lr 2.797e-05 [train_loss] loss -1.1031 (5.232 secs)\n",
      "anp:anp_periodic step 85000 lr 2.725e-05 [train_loss] loss -1.0886 (5.326 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 141.36it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.1762 tar_ll -5.1839 (21.225 secs)\n",
      "\n",
      "anp:anp_periodic step 85200 lr 2.654e-05 [train_loss] loss -1.0891 (5.433 secs)\n",
      "anp:anp_periodic step 85400 lr 2.584e-05 [train_loss] loss -1.0816 (5.502 secs)\n",
      "anp:anp_periodic step 85600 lr 2.515e-05 [train_loss] loss -1.0588 (5.723 secs)\n",
      "anp:anp_periodic step 85800 lr 2.447e-05 [train_loss] loss -1.1187 (5.390 secs)\n",
      "anp:anp_periodic step 86000 lr 2.379e-05 [train_loss] loss -1.1010 (5.744 secs)\n",
      "anp:anp_periodic step 86200 lr 2.313e-05 [train_loss] loss -1.0859 (5.609 secs)\n",
      "anp:anp_periodic step 86400 lr 2.247e-05 [train_loss] loss -1.0815 (5.362 secs)\n",
      "anp:anp_periodic step 86600 lr 2.183e-05 [train_loss] loss -1.0835 (5.662 secs)\n",
      "anp:anp_periodic step 86800 lr 2.119e-05 [train_loss] loss -1.0979 (5.345 secs)\n",
      "anp:anp_periodic step 87000 lr 2.056e-05 [train_loss] loss -1.0829 (5.410 secs)\n",
      "anp:anp_periodic step 87200 lr 1.994e-05 [train_loss] loss -1.1041 (5.615 secs)\n",
      "anp:anp_periodic step 87400 lr 1.933e-05 [train_loss] loss -1.0680 (5.272 secs)\n",
      "anp:anp_periodic step 87600 lr 1.873e-05 [train_loss] loss -1.0955 (5.269 secs)\n",
      "anp:anp_periodic step 87800 lr 1.814e-05 [train_loss] loss -1.0907 (5.593 secs)\n",
      "anp:anp_periodic step 88000 lr 1.756e-05 [train_loss] loss -1.0993 (5.361 secs)\n",
      "anp:anp_periodic step 88200 lr 1.698e-05 [train_loss] loss -1.0613 (5.427 secs)\n",
      "anp:anp_periodic step 88400 lr 1.642e-05 [train_loss] loss -1.0670 (5.597 secs)\n",
      "anp:anp_periodic step 88600 lr 1.586e-05 [train_loss] loss -1.0777 (5.148 secs)\n",
      "anp:anp_periodic step 88800 lr 1.532e-05 [train_loss] loss -1.0665 (5.421 secs)\n",
      "anp:anp_periodic step 89000 lr 1.478e-05 [train_loss] loss -1.1187 (5.371 secs)\n",
      "anp:anp_periodic step 89200 lr 1.425e-05 [train_loss] loss -1.1000 (5.511 secs)\n",
      "anp:anp_periodic step 89400 lr 1.373e-05 [train_loss] loss -1.0341 (5.254 secs)\n",
      "anp:anp_periodic step 89600 lr 1.323e-05 [train_loss] loss -1.0824 (5.450 secs)\n",
      "anp:anp_periodic step 89800 lr 1.273e-05 [train_loss] loss -1.0855 (5.312 secs)\n",
      "anp:anp_periodic step 90000 lr 1.224e-05 [train_loss] loss -1.1037 (5.218 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 144.18it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.2081 tar_ll -4.8175 (20.810 secs)\n",
      "\n",
      "anp:anp_periodic step 90200 lr 1.176e-05 [train_loss] loss -1.1197 (5.326 secs)\n",
      "anp:anp_periodic step 90400 lr 1.128e-05 [train_loss] loss -1.0505 (5.334 secs)\n",
      "anp:anp_periodic step 90600 lr 1.082e-05 [train_loss] loss -1.1031 (5.420 secs)\n",
      "anp:anp_periodic step 90800 lr 1.037e-05 [train_loss] loss -1.1054 (5.318 secs)\n",
      "anp:anp_periodic step 91000 lr 9.927e-06 [train_loss] loss -1.0725 (5.228 secs)\n",
      "anp:anp_periodic step 91200 lr 9.493e-06 [train_loss] loss -1.1162 (5.312 secs)\n",
      "anp:anp_periodic step 91400 lr 9.069e-06 [train_loss] loss -1.1042 (5.188 secs)\n",
      "anp:anp_periodic step 91600 lr 8.655e-06 [train_loss] loss -1.0682 (5.132 secs)\n",
      "anp:anp_periodic step 91800 lr 8.250e-06 [train_loss] loss -1.0232 (5.337 secs)\n",
      "anp:anp_periodic step 92000 lr 7.854e-06 [train_loss] loss -1.0921 (5.297 secs)\n",
      "anp:anp_periodic step 92200 lr 7.468e-06 [train_loss] loss -1.1127 (5.391 secs)\n",
      "anp:anp_periodic step 92400 lr 7.092e-06 [train_loss] loss -1.1092 (5.221 secs)\n",
      "anp:anp_periodic step 92600 lr 6.725e-06 [train_loss] loss -1.1254 (5.371 secs)\n",
      "anp:anp_periodic step 92800 lr 6.368e-06 [train_loss] loss -1.0174 (5.328 secs)\n",
      "anp:anp_periodic step 93000 lr 6.021e-06 [train_loss] loss -1.1091 (5.340 secs)\n",
      "anp:anp_periodic step 93200 lr 5.683e-06 [train_loss] loss -1.0918 (5.453 secs)\n",
      "anp:anp_periodic step 93400 lr 5.355e-06 [train_loss] loss -1.0881 (5.230 secs)\n",
      "anp:anp_periodic step 93600 lr 5.036e-06 [train_loss] loss -1.1109 (5.675 secs)\n",
      "anp:anp_periodic step 93800 lr 4.727e-06 [train_loss] loss -1.1224 (5.177 secs)\n",
      "anp:anp_periodic step 94000 lr 4.428e-06 [train_loss] loss -1.1070 (5.648 secs)\n",
      "anp:anp_periodic step 94200 lr 4.139e-06 [train_loss] loss -1.0948 (5.582 secs)\n",
      "anp:anp_periodic step 94400 lr 3.859e-06 [train_loss] loss -1.0873 (5.350 secs)\n",
      "anp:anp_periodic step 94600 lr 3.589e-06 [train_loss] loss -1.0830 (5.471 secs)\n",
      "anp:anp_periodic step 94800 lr 3.329e-06 [train_loss] loss -1.0545 (5.389 secs)\n",
      "anp:anp_periodic step 95000 lr 3.078e-06 [train_loss] loss -1.0719 (5.653 secs)\n",
      "100%|##########| 3000/3000 [00:21<00:00, 141.89it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.2091 tar_ll -4.9271 (21.146 secs)\n",
      "\n",
      "anp:anp_periodic step 95200 lr 2.837e-06 [train_loss] loss -1.1093 (5.444 secs)\n",
      "anp:anp_periodic step 95400 lr 2.606e-06 [train_loss] loss -1.0980 (5.359 secs)\n",
      "anp:anp_periodic step 95600 lr 2.385e-06 [train_loss] loss -1.1330 (5.634 secs)\n",
      "anp:anp_periodic step 95800 lr 2.173e-06 [train_loss] loss -1.0855 (5.324 secs)\n",
      "anp:anp_periodic step 96000 lr 1.971e-06 [train_loss] loss -1.1443 (5.508 secs)\n",
      "anp:anp_periodic step 96200 lr 1.779e-06 [train_loss] loss -1.1070 (5.460 secs)\n",
      "anp:anp_periodic step 96400 lr 1.597e-06 [train_loss] loss -1.1016 (5.277 secs)\n",
      "anp:anp_periodic step 96600 lr 1.425e-06 [train_loss] loss -1.1024 (5.315 secs)\n",
      "anp:anp_periodic step 96800 lr 1.262e-06 [train_loss] loss -1.0830 (5.250 secs)\n",
      "anp:anp_periodic step 97000 lr 1.110e-06 [train_loss] loss -1.1280 (5.196 secs)\n",
      "anp:anp_periodic step 97200 lr 9.666e-07 [train_loss] loss -1.0806 (5.373 secs)\n",
      "anp:anp_periodic step 97400 lr 8.335e-07 [train_loss] loss -1.1163 (5.557 secs)\n",
      "anp:anp_periodic step 97600 lr 7.103e-07 [train_loss] loss -1.0715 (5.289 secs)\n",
      "anp:anp_periodic step 97800 lr 5.969e-07 [train_loss] loss -1.0542 (5.428 secs)\n",
      "anp:anp_periodic step 98000 lr 4.933e-07 [train_loss] loss -1.1224 (5.542 secs)\n",
      "anp:anp_periodic step 98200 lr 3.996e-07 [train_loss] loss -1.0774 (5.513 secs)\n",
      "anp:anp_periodic step 98400 lr 3.158e-07 [train_loss] loss -1.0518 (5.276 secs)\n",
      "anp:anp_periodic step 98600 lr 2.418e-07 [train_loss] loss -1.0987 (5.295 secs)\n",
      "anp:anp_periodic step 98800 lr 1.776e-07 [train_loss] loss -1.0749 (5.336 secs)\n",
      "anp:anp_periodic step 99000 lr 1.234e-07 [train_loss] loss -1.0914 (5.301 secs)\n",
      "anp:anp_periodic step 99200 lr 7.895e-08 [train_loss] loss -1.0562 (5.438 secs)\n",
      "anp:anp_periodic step 99400 lr 4.441e-08 [train_loss] loss -1.0778 (5.122 secs)\n",
      "anp:anp_periodic step 99600 lr 1.974e-08 [train_loss] loss -1.0814 (5.159 secs)\n",
      "anp:anp_periodic step 99800 lr 4.935e-09 [train_loss] loss -1.1202 (5.228 secs)\n",
      "anp:anp_periodic step 100000 lr 0.000e+00 [train_loss] loss -1.0421 (4.996 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 147.40it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.2073 tar_ll -4.9742 (20.356 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:20<00:00, 143.86it/s]\n",
      "anp:anp_periodic periodic ctx_ll 0.2077 tar_ll -4.9529 (20.856 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3146506.75 miliseconds\n",
      "Execution time: 3146.50675 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 196.248046875 MB\n",
      "Memory Usage Change: 179.998046875 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='anp', name='anp_periodic',val_seed=100, val_l=None,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358f952-23b5-4226-9665-3a8228121233",
   "metadata": {},
   "source": [
    "## BNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b22f803c-d61e-4cf3-864f-484029161e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: bnp-bnp_periodic\n",
      "Total number of parameters: 248450\n",
      "\n",
      "bnp:bnp_periodic step 200 lr 5.000e-04 [train_loss] ll_base -0.6587 ll -0.6558 loss 1.3145 (4.551 secs)\n",
      "bnp:bnp_periodic step 400 lr 5.000e-04 [train_loss] ll_base -0.5818 ll -0.5794 loss 1.1612 (4.533 secs)\n",
      "bnp:bnp_periodic step 600 lr 5.000e-04 [train_loss] ll_base -0.5482 ll -0.5476 loss 1.0958 (4.683 secs)\n",
      "bnp:bnp_periodic step 800 lr 4.999e-04 [train_loss] ll_base -0.5643 ll -0.5636 loss 1.1279 (4.604 secs)\n",
      "bnp:bnp_periodic step 1000 lr 4.999e-04 [train_loss] ll_base -0.5446 ll -0.5418 loss 1.0864 (4.643 secs)\n",
      "bnp:bnp_periodic step 1200 lr 4.998e-04 [train_loss] ll_base -0.5307 ll -0.5273 loss 1.0580 (4.684 secs)\n",
      "bnp:bnp_periodic step 1400 lr 4.998e-04 [train_loss] ll_base -0.5348 ll -0.5232 loss 1.0580 (4.777 secs)\n",
      "bnp:bnp_periodic step 1600 lr 4.997e-04 [train_loss] ll_base -0.4960 ll -0.4831 loss 0.9791 (4.639 secs)\n",
      "bnp:bnp_periodic step 1800 lr 4.996e-04 [train_loss] ll_base -0.4493 ll -0.4315 loss 0.8807 (4.591 secs)\n",
      "bnp:bnp_periodic step 2000 lr 4.995e-04 [train_loss] ll_base -0.4507 ll -0.4316 loss 0.8823 (4.795 secs)\n",
      "bnp:bnp_periodic step 2200 lr 4.994e-04 [train_loss] ll_base -0.4178 ll -0.4019 loss 0.8197 (4.647 secs)\n",
      "bnp:bnp_periodic step 2400 lr 4.993e-04 [train_loss] ll_base -0.4334 ll -0.4167 loss 0.8501 (4.413 secs)\n",
      "bnp:bnp_periodic step 2600 lr 4.992e-04 [train_loss] ll_base -0.4422 ll -0.4252 loss 0.8674 (4.813 secs)\n",
      "bnp:bnp_periodic step 2800 lr 4.990e-04 [train_loss] ll_base -0.4120 ll -0.3878 loss 0.7999 (4.791 secs)\n",
      "bnp:bnp_periodic step 3000 lr 4.989e-04 [train_loss] ll_base -0.3993 ll -0.3726 loss 0.7720 (4.629 secs)\n",
      "bnp:bnp_periodic step 3200 lr 4.987e-04 [train_loss] ll_base -0.3651 ll -0.3308 loss 0.6958 (4.511 secs)\n",
      "bnp:bnp_periodic step 3400 lr 4.986e-04 [train_loss] ll_base -0.3886 ll -0.3488 loss 0.7374 (4.726 secs)\n",
      "bnp:bnp_periodic step 3600 lr 4.984e-04 [train_loss] ll_base -0.3516 ll -0.3175 loss 0.6692 (4.579 secs)\n",
      "bnp:bnp_periodic step 3800 lr 4.982e-04 [train_loss] ll_base -0.3620 ll -0.3311 loss 0.6931 (4.609 secs)\n",
      "bnp:bnp_periodic step 4000 lr 4.980e-04 [train_loss] ll_base -0.3348 ll -0.3030 loss 0.6378 (4.593 secs)\n",
      "bnp:bnp_periodic step 4200 lr 4.978e-04 [train_loss] ll_base -0.2889 ll -0.2528 loss 0.5417 (4.824 secs)\n",
      "bnp:bnp_periodic step 4400 lr 4.976e-04 [train_loss] ll_base -0.3266 ll -0.2879 loss 0.6146 (4.559 secs)\n",
      "bnp:bnp_periodic step 4600 lr 4.974e-04 [train_loss] ll_base -0.2827 ll -0.2460 loss 0.5287 (4.545 secs)\n",
      "bnp:bnp_periodic step 4800 lr 4.972e-04 [train_loss] ll_base -0.3025 ll -0.2616 loss 0.5641 (4.716 secs)\n",
      "bnp:bnp_periodic step 5000 lr 4.969e-04 [train_loss] ll_base -0.2863 ll -0.2449 loss 0.5312 (4.501 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 108.96it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.4331 tar_ll -0.7801 (27.536 secs)\n",
      "\n",
      "bnp:bnp_periodic step 5200 lr 4.967e-04 [train_loss] ll_base -0.3042 ll -0.2543 loss 0.5585 (4.655 secs)\n",
      "bnp:bnp_periodic step 5400 lr 4.964e-04 [train_loss] ll_base -0.2764 ll -0.2302 loss 0.5066 (4.575 secs)\n",
      "bnp:bnp_periodic step 5600 lr 4.961e-04 [train_loss] ll_base -0.2128 ll -0.1677 loss 0.3806 (4.694 secs)\n",
      "bnp:bnp_periodic step 5800 lr 4.959e-04 [train_loss] ll_base -0.2216 ll -0.1746 loss 0.3962 (4.476 secs)\n",
      "bnp:bnp_periodic step 6000 lr 4.956e-04 [train_loss] ll_base -0.2424 ll -0.1984 loss 0.4408 (4.519 secs)\n",
      "bnp:bnp_periodic step 6200 lr 4.953e-04 [train_loss] ll_base -0.1766 ll -0.1348 loss 0.3114 (4.450 secs)\n",
      "bnp:bnp_periodic step 6400 lr 4.950e-04 [train_loss] ll_base -0.2165 ll -0.1659 loss 0.3824 (4.793 secs)\n",
      "bnp:bnp_periodic step 6600 lr 4.946e-04 [train_loss] ll_base -0.2406 ll -0.1824 loss 0.4230 (4.388 secs)\n",
      "bnp:bnp_periodic step 6800 lr 4.943e-04 [train_loss] ll_base -0.1776 ll -0.1288 loss 0.3064 (4.596 secs)\n",
      "bnp:bnp_periodic step 7000 lr 4.940e-04 [train_loss] ll_base -0.2199 ll -0.1660 loss 0.3860 (4.796 secs)\n",
      "bnp:bnp_periodic step 7200 lr 4.936e-04 [train_loss] ll_base -0.1802 ll -0.1267 loss 0.3069 (4.588 secs)\n",
      "bnp:bnp_periodic step 7400 lr 4.933e-04 [train_loss] ll_base -0.1443 ll -0.0916 loss 0.2359 (4.446 secs)\n",
      "bnp:bnp_periodic step 7600 lr 4.929e-04 [train_loss] ll_base -0.1488 ll -0.0899 loss 0.2388 (4.488 secs)\n",
      "bnp:bnp_periodic step 7800 lr 4.925e-04 [train_loss] ll_base -0.1901 ll -0.1322 loss 0.3223 (4.642 secs)\n",
      "bnp:bnp_periodic step 8000 lr 4.921e-04 [train_loss] ll_base -0.1835 ll -0.1306 loss 0.3141 (4.676 secs)\n",
      "bnp:bnp_periodic step 8200 lr 4.918e-04 [train_loss] ll_base -0.1602 ll -0.1044 loss 0.2646 (4.811 secs)\n",
      "bnp:bnp_periodic step 8400 lr 4.913e-04 [train_loss] ll_base -0.1534 ll -0.0901 loss 0.2435 (4.744 secs)\n",
      "bnp:bnp_periodic step 8600 lr 4.909e-04 [train_loss] ll_base -0.1115 ll -0.0497 loss 0.1612 (4.878 secs)\n",
      "bnp:bnp_periodic step 8800 lr 4.905e-04 [train_loss] ll_base -0.1013 ll -0.0318 loss 0.1330 (4.894 secs)\n",
      "bnp:bnp_periodic step 9000 lr 4.901e-04 [train_loss] ll_base -0.1506 ll -0.0893 loss 0.2400 (4.830 secs)\n",
      "bnp:bnp_periodic step 9200 lr 4.896e-04 [train_loss] ll_base -0.1200 ll -0.0633 loss 0.1833 (4.872 secs)\n",
      "bnp:bnp_periodic step 9400 lr 4.892e-04 [train_loss] ll_base -0.1199 ll -0.0500 loss 0.1699 (4.842 secs)\n",
      "bnp:bnp_periodic step 9600 lr 4.887e-04 [train_loss] ll_base -0.0821 ll -0.0178 loss 0.0999 (4.794 secs)\n",
      "bnp:bnp_periodic step 9800 lr 4.882e-04 [train_loss] ll_base -0.1204 ll -0.0521 loss 0.1726 (4.875 secs)\n",
      "bnp:bnp_periodic step 10000 lr 4.878e-04 [train_loss] ll_base -0.0972 ll -0.0307 loss 0.1280 (4.837 secs)\n",
      "100%|##########| 3000/3000 [00:28<00:00, 104.66it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.3800 tar_ll -0.8215 (28.666 secs)\n",
      "\n",
      "bnp:bnp_periodic step 10200 lr 4.873e-04 [train_loss] ll_base -0.1121 ll -0.0402 loss 0.1523 (4.636 secs)\n",
      "bnp:bnp_periodic step 10400 lr 4.868e-04 [train_loss] ll_base -0.0593 ll 0.0079 loss 0.0514 (4.647 secs)\n",
      "bnp:bnp_periodic step 10600 lr 4.863e-04 [train_loss] ll_base -0.1152 ll -0.0516 loss 0.1668 (4.720 secs)\n",
      "bnp:bnp_periodic step 10800 lr 4.857e-04 [train_loss] ll_base -0.0637 ll 0.0073 loss 0.0563 (4.794 secs)\n",
      "bnp:bnp_periodic step 11000 lr 4.852e-04 [train_loss] ll_base -0.0616 ll 0.0061 loss 0.0555 (4.838 secs)\n",
      "bnp:bnp_periodic step 11200 lr 4.847e-04 [train_loss] ll_base -0.0973 ll -0.0307 loss 0.1280 (5.076 secs)\n",
      "bnp:bnp_periodic step 11400 lr 4.841e-04 [train_loss] ll_base -0.0559 ll 0.0080 loss 0.0479 (4.804 secs)\n",
      "bnp:bnp_periodic step 11600 lr 4.836e-04 [train_loss] ll_base -0.0308 ll 0.0399 loss -0.0091 (4.752 secs)\n",
      "bnp:bnp_periodic step 11800 lr 4.830e-04 [train_loss] ll_base -0.0810 ll -0.0143 loss 0.0953 (4.885 secs)\n",
      "bnp:bnp_periodic step 12000 lr 4.824e-04 [train_loss] ll_base 0.0036 ll 0.0705 loss -0.0740 (4.490 secs)\n",
      "bnp:bnp_periodic step 12200 lr 4.819e-04 [train_loss] ll_base 0.0041 ll 0.0808 loss -0.0849 (4.666 secs)\n",
      "bnp:bnp_periodic step 12400 lr 4.813e-04 [train_loss] ll_base -0.0604 ll 0.0176 loss 0.0427 (4.745 secs)\n",
      "bnp:bnp_periodic step 12600 lr 4.807e-04 [train_loss] ll_base -0.0159 ll 0.0643 loss -0.0484 (4.579 secs)\n",
      "bnp:bnp_periodic step 12800 lr 4.801e-04 [train_loss] ll_base -0.0602 ll 0.0310 loss 0.0293 (4.536 secs)\n",
      "bnp:bnp_periodic step 13000 lr 4.794e-04 [train_loss] ll_base 0.0024 ll 0.0752 loss -0.0776 (4.705 secs)\n",
      "bnp:bnp_periodic step 13200 lr 4.788e-04 [train_loss] ll_base 0.0145 ll 0.0885 loss -0.1030 (4.886 secs)\n",
      "bnp:bnp_periodic step 13400 lr 4.782e-04 [train_loss] ll_base -0.0267 ll 0.0428 loss -0.0161 (4.591 secs)\n",
      "bnp:bnp_periodic step 13600 lr 4.775e-04 [train_loss] ll_base -0.0035 ll 0.0699 loss -0.0664 (4.522 secs)\n",
      "bnp:bnp_periodic step 13800 lr 4.769e-04 [train_loss] ll_base -0.0097 ll 0.0624 loss -0.0527 (4.857 secs)\n",
      "bnp:bnp_periodic step 14000 lr 4.762e-04 [train_loss] ll_base -0.0054 ll 0.0648 loss -0.0594 (4.885 secs)\n",
      "bnp:bnp_periodic step 14200 lr 4.755e-04 [train_loss] ll_base 0.0202 ll 0.0940 loss -0.1142 (4.659 secs)\n",
      "bnp:bnp_periodic step 14400 lr 4.749e-04 [train_loss] ll_base -0.0414 ll 0.0342 loss 0.0072 (4.774 secs)\n",
      "bnp:bnp_periodic step 14600 lr 4.742e-04 [train_loss] ll_base 0.0047 ll 0.0850 loss -0.0897 (4.813 secs)\n",
      "bnp:bnp_periodic step 14800 lr 4.735e-04 [train_loss] ll_base 0.0163 ll 0.0906 loss -0.1069 (4.548 secs)\n",
      "bnp:bnp_periodic step 15000 lr 4.728e-04 [train_loss] ll_base 0.0165 ll 0.0955 loss -0.1120 (4.662 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 109.42it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.3576 tar_ll -0.8596 (27.420 secs)\n",
      "\n",
      "bnp:bnp_periodic step 15200 lr 4.720e-04 [train_loss] ll_base 0.0746 ll 0.1465 loss -0.2212 (4.567 secs)\n",
      "bnp:bnp_periodic step 15400 lr 4.713e-04 [train_loss] ll_base 0.0038 ll 0.0832 loss -0.0870 (4.492 secs)\n",
      "bnp:bnp_periodic step 15600 lr 4.706e-04 [train_loss] ll_base 0.0651 ll 0.1415 loss -0.2067 (4.765 secs)\n",
      "bnp:bnp_periodic step 15800 lr 4.698e-04 [train_loss] ll_base 0.0452 ll 0.1340 loss -0.1792 (4.542 secs)\n",
      "bnp:bnp_periodic step 16000 lr 4.691e-04 [train_loss] ll_base 0.0302 ll 0.1045 loss -0.1347 (4.760 secs)\n",
      "bnp:bnp_periodic step 16200 lr 4.683e-04 [train_loss] ll_base 0.0378 ll 0.1148 loss -0.1525 (4.517 secs)\n",
      "bnp:bnp_periodic step 16400 lr 4.675e-04 [train_loss] ll_base 0.0582 ll 0.1438 loss -0.2020 (4.585 secs)\n",
      "bnp:bnp_periodic step 16600 lr 4.668e-04 [train_loss] ll_base 0.0561 ll 0.1369 loss -0.1931 (4.537 secs)\n",
      "bnp:bnp_periodic step 16800 lr 4.660e-04 [train_loss] ll_base 0.0888 ll 0.1676 loss -0.2564 (4.731 secs)\n",
      "bnp:bnp_periodic step 17000 lr 4.652e-04 [train_loss] ll_base 0.0502 ll 0.1282 loss -0.1784 (4.845 secs)\n",
      "bnp:bnp_periodic step 17200 lr 4.644e-04 [train_loss] ll_base 0.1016 ll 0.1870 loss -0.2886 (4.671 secs)\n",
      "bnp:bnp_periodic step 17400 lr 4.636e-04 [train_loss] ll_base 0.0421 ll 0.1204 loss -0.1625 (4.671 secs)\n",
      "bnp:bnp_periodic step 17600 lr 4.627e-04 [train_loss] ll_base 0.0750 ll 0.1686 loss -0.2437 (4.748 secs)\n",
      "bnp:bnp_periodic step 17800 lr 4.619e-04 [train_loss] ll_base 0.0417 ll 0.1295 loss -0.1712 (4.636 secs)\n",
      "bnp:bnp_periodic step 18000 lr 4.611e-04 [train_loss] ll_base 0.0273 ll 0.1050 loss -0.1323 (4.897 secs)\n",
      "bnp:bnp_periodic step 18200 lr 4.602e-04 [train_loss] ll_base 0.1212 ll 0.2029 loss -0.3242 (4.591 secs)\n",
      "bnp:bnp_periodic step 18400 lr 4.594e-04 [train_loss] ll_base 0.0572 ll 0.1387 loss -0.1959 (4.491 secs)\n",
      "bnp:bnp_periodic step 18600 lr 4.585e-04 [train_loss] ll_base 0.0633 ll 0.1547 loss -0.2180 (4.665 secs)\n",
      "bnp:bnp_periodic step 18800 lr 4.576e-04 [train_loss] ll_base 0.1051 ll 0.1927 loss -0.2978 (4.986 secs)\n",
      "bnp:bnp_periodic step 19000 lr 4.568e-04 [train_loss] ll_base 0.0934 ll 0.1720 loss -0.2653 (4.473 secs)\n",
      "bnp:bnp_periodic step 19200 lr 4.559e-04 [train_loss] ll_base 0.0896 ll 0.1710 loss -0.2607 (4.667 secs)\n",
      "bnp:bnp_periodic step 19400 lr 4.550e-04 [train_loss] ll_base 0.0836 ll 0.1622 loss -0.2458 (4.687 secs)\n",
      "bnp:bnp_periodic step 19600 lr 4.541e-04 [train_loss] ll_base 0.0474 ll 0.1206 loss -0.1679 (4.719 secs)\n",
      "bnp:bnp_periodic step 19800 lr 4.532e-04 [train_loss] ll_base 0.1068 ll 0.1908 loss -0.2976 (4.651 secs)\n",
      "bnp:bnp_periodic step 20000 lr 4.523e-04 [train_loss] ll_base 0.0814 ll 0.1630 loss -0.2444 (4.609 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 109.25it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.3323 tar_ll -0.8908 (27.463 secs)\n",
      "\n",
      "bnp:bnp_periodic step 20200 lr 4.513e-04 [train_loss] ll_base 0.1338 ll 0.2123 loss -0.3461 (4.676 secs)\n",
      "bnp:bnp_periodic step 20400 lr 4.504e-04 [train_loss] ll_base 0.0949 ll 0.1774 loss -0.2724 (4.475 secs)\n",
      "bnp:bnp_periodic step 20600 lr 4.494e-04 [train_loss] ll_base 0.1211 ll 0.2060 loss -0.3271 (4.643 secs)\n",
      "bnp:bnp_periodic step 20800 lr 4.485e-04 [train_loss] ll_base 0.1235 ll 0.2060 loss -0.3295 (4.667 secs)\n",
      "bnp:bnp_periodic step 21000 lr 4.475e-04 [train_loss] ll_base 0.1203 ll 0.2078 loss -0.3281 (4.707 secs)\n",
      "bnp:bnp_periodic step 21200 lr 4.466e-04 [train_loss] ll_base 0.0818 ll 0.1633 loss -0.2451 (4.655 secs)\n",
      "bnp:bnp_periodic step 21400 lr 4.456e-04 [train_loss] ll_base 0.1191 ll 0.1969 loss -0.3160 (4.512 secs)\n",
      "bnp:bnp_periodic step 21600 lr 4.446e-04 [train_loss] ll_base 0.0979 ll 0.1847 loss -0.2826 (4.737 secs)\n",
      "bnp:bnp_periodic step 21800 lr 4.436e-04 [train_loss] ll_base 0.1357 ll 0.2144 loss -0.3502 (4.634 secs)\n",
      "bnp:bnp_periodic step 22000 lr 4.426e-04 [train_loss] ll_base 0.0754 ll 0.1569 loss -0.2323 (4.567 secs)\n",
      "bnp:bnp_periodic step 22200 lr 4.416e-04 [train_loss] ll_base 0.1123 ll 0.1991 loss -0.3115 (4.599 secs)\n",
      "bnp:bnp_periodic step 22400 lr 4.406e-04 [train_loss] ll_base 0.1681 ll 0.2418 loss -0.4100 (4.648 secs)\n",
      "bnp:bnp_periodic step 22600 lr 4.396e-04 [train_loss] ll_base 0.0666 ll 0.1484 loss -0.2150 (4.535 secs)\n",
      "bnp:bnp_periodic step 22800 lr 4.386e-04 [train_loss] ll_base 0.1160 ll 0.2007 loss -0.3167 (4.592 secs)\n",
      "bnp:bnp_periodic step 23000 lr 4.375e-04 [train_loss] ll_base 0.1379 ll 0.2201 loss -0.3580 (4.684 secs)\n",
      "bnp:bnp_periodic step 23200 lr 4.365e-04 [train_loss] ll_base 0.1219 ll 0.2081 loss -0.3299 (4.743 secs)\n",
      "bnp:bnp_periodic step 23400 lr 4.354e-04 [train_loss] ll_base 0.1407 ll 0.2275 loss -0.3683 (4.527 secs)\n",
      "bnp:bnp_periodic step 23600 lr 4.344e-04 [train_loss] ll_base 0.1124 ll 0.1948 loss -0.3072 (4.591 secs)\n",
      "bnp:bnp_periodic step 23800 lr 4.333e-04 [train_loss] ll_base 0.1543 ll 0.2365 loss -0.3908 (4.769 secs)\n",
      "bnp:bnp_periodic step 24000 lr 4.322e-04 [train_loss] ll_base 0.1322 ll 0.2193 loss -0.3515 (4.585 secs)\n",
      "bnp:bnp_periodic step 24200 lr 4.312e-04 [train_loss] ll_base 0.1434 ll 0.2317 loss -0.3750 (4.546 secs)\n",
      "bnp:bnp_periodic step 24400 lr 4.301e-04 [train_loss] ll_base 0.1306 ll 0.2120 loss -0.3426 (4.653 secs)\n",
      "bnp:bnp_periodic step 24600 lr 4.290e-04 [train_loss] ll_base 0.1396 ll 0.2198 loss -0.3594 (4.563 secs)\n",
      "bnp:bnp_periodic step 24800 lr 4.279e-04 [train_loss] ll_base 0.1485 ll 0.2304 loss -0.3788 (4.610 secs)\n",
      "bnp:bnp_periodic step 25000 lr 4.268e-04 [train_loss] ll_base 0.1248 ll 0.2082 loss -0.3330 (4.655 secs)\n",
      "100%|##########| 3000/3000 [00:28<00:00, 106.60it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.3196 tar_ll -0.9324 (28.147 secs)\n",
      "\n",
      "bnp:bnp_periodic step 25200 lr 4.257e-04 [train_loss] ll_base 0.1537 ll 0.2410 loss -0.3947 (4.859 secs)\n",
      "bnp:bnp_periodic step 25400 lr 4.245e-04 [train_loss] ll_base 0.1489 ll 0.2278 loss -0.3768 (4.580 secs)\n",
      "bnp:bnp_periodic step 25600 lr 4.234e-04 [train_loss] ll_base 0.1933 ll 0.2672 loss -0.4605 (4.656 secs)\n",
      "bnp:bnp_periodic step 25800 lr 4.223e-04 [train_loss] ll_base 0.1770 ll 0.2661 loss -0.4431 (4.764 secs)\n",
      "bnp:bnp_periodic step 26000 lr 4.211e-04 [train_loss] ll_base 0.1397 ll 0.2197 loss -0.3593 (4.830 secs)\n",
      "bnp:bnp_periodic step 26200 lr 4.200e-04 [train_loss] ll_base 0.1543 ll 0.2466 loss -0.4008 (4.618 secs)\n",
      "bnp:bnp_periodic step 26400 lr 4.188e-04 [train_loss] ll_base 0.1261 ll 0.2189 loss -0.3450 (4.740 secs)\n",
      "bnp:bnp_periodic step 26600 lr 4.177e-04 [train_loss] ll_base 0.1679 ll 0.2489 loss -0.4167 (4.783 secs)\n",
      "bnp:bnp_periodic step 26800 lr 4.165e-04 [train_loss] ll_base 0.1731 ll 0.2542 loss -0.4273 (4.592 secs)\n",
      "bnp:bnp_periodic step 27000 lr 4.153e-04 [train_loss] ll_base 0.2045 ll 0.2881 loss -0.4926 (4.590 secs)\n",
      "bnp:bnp_periodic step 27200 lr 4.141e-04 [train_loss] ll_base 0.1967 ll 0.2831 loss -0.4798 (4.845 secs)\n",
      "bnp:bnp_periodic step 27400 lr 4.130e-04 [train_loss] ll_base 0.1925 ll 0.2837 loss -0.4761 (4.720 secs)\n",
      "bnp:bnp_periodic step 27600 lr 4.118e-04 [train_loss] ll_base 0.1496 ll 0.2366 loss -0.3862 (4.590 secs)\n",
      "bnp:bnp_periodic step 27800 lr 4.106e-04 [train_loss] ll_base 0.1780 ll 0.2656 loss -0.4436 (4.683 secs)\n",
      "bnp:bnp_periodic step 28000 lr 4.094e-04 [train_loss] ll_base 0.1803 ll 0.2681 loss -0.4484 (4.861 secs)\n",
      "bnp:bnp_periodic step 28200 lr 4.081e-04 [train_loss] ll_base 0.1868 ll 0.2657 loss -0.4525 (4.591 secs)\n",
      "bnp:bnp_periodic step 28400 lr 4.069e-04 [train_loss] ll_base 0.2022 ll 0.2772 loss -0.4794 (4.661 secs)\n",
      "bnp:bnp_periodic step 28600 lr 4.057e-04 [train_loss] ll_base 0.2077 ll 0.2892 loss -0.4968 (4.803 secs)\n",
      "bnp:bnp_periodic step 28800 lr 4.045e-04 [train_loss] ll_base 0.2419 ll 0.3239 loss -0.5658 (4.698 secs)\n",
      "bnp:bnp_periodic step 29000 lr 4.032e-04 [train_loss] ll_base 0.1582 ll 0.2346 loss -0.3928 (4.600 secs)\n",
      "bnp:bnp_periodic step 29200 lr 4.020e-04 [train_loss] ll_base 0.1536 ll 0.2378 loss -0.3914 (4.521 secs)\n",
      "bnp:bnp_periodic step 29400 lr 4.007e-04 [train_loss] ll_base 0.2133 ll 0.2969 loss -0.5102 (4.678 secs)\n",
      "bnp:bnp_periodic step 29600 lr 3.995e-04 [train_loss] ll_base 0.1964 ll 0.2906 loss -0.4870 (4.439 secs)\n",
      "bnp:bnp_periodic step 29800 lr 3.982e-04 [train_loss] ll_base 0.2061 ll 0.2950 loss -0.5011 (4.587 secs)\n",
      "bnp:bnp_periodic step 30000 lr 3.969e-04 [train_loss] ll_base 0.2017 ll 0.2931 loss -0.4949 (4.734 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 107.97it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2914 tar_ll -0.9211 (27.789 secs)\n",
      "\n",
      "bnp:bnp_periodic step 30200 lr 3.957e-04 [train_loss] ll_base 0.2332 ll 0.3136 loss -0.5468 (4.521 secs)\n",
      "bnp:bnp_periodic step 30400 lr 3.944e-04 [train_loss] ll_base 0.1956 ll 0.2782 loss -0.4739 (4.536 secs)\n",
      "bnp:bnp_periodic step 30600 lr 3.931e-04 [train_loss] ll_base 0.2075 ll 0.2917 loss -0.4993 (4.480 secs)\n",
      "bnp:bnp_periodic step 30800 lr 3.918e-04 [train_loss] ll_base 0.1991 ll 0.2849 loss -0.4839 (4.678 secs)\n",
      "bnp:bnp_periodic step 31000 lr 3.905e-04 [train_loss] ll_base 0.1086 ll 0.2035 loss -0.3120 (4.474 secs)\n",
      "bnp:bnp_periodic step 31200 lr 3.892e-04 [train_loss] ll_base 0.2170 ll 0.3020 loss -0.5190 (4.620 secs)\n",
      "bnp:bnp_periodic step 31400 lr 3.879e-04 [train_loss] ll_base 0.2082 ll 0.2953 loss -0.5035 (4.544 secs)\n",
      "bnp:bnp_periodic step 31600 lr 3.866e-04 [train_loss] ll_base 0.2188 ll 0.3036 loss -0.5224 (4.433 secs)\n",
      "bnp:bnp_periodic step 31800 lr 3.853e-04 [train_loss] ll_base 0.2049 ll 0.2834 loss -0.4883 (4.410 secs)\n",
      "bnp:bnp_periodic step 32000 lr 3.840e-04 [train_loss] ll_base 0.2515 ll 0.3387 loss -0.5902 (4.491 secs)\n",
      "bnp:bnp_periodic step 32200 lr 3.826e-04 [train_loss] ll_base 0.1999 ll 0.2844 loss -0.4843 (4.686 secs)\n",
      "bnp:bnp_periodic step 32400 lr 3.813e-04 [train_loss] ll_base 0.2342 ll 0.3181 loss -0.5523 (4.486 secs)\n",
      "bnp:bnp_periodic step 32600 lr 3.800e-04 [train_loss] ll_base 0.2705 ll 0.3519 loss -0.6224 (4.662 secs)\n",
      "bnp:bnp_periodic step 32800 lr 3.786e-04 [train_loss] ll_base 0.2792 ll 0.3641 loss -0.6433 (4.546 secs)\n",
      "bnp:bnp_periodic step 33000 lr 3.773e-04 [train_loss] ll_base 0.2019 ll 0.2906 loss -0.4925 (4.769 secs)\n",
      "bnp:bnp_periodic step 33200 lr 3.759e-04 [train_loss] ll_base 0.1738 ll 0.2558 loss -0.4297 (4.590 secs)\n",
      "bnp:bnp_periodic step 33400 lr 3.745e-04 [train_loss] ll_base 0.2328 ll 0.3188 loss -0.5516 (4.576 secs)\n",
      "bnp:bnp_periodic step 33600 lr 3.732e-04 [train_loss] ll_base 0.2456 ll 0.3278 loss -0.5734 (4.855 secs)\n",
      "bnp:bnp_periodic step 33800 lr 3.718e-04 [train_loss] ll_base 0.2506 ll 0.3344 loss -0.5850 (4.684 secs)\n",
      "bnp:bnp_periodic step 34000 lr 3.704e-04 [train_loss] ll_base 0.2178 ll 0.3073 loss -0.5251 (4.547 secs)\n",
      "bnp:bnp_periodic step 34200 lr 3.691e-04 [train_loss] ll_base 0.2698 ll 0.3533 loss -0.6231 (4.602 secs)\n",
      "bnp:bnp_periodic step 34400 lr 3.677e-04 [train_loss] ll_base 0.2250 ll 0.3107 loss -0.5357 (4.662 secs)\n",
      "bnp:bnp_periodic step 34600 lr 3.663e-04 [train_loss] ll_base 0.2614 ll 0.3415 loss -0.6029 (4.464 secs)\n",
      "bnp:bnp_periodic step 34800 lr 3.649e-04 [train_loss] ll_base 0.2357 ll 0.3145 loss -0.5502 (4.616 secs)\n",
      "bnp:bnp_periodic step 35000 lr 3.635e-04 [train_loss] ll_base 0.2528 ll 0.3287 loss -0.5815 (4.702 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 109.26it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2900 tar_ll -0.9310 (27.460 secs)\n",
      "\n",
      "bnp:bnp_periodic step 35200 lr 3.621e-04 [train_loss] ll_base 0.2415 ll 0.3171 loss -0.5586 (4.940 secs)\n",
      "bnp:bnp_periodic step 35400 lr 3.607e-04 [train_loss] ll_base 0.2016 ll 0.2844 loss -0.4859 (4.459 secs)\n",
      "bnp:bnp_periodic step 35600 lr 3.593e-04 [train_loss] ll_base 0.2692 ll 0.3516 loss -0.6208 (4.475 secs)\n",
      "bnp:bnp_periodic step 35800 lr 3.579e-04 [train_loss] ll_base 0.2492 ll 0.3369 loss -0.5861 (4.730 secs)\n",
      "bnp:bnp_periodic step 36000 lr 3.564e-04 [train_loss] ll_base 0.2607 ll 0.3502 loss -0.6109 (4.512 secs)\n",
      "bnp:bnp_periodic step 36200 lr 3.550e-04 [train_loss] ll_base 0.2564 ll 0.3389 loss -0.5953 (4.649 secs)\n",
      "bnp:bnp_periodic step 36400 lr 3.536e-04 [train_loss] ll_base 0.2629 ll 0.3516 loss -0.6145 (4.491 secs)\n",
      "bnp:bnp_periodic step 36600 lr 3.522e-04 [train_loss] ll_base 0.2598 ll 0.3461 loss -0.6059 (4.716 secs)\n",
      "bnp:bnp_periodic step 36800 lr 3.507e-04 [train_loss] ll_base 0.2680 ll 0.3555 loss -0.6235 (4.517 secs)\n",
      "bnp:bnp_periodic step 37000 lr 3.493e-04 [train_loss] ll_base 0.2734 ll 0.3627 loss -0.6361 (4.517 secs)\n",
      "bnp:bnp_periodic step 37200 lr 3.478e-04 [train_loss] ll_base 0.3245 ll 0.4028 loss -0.7274 (4.623 secs)\n",
      "bnp:bnp_periodic step 37400 lr 3.464e-04 [train_loss] ll_base 0.2609 ll 0.3410 loss -0.6018 (4.318 secs)\n",
      "bnp:bnp_periodic step 37600 lr 3.449e-04 [train_loss] ll_base 0.2435 ll 0.3290 loss -0.5725 (4.373 secs)\n",
      "bnp:bnp_periodic step 37800 lr 3.435e-04 [train_loss] ll_base 0.2157 ll 0.3055 loss -0.5212 (4.584 secs)\n",
      "bnp:bnp_periodic step 38000 lr 3.420e-04 [train_loss] ll_base 0.2643 ll 0.3394 loss -0.6036 (4.650 secs)\n",
      "bnp:bnp_periodic step 38200 lr 3.406e-04 [train_loss] ll_base 0.3229 ll 0.4014 loss -0.7243 (4.463 secs)\n",
      "bnp:bnp_periodic step 38400 lr 3.391e-04 [train_loss] ll_base 0.2647 ll 0.3458 loss -0.6105 (4.541 secs)\n",
      "bnp:bnp_periodic step 38600 lr 3.376e-04 [train_loss] ll_base 0.2619 ll 0.3386 loss -0.6005 (4.492 secs)\n",
      "bnp:bnp_periodic step 38800 lr 3.362e-04 [train_loss] ll_base 0.2630 ll 0.3528 loss -0.6157 (4.658 secs)\n",
      "bnp:bnp_periodic step 39000 lr 3.347e-04 [train_loss] ll_base 0.2380 ll 0.3246 loss -0.5626 (4.444 secs)\n",
      "bnp:bnp_periodic step 39200 lr 3.332e-04 [train_loss] ll_base 0.1926 ll 0.2726 loss -0.4652 (4.501 secs)\n",
      "bnp:bnp_periodic step 39400 lr 3.317e-04 [train_loss] ll_base 0.2681 ll 0.3520 loss -0.6202 (4.623 secs)\n",
      "bnp:bnp_periodic step 39600 lr 3.302e-04 [train_loss] ll_base 0.2569 ll 0.3462 loss -0.6031 (4.418 secs)\n",
      "bnp:bnp_periodic step 39800 lr 3.287e-04 [train_loss] ll_base 0.3140 ll 0.4025 loss -0.7165 (4.500 secs)\n",
      "bnp:bnp_periodic step 40000 lr 3.273e-04 [train_loss] ll_base 0.3309 ll 0.4090 loss -0.7399 (4.465 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 109.66it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2706 tar_ll -0.9108 (27.361 secs)\n",
      "\n",
      "bnp:bnp_periodic step 40200 lr 3.258e-04 [train_loss] ll_base 0.2942 ll 0.3718 loss -0.6660 (4.637 secs)\n",
      "bnp:bnp_periodic step 40400 lr 3.243e-04 [train_loss] ll_base 0.2561 ll 0.3442 loss -0.6002 (4.420 secs)\n",
      "bnp:bnp_periodic step 40600 lr 3.228e-04 [train_loss] ll_base 0.3287 ll 0.4095 loss -0.7382 (4.685 secs)\n",
      "bnp:bnp_periodic step 40800 lr 3.213e-04 [train_loss] ll_base 0.3220 ll 0.4013 loss -0.7233 (4.583 secs)\n",
      "bnp:bnp_periodic step 41000 lr 3.197e-04 [train_loss] ll_base 0.3004 ll 0.3809 loss -0.6813 (4.688 secs)\n",
      "bnp:bnp_periodic step 41200 lr 3.182e-04 [train_loss] ll_base 0.2820 ll 0.3716 loss -0.6536 (4.582 secs)\n",
      "bnp:bnp_periodic step 41400 lr 3.167e-04 [train_loss] ll_base 0.2723 ll 0.3601 loss -0.6324 (4.512 secs)\n",
      "bnp:bnp_periodic step 41600 lr 3.152e-04 [train_loss] ll_base 0.2566 ll 0.3316 loss -0.5882 (4.624 secs)\n",
      "bnp:bnp_periodic step 41800 lr 3.137e-04 [train_loss] ll_base 0.3244 ll 0.4087 loss -0.7330 (4.594 secs)\n",
      "bnp:bnp_periodic step 42000 lr 3.122e-04 [train_loss] ll_base 0.3147 ll 0.4011 loss -0.7157 (4.535 secs)\n",
      "bnp:bnp_periodic step 42200 lr 3.106e-04 [train_loss] ll_base 0.2958 ll 0.3797 loss -0.6756 (4.744 secs)\n",
      "bnp:bnp_periodic step 42400 lr 3.091e-04 [train_loss] ll_base 0.3166 ll 0.3873 loss -0.7039 (4.729 secs)\n",
      "bnp:bnp_periodic step 42600 lr 3.076e-04 [train_loss] ll_base 0.3033 ll 0.3814 loss -0.6847 (4.562 secs)\n",
      "bnp:bnp_periodic step 42800 lr 3.061e-04 [train_loss] ll_base 0.2518 ll 0.3423 loss -0.5941 (4.655 secs)\n",
      "bnp:bnp_periodic step 43000 lr 3.045e-04 [train_loss] ll_base 0.3064 ll 0.3900 loss -0.6963 (4.730 secs)\n",
      "bnp:bnp_periodic step 43200 lr 3.030e-04 [train_loss] ll_base 0.2531 ll 0.3439 loss -0.5970 (4.545 secs)\n",
      "bnp:bnp_periodic step 43400 lr 3.015e-04 [train_loss] ll_base 0.3402 ll 0.4191 loss -0.7594 (4.450 secs)\n",
      "bnp:bnp_periodic step 43600 lr 2.999e-04 [train_loss] ll_base 0.3187 ll 0.3992 loss -0.7179 (4.710 secs)\n",
      "bnp:bnp_periodic step 43800 lr 2.984e-04 [train_loss] ll_base 0.3629 ll 0.4462 loss -0.8092 (4.547 secs)\n",
      "bnp:bnp_periodic step 44000 lr 2.968e-04 [train_loss] ll_base 0.3337 ll 0.4137 loss -0.7473 (4.736 secs)\n",
      "bnp:bnp_periodic step 44200 lr 2.953e-04 [train_loss] ll_base 0.3198 ll 0.3979 loss -0.7178 (4.532 secs)\n",
      "bnp:bnp_periodic step 44400 lr 2.938e-04 [train_loss] ll_base 0.2898 ll 0.3735 loss -0.6633 (4.762 secs)\n",
      "bnp:bnp_periodic step 44600 lr 2.922e-04 [train_loss] ll_base 0.2667 ll 0.3425 loss -0.6091 (4.603 secs)\n",
      "bnp:bnp_periodic step 44800 lr 2.907e-04 [train_loss] ll_base 0.3106 ll 0.3864 loss -0.6970 (4.712 secs)\n",
      "bnp:bnp_periodic step 45000 lr 2.891e-04 [train_loss] ll_base 0.3465 ll 0.4304 loss -0.7768 (4.715 secs)\n",
      "100%|##########| 3000/3000 [00:28<00:00, 106.60it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2657 tar_ll -0.9513 (28.146 secs)\n",
      "\n",
      "bnp:bnp_periodic step 45200 lr 2.876e-04 [train_loss] ll_base 0.3169 ll 0.3971 loss -0.7139 (4.908 secs)\n",
      "bnp:bnp_periodic step 45400 lr 2.860e-04 [train_loss] ll_base 0.2765 ll 0.3611 loss -0.6376 (4.519 secs)\n",
      "bnp:bnp_periodic step 45600 lr 2.844e-04 [train_loss] ll_base 0.3319 ll 0.4083 loss -0.7402 (4.464 secs)\n",
      "bnp:bnp_periodic step 45800 lr 2.829e-04 [train_loss] ll_base 0.3377 ll 0.4160 loss -0.7537 (4.619 secs)\n",
      "bnp:bnp_periodic step 46000 lr 2.813e-04 [train_loss] ll_base 0.3786 ll 0.4592 loss -0.8378 (4.553 secs)\n",
      "bnp:bnp_periodic step 46200 lr 2.798e-04 [train_loss] ll_base 0.2794 ll 0.3559 loss -0.6353 (4.696 secs)\n",
      "bnp:bnp_periodic step 46400 lr 2.782e-04 [train_loss] ll_base 0.3287 ll 0.4082 loss -0.7369 (4.646 secs)\n",
      "bnp:bnp_periodic step 46600 lr 2.767e-04 [train_loss] ll_base 0.2920 ll 0.3710 loss -0.6630 (4.838 secs)\n",
      "bnp:bnp_periodic step 46800 lr 2.751e-04 [train_loss] ll_base 0.3307 ll 0.4030 loss -0.7337 (4.582 secs)\n",
      "bnp:bnp_periodic step 47000 lr 2.735e-04 [train_loss] ll_base 0.2910 ll 0.3788 loss -0.6698 (4.356 secs)\n",
      "bnp:bnp_periodic step 47200 lr 2.720e-04 [train_loss] ll_base 0.3428 ll 0.4193 loss -0.7621 (4.885 secs)\n",
      "bnp:bnp_periodic step 47400 lr 2.704e-04 [train_loss] ll_base 0.3240 ll 0.4025 loss -0.7265 (4.463 secs)\n",
      "bnp:bnp_periodic step 47600 lr 2.688e-04 [train_loss] ll_base 0.2867 ll 0.3733 loss -0.6599 (4.357 secs)\n",
      "bnp:bnp_periodic step 47800 lr 2.673e-04 [train_loss] ll_base 0.3228 ll 0.4032 loss -0.7261 (4.647 secs)\n",
      "bnp:bnp_periodic step 48000 lr 2.657e-04 [train_loss] ll_base 0.2775 ll 0.3523 loss -0.6299 (4.561 secs)\n",
      "bnp:bnp_periodic step 48200 lr 2.641e-04 [train_loss] ll_base 0.3239 ll 0.3990 loss -0.7229 (4.440 secs)\n",
      "bnp:bnp_periodic step 48400 lr 2.626e-04 [train_loss] ll_base 0.3344 ll 0.4109 loss -0.7453 (4.393 secs)\n",
      "bnp:bnp_periodic step 48600 lr 2.610e-04 [train_loss] ll_base 0.3183 ll 0.4043 loss -0.7225 (4.499 secs)\n",
      "bnp:bnp_periodic step 48800 lr 2.594e-04 [train_loss] ll_base 0.3210 ll 0.3982 loss -0.7193 (4.676 secs)\n",
      "bnp:bnp_periodic step 49000 lr 2.579e-04 [train_loss] ll_base 0.3434 ll 0.4256 loss -0.7690 (4.467 secs)\n",
      "bnp:bnp_periodic step 49200 lr 2.563e-04 [train_loss] ll_base 0.3538 ll 0.4362 loss -0.7901 (4.400 secs)\n",
      "bnp:bnp_periodic step 49400 lr 2.547e-04 [train_loss] ll_base 0.3517 ll 0.4245 loss -0.7762 (4.457 secs)\n",
      "bnp:bnp_periodic step 49600 lr 2.531e-04 [train_loss] ll_base 0.3233 ll 0.4093 loss -0.7326 (4.614 secs)\n",
      "bnp:bnp_periodic step 49800 lr 2.516e-04 [train_loss] ll_base 0.3273 ll 0.4023 loss -0.7296 (4.557 secs)\n",
      "bnp:bnp_periodic step 50000 lr 2.500e-04 [train_loss] ll_base 0.3379 ll 0.4178 loss -0.7558 (4.852 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 108.76it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2672 tar_ll -0.8967 (27.585 secs)\n",
      "\n",
      "bnp:bnp_periodic step 50200 lr 2.484e-04 [train_loss] ll_base 0.3224 ll 0.3954 loss -0.7178 (4.676 secs)\n",
      "bnp:bnp_periodic step 50400 lr 2.469e-04 [train_loss] ll_base 0.3335 ll 0.4177 loss -0.7512 (4.536 secs)\n",
      "bnp:bnp_periodic step 50600 lr 2.453e-04 [train_loss] ll_base 0.3692 ll 0.4436 loss -0.8128 (4.598 secs)\n",
      "bnp:bnp_periodic step 50800 lr 2.437e-04 [train_loss] ll_base 0.3784 ll 0.4545 loss -0.8329 (4.680 secs)\n",
      "bnp:bnp_periodic step 51000 lr 2.421e-04 [train_loss] ll_base 0.3658 ll 0.4462 loss -0.8121 (4.703 secs)\n",
      "bnp:bnp_periodic step 51200 lr 2.406e-04 [train_loss] ll_base 0.4020 ll 0.4846 loss -0.8867 (4.536 secs)\n",
      "bnp:bnp_periodic step 51400 lr 2.390e-04 [train_loss] ll_base 0.3975 ll 0.4930 loss -0.8905 (4.602 secs)\n",
      "bnp:bnp_periodic step 51600 lr 2.374e-04 [train_loss] ll_base 0.3481 ll 0.4236 loss -0.7717 (4.832 secs)\n",
      "bnp:bnp_periodic step 51800 lr 2.359e-04 [train_loss] ll_base 0.3973 ll 0.4755 loss -0.8729 (4.576 secs)\n",
      "bnp:bnp_periodic step 52000 lr 2.343e-04 [train_loss] ll_base 0.3515 ll 0.4296 loss -0.7811 (4.568 secs)\n",
      "bnp:bnp_periodic step 52200 lr 2.327e-04 [train_loss] ll_base 0.4093 ll 0.4799 loss -0.8892 (4.898 secs)\n",
      "bnp:bnp_periodic step 52400 lr 2.312e-04 [train_loss] ll_base 0.3779 ll 0.4599 loss -0.8378 (4.788 secs)\n",
      "bnp:bnp_periodic step 52600 lr 2.296e-04 [train_loss] ll_base 0.3518 ll 0.4360 loss -0.7877 (4.735 secs)\n",
      "bnp:bnp_periodic step 52800 lr 2.280e-04 [train_loss] ll_base 0.3855 ll 0.4633 loss -0.8488 (4.612 secs)\n",
      "bnp:bnp_periodic step 53000 lr 2.265e-04 [train_loss] ll_base 0.3688 ll 0.4451 loss -0.8138 (4.660 secs)\n",
      "bnp:bnp_periodic step 53200 lr 2.249e-04 [train_loss] ll_base 0.3939 ll 0.4703 loss -0.8643 (4.784 secs)\n",
      "bnp:bnp_periodic step 53400 lr 2.233e-04 [train_loss] ll_base 0.3417 ll 0.4203 loss -0.7619 (4.614 secs)\n",
      "bnp:bnp_periodic step 53600 lr 2.218e-04 [train_loss] ll_base 0.3826 ll 0.4620 loss -0.8446 (4.677 secs)\n",
      "bnp:bnp_periodic step 53800 lr 2.202e-04 [train_loss] ll_base 0.3980 ll 0.4739 loss -0.8718 (4.541 secs)\n",
      "bnp:bnp_periodic step 54000 lr 2.187e-04 [train_loss] ll_base 0.4126 ll 0.4848 loss -0.8974 (4.490 secs)\n",
      "bnp:bnp_periodic step 54200 lr 2.171e-04 [train_loss] ll_base 0.3566 ll 0.4318 loss -0.7884 (4.539 secs)\n",
      "bnp:bnp_periodic step 54400 lr 2.156e-04 [train_loss] ll_base 0.3897 ll 0.4603 loss -0.8500 (4.722 secs)\n",
      "bnp:bnp_periodic step 54600 lr 2.140e-04 [train_loss] ll_base 0.4086 ll 0.4784 loss -0.8869 (4.557 secs)\n",
      "bnp:bnp_periodic step 54800 lr 2.124e-04 [train_loss] ll_base 0.3556 ll 0.4305 loss -0.7861 (4.558 secs)\n",
      "bnp:bnp_periodic step 55000 lr 2.109e-04 [train_loss] ll_base 0.4358 ll 0.5162 loss -0.9520 (4.710 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 109.19it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2531 tar_ll -0.9348 (27.476 secs)\n",
      "\n",
      "bnp:bnp_periodic step 55200 lr 2.093e-04 [train_loss] ll_base 0.3573 ll 0.4353 loss -0.7926 (4.941 secs)\n",
      "bnp:bnp_periodic step 55400 lr 2.078e-04 [train_loss] ll_base 0.3696 ll 0.4427 loss -0.8123 (4.690 secs)\n",
      "bnp:bnp_periodic step 55600 lr 2.062e-04 [train_loss] ll_base 0.3967 ll 0.4705 loss -0.8671 (4.504 secs)\n",
      "bnp:bnp_periodic step 55800 lr 2.047e-04 [train_loss] ll_base 0.4147 ll 0.4888 loss -0.9035 (4.678 secs)\n",
      "bnp:bnp_periodic step 56000 lr 2.032e-04 [train_loss] ll_base 0.3608 ll 0.4443 loss -0.8052 (4.567 secs)\n",
      "bnp:bnp_periodic step 56200 lr 2.016e-04 [train_loss] ll_base 0.3120 ll 0.4055 loss -0.7175 (4.558 secs)\n",
      "bnp:bnp_periodic step 56400 lr 2.001e-04 [train_loss] ll_base 0.3656 ll 0.4395 loss -0.8051 (4.670 secs)\n",
      "bnp:bnp_periodic step 56600 lr 1.985e-04 [train_loss] ll_base 0.3778 ll 0.4498 loss -0.8276 (4.667 secs)\n",
      "bnp:bnp_periodic step 56800 lr 1.970e-04 [train_loss] ll_base 0.3322 ll 0.4028 loss -0.7350 (4.615 secs)\n",
      "bnp:bnp_periodic step 57000 lr 1.955e-04 [train_loss] ll_base 0.3527 ll 0.4321 loss -0.7848 (4.387 secs)\n",
      "bnp:bnp_periodic step 57200 lr 1.939e-04 [train_loss] ll_base 0.3805 ll 0.4595 loss -0.8400 (4.532 secs)\n",
      "bnp:bnp_periodic step 57400 lr 1.924e-04 [train_loss] ll_base 0.3669 ll 0.4439 loss -0.8108 (4.555 secs)\n",
      "bnp:bnp_periodic step 57600 lr 1.909e-04 [train_loss] ll_base 0.3673 ll 0.4546 loss -0.8219 (4.598 secs)\n",
      "bnp:bnp_periodic step 57800 lr 1.894e-04 [train_loss] ll_base 0.3930 ll 0.4675 loss -0.8605 (4.462 secs)\n",
      "bnp:bnp_periodic step 58000 lr 1.878e-04 [train_loss] ll_base 0.3498 ll 0.4182 loss -0.7680 (4.773 secs)\n",
      "bnp:bnp_periodic step 58200 lr 1.863e-04 [train_loss] ll_base 0.4018 ll 0.4682 loss -0.8700 (4.625 secs)\n",
      "bnp:bnp_periodic step 58400 lr 1.848e-04 [train_loss] ll_base 0.3933 ll 0.4650 loss -0.8583 (4.609 secs)\n",
      "bnp:bnp_periodic step 58600 lr 1.833e-04 [train_loss] ll_base 0.4157 ll 0.4910 loss -0.9067 (4.816 secs)\n",
      "bnp:bnp_periodic step 58800 lr 1.818e-04 [train_loss] ll_base 0.3690 ll 0.4456 loss -0.8146 (4.617 secs)\n",
      "bnp:bnp_periodic step 59000 lr 1.803e-04 [train_loss] ll_base 0.3796 ll 0.4526 loss -0.8321 (4.667 secs)\n",
      "bnp:bnp_periodic step 59200 lr 1.787e-04 [train_loss] ll_base 0.3760 ll 0.4495 loss -0.8254 (4.749 secs)\n",
      "bnp:bnp_periodic step 59400 lr 1.772e-04 [train_loss] ll_base 0.3909 ll 0.4666 loss -0.8575 (4.897 secs)\n",
      "bnp:bnp_periodic step 59600 lr 1.757e-04 [train_loss] ll_base 0.3868 ll 0.4624 loss -0.8492 (4.720 secs)\n",
      "bnp:bnp_periodic step 59800 lr 1.742e-04 [train_loss] ll_base 0.3721 ll 0.4414 loss -0.8135 (4.688 secs)\n",
      "bnp:bnp_periodic step 60000 lr 1.727e-04 [train_loss] ll_base 0.3796 ll 0.4593 loss -0.8389 (4.694 secs)\n",
      "100%|##########| 3000/3000 [00:28<00:00, 106.90it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2481 tar_ll -0.9442 (28.066 secs)\n",
      "\n",
      "bnp:bnp_periodic step 60200 lr 1.713e-04 [train_loss] ll_base 0.4024 ll 0.4780 loss -0.8805 (4.648 secs)\n",
      "bnp:bnp_periodic step 60400 lr 1.698e-04 [train_loss] ll_base 0.3768 ll 0.4478 loss -0.8246 (4.795 secs)\n",
      "bnp:bnp_periodic step 60600 lr 1.683e-04 [train_loss] ll_base 0.4099 ll 0.4783 loss -0.8882 (4.629 secs)\n",
      "bnp:bnp_periodic step 60800 lr 1.668e-04 [train_loss] ll_base 0.3860 ll 0.4627 loss -0.8487 (4.727 secs)\n",
      "bnp:bnp_periodic step 61000 lr 1.653e-04 [train_loss] ll_base 0.4031 ll 0.4761 loss -0.8791 (4.612 secs)\n",
      "bnp:bnp_periodic step 61200 lr 1.638e-04 [train_loss] ll_base 0.3921 ll 0.4656 loss -0.8577 (4.728 secs)\n",
      "bnp:bnp_periodic step 61400 lr 1.624e-04 [train_loss] ll_base 0.3860 ll 0.4592 loss -0.8452 (4.583 secs)\n",
      "bnp:bnp_periodic step 61600 lr 1.609e-04 [train_loss] ll_base 0.3713 ll 0.4559 loss -0.8272 (4.460 secs)\n",
      "bnp:bnp_periodic step 61800 lr 1.594e-04 [train_loss] ll_base 0.4162 ll 0.4840 loss -0.9002 (4.658 secs)\n",
      "bnp:bnp_periodic step 62000 lr 1.580e-04 [train_loss] ll_base 0.3892 ll 0.4607 loss -0.8500 (4.535 secs)\n",
      "bnp:bnp_periodic step 62200 lr 1.565e-04 [train_loss] ll_base 0.3419 ll 0.4191 loss -0.7609 (4.641 secs)\n",
      "bnp:bnp_periodic step 62400 lr 1.551e-04 [train_loss] ll_base 0.4127 ll 0.4878 loss -0.9006 (4.456 secs)\n",
      "bnp:bnp_periodic step 62600 lr 1.536e-04 [train_loss] ll_base 0.3741 ll 0.4428 loss -0.8169 (4.452 secs)\n",
      "bnp:bnp_periodic step 62800 lr 1.522e-04 [train_loss] ll_base 0.4518 ll 0.5193 loss -0.9711 (4.611 secs)\n",
      "bnp:bnp_periodic step 63000 lr 1.507e-04 [train_loss] ll_base 0.4619 ll 0.5277 loss -0.9896 (4.619 secs)\n",
      "bnp:bnp_periodic step 63200 lr 1.493e-04 [train_loss] ll_base 0.3884 ll 0.4613 loss -0.8497 (4.475 secs)\n",
      "bnp:bnp_periodic step 63400 lr 1.478e-04 [train_loss] ll_base 0.4081 ll 0.4838 loss -0.8919 (4.617 secs)\n",
      "bnp:bnp_periodic step 63600 lr 1.464e-04 [train_loss] ll_base 0.3779 ll 0.4440 loss -0.8219 (4.738 secs)\n",
      "bnp:bnp_periodic step 63800 lr 1.450e-04 [train_loss] ll_base 0.3944 ll 0.4787 loss -0.8732 (4.542 secs)\n",
      "bnp:bnp_periodic step 64000 lr 1.436e-04 [train_loss] ll_base 0.4140 ll 0.4903 loss -0.9044 (4.561 secs)\n",
      "bnp:bnp_periodic step 64200 lr 1.421e-04 [train_loss] ll_base 0.3785 ll 0.4477 loss -0.8262 (4.639 secs)\n",
      "bnp:bnp_periodic step 64400 lr 1.407e-04 [train_loss] ll_base 0.4476 ll 0.5204 loss -0.9680 (4.590 secs)\n",
      "bnp:bnp_periodic step 64600 lr 1.393e-04 [train_loss] ll_base 0.4452 ll 0.5171 loss -0.9623 (4.382 secs)\n",
      "bnp:bnp_periodic step 64800 lr 1.379e-04 [train_loss] ll_base 0.3957 ll 0.4691 loss -0.8648 (4.424 secs)\n",
      "bnp:bnp_periodic step 65000 lr 1.365e-04 [train_loss] ll_base 0.4777 ll 0.5485 loss -1.0262 (4.496 secs)\n",
      "100%|##########| 3000/3000 [00:26<00:00, 111.61it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2450 tar_ll -0.9137 (26.882 secs)\n",
      "\n",
      "bnp:bnp_periodic step 65200 lr 1.351e-04 [train_loss] ll_base 0.4222 ll 0.4976 loss -0.9198 (4.650 secs)\n",
      "bnp:bnp_periodic step 65400 lr 1.337e-04 [train_loss] ll_base 0.4077 ll 0.4861 loss -0.8938 (4.568 secs)\n",
      "bnp:bnp_periodic step 65600 lr 1.323e-04 [train_loss] ll_base 0.4074 ll 0.4755 loss -0.8829 (4.632 secs)\n",
      "bnp:bnp_periodic step 65800 lr 1.309e-04 [train_loss] ll_base 0.4163 ll 0.4899 loss -0.9062 (4.815 secs)\n",
      "bnp:bnp_periodic step 66000 lr 1.296e-04 [train_loss] ll_base 0.4160 ll 0.4859 loss -0.9019 (4.636 secs)\n",
      "bnp:bnp_periodic step 66200 lr 1.282e-04 [train_loss] ll_base 0.4691 ll 0.5378 loss -1.0070 (4.636 secs)\n",
      "bnp:bnp_periodic step 66400 lr 1.268e-04 [train_loss] ll_base 0.3940 ll 0.4674 loss -0.8613 (4.644 secs)\n",
      "bnp:bnp_periodic step 66600 lr 1.255e-04 [train_loss] ll_base 0.3862 ll 0.4630 loss -0.8492 (4.791 secs)\n",
      "bnp:bnp_periodic step 66800 lr 1.241e-04 [train_loss] ll_base 0.4664 ll 0.5430 loss -1.0094 (4.756 secs)\n",
      "bnp:bnp_periodic step 67000 lr 1.227e-04 [train_loss] ll_base 0.3923 ll 0.4700 loss -0.8623 (4.630 secs)\n",
      "bnp:bnp_periodic step 67200 lr 1.214e-04 [train_loss] ll_base 0.4332 ll 0.5073 loss -0.9405 (4.853 secs)\n",
      "bnp:bnp_periodic step 67400 lr 1.200e-04 [train_loss] ll_base 0.4300 ll 0.5026 loss -0.9326 (4.541 secs)\n",
      "bnp:bnp_periodic step 67600 lr 1.187e-04 [train_loss] ll_base 0.4590 ll 0.5256 loss -0.9847 (4.743 secs)\n",
      "bnp:bnp_periodic step 67800 lr 1.174e-04 [train_loss] ll_base 0.4309 ll 0.5034 loss -0.9344 (4.836 secs)\n",
      "bnp:bnp_periodic step 68000 lr 1.160e-04 [train_loss] ll_base 0.4378 ll 0.5127 loss -0.9505 (4.669 secs)\n",
      "bnp:bnp_periodic step 68200 lr 1.147e-04 [train_loss] ll_base 0.4078 ll 0.4898 loss -0.8976 (4.760 secs)\n",
      "bnp:bnp_periodic step 68400 lr 1.134e-04 [train_loss] ll_base 0.4381 ll 0.5113 loss -0.9494 (4.623 secs)\n",
      "bnp:bnp_periodic step 68600 lr 1.121e-04 [train_loss] ll_base 0.3631 ll 0.4438 loss -0.8069 (4.628 secs)\n",
      "bnp:bnp_periodic step 68800 lr 1.108e-04 [train_loss] ll_base 0.4033 ll 0.4775 loss -0.8808 (4.684 secs)\n",
      "bnp:bnp_periodic step 69000 lr 1.095e-04 [train_loss] ll_base 0.4104 ll 0.4814 loss -0.8918 (4.815 secs)\n",
      "bnp:bnp_periodic step 69200 lr 1.082e-04 [train_loss] ll_base 0.4030 ll 0.4689 loss -0.8719 (4.707 secs)\n",
      "bnp:bnp_periodic step 69400 lr 1.069e-04 [train_loss] ll_base 0.4263 ll 0.4949 loss -0.9212 (4.756 secs)\n",
      "bnp:bnp_periodic step 69600 lr 1.056e-04 [train_loss] ll_base 0.3922 ll 0.4644 loss -0.8566 (4.710 secs)\n",
      "bnp:bnp_periodic step 69800 lr 1.043e-04 [train_loss] ll_base 0.4185 ll 0.4861 loss -0.9046 (4.815 secs)\n",
      "bnp:bnp_periodic step 70000 lr 1.031e-04 [train_loss] ll_base 0.4353 ll 0.5086 loss -0.9439 (4.697 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 107.87it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2451 tar_ll -0.9115 (27.814 secs)\n",
      "\n",
      "bnp:bnp_periodic step 70200 lr 1.018e-04 [train_loss] ll_base 0.4323 ll 0.5033 loss -0.9355 (4.587 secs)\n",
      "bnp:bnp_periodic step 70400 lr 1.005e-04 [train_loss] ll_base 0.4340 ll 0.5066 loss -0.9407 (4.399 secs)\n",
      "bnp:bnp_periodic step 70600 lr 9.927e-05 [train_loss] ll_base 0.3994 ll 0.4724 loss -0.8718 (4.719 secs)\n",
      "bnp:bnp_periodic step 70800 lr 9.802e-05 [train_loss] ll_base 0.4395 ll 0.5073 loss -0.9468 (4.741 secs)\n",
      "bnp:bnp_periodic step 71000 lr 9.677e-05 [train_loss] ll_base 0.4394 ll 0.5036 loss -0.9430 (4.722 secs)\n",
      "bnp:bnp_periodic step 71200 lr 9.554e-05 [train_loss] ll_base 0.4845 ll 0.5592 loss -1.0437 (4.673 secs)\n",
      "bnp:bnp_periodic step 71400 lr 9.430e-05 [train_loss] ll_base 0.4436 ll 0.5126 loss -0.9563 (4.636 secs)\n",
      "bnp:bnp_periodic step 71600 lr 9.308e-05 [train_loss] ll_base 0.3717 ll 0.4496 loss -0.8214 (4.577 secs)\n",
      "bnp:bnp_periodic step 71800 lr 9.186e-05 [train_loss] ll_base 0.4350 ll 0.5139 loss -0.9489 (4.478 secs)\n",
      "bnp:bnp_periodic step 72000 lr 9.064e-05 [train_loss] ll_base 0.4100 ll 0.4838 loss -0.8937 (4.679 secs)\n",
      "bnp:bnp_periodic step 72200 lr 8.944e-05 [train_loss] ll_base 0.4218 ll 0.4885 loss -0.9104 (4.457 secs)\n",
      "bnp:bnp_periodic step 72400 lr 8.824e-05 [train_loss] ll_base 0.4562 ll 0.5223 loss -0.9785 (4.649 secs)\n",
      "bnp:bnp_periodic step 72600 lr 8.704e-05 [train_loss] ll_base 0.4209 ll 0.4953 loss -0.9162 (4.421 secs)\n",
      "bnp:bnp_periodic step 72800 lr 8.585e-05 [train_loss] ll_base 0.3468 ll 0.4217 loss -0.7686 (4.762 secs)\n",
      "bnp:bnp_periodic step 73000 lr 8.467e-05 [train_loss] ll_base 0.4177 ll 0.4871 loss -0.9049 (4.486 secs)\n",
      "bnp:bnp_periodic step 73200 lr 8.350e-05 [train_loss] ll_base 0.4279 ll 0.4918 loss -0.9197 (4.566 secs)\n",
      "bnp:bnp_periodic step 73400 lr 8.233e-05 [train_loss] ll_base 0.4198 ll 0.4905 loss -0.9104 (4.494 secs)\n",
      "bnp:bnp_periodic step 73600 lr 8.117e-05 [train_loss] ll_base 0.4171 ll 0.4916 loss -0.9087 (4.720 secs)\n",
      "bnp:bnp_periodic step 73800 lr 8.001e-05 [train_loss] ll_base 0.4397 ll 0.5066 loss -0.9463 (4.450 secs)\n",
      "bnp:bnp_periodic step 74000 lr 7.886e-05 [train_loss] ll_base 0.4531 ll 0.5274 loss -0.9805 (4.453 secs)\n",
      "bnp:bnp_periodic step 74200 lr 7.772e-05 [train_loss] ll_base 0.4197 ll 0.4898 loss -0.9095 (4.704 secs)\n",
      "bnp:bnp_periodic step 74400 lr 7.659e-05 [train_loss] ll_base 0.4077 ll 0.4787 loss -0.8865 (4.516 secs)\n",
      "bnp:bnp_periodic step 74600 lr 7.546e-05 [train_loss] ll_base 0.4817 ll 0.5443 loss -1.0260 (4.499 secs)\n",
      "bnp:bnp_periodic step 74800 lr 7.434e-05 [train_loss] ll_base 0.4444 ll 0.5249 loss -0.9693 (4.641 secs)\n",
      "bnp:bnp_periodic step 75000 lr 7.322e-05 [train_loss] ll_base 0.4274 ll 0.5009 loss -0.9282 (4.556 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 108.25it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2343 tar_ll -0.9281 (27.716 secs)\n",
      "\n",
      "bnp:bnp_periodic step 75200 lr 7.212e-05 [train_loss] ll_base 0.4304 ll 0.5025 loss -0.9329 (4.640 secs)\n",
      "bnp:bnp_periodic step 75400 lr 7.102e-05 [train_loss] ll_base 0.4164 ll 0.4824 loss -0.8987 (4.490 secs)\n",
      "bnp:bnp_periodic step 75600 lr 6.992e-05 [train_loss] ll_base 0.4766 ll 0.5431 loss -1.0197 (4.746 secs)\n",
      "bnp:bnp_periodic step 75800 lr 6.884e-05 [train_loss] ll_base 0.4593 ll 0.5249 loss -0.9841 (4.652 secs)\n",
      "bnp:bnp_periodic step 76000 lr 6.776e-05 [train_loss] ll_base 0.3796 ll 0.4498 loss -0.8294 (4.607 secs)\n",
      "bnp:bnp_periodic step 76200 lr 6.669e-05 [train_loss] ll_base 0.4359 ll 0.5032 loss -0.9391 (4.537 secs)\n",
      "bnp:bnp_periodic step 76400 lr 6.562e-05 [train_loss] ll_base 0.4301 ll 0.5079 loss -0.9380 (4.726 secs)\n",
      "bnp:bnp_periodic step 76600 lr 6.456e-05 [train_loss] ll_base 0.4282 ll 0.5013 loss -0.9295 (4.679 secs)\n",
      "bnp:bnp_periodic step 76800 lr 6.351e-05 [train_loss] ll_base 0.4080 ll 0.4776 loss -0.8856 (4.587 secs)\n",
      "bnp:bnp_periodic step 77000 lr 6.247e-05 [train_loss] ll_base 0.4881 ll 0.5510 loss -1.0391 (4.601 secs)\n",
      "bnp:bnp_periodic step 77200 lr 6.144e-05 [train_loss] ll_base 0.4513 ll 0.5291 loss -0.9803 (4.722 secs)\n",
      "bnp:bnp_periodic step 77400 lr 6.041e-05 [train_loss] ll_base 0.4567 ll 0.5285 loss -0.9853 (4.662 secs)\n",
      "bnp:bnp_periodic step 77600 lr 5.939e-05 [train_loss] ll_base 0.4527 ll 0.5178 loss -0.9704 (4.616 secs)\n",
      "bnp:bnp_periodic step 77800 lr 5.838e-05 [train_loss] ll_base 0.4759 ll 0.5390 loss -1.0149 (4.705 secs)\n",
      "bnp:bnp_periodic step 78000 lr 5.737e-05 [train_loss] ll_base 0.4824 ll 0.5495 loss -1.0319 (4.625 secs)\n",
      "bnp:bnp_periodic step 78200 lr 5.637e-05 [train_loss] ll_base 0.4681 ll 0.5310 loss -0.9992 (4.628 secs)\n",
      "bnp:bnp_periodic step 78400 lr 5.538e-05 [train_loss] ll_base 0.4097 ll 0.4822 loss -0.8920 (4.617 secs)\n",
      "bnp:bnp_periodic step 78600 lr 5.440e-05 [train_loss] ll_base 0.4425 ll 0.5144 loss -0.9569 (4.576 secs)\n",
      "bnp:bnp_periodic step 78800 lr 5.343e-05 [train_loss] ll_base 0.4555 ll 0.5243 loss -0.9798 (4.515 secs)\n",
      "bnp:bnp_periodic step 79000 lr 5.246e-05 [train_loss] ll_base 0.4547 ll 0.5224 loss -0.9771 (4.531 secs)\n",
      "bnp:bnp_periodic step 79200 lr 5.150e-05 [train_loss] ll_base 0.4889 ll 0.5681 loss -1.0570 (4.577 secs)\n",
      "bnp:bnp_periodic step 79400 lr 5.055e-05 [train_loss] ll_base 0.4504 ll 0.5167 loss -0.9672 (4.597 secs)\n",
      "bnp:bnp_periodic step 79600 lr 4.961e-05 [train_loss] ll_base 0.4431 ll 0.5107 loss -0.9538 (4.466 secs)\n",
      "bnp:bnp_periodic step 79800 lr 4.867e-05 [train_loss] ll_base 0.4864 ll 0.5537 loss -1.0401 (4.589 secs)\n",
      "bnp:bnp_periodic step 80000 lr 4.775e-05 [train_loss] ll_base 0.4288 ll 0.4945 loss -0.9234 (4.523 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 107.80it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2353 tar_ll -0.9439 (27.830 secs)\n",
      "\n",
      "bnp:bnp_periodic step 80200 lr 4.683e-05 [train_loss] ll_base 0.4812 ll 0.5575 loss -1.0388 (4.635 secs)\n",
      "bnp:bnp_periodic step 80400 lr 4.592e-05 [train_loss] ll_base 0.4330 ll 0.4991 loss -0.9320 (4.574 secs)\n",
      "bnp:bnp_periodic step 80600 lr 4.501e-05 [train_loss] ll_base 0.4621 ll 0.5334 loss -0.9955 (4.505 secs)\n",
      "bnp:bnp_periodic step 80800 lr 4.412e-05 [train_loss] ll_base 0.4747 ll 0.5407 loss -1.0154 (4.574 secs)\n",
      "bnp:bnp_periodic step 81000 lr 4.323e-05 [train_loss] ll_base 0.4341 ll 0.5035 loss -0.9376 (4.277 secs)\n",
      "bnp:bnp_periodic step 81200 lr 4.235e-05 [train_loss] ll_base 0.4092 ll 0.4983 loss -0.9075 (4.472 secs)\n",
      "bnp:bnp_periodic step 81400 lr 4.148e-05 [train_loss] ll_base 0.4350 ll 0.5124 loss -0.9474 (4.664 secs)\n",
      "bnp:bnp_periodic step 81600 lr 4.062e-05 [train_loss] ll_base 0.4205 ll 0.4932 loss -0.9136 (4.622 secs)\n",
      "bnp:bnp_periodic step 81800 lr 3.976e-05 [train_loss] ll_base 0.4907 ll 0.5629 loss -1.0536 (4.369 secs)\n",
      "bnp:bnp_periodic step 82000 lr 3.892e-05 [train_loss] ll_base 0.4843 ll 0.5521 loss -1.0364 (4.410 secs)\n",
      "bnp:bnp_periodic step 82200 lr 3.808e-05 [train_loss] ll_base 0.4184 ll 0.4947 loss -0.9131 (4.463 secs)\n",
      "bnp:bnp_periodic step 82400 lr 3.725e-05 [train_loss] ll_base 0.4714 ll 0.5403 loss -1.0118 (4.526 secs)\n",
      "bnp:bnp_periodic step 82600 lr 3.643e-05 [train_loss] ll_base 0.4947 ll 0.5623 loss -1.0571 (4.607 secs)\n",
      "bnp:bnp_periodic step 82800 lr 3.562e-05 [train_loss] ll_base 0.4534 ll 0.5220 loss -0.9753 (4.772 secs)\n",
      "bnp:bnp_periodic step 83000 lr 3.481e-05 [train_loss] ll_base 0.3882 ll 0.4588 loss -0.8470 (4.585 secs)\n",
      "bnp:bnp_periodic step 83200 lr 3.402e-05 [train_loss] ll_base 0.4502 ll 0.5251 loss -0.9753 (4.670 secs)\n",
      "bnp:bnp_periodic step 83400 lr 3.323e-05 [train_loss] ll_base 0.4488 ll 0.5310 loss -0.9797 (4.775 secs)\n",
      "bnp:bnp_periodic step 83600 lr 3.245e-05 [train_loss] ll_base 0.4230 ll 0.4913 loss -0.9143 (4.852 secs)\n",
      "bnp:bnp_periodic step 83800 lr 3.168e-05 [train_loss] ll_base 0.4873 ll 0.5581 loss -1.0455 (4.683 secs)\n",
      "bnp:bnp_periodic step 84000 lr 3.092e-05 [train_loss] ll_base 0.4590 ll 0.5369 loss -0.9959 (4.622 secs)\n",
      "bnp:bnp_periodic step 84200 lr 3.017e-05 [train_loss] ll_base 0.4319 ll 0.5050 loss -0.9369 (4.801 secs)\n",
      "bnp:bnp_periodic step 84400 lr 2.943e-05 [train_loss] ll_base 0.4339 ll 0.5124 loss -0.9463 (4.742 secs)\n",
      "bnp:bnp_periodic step 84600 lr 2.869e-05 [train_loss] ll_base 0.4811 ll 0.5463 loss -1.0274 (4.703 secs)\n",
      "bnp:bnp_periodic step 84800 lr 2.797e-05 [train_loss] ll_base 0.4501 ll 0.5137 loss -0.9638 (4.657 secs)\n",
      "bnp:bnp_periodic step 85000 lr 2.725e-05 [train_loss] ll_base 0.4485 ll 0.5213 loss -0.9698 (4.597 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 108.32it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2304 tar_ll -0.9301 (27.701 secs)\n",
      "\n",
      "bnp:bnp_periodic step 85200 lr 2.654e-05 [train_loss] ll_base 0.4707 ll 0.5358 loss -1.0066 (4.592 secs)\n",
      "bnp:bnp_periodic step 85400 lr 2.584e-05 [train_loss] ll_base 0.4762 ll 0.5475 loss -1.0237 (4.599 secs)\n",
      "bnp:bnp_periodic step 85600 lr 2.515e-05 [train_loss] ll_base 0.4497 ll 0.5221 loss -0.9718 (4.941 secs)\n",
      "bnp:bnp_periodic step 85800 lr 2.447e-05 [train_loss] ll_base 0.4486 ll 0.5168 loss -0.9654 (4.644 secs)\n",
      "bnp:bnp_periodic step 86000 lr 2.379e-05 [train_loss] ll_base 0.4651 ll 0.5350 loss -1.0001 (4.557 secs)\n",
      "bnp:bnp_periodic step 86200 lr 2.313e-05 [train_loss] ll_base 0.4694 ll 0.5363 loss -1.0057 (4.606 secs)\n",
      "bnp:bnp_periodic step 86400 lr 2.247e-05 [train_loss] ll_base 0.4602 ll 0.5303 loss -0.9905 (4.663 secs)\n",
      "bnp:bnp_periodic step 86600 lr 2.183e-05 [train_loss] ll_base 0.4877 ll 0.5574 loss -1.0451 (4.675 secs)\n",
      "bnp:bnp_periodic step 86800 lr 2.119e-05 [train_loss] ll_base 0.4670 ll 0.5362 loss -1.0032 (4.556 secs)\n",
      "bnp:bnp_periodic step 87000 lr 2.056e-05 [train_loss] ll_base 0.4353 ll 0.5090 loss -0.9443 (4.620 secs)\n",
      "bnp:bnp_periodic step 87200 lr 1.994e-05 [train_loss] ll_base 0.4504 ll 0.5267 loss -0.9771 (4.533 secs)\n",
      "bnp:bnp_periodic step 87400 lr 1.933e-05 [train_loss] ll_base 0.5065 ll 0.5683 loss -1.0748 (4.573 secs)\n",
      "bnp:bnp_periodic step 87600 lr 1.873e-05 [train_loss] ll_base 0.4329 ll 0.5012 loss -0.9341 (4.575 secs)\n",
      "bnp:bnp_periodic step 87800 lr 1.814e-05 [train_loss] ll_base 0.4792 ll 0.5431 loss -1.0222 (4.635 secs)\n",
      "bnp:bnp_periodic step 88000 lr 1.756e-05 [train_loss] ll_base 0.4506 ll 0.5290 loss -0.9797 (4.467 secs)\n",
      "bnp:bnp_periodic step 88200 lr 1.698e-05 [train_loss] ll_base 0.4578 ll 0.5311 loss -0.9888 (4.609 secs)\n",
      "bnp:bnp_periodic step 88400 lr 1.642e-05 [train_loss] ll_base 0.4795 ll 0.5508 loss -1.0303 (4.518 secs)\n",
      "bnp:bnp_periodic step 88600 lr 1.586e-05 [train_loss] ll_base 0.4099 ll 0.4835 loss -0.8934 (4.553 secs)\n",
      "bnp:bnp_periodic step 88800 lr 1.532e-05 [train_loss] ll_base 0.4486 ll 0.5244 loss -0.9730 (4.621 secs)\n",
      "bnp:bnp_periodic step 89000 lr 1.478e-05 [train_loss] ll_base 0.4807 ll 0.5388 loss -1.0195 (4.484 secs)\n",
      "bnp:bnp_periodic step 89200 lr 1.425e-05 [train_loss] ll_base 0.4985 ll 0.5622 loss -1.0607 (4.637 secs)\n",
      "bnp:bnp_periodic step 89400 lr 1.373e-05 [train_loss] ll_base 0.4357 ll 0.5054 loss -0.9412 (4.520 secs)\n",
      "bnp:bnp_periodic step 89600 lr 1.323e-05 [train_loss] ll_base 0.4629 ll 0.5288 loss -0.9917 (4.594 secs)\n",
      "bnp:bnp_periodic step 89800 lr 1.273e-05 [train_loss] ll_base 0.4523 ll 0.5186 loss -0.9709 (4.586 secs)\n",
      "bnp:bnp_periodic step 90000 lr 1.224e-05 [train_loss] ll_base 0.4556 ll 0.5294 loss -0.9850 (4.552 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 108.78it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2280 tar_ll -0.9427 (27.583 secs)\n",
      "\n",
      "bnp:bnp_periodic step 90200 lr 1.176e-05 [train_loss] ll_base 0.5026 ll 0.5729 loss -1.0755 (4.629 secs)\n",
      "bnp:bnp_periodic step 90400 lr 1.128e-05 [train_loss] ll_base 0.4752 ll 0.5392 loss -1.0144 (4.551 secs)\n",
      "bnp:bnp_periodic step 90600 lr 1.082e-05 [train_loss] ll_base 0.4992 ll 0.5674 loss -1.0667 (4.604 secs)\n",
      "bnp:bnp_periodic step 90800 lr 1.037e-05 [train_loss] ll_base 0.4667 ll 0.5318 loss -0.9985 (4.726 secs)\n",
      "bnp:bnp_periodic step 91000 lr 9.927e-06 [train_loss] ll_base 0.5255 ll 0.5896 loss -1.1151 (4.631 secs)\n",
      "bnp:bnp_periodic step 91200 lr 9.493e-06 [train_loss] ll_base 0.4553 ll 0.5233 loss -0.9786 (4.479 secs)\n",
      "bnp:bnp_periodic step 91400 lr 9.069e-06 [train_loss] ll_base 0.4543 ll 0.5250 loss -0.9793 (4.720 secs)\n",
      "bnp:bnp_periodic step 91600 lr 8.655e-06 [train_loss] ll_base 0.4621 ll 0.5285 loss -0.9906 (4.555 secs)\n",
      "bnp:bnp_periodic step 91800 lr 8.250e-06 [train_loss] ll_base 0.4599 ll 0.5257 loss -0.9856 (4.663 secs)\n",
      "bnp:bnp_periodic step 92000 lr 7.854e-06 [train_loss] ll_base 0.4222 ll 0.4959 loss -0.9180 (4.658 secs)\n",
      "bnp:bnp_periodic step 92200 lr 7.468e-06 [train_loss] ll_base 0.5181 ll 0.5869 loss -1.1050 (4.625 secs)\n",
      "bnp:bnp_periodic step 92400 lr 7.092e-06 [train_loss] ll_base 0.4784 ll 0.5485 loss -1.0269 (4.703 secs)\n",
      "bnp:bnp_periodic step 92600 lr 6.725e-06 [train_loss] ll_base 0.5123 ll 0.5804 loss -1.0927 (4.595 secs)\n",
      "bnp:bnp_periodic step 92800 lr 6.368e-06 [train_loss] ll_base 0.4864 ll 0.5553 loss -1.0416 (4.672 secs)\n",
      "bnp:bnp_periodic step 93000 lr 6.021e-06 [train_loss] ll_base 0.4934 ll 0.5650 loss -1.0584 (4.495 secs)\n",
      "bnp:bnp_periodic step 93200 lr 5.683e-06 [train_loss] ll_base 0.4594 ll 0.5250 loss -0.9844 (4.649 secs)\n",
      "bnp:bnp_periodic step 93400 lr 5.355e-06 [train_loss] ll_base 0.4537 ll 0.5159 loss -0.9697 (4.623 secs)\n",
      "bnp:bnp_periodic step 93600 lr 5.036e-06 [train_loss] ll_base 0.4119 ll 0.4890 loss -0.9009 (4.748 secs)\n",
      "bnp:bnp_periodic step 93800 lr 4.727e-06 [train_loss] ll_base 0.4707 ll 0.5344 loss -1.0051 (4.624 secs)\n",
      "bnp:bnp_periodic step 94000 lr 4.428e-06 [train_loss] ll_base 0.4610 ll 0.5300 loss -0.9909 (4.569 secs)\n",
      "bnp:bnp_periodic step 94200 lr 4.139e-06 [train_loss] ll_base 0.4770 ll 0.5452 loss -1.0221 (4.628 secs)\n",
      "bnp:bnp_periodic step 94400 lr 3.859e-06 [train_loss] ll_base 0.4738 ll 0.5376 loss -1.0114 (4.635 secs)\n",
      "bnp:bnp_periodic step 94600 lr 3.589e-06 [train_loss] ll_base 0.4989 ll 0.5713 loss -1.0702 (4.553 secs)\n",
      "bnp:bnp_periodic step 94800 lr 3.329e-06 [train_loss] ll_base 0.4467 ll 0.5230 loss -0.9697 (4.767 secs)\n",
      "bnp:bnp_periodic step 95000 lr 3.078e-06 [train_loss] ll_base 0.4582 ll 0.5177 loss -0.9759 (4.513 secs)\n",
      "100%|##########| 3000/3000 [00:28<00:00, 106.93it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2288 tar_ll -0.9381 (28.059 secs)\n",
      "\n",
      "bnp:bnp_periodic step 95200 lr 2.837e-06 [train_loss] ll_base 0.5070 ll 0.5738 loss -1.0808 (4.651 secs)\n",
      "bnp:bnp_periodic step 95400 lr 2.606e-06 [train_loss] ll_base 0.4890 ll 0.5540 loss -1.0429 (4.471 secs)\n",
      "bnp:bnp_periodic step 95600 lr 2.385e-06 [train_loss] ll_base 0.4395 ll 0.5071 loss -0.9466 (4.620 secs)\n",
      "bnp:bnp_periodic step 95800 lr 2.173e-06 [train_loss] ll_base 0.4770 ll 0.5409 loss -1.0178 (4.446 secs)\n",
      "bnp:bnp_periodic step 96000 lr 1.971e-06 [train_loss] ll_base 0.4901 ll 0.5553 loss -1.0454 (4.687 secs)\n",
      "bnp:bnp_periodic step 96200 lr 1.779e-06 [train_loss] ll_base 0.4665 ll 0.5394 loss -1.0059 (4.502 secs)\n",
      "bnp:bnp_periodic step 96400 lr 1.597e-06 [train_loss] ll_base 0.4401 ll 0.5183 loss -0.9585 (4.455 secs)\n",
      "bnp:bnp_periodic step 96600 lr 1.425e-06 [train_loss] ll_base 0.4555 ll 0.5261 loss -0.9816 (4.527 secs)\n",
      "bnp:bnp_periodic step 96800 lr 1.262e-06 [train_loss] ll_base 0.4493 ll 0.5236 loss -0.9729 (4.428 secs)\n",
      "bnp:bnp_periodic step 97000 lr 1.110e-06 [train_loss] ll_base 0.4688 ll 0.5356 loss -1.0044 (4.542 secs)\n",
      "bnp:bnp_periodic step 97200 lr 9.666e-07 [train_loss] ll_base 0.4434 ll 0.5143 loss -0.9577 (4.357 secs)\n",
      "bnp:bnp_periodic step 97400 lr 8.335e-07 [train_loss] ll_base 0.4837 ll 0.5549 loss -1.0386 (4.504 secs)\n",
      "bnp:bnp_periodic step 97600 lr 7.103e-07 [train_loss] ll_base 0.4352 ll 0.5095 loss -0.9447 (4.569 secs)\n",
      "bnp:bnp_periodic step 97800 lr 5.969e-07 [train_loss] ll_base 0.4653 ll 0.5344 loss -0.9998 (4.603 secs)\n",
      "bnp:bnp_periodic step 98000 lr 4.933e-07 [train_loss] ll_base 0.4784 ll 0.5490 loss -1.0274 (4.410 secs)\n",
      "bnp:bnp_periodic step 98200 lr 3.996e-07 [train_loss] ll_base 0.4779 ll 0.5388 loss -1.0167 (4.616 secs)\n",
      "bnp:bnp_periodic step 98400 lr 3.158e-07 [train_loss] ll_base 0.4758 ll 0.5500 loss -1.0258 (4.489 secs)\n",
      "bnp:bnp_periodic step 98600 lr 2.418e-07 [train_loss] ll_base 0.4575 ll 0.5199 loss -0.9774 (4.585 secs)\n",
      "bnp:bnp_periodic step 98800 lr 1.776e-07 [train_loss] ll_base 0.4578 ll 0.5265 loss -0.9844 (4.425 secs)\n",
      "bnp:bnp_periodic step 99000 lr 1.234e-07 [train_loss] ll_base 0.4406 ll 0.5101 loss -0.9507 (4.535 secs)\n",
      "bnp:bnp_periodic step 99200 lr 7.895e-08 [train_loss] ll_base 0.5091 ll 0.5790 loss -1.0881 (4.561 secs)\n",
      "bnp:bnp_periodic step 99400 lr 4.441e-08 [train_loss] ll_base 0.4587 ll 0.5272 loss -0.9858 (4.527 secs)\n",
      "bnp:bnp_periodic step 99600 lr 1.974e-08 [train_loss] ll_base 0.4968 ll 0.5666 loss -1.0634 (4.596 secs)\n",
      "bnp:bnp_periodic step 99800 lr 4.935e-09 [train_loss] ll_base 0.4936 ll 0.5718 loss -1.0654 (4.688 secs)\n",
      "bnp:bnp_periodic step 100000 lr 0.000e+00 [train_loss] ll_base 0.4468 ll 0.5172 loss -0.9640 (4.623 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 109.57it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2284 tar_ll -0.9386 (27.386 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:27<00:00, 109.85it/s]\n",
      "bnp:bnp_periodic periodic ctx_ll -0.2283 tar_ll -0.9390 (27.317 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 2922860.75 miliseconds\n",
      "Execution time: 2922.86075 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 250.07958984375 MB\n",
      "Memory Usage Change: 233.82958984375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='bnp', name='bnp_periodic',val_seed=100, val_l=None,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9088b2f-2f81-422d-9d7b-50294a2620fc",
   "metadata": {},
   "source": [
    "## BANP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba41473d-9998-4280-baf0-324a0ec2aad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: banp-banp_periodic\n",
      "Total number of parameters: 364674\n",
      "\n",
      "banp:banp_periodic step 200 lr 5.000e-04 [train_loss] ll_base -0.5163 ll -0.4822 loss 0.9985 (8.013 secs)\n",
      "banp:banp_periodic step 400 lr 5.000e-04 [train_loss] ll_base -0.2220 ll -0.1436 loss 0.3656 (7.778 secs)\n",
      "banp:banp_periodic step 600 lr 5.000e-04 [train_loss] ll_base 0.0331 ll 0.1262 loss -0.1593 (7.996 secs)\n",
      "banp:banp_periodic step 800 lr 4.999e-04 [train_loss] ll_base 0.2386 ll 0.3197 loss -0.5583 (7.852 secs)\n",
      "banp:banp_periodic step 1000 lr 4.999e-04 [train_loss] ll_base 0.3816 ll 0.4631 loss -0.8447 (8.150 secs)\n",
      "banp:banp_periodic step 1200 lr 4.998e-04 [train_loss] ll_base 0.4937 ll 0.5721 loss -1.0657 (7.876 secs)\n",
      "banp:banp_periodic step 1400 lr 4.998e-04 [train_loss] ll_base 0.5252 ll 0.5996 loss -1.1248 (7.823 secs)\n",
      "banp:banp_periodic step 1600 lr 4.997e-04 [train_loss] ll_base 0.4091 ll 0.4686 loss -0.8777 (7.678 secs)\n",
      "banp:banp_periodic step 1800 lr 4.996e-04 [train_loss] ll_base 0.6164 ll 0.6803 loss -1.2967 (7.839 secs)\n",
      "banp:banp_periodic step 2000 lr 4.995e-04 [train_loss] ll_base 0.7213 ll 0.7787 loss -1.5000 (7.802 secs)\n",
      "banp:banp_periodic step 2200 lr 4.994e-04 [train_loss] ll_base 0.6766 ll 0.7339 loss -1.4106 (7.883 secs)\n",
      "banp:banp_periodic step 2400 lr 4.993e-04 [train_loss] ll_base 0.6200 ll 0.6617 loss -1.2817 (7.648 secs)\n",
      "banp:banp_periodic step 2600 lr 4.992e-04 [train_loss] ll_base 0.3316 ll 0.3705 loss -0.7021 (7.813 secs)\n",
      "banp:banp_periodic step 2800 lr 4.990e-04 [train_loss] ll_base 0.5620 ll 0.6134 loss -1.1754 (7.785 secs)\n",
      "banp:banp_periodic step 3000 lr 4.989e-04 [train_loss] ll_base 0.6490 ll 0.6970 loss -1.3460 (7.934 secs)\n",
      "banp:banp_periodic step 3200 lr 4.987e-04 [train_loss] ll_base 0.7390 ll 0.7826 loss -1.5215 (7.608 secs)\n",
      "banp:banp_periodic step 3400 lr 4.986e-04 [train_loss] ll_base 0.7365 ll 0.7776 loss -1.5142 (7.890 secs)\n",
      "banp:banp_periodic step 3600 lr 4.984e-04 [train_loss] ll_base 0.5642 ll 0.6164 loss -1.1806 (7.669 secs)\n",
      "banp:banp_periodic step 3800 lr 4.982e-04 [train_loss] ll_base 0.7067 ll 0.7437 loss -1.4504 (7.781 secs)\n",
      "banp:banp_periodic step 4000 lr 4.980e-04 [train_loss] ll_base 0.7797 ll 0.8185 loss -1.5982 (7.859 secs)\n",
      "banp:banp_periodic step 4200 lr 4.978e-04 [train_loss] ll_base 0.6471 ll 0.6846 loss -1.3317 (7.747 secs)\n",
      "banp:banp_periodic step 4400 lr 4.976e-04 [train_loss] ll_base 0.5705 ll 0.6246 loss -1.1952 (7.855 secs)\n",
      "banp:banp_periodic step 4600 lr 4.974e-04 [train_loss] ll_base 0.7712 ll 0.8137 loss -1.5850 (8.044 secs)\n",
      "banp:banp_periodic step 4800 lr 4.972e-04 [train_loss] ll_base 0.8248 ll 0.8544 loss -1.6792 (8.004 secs)\n",
      "banp:banp_periodic step 5000 lr 4.969e-04 [train_loss] ll_base 0.7817 ll 0.8186 loss -1.6004 (8.064 secs)\n",
      "100%|##########| 3000/3000 [01:26<00:00, 34.82it/s]\n",
      "banp:banp_periodic periodic ctx_ll -0.1142 tar_ll -2.4932 (86.170 secs)\n",
      "\n",
      "banp:banp_periodic step 5200 lr 4.967e-04 [train_loss] ll_base 0.7218 ll 0.7720 loss -1.4937 (8.052 secs)\n",
      "banp:banp_periodic step 5400 lr 4.964e-04 [train_loss] ll_base 0.8802 ll 0.9153 loss -1.7955 (7.783 secs)\n",
      "banp:banp_periodic step 5600 lr 4.961e-04 [train_loss] ll_base 0.8556 ll 0.8864 loss -1.7420 (7.977 secs)\n",
      "banp:banp_periodic step 5800 lr 4.959e-04 [train_loss] ll_base 0.8778 ll 0.9065 loss -1.7843 (7.658 secs)\n",
      "banp:banp_periodic step 6000 lr 4.956e-04 [train_loss] ll_base 0.8956 ll 0.9236 loss -1.8192 (7.747 secs)\n",
      "banp:banp_periodic step 6200 lr 4.953e-04 [train_loss] ll_base 0.8681 ll 0.8938 loss -1.7619 (7.796 secs)\n",
      "banp:banp_periodic step 6400 lr 4.950e-04 [train_loss] ll_base 0.8610 ll 0.8952 loss -1.7562 (7.788 secs)\n",
      "banp:banp_periodic step 6600 lr 4.946e-04 [train_loss] ll_base 0.9097 ll 0.9355 loss -1.8451 (7.579 secs)\n",
      "banp:banp_periodic step 6800 lr 4.943e-04 [train_loss] ll_base 0.9268 ll 0.9464 loss -1.8732 (7.726 secs)\n",
      "banp:banp_periodic step 7000 lr 4.940e-04 [train_loss] ll_base 0.8965 ll 0.9190 loss -1.8155 (7.798 secs)\n",
      "banp:banp_periodic step 7200 lr 4.936e-04 [train_loss] ll_base 0.9052 ll 0.9271 loss -1.8323 (7.785 secs)\n",
      "banp:banp_periodic step 7400 lr 4.933e-04 [train_loss] ll_base 0.8846 ll 0.9188 loss -1.8034 (7.773 secs)\n",
      "banp:banp_periodic step 7600 lr 4.929e-04 [train_loss] ll_base 0.8397 ll 0.8759 loss -1.7157 (7.632 secs)\n",
      "banp:banp_periodic step 7800 lr 4.925e-04 [train_loss] ll_base 0.9075 ll 0.9360 loss -1.8435 (7.469 secs)\n",
      "banp:banp_periodic step 8000 lr 4.921e-04 [train_loss] ll_base 0.9022 ll 0.9288 loss -1.8310 (7.647 secs)\n",
      "banp:banp_periodic step 8200 lr 4.918e-04 [train_loss] ll_base 0.8963 ll 0.9186 loss -1.8149 (7.622 secs)\n",
      "banp:banp_periodic step 8400 lr 4.913e-04 [train_loss] ll_base 0.8723 ll 0.9080 loss -1.7803 (7.766 secs)\n",
      "banp:banp_periodic step 8600 lr 4.909e-04 [train_loss] ll_base 0.7769 ll 0.8076 loss -1.5845 (7.836 secs)\n",
      "banp:banp_periodic step 8800 lr 4.905e-04 [train_loss] ll_base 0.7863 ll 0.8135 loss -1.5997 (7.560 secs)\n",
      "banp:banp_periodic step 9000 lr 4.901e-04 [train_loss] ll_base 0.8699 ll 0.9031 loss -1.7730 (8.124 secs)\n",
      "banp:banp_periodic step 9200 lr 4.896e-04 [train_loss] ll_base 0.9163 ll 0.9374 loss -1.8537 (7.854 secs)\n",
      "banp:banp_periodic step 9400 lr 4.892e-04 [train_loss] ll_base 0.9073 ll 0.9289 loss -1.8362 (8.003 secs)\n",
      "banp:banp_periodic step 9600 lr 4.887e-04 [train_loss] ll_base 0.7909 ll 0.8192 loss -1.6102 (7.821 secs)\n",
      "banp:banp_periodic step 9800 lr 4.882e-04 [train_loss] ll_base 0.9070 ll 0.9334 loss -1.8404 (8.029 secs)\n",
      "banp:banp_periodic step 10000 lr 4.878e-04 [train_loss] ll_base 0.8625 ll 0.8930 loss -1.7554 (7.826 secs)\n",
      "100%|##########| 3000/3000 [01:24<00:00, 35.41it/s]\n",
      "banp:banp_periodic periodic ctx_ll -0.0240 tar_ll -2.1463 (84.720 secs)\n",
      "\n",
      "banp:banp_periodic step 10200 lr 4.873e-04 [train_loss] ll_base 0.8406 ll 0.8714 loss -1.7120 (8.077 secs)\n",
      "banp:banp_periodic step 10400 lr 4.868e-04 [train_loss] ll_base 0.9139 ll 0.9319 loss -1.8458 (8.039 secs)\n",
      "banp:banp_periodic step 10600 lr 4.863e-04 [train_loss] ll_base 0.9420 ll 0.9649 loss -1.9070 (7.978 secs)\n",
      "banp:banp_periodic step 10800 lr 4.857e-04 [train_loss] ll_base 0.9255 ll 0.9447 loss -1.8702 (7.821 secs)\n",
      "banp:banp_periodic step 11000 lr 4.852e-04 [train_loss] ll_base 0.6753 ll 0.7368 loss -1.4122 (7.939 secs)\n",
      "banp:banp_periodic step 11200 lr 4.847e-04 [train_loss] ll_base 0.9372 ll 0.9610 loss -1.8983 (7.830 secs)\n",
      "banp:banp_periodic step 11400 lr 4.841e-04 [train_loss] ll_base 0.8883 ll 0.9067 loss -1.7950 (7.619 secs)\n",
      "banp:banp_periodic step 11600 lr 4.836e-04 [train_loss] ll_base 0.9796 ll 0.9968 loss -1.9765 (7.816 secs)\n",
      "banp:banp_periodic step 11800 lr 4.830e-04 [train_loss] ll_base 0.8831 ll 0.9039 loss -1.7870 (7.891 secs)\n",
      "banp:banp_periodic step 12000 lr 4.824e-04 [train_loss] ll_base 0.8867 ll 0.9166 loss -1.8033 (7.970 secs)\n",
      "banp:banp_periodic step 12200 lr 4.819e-04 [train_loss] ll_base 0.9404 ll 0.9581 loss -1.8985 (7.890 secs)\n",
      "banp:banp_periodic step 12400 lr 4.813e-04 [train_loss] ll_base 0.9009 ll 0.9163 loss -1.8173 (7.973 secs)\n",
      "banp:banp_periodic step 12600 lr 4.807e-04 [train_loss] ll_base 0.9179 ll 0.9409 loss -1.8588 (7.896 secs)\n",
      "banp:banp_periodic step 12800 lr 4.801e-04 [train_loss] ll_base 0.9628 ll 0.9859 loss -1.9487 (8.019 secs)\n",
      "banp:banp_periodic step 13000 lr 4.794e-04 [train_loss] ll_base 0.9876 ll 1.0048 loss -1.9924 (7.741 secs)\n",
      "banp:banp_periodic step 13200 lr 4.788e-04 [train_loss] ll_base 0.9856 ll 1.0024 loss -1.9880 (7.805 secs)\n",
      "banp:banp_periodic step 13400 lr 4.782e-04 [train_loss] ll_base 0.9842 ll 1.0072 loss -1.9914 (7.688 secs)\n",
      "banp:banp_periodic step 13600 lr 4.775e-04 [train_loss] ll_base 1.0600 ll 1.0750 loss -2.1350 (7.966 secs)\n",
      "banp:banp_periodic step 13800 lr 4.769e-04 [train_loss] ll_base 0.9159 ll 0.9355 loss -1.8514 (7.800 secs)\n",
      "banp:banp_periodic step 14000 lr 4.762e-04 [train_loss] ll_base 0.9581 ll 0.9715 loss -1.9296 (7.691 secs)\n",
      "banp:banp_periodic step 14200 lr 4.755e-04 [train_loss] ll_base 0.9805 ll 1.0010 loss -1.9816 (7.886 secs)\n",
      "banp:banp_periodic step 14400 lr 4.749e-04 [train_loss] ll_base 0.9628 ll 0.9836 loss -1.9464 (7.818 secs)\n",
      "banp:banp_periodic step 14600 lr 4.742e-04 [train_loss] ll_base 0.9345 ll 0.9523 loss -1.8868 (7.896 secs)\n",
      "banp:banp_periodic step 14800 lr 4.735e-04 [train_loss] ll_base 0.9201 ll 0.9384 loss -1.8585 (7.867 secs)\n",
      "banp:banp_periodic step 15000 lr 4.728e-04 [train_loss] ll_base 0.9563 ll 0.9732 loss -1.9296 (7.921 secs)\n",
      "100%|##########| 3000/3000 [01:24<00:00, 35.60it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.1709 tar_ll -3.0754 (84.275 secs)\n",
      "\n",
      "banp:banp_periodic step 15200 lr 4.720e-04 [train_loss] ll_base 0.8893 ll 0.9175 loss -1.8068 (8.075 secs)\n",
      "banp:banp_periodic step 15400 lr 4.713e-04 [train_loss] ll_base 0.9518 ll 0.9728 loss -1.9247 (7.597 secs)\n",
      "banp:banp_periodic step 15600 lr 4.706e-04 [train_loss] ll_base 0.9269 ll 0.9498 loss -1.8766 (7.901 secs)\n",
      "banp:banp_periodic step 15800 lr 4.698e-04 [train_loss] ll_base 0.9750 ll 0.9895 loss -1.9646 (7.695 secs)\n",
      "banp:banp_periodic step 16000 lr 4.691e-04 [train_loss] ll_base 0.8085 ll 0.8380 loss -1.6465 (7.730 secs)\n",
      "banp:banp_periodic step 16200 lr 4.683e-04 [train_loss] ll_base 0.8272 ll 0.8669 loss -1.6941 (7.943 secs)\n",
      "banp:banp_periodic step 16400 lr 4.675e-04 [train_loss] ll_base 0.9028 ll 0.9291 loss -1.8319 (8.256 secs)\n",
      "banp:banp_periodic step 16600 lr 4.668e-04 [train_loss] ll_base 1.0238 ll 1.0367 loss -2.0605 (7.989 secs)\n",
      "banp:banp_periodic step 16800 lr 4.660e-04 [train_loss] ll_base 0.8787 ll 0.8966 loss -1.7752 (7.925 secs)\n",
      "banp:banp_periodic step 17000 lr 4.652e-04 [train_loss] ll_base 0.9906 ll 1.0055 loss -1.9961 (7.984 secs)\n",
      "banp:banp_periodic step 17200 lr 4.644e-04 [train_loss] ll_base 0.9210 ll 0.9418 loss -1.8628 (7.946 secs)\n",
      "banp:banp_periodic step 17400 lr 4.636e-04 [train_loss] ll_base 1.0074 ll 1.0201 loss -2.0274 (7.730 secs)\n",
      "banp:banp_periodic step 17600 lr 4.627e-04 [train_loss] ll_base 0.9329 ll 0.9535 loss -1.8865 (8.074 secs)\n",
      "banp:banp_periodic step 17800 lr 4.619e-04 [train_loss] ll_base 1.0096 ll 1.0223 loss -2.0320 (7.824 secs)\n",
      "banp:banp_periodic step 18000 lr 4.611e-04 [train_loss] ll_base 1.0267 ll 1.0413 loss -2.0680 (8.005 secs)\n",
      "banp:banp_periodic step 18200 lr 4.602e-04 [train_loss] ll_base 0.9394 ll 0.9516 loss -1.8910 (7.913 secs)\n",
      "banp:banp_periodic step 18400 lr 4.594e-04 [train_loss] ll_base 0.9750 ll 0.9922 loss -1.9672 (7.942 secs)\n",
      "banp:banp_periodic step 18600 lr 4.585e-04 [train_loss] ll_base 0.9577 ll 0.9756 loss -1.9333 (7.956 secs)\n",
      "banp:banp_periodic step 18800 lr 4.576e-04 [train_loss] ll_base 0.9720 ll 0.9857 loss -1.9577 (8.250 secs)\n",
      "banp:banp_periodic step 19000 lr 4.568e-04 [train_loss] ll_base 0.9699 ll 0.9859 loss -1.9558 (8.321 secs)\n",
      "banp:banp_periodic step 19200 lr 4.559e-04 [train_loss] ll_base 0.9065 ll 0.9278 loss -1.8343 (7.877 secs)\n",
      "banp:banp_periodic step 19400 lr 4.550e-04 [train_loss] ll_base 1.0053 ll 1.0241 loss -2.0294 (7.727 secs)\n",
      "banp:banp_periodic step 19600 lr 4.541e-04 [train_loss] ll_base 0.9629 ll 0.9811 loss -1.9440 (7.709 secs)\n",
      "banp:banp_periodic step 19800 lr 4.532e-04 [train_loss] ll_base 0.9946 ll 1.0138 loss -2.0084 (7.679 secs)\n",
      "banp:banp_periodic step 20000 lr 4.523e-04 [train_loss] ll_base 0.9210 ll 0.9390 loss -1.8600 (7.679 secs)\n",
      "100%|##########| 3000/3000 [01:24<00:00, 35.71it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.2898 tar_ll -2.7670 (84.006 secs)\n",
      "\n",
      "banp:banp_periodic step 20200 lr 4.513e-04 [train_loss] ll_base 0.8905 ll 0.9159 loss -1.8064 (8.202 secs)\n",
      "banp:banp_periodic step 20400 lr 4.504e-04 [train_loss] ll_base 0.7977 ll 0.8393 loss -1.6370 (7.909 secs)\n",
      "banp:banp_periodic step 20600 lr 4.494e-04 [train_loss] ll_base 0.8859 ll 0.9099 loss -1.7958 (8.179 secs)\n",
      "banp:banp_periodic step 20800 lr 4.485e-04 [train_loss] ll_base 1.0162 ll 1.0350 loss -2.0512 (7.914 secs)\n",
      "banp:banp_periodic step 21000 lr 4.475e-04 [train_loss] ll_base 0.9488 ll 0.9664 loss -1.9151 (7.997 secs)\n",
      "banp:banp_periodic step 21200 lr 4.466e-04 [train_loss] ll_base 0.9811 ll 0.9936 loss -1.9748 (8.067 secs)\n",
      "banp:banp_periodic step 21400 lr 4.456e-04 [train_loss] ll_base 0.9858 ll 1.0013 loss -1.9871 (7.943 secs)\n",
      "banp:banp_periodic step 21600 lr 4.446e-04 [train_loss] ll_base 0.9066 ll 0.9335 loss -1.8401 (7.951 secs)\n",
      "banp:banp_periodic step 21800 lr 4.436e-04 [train_loss] ll_base 0.9404 ll 0.9607 loss -1.9011 (7.909 secs)\n",
      "banp:banp_periodic step 22000 lr 4.426e-04 [train_loss] ll_base 0.9803 ll 0.9972 loss -1.9774 (7.946 secs)\n",
      "banp:banp_periodic step 22200 lr 4.416e-04 [train_loss] ll_base 0.9190 ll 0.9306 loss -1.8496 (8.065 secs)\n",
      "banp:banp_periodic step 22400 lr 4.406e-04 [train_loss] ll_base 1.0506 ll 1.0625 loss -2.1131 (7.836 secs)\n",
      "banp:banp_periodic step 22600 lr 4.396e-04 [train_loss] ll_base 1.0138 ll 1.0252 loss -2.0389 (7.832 secs)\n",
      "banp:banp_periodic step 22800 lr 4.386e-04 [train_loss] ll_base 0.9513 ll 0.9650 loss -1.9163 (7.776 secs)\n",
      "banp:banp_periodic step 23000 lr 4.375e-04 [train_loss] ll_base 0.9806 ll 0.9954 loss -1.9761 (7.893 secs)\n",
      "banp:banp_periodic step 23200 lr 4.365e-04 [train_loss] ll_base 0.9229 ll 0.9413 loss -1.8642 (7.672 secs)\n",
      "banp:banp_periodic step 23400 lr 4.354e-04 [train_loss] ll_base 0.9180 ll 0.9312 loss -1.8492 (7.900 secs)\n",
      "banp:banp_periodic step 23600 lr 4.344e-04 [train_loss] ll_base 0.9737 ll 0.9885 loss -1.9622 (7.715 secs)\n",
      "banp:banp_periodic step 23800 lr 4.333e-04 [train_loss] ll_base 0.9441 ll 0.9617 loss -1.9059 (7.883 secs)\n",
      "banp:banp_periodic step 24000 lr 4.322e-04 [train_loss] ll_base 0.9891 ll 1.0021 loss -1.9912 (8.067 secs)\n",
      "banp:banp_periodic step 24200 lr 4.312e-04 [train_loss] ll_base 0.9881 ll 0.9994 loss -1.9875 (7.740 secs)\n",
      "banp:banp_periodic step 24400 lr 4.301e-04 [train_loss] ll_base 0.9731 ll 0.9896 loss -1.9626 (7.823 secs)\n",
      "banp:banp_periodic step 24600 lr 4.290e-04 [train_loss] ll_base 0.9329 ll 0.9538 loss -1.8867 (7.750 secs)\n",
      "banp:banp_periodic step 24800 lr 4.279e-04 [train_loss] ll_base 1.0045 ll 1.0170 loss -2.0215 (7.785 secs)\n",
      "banp:banp_periodic step 25000 lr 4.268e-04 [train_loss] ll_base 1.0507 ll 1.0651 loss -2.1158 (7.646 secs)\n",
      "100%|##########| 3000/3000 [01:23<00:00, 35.73it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.4550 tar_ll -2.4520 (83.966 secs)\n",
      "\n",
      "banp:banp_periodic step 25200 lr 4.257e-04 [train_loss] ll_base 1.0017 ll 1.0144 loss -2.0161 (8.093 secs)\n",
      "banp:banp_periodic step 25400 lr 4.245e-04 [train_loss] ll_base 0.9568 ll 0.9751 loss -1.9319 (9.097 secs)\n",
      "banp:banp_periodic step 25600 lr 4.234e-04 [train_loss] ll_base 0.9758 ll 0.9923 loss -1.9681 (8.604 secs)\n",
      "banp:banp_periodic step 25800 lr 4.223e-04 [train_loss] ll_base 1.0050 ll 1.0143 loss -2.0193 (8.039 secs)\n",
      "banp:banp_periodic step 26000 lr 4.211e-04 [train_loss] ll_base 0.9471 ll 0.9635 loss -1.9106 (7.942 secs)\n",
      "banp:banp_periodic step 26200 lr 4.200e-04 [train_loss] ll_base 1.0160 ll 1.0317 loss -2.0477 (8.084 secs)\n",
      "banp:banp_periodic step 26400 lr 4.188e-04 [train_loss] ll_base 0.9854 ll 0.9969 loss -1.9824 (7.969 secs)\n",
      "banp:banp_periodic step 26600 lr 4.177e-04 [train_loss] ll_base 0.9872 ll 1.0027 loss -1.9899 (7.963 secs)\n",
      "banp:banp_periodic step 26800 lr 4.165e-04 [train_loss] ll_base 0.9125 ll 0.9320 loss -1.8445 (8.010 secs)\n",
      "banp:banp_periodic step 27000 lr 4.153e-04 [train_loss] ll_base 0.9815 ll 0.9929 loss -1.9745 (7.930 secs)\n",
      "banp:banp_periodic step 27200 lr 4.141e-04 [train_loss] ll_base 1.0474 ll 1.0592 loss -2.1066 (7.877 secs)\n",
      "banp:banp_periodic step 27400 lr 4.130e-04 [train_loss] ll_base 0.9557 ll 0.9750 loss -1.9307 (7.804 secs)\n",
      "banp:banp_periodic step 27600 lr 4.118e-04 [train_loss] ll_base 0.9737 ll 0.9886 loss -1.9623 (7.680 secs)\n",
      "banp:banp_periodic step 27800 lr 4.106e-04 [train_loss] ll_base 0.8909 ll 0.9115 loss -1.8024 (7.882 secs)\n",
      "banp:banp_periodic step 28000 lr 4.094e-04 [train_loss] ll_base 1.0010 ll 1.0175 loss -2.0185 (7.776 secs)\n",
      "banp:banp_periodic step 28200 lr 4.081e-04 [train_loss] ll_base 1.0085 ll 1.0171 loss -2.0255 (7.704 secs)\n",
      "banp:banp_periodic step 28400 lr 4.069e-04 [train_loss] ll_base 0.9882 ll 1.0047 loss -1.9929 (7.684 secs)\n",
      "banp:banp_periodic step 28600 lr 4.057e-04 [train_loss] ll_base 1.0142 ll 1.0304 loss -2.0446 (7.811 secs)\n",
      "banp:banp_periodic step 28800 lr 4.045e-04 [train_loss] ll_base 1.0009 ll 1.0152 loss -2.0161 (7.736 secs)\n",
      "banp:banp_periodic step 29000 lr 4.032e-04 [train_loss] ll_base 0.9606 ll 0.9713 loss -1.9319 (7.766 secs)\n",
      "banp:banp_periodic step 29200 lr 4.020e-04 [train_loss] ll_base 1.0193 ll 1.0301 loss -2.0494 (7.712 secs)\n",
      "banp:banp_periodic step 29400 lr 4.007e-04 [train_loss] ll_base 1.0298 ll 1.0421 loss -2.0719 (7.861 secs)\n",
      "banp:banp_periodic step 29600 lr 3.995e-04 [train_loss] ll_base 1.0471 ll 1.0596 loss -2.1067 (7.619 secs)\n",
      "banp:banp_periodic step 29800 lr 3.982e-04 [train_loss] ll_base 1.0127 ll 1.0255 loss -2.0382 (7.986 secs)\n",
      "banp:banp_periodic step 30000 lr 3.969e-04 [train_loss] ll_base 0.9202 ll 0.9404 loss -1.8606 (7.930 secs)\n",
      "100%|##########| 3000/3000 [01:26<00:00, 34.84it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.3692 tar_ll -2.2771 (86.111 secs)\n",
      "\n",
      "banp:banp_periodic step 30200 lr 3.957e-04 [train_loss] ll_base 0.9787 ll 0.9893 loss -1.9680 (9.386 secs)\n",
      "banp:banp_periodic step 30400 lr 3.944e-04 [train_loss] ll_base 0.9297 ll 0.9441 loss -1.8738 (9.712 secs)\n",
      "banp:banp_periodic step 30600 lr 3.931e-04 [train_loss] ll_base 0.9841 ll 0.9978 loss -1.9819 (8.221 secs)\n",
      "banp:banp_periodic step 30800 lr 3.918e-04 [train_loss] ll_base 0.9926 ll 1.0041 loss -1.9967 (8.024 secs)\n",
      "banp:banp_periodic step 31000 lr 3.905e-04 [train_loss] ll_base 0.9428 ll 0.9534 loss -1.8962 (7.676 secs)\n",
      "banp:banp_periodic step 31200 lr 3.892e-04 [train_loss] ll_base 0.9760 ll 0.9865 loss -1.9625 (7.807 secs)\n",
      "banp:banp_periodic step 31400 lr 3.879e-04 [train_loss] ll_base 1.0128 ll 1.0247 loss -2.0375 (7.707 secs)\n",
      "banp:banp_periodic step 31600 lr 3.866e-04 [train_loss] ll_base 0.9688 ll 0.9801 loss -1.9488 (7.818 secs)\n",
      "banp:banp_periodic step 31800 lr 3.853e-04 [train_loss] ll_base 0.9645 ll 0.9744 loss -1.9388 (7.637 secs)\n",
      "banp:banp_periodic step 32000 lr 3.840e-04 [train_loss] ll_base 0.9467 ll 0.9618 loss -1.9085 (7.747 secs)\n",
      "banp:banp_periodic step 32200 lr 3.826e-04 [train_loss] ll_base 0.9981 ll 1.0101 loss -2.0082 (7.702 secs)\n",
      "banp:banp_periodic step 32400 lr 3.813e-04 [train_loss] ll_base 1.0360 ll 1.0465 loss -2.0825 (7.820 secs)\n",
      "banp:banp_periodic step 32600 lr 3.800e-04 [train_loss] ll_base 1.0759 ll 1.0861 loss -2.1621 (7.716 secs)\n",
      "banp:banp_periodic step 32800 lr 3.786e-04 [train_loss] ll_base 0.8475 ll 0.8703 loss -1.7178 (7.893 secs)\n",
      "banp:banp_periodic step 33000 lr 3.773e-04 [train_loss] ll_base 0.9895 ll 1.0011 loss -1.9906 (7.695 secs)\n",
      "banp:banp_periodic step 33200 lr 3.759e-04 [train_loss] ll_base 1.0069 ll 1.0181 loss -2.0250 (7.601 secs)\n",
      "banp:banp_periodic step 33400 lr 3.745e-04 [train_loss] ll_base 1.0079 ll 1.0176 loss -2.0255 (7.649 secs)\n",
      "banp:banp_periodic step 33600 lr 3.732e-04 [train_loss] ll_base 0.9486 ll 0.9619 loss -1.9105 (7.733 secs)\n",
      "banp:banp_periodic step 33800 lr 3.718e-04 [train_loss] ll_base 1.0212 ll 1.0340 loss -2.0552 (7.848 secs)\n",
      "banp:banp_periodic step 34000 lr 3.704e-04 [train_loss] ll_base 0.8834 ll 0.8942 loss -1.7776 (7.894 secs)\n",
      "banp:banp_periodic step 34200 lr 3.691e-04 [train_loss] ll_base 0.9751 ll 0.9883 loss -1.9635 (8.026 secs)\n",
      "banp:banp_periodic step 34400 lr 3.677e-04 [train_loss] ll_base 1.0280 ll 1.0397 loss -2.0678 (7.887 secs)\n",
      "banp:banp_periodic step 34600 lr 3.663e-04 [train_loss] ll_base 1.0167 ll 1.0273 loss -2.0440 (7.901 secs)\n",
      "banp:banp_periodic step 34800 lr 3.649e-04 [train_loss] ll_base 0.9823 ll 0.9962 loss -1.9785 (8.025 secs)\n",
      "banp:banp_periodic step 35000 lr 3.635e-04 [train_loss] ll_base 1.0102 ll 1.0221 loss -2.0323 (7.814 secs)\n",
      "100%|##########| 3000/3000 [01:24<00:00, 35.65it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.4316 tar_ll -2.5014 (84.160 secs)\n",
      "\n",
      "banp:banp_periodic step 35200 lr 3.621e-04 [train_loss] ll_base 1.0317 ll 1.0394 loss -2.0711 (8.802 secs)\n",
      "banp:banp_periodic step 35400 lr 3.607e-04 [train_loss] ll_base 0.9692 ll 0.9778 loss -1.9469 (9.197 secs)\n",
      "banp:banp_periodic step 35600 lr 3.593e-04 [train_loss] ll_base 0.9639 ll 0.9748 loss -1.9387 (8.560 secs)\n",
      "banp:banp_periodic step 35800 lr 3.579e-04 [train_loss] ll_base 1.0198 ll 1.0288 loss -2.0486 (7.932 secs)\n",
      "banp:banp_periodic step 36000 lr 3.564e-04 [train_loss] ll_base 0.9506 ll 0.9630 loss -1.9136 (8.050 secs)\n",
      "banp:banp_periodic step 36200 lr 3.550e-04 [train_loss] ll_base 1.0095 ll 1.0231 loss -2.0326 (7.877 secs)\n",
      "banp:banp_periodic step 36400 lr 3.536e-04 [train_loss] ll_base 1.0336 ll 1.0445 loss -2.0780 (7.863 secs)\n",
      "banp:banp_periodic step 36600 lr 3.522e-04 [train_loss] ll_base 0.9874 ll 0.9975 loss -1.9850 (7.876 secs)\n",
      "banp:banp_periodic step 36800 lr 3.507e-04 [train_loss] ll_base 1.0880 ll 1.0990 loss -2.1870 (7.782 secs)\n",
      "banp:banp_periodic step 37000 lr 3.493e-04 [train_loss] ll_base 1.0127 ll 1.0251 loss -2.0378 (7.883 secs)\n",
      "banp:banp_periodic step 37200 lr 3.478e-04 [train_loss] ll_base 0.9699 ll 0.9823 loss -1.9522 (7.763 secs)\n",
      "banp:banp_periodic step 37400 lr 3.464e-04 [train_loss] ll_base 1.0263 ll 1.0365 loss -2.0628 (7.739 secs)\n",
      "banp:banp_periodic step 37600 lr 3.449e-04 [train_loss] ll_base 1.0386 ll 1.0533 loss -2.0918 (7.901 secs)\n",
      "banp:banp_periodic step 37800 lr 3.435e-04 [train_loss] ll_base 0.9986 ll 1.0072 loss -2.0058 (7.838 secs)\n",
      "banp:banp_periodic step 38000 lr 3.420e-04 [train_loss] ll_base 1.0096 ll 1.0201 loss -2.0297 (8.074 secs)\n",
      "banp:banp_periodic step 38200 lr 3.406e-04 [train_loss] ll_base 1.0379 ll 1.0462 loss -2.0841 (7.891 secs)\n",
      "banp:banp_periodic step 38400 lr 3.391e-04 [train_loss] ll_base 1.0025 ll 1.0156 loss -2.0181 (7.873 secs)\n",
      "banp:banp_periodic step 38600 lr 3.376e-04 [train_loss] ll_base 1.0305 ll 1.0417 loss -2.0722 (7.828 secs)\n",
      "banp:banp_periodic step 38800 lr 3.362e-04 [train_loss] ll_base 1.0203 ll 1.0286 loss -2.0489 (8.020 secs)\n",
      "banp:banp_periodic step 39000 lr 3.347e-04 [train_loss] ll_base 1.0048 ll 1.0139 loss -2.0186 (7.806 secs)\n",
      "banp:banp_periodic step 39200 lr 3.332e-04 [train_loss] ll_base 0.9131 ll 0.9284 loss -1.8415 (8.004 secs)\n",
      "banp:banp_periodic step 39400 lr 3.317e-04 [train_loss] ll_base 0.9306 ll 0.9466 loss -1.8772 (7.969 secs)\n",
      "banp:banp_periodic step 39600 lr 3.302e-04 [train_loss] ll_base 0.9724 ll 0.9842 loss -1.9565 (7.993 secs)\n",
      "banp:banp_periodic step 39800 lr 3.287e-04 [train_loss] ll_base 0.9764 ll 0.9887 loss -1.9651 (7.741 secs)\n",
      "banp:banp_periodic step 40000 lr 3.273e-04 [train_loss] ll_base 1.0406 ll 1.0515 loss -2.0921 (7.992 secs)\n",
      "100%|##########| 3000/3000 [01:25<00:00, 35.04it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.4972 tar_ll -2.3623 (85.619 secs)\n",
      "\n",
      "banp:banp_periodic step 40200 lr 3.258e-04 [train_loss] ll_base 1.0170 ll 1.0282 loss -2.0451 (10.085 secs)\n",
      "banp:banp_periodic step 40400 lr 3.243e-04 [train_loss] ll_base 1.0170 ll 1.0249 loss -2.0419 (9.096 secs)\n",
      "banp:banp_periodic step 40600 lr 3.228e-04 [train_loss] ll_base 1.0146 ll 1.0231 loss -2.0377 (7.675 secs)\n",
      "banp:banp_periodic step 40800 lr 3.213e-04 [train_loss] ll_base 1.0129 ll 1.0215 loss -2.0344 (7.769 secs)\n",
      "banp:banp_periodic step 41000 lr 3.197e-04 [train_loss] ll_base 0.9882 ll 0.9985 loss -1.9867 (7.744 secs)\n",
      "banp:banp_periodic step 41200 lr 3.182e-04 [train_loss] ll_base 0.9770 ll 0.9887 loss -1.9657 (7.819 secs)\n",
      "banp:banp_periodic step 41400 lr 3.167e-04 [train_loss] ll_base 1.0445 ll 1.0531 loss -2.0975 (7.924 secs)\n",
      "banp:banp_periodic step 41600 lr 3.152e-04 [train_loss] ll_base 1.0444 ll 1.0543 loss -2.0987 (7.980 secs)\n",
      "banp:banp_periodic step 41800 lr 3.137e-04 [train_loss] ll_base 0.9808 ll 0.9949 loss -1.9757 (8.043 secs)\n",
      "banp:banp_periodic step 42000 lr 3.122e-04 [train_loss] ll_base 0.9993 ll 1.0082 loss -2.0075 (7.952 secs)\n",
      "banp:banp_periodic step 42200 lr 3.106e-04 [train_loss] ll_base 1.0703 ll 1.0807 loss -2.1510 (8.219 secs)\n",
      "banp:banp_periodic step 42400 lr 3.091e-04 [train_loss] ll_base 1.0291 ll 1.0400 loss -2.0690 (7.730 secs)\n",
      "banp:banp_periodic step 42600 lr 3.076e-04 [train_loss] ll_base 1.0081 ll 1.0177 loss -2.0257 (8.138 secs)\n",
      "banp:banp_periodic step 42800 lr 3.061e-04 [train_loss] ll_base 1.0298 ll 1.0436 loss -2.0734 (8.030 secs)\n",
      "banp:banp_periodic step 43000 lr 3.045e-04 [train_loss] ll_base 0.9817 ll 0.9912 loss -1.9730 (7.946 secs)\n",
      "banp:banp_periodic step 43200 lr 3.030e-04 [train_loss] ll_base 1.0688 ll 1.0797 loss -2.1485 (8.029 secs)\n",
      "banp:banp_periodic step 43400 lr 3.015e-04 [train_loss] ll_base 0.9493 ll 0.9662 loss -1.9154 (8.000 secs)\n",
      "banp:banp_periodic step 43600 lr 2.999e-04 [train_loss] ll_base 1.0724 ll 1.0850 loss -2.1574 (7.749 secs)\n",
      "banp:banp_periodic step 43800 lr 2.984e-04 [train_loss] ll_base 0.9977 ll 1.0093 loss -2.0070 (7.927 secs)\n",
      "banp:banp_periodic step 44000 lr 2.968e-04 [train_loss] ll_base 1.0414 ll 1.0517 loss -2.0931 (7.822 secs)\n",
      "banp:banp_periodic step 44200 lr 2.953e-04 [train_loss] ll_base 1.0191 ll 1.0300 loss -2.0491 (8.110 secs)\n",
      "banp:banp_periodic step 44400 lr 2.938e-04 [train_loss] ll_base 1.0150 ll 1.0239 loss -2.0390 (7.824 secs)\n",
      "banp:banp_periodic step 44600 lr 2.922e-04 [train_loss] ll_base 1.0138 ll 1.0249 loss -2.0388 (7.846 secs)\n",
      "banp:banp_periodic step 44800 lr 2.907e-04 [train_loss] ll_base 0.9875 ll 0.9993 loss -1.9868 (7.793 secs)\n",
      "banp:banp_periodic step 45000 lr 2.891e-04 [train_loss] ll_base 1.0244 ll 1.0368 loss -2.0612 (7.911 secs)\n",
      "100%|##########| 3000/3000 [01:24<00:00, 35.55it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.2841 tar_ll -3.4350 (84.383 secs)\n",
      "\n",
      "banp:banp_periodic step 45200 lr 2.876e-04 [train_loss] ll_base 0.9991 ll 1.0061 loss -2.0053 (8.069 secs)\n",
      "banp:banp_periodic step 45400 lr 2.860e-04 [train_loss] ll_base 1.0341 ll 1.0433 loss -2.0775 (9.219 secs)\n",
      "banp:banp_periodic step 45600 lr 2.844e-04 [train_loss] ll_base 1.0240 ll 1.0345 loss -2.0585 (9.185 secs)\n",
      "banp:banp_periodic step 45800 lr 2.829e-04 [train_loss] ll_base 1.0549 ll 1.0644 loss -2.1192 (8.114 secs)\n",
      "banp:banp_periodic step 46000 lr 2.813e-04 [train_loss] ll_base 1.0047 ll 1.0156 loss -2.0203 (8.176 secs)\n",
      "banp:banp_periodic step 46200 lr 2.798e-04 [train_loss] ll_base 1.0055 ll 1.0155 loss -2.0210 (8.098 secs)\n",
      "banp:banp_periodic step 46400 lr 2.782e-04 [train_loss] ll_base 1.0241 ll 1.0312 loss -2.0553 (8.006 secs)\n",
      "banp:banp_periodic step 46600 lr 2.767e-04 [train_loss] ll_base 0.9870 ll 0.9985 loss -1.9855 (8.098 secs)\n",
      "banp:banp_periodic step 46800 lr 2.751e-04 [train_loss] ll_base 1.0436 ll 1.0571 loss -2.1007 (8.106 secs)\n",
      "banp:banp_periodic step 47000 lr 2.735e-04 [train_loss] ll_base 1.0268 ll 1.0340 loss -2.0607 (7.963 secs)\n",
      "banp:banp_periodic step 47200 lr 2.720e-04 [train_loss] ll_base 1.0177 ll 1.0246 loss -2.0423 (7.986 secs)\n",
      "banp:banp_periodic step 47400 lr 2.704e-04 [train_loss] ll_base 1.0040 ll 1.0118 loss -2.0158 (7.991 secs)\n",
      "banp:banp_periodic step 47600 lr 2.688e-04 [train_loss] ll_base 1.0062 ll 1.0165 loss -2.0227 (8.001 secs)\n",
      "banp:banp_periodic step 47800 lr 2.673e-04 [train_loss] ll_base 1.0100 ll 1.0196 loss -2.0295 (7.902 secs)\n",
      "banp:banp_periodic step 48000 lr 2.657e-04 [train_loss] ll_base 1.0521 ll 1.0598 loss -2.1119 (8.190 secs)\n",
      "banp:banp_periodic step 48200 lr 2.641e-04 [train_loss] ll_base 1.0140 ll 1.0214 loss -2.0353 (7.808 secs)\n",
      "banp:banp_periodic step 48400 lr 2.626e-04 [train_loss] ll_base 1.0594 ll 1.0676 loss -2.1270 (7.800 secs)\n",
      "banp:banp_periodic step 48600 lr 2.610e-04 [train_loss] ll_base 1.0770 ll 1.0843 loss -2.1613 (7.871 secs)\n",
      "banp:banp_periodic step 48800 lr 2.594e-04 [train_loss] ll_base 1.0522 ll 1.0618 loss -2.1140 (7.650 secs)\n",
      "banp:banp_periodic step 49000 lr 2.579e-04 [train_loss] ll_base 1.0171 ll 1.0269 loss -2.0440 (7.844 secs)\n",
      "banp:banp_periodic step 49200 lr 2.563e-04 [train_loss] ll_base 0.9985 ll 1.0085 loss -2.0070 (8.000 secs)\n",
      "banp:banp_periodic step 49400 lr 2.547e-04 [train_loss] ll_base 0.9861 ll 0.9953 loss -1.9814 (7.773 secs)\n",
      "banp:banp_periodic step 49600 lr 2.531e-04 [train_loss] ll_base 1.0339 ll 1.0422 loss -2.0762 (7.993 secs)\n",
      "banp:banp_periodic step 49800 lr 2.516e-04 [train_loss] ll_base 1.0613 ll 1.0711 loss -2.1323 (7.719 secs)\n",
      "banp:banp_periodic step 50000 lr 2.500e-04 [train_loss] ll_base 1.0080 ll 1.0176 loss -2.0256 (7.895 secs)\n",
      "100%|##########| 3000/3000 [01:24<00:00, 35.71it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.4184 tar_ll -2.7895 (84.004 secs)\n",
      "\n",
      "banp:banp_periodic step 50200 lr 2.484e-04 [train_loss] ll_base 0.9565 ll 0.9720 loss -1.9285 (9.512 secs)\n",
      "banp:banp_periodic step 50400 lr 2.469e-04 [train_loss] ll_base 1.0660 ll 1.0745 loss -2.1405 (8.918 secs)\n",
      "banp:banp_periodic step 50600 lr 2.453e-04 [train_loss] ll_base 1.0462 ll 1.0545 loss -2.1007 (8.658 secs)\n",
      "banp:banp_periodic step 50800 lr 2.437e-04 [train_loss] ll_base 1.0392 ll 1.0471 loss -2.0863 (7.908 secs)\n",
      "banp:banp_periodic step 51000 lr 2.421e-04 [train_loss] ll_base 1.0515 ll 1.0582 loss -2.1097 (8.002 secs)\n",
      "banp:banp_periodic step 51200 lr 2.406e-04 [train_loss] ll_base 1.0073 ll 1.0182 loss -2.0255 (7.995 secs)\n",
      "banp:banp_periodic step 51400 lr 2.390e-04 [train_loss] ll_base 1.0693 ll 1.0768 loss -2.1461 (8.240 secs)\n",
      "banp:banp_periodic step 51600 lr 2.374e-04 [train_loss] ll_base 0.9357 ll 0.9448 loss -1.8805 (8.002 secs)\n",
      "banp:banp_periodic step 51800 lr 2.359e-04 [train_loss] ll_base 1.0467 ll 1.0574 loss -2.1041 (8.018 secs)\n",
      "banp:banp_periodic step 52000 lr 2.343e-04 [train_loss] ll_base 1.0337 ll 1.0427 loss -2.0764 (7.746 secs)\n",
      "banp:banp_periodic step 52200 lr 2.327e-04 [train_loss] ll_base 1.0164 ll 1.0256 loss -2.0420 (7.860 secs)\n",
      "banp:banp_periodic step 52400 lr 2.312e-04 [train_loss] ll_base 1.0643 ll 1.0730 loss -2.1373 (7.730 secs)\n",
      "banp:banp_periodic step 52600 lr 2.296e-04 [train_loss] ll_base 1.0507 ll 1.0588 loss -2.1094 (7.830 secs)\n",
      "banp:banp_periodic step 52800 lr 2.280e-04 [train_loss] ll_base 0.9893 ll 0.9993 loss -1.9885 (7.702 secs)\n",
      "banp:banp_periodic step 53000 lr 2.265e-04 [train_loss] ll_base 0.9826 ll 0.9945 loss -1.9771 (7.770 secs)\n",
      "banp:banp_periodic step 53200 lr 2.249e-04 [train_loss] ll_base 0.9473 ll 0.9554 loss -1.9027 (7.800 secs)\n",
      "banp:banp_periodic step 53400 lr 2.233e-04 [train_loss] ll_base 1.0343 ll 1.0423 loss -2.0766 (7.846 secs)\n",
      "banp:banp_periodic step 53600 lr 2.218e-04 [train_loss] ll_base 1.0629 ll 1.0697 loss -2.1326 (7.712 secs)\n",
      "banp:banp_periodic step 53800 lr 2.202e-04 [train_loss] ll_base 1.0279 ll 1.0366 loss -2.0645 (7.928 secs)\n",
      "banp:banp_periodic step 54000 lr 2.187e-04 [train_loss] ll_base 1.0237 ll 1.0331 loss -2.0569 (7.917 secs)\n",
      "banp:banp_periodic step 54200 lr 2.171e-04 [train_loss] ll_base 1.0169 ll 1.0283 loss -2.0451 (7.845 secs)\n",
      "banp:banp_periodic step 54400 lr 2.156e-04 [train_loss] ll_base 1.0390 ll 1.0477 loss -2.0867 (7.702 secs)\n",
      "banp:banp_periodic step 54600 lr 2.140e-04 [train_loss] ll_base 1.0269 ll 1.0372 loss -2.0640 (7.869 secs)\n",
      "banp:banp_periodic step 54800 lr 2.124e-04 [train_loss] ll_base 1.0070 ll 1.0186 loss -2.0257 (7.700 secs)\n",
      "banp:banp_periodic step 55000 lr 2.109e-04 [train_loss] ll_base 1.0578 ll 1.0655 loss -2.1233 (7.720 secs)\n",
      "100%|##########| 3000/3000 [01:26<00:00, 34.53it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.4503 tar_ll -2.8441 (86.879 secs)\n",
      "\n",
      "banp:banp_periodic step 55200 lr 2.093e-04 [train_loss] ll_base 1.0218 ll 1.0308 loss -2.0527 (10.034 secs)\n",
      "banp:banp_periodic step 55400 lr 2.078e-04 [train_loss] ll_base 1.0339 ll 1.0422 loss -2.0761 (9.388 secs)\n",
      "banp:banp_periodic step 55600 lr 2.062e-04 [train_loss] ll_base 1.0145 ll 1.0234 loss -2.0378 (8.122 secs)\n",
      "banp:banp_periodic step 55800 lr 2.047e-04 [train_loss] ll_base 1.0469 ll 1.0571 loss -2.1040 (7.828 secs)\n",
      "banp:banp_periodic step 56000 lr 2.032e-04 [train_loss] ll_base 0.9866 ll 0.9959 loss -1.9825 (8.017 secs)\n",
      "banp:banp_periodic step 56200 lr 2.016e-04 [train_loss] ll_base 1.0776 ll 1.0825 loss -2.1601 (7.995 secs)\n",
      "banp:banp_periodic step 56400 lr 2.001e-04 [train_loss] ll_base 1.0349 ll 1.0421 loss -2.0771 (7.895 secs)\n",
      "banp:banp_periodic step 56600 lr 1.985e-04 [train_loss] ll_base 1.0054 ll 1.0163 loss -2.0217 (7.734 secs)\n",
      "banp:banp_periodic step 56800 lr 1.970e-04 [train_loss] ll_base 0.9956 ll 1.0068 loss -2.0025 (7.612 secs)\n",
      "banp:banp_periodic step 57000 lr 1.955e-04 [train_loss] ll_base 0.9834 ll 0.9942 loss -1.9776 (7.725 secs)\n",
      "banp:banp_periodic step 57200 lr 1.939e-04 [train_loss] ll_base 1.0599 ll 1.0701 loss -2.1300 (7.900 secs)\n",
      "banp:banp_periodic step 57400 lr 1.924e-04 [train_loss] ll_base 1.0207 ll 1.0301 loss -2.0508 (7.741 secs)\n",
      "banp:banp_periodic step 57600 lr 1.909e-04 [train_loss] ll_base 1.0341 ll 1.0443 loss -2.0784 (7.923 secs)\n",
      "banp:banp_periodic step 57800 lr 1.894e-04 [train_loss] ll_base 1.0136 ll 1.0206 loss -2.0341 (7.803 secs)\n",
      "banp:banp_periodic step 58000 lr 1.878e-04 [train_loss] ll_base 1.0055 ll 1.0166 loss -2.0221 (7.744 secs)\n",
      "banp:banp_periodic step 58200 lr 1.863e-04 [train_loss] ll_base 0.9940 ll 1.0015 loss -1.9955 (7.836 secs)\n",
      "banp:banp_periodic step 58400 lr 1.848e-04 [train_loss] ll_base 1.0252 ll 1.0352 loss -2.0603 (7.855 secs)\n",
      "banp:banp_periodic step 58600 lr 1.833e-04 [train_loss] ll_base 0.9792 ll 0.9871 loss -1.9663 (7.659 secs)\n",
      "banp:banp_periodic step 58800 lr 1.818e-04 [train_loss] ll_base 1.0393 ll 1.0450 loss -2.0843 (7.668 secs)\n",
      "banp:banp_periodic step 59000 lr 1.803e-04 [train_loss] ll_base 1.0164 ll 1.0229 loss -2.0393 (7.748 secs)\n",
      "banp:banp_periodic step 59200 lr 1.787e-04 [train_loss] ll_base 1.0885 ll 1.0962 loss -2.1848 (7.976 secs)\n",
      "banp:banp_periodic step 59400 lr 1.772e-04 [train_loss] ll_base 0.9949 ll 1.0048 loss -1.9997 (7.888 secs)\n",
      "banp:banp_periodic step 59600 lr 1.757e-04 [train_loss] ll_base 1.0690 ll 1.0756 loss -2.1446 (7.920 secs)\n",
      "banp:banp_periodic step 59800 lr 1.742e-04 [train_loss] ll_base 1.0783 ll 1.0857 loss -2.1639 (7.999 secs)\n",
      "banp:banp_periodic step 60000 lr 1.727e-04 [train_loss] ll_base 1.0786 ll 1.0885 loss -2.1671 (7.964 secs)\n",
      "100%|##########| 3000/3000 [01:27<00:00, 34.47it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.4070 tar_ll -2.8481 (87.046 secs)\n",
      "\n",
      "banp:banp_periodic step 60200 lr 1.713e-04 [train_loss] ll_base 1.0294 ll 1.0341 loss -2.0635 (10.614 secs)\n",
      "banp:banp_periodic step 60400 lr 1.698e-04 [train_loss] ll_base 1.0191 ll 1.0280 loss -2.0471 (8.716 secs)\n",
      "banp:banp_periodic step 60600 lr 1.683e-04 [train_loss] ll_base 1.0247 ll 1.0326 loss -2.0573 (8.205 secs)\n",
      "banp:banp_periodic step 60800 lr 1.668e-04 [train_loss] ll_base 0.9757 ll 0.9827 loss -1.9584 (7.955 secs)\n",
      "banp:banp_periodic step 61000 lr 1.653e-04 [train_loss] ll_base 1.0136 ll 1.0213 loss -2.0349 (8.045 secs)\n",
      "banp:banp_periodic step 61200 lr 1.638e-04 [train_loss] ll_base 1.0034 ll 1.0135 loss -2.0169 (7.884 secs)\n",
      "banp:banp_periodic step 61400 lr 1.624e-04 [train_loss] ll_base 1.0038 ll 1.0125 loss -2.0162 (7.775 secs)\n",
      "banp:banp_periodic step 61600 lr 1.609e-04 [train_loss] ll_base 1.0598 ll 1.0712 loss -2.1310 (7.954 secs)\n",
      "banp:banp_periodic step 61800 lr 1.594e-04 [train_loss] ll_base 1.0607 ll 1.0675 loss -2.1281 (7.823 secs)\n",
      "banp:banp_periodic step 62000 lr 1.580e-04 [train_loss] ll_base 1.0644 ll 1.0720 loss -2.1364 (7.786 secs)\n",
      "banp:banp_periodic step 62200 lr 1.565e-04 [train_loss] ll_base 1.0630 ll 1.0720 loss -2.1350 (7.937 secs)\n",
      "banp:banp_periodic step 62400 lr 1.551e-04 [train_loss] ll_base 0.9862 ll 0.9959 loss -1.9821 (7.823 secs)\n",
      "banp:banp_periodic step 62600 lr 1.536e-04 [train_loss] ll_base 1.0324 ll 1.0407 loss -2.0731 (7.900 secs)\n",
      "banp:banp_periodic step 62800 lr 1.522e-04 [train_loss] ll_base 1.0331 ll 1.0405 loss -2.0736 (7.863 secs)\n",
      "banp:banp_periodic step 63000 lr 1.507e-04 [train_loss] ll_base 1.0761 ll 1.0826 loss -2.1587 (7.984 secs)\n",
      "banp:banp_periodic step 63200 lr 1.493e-04 [train_loss] ll_base 1.0527 ll 1.0624 loss -2.1152 (7.811 secs)\n",
      "banp:banp_periodic step 63400 lr 1.478e-04 [train_loss] ll_base 0.9949 ll 1.0082 loss -2.0031 (7.929 secs)\n",
      "banp:banp_periodic step 63600 lr 1.464e-04 [train_loss] ll_base 1.0419 ll 1.0541 loss -2.0960 (7.864 secs)\n",
      "banp:banp_periodic step 63800 lr 1.450e-04 [train_loss] ll_base 0.9772 ll 0.9886 loss -1.9658 (7.960 secs)\n",
      "banp:banp_periodic step 64000 lr 1.436e-04 [train_loss] ll_base 1.0670 ll 1.0758 loss -2.1427 (7.845 secs)\n",
      "banp:banp_periodic step 64200 lr 1.421e-04 [train_loss] ll_base 1.0493 ll 1.0549 loss -2.1042 (8.065 secs)\n",
      "banp:banp_periodic step 64400 lr 1.407e-04 [train_loss] ll_base 1.0365 ll 1.0457 loss -2.0822 (7.910 secs)\n",
      "banp:banp_periodic step 64600 lr 1.393e-04 [train_loss] ll_base 1.0375 ll 1.0438 loss -2.0813 (7.816 secs)\n",
      "banp:banp_periodic step 64800 lr 1.379e-04 [train_loss] ll_base 1.0905 ll 1.0952 loss -2.1857 (7.810 secs)\n",
      "banp:banp_periodic step 65000 lr 1.365e-04 [train_loss] ll_base 1.0744 ll 1.0805 loss -2.1549 (8.107 secs)\n",
      "100%|##########| 3000/3000 [01:26<00:00, 34.75it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.4688 tar_ll -2.7944 (86.341 secs)\n",
      "\n",
      "banp:banp_periodic step 65200 lr 1.351e-04 [train_loss] ll_base 1.0639 ll 1.0694 loss -2.1333 (10.391 secs)\n",
      "banp:banp_periodic step 65400 lr 1.337e-04 [train_loss] ll_base 1.0507 ll 1.0591 loss -2.1097 (8.639 secs)\n",
      "banp:banp_periodic step 65600 lr 1.323e-04 [train_loss] ll_base 1.0207 ll 1.0328 loss -2.0535 (7.844 secs)\n",
      "banp:banp_periodic step 65800 lr 1.309e-04 [train_loss] ll_base 1.0242 ll 1.0313 loss -2.0556 (7.540 secs)\n",
      "banp:banp_periodic step 66000 lr 1.296e-04 [train_loss] ll_base 1.0290 ll 1.0381 loss -2.0670 (7.809 secs)\n",
      "banp:banp_periodic step 66200 lr 1.282e-04 [train_loss] ll_base 1.0425 ll 1.0482 loss -2.0907 (7.692 secs)\n",
      "banp:banp_periodic step 66400 lr 1.268e-04 [train_loss] ll_base 1.0320 ll 1.0399 loss -2.0720 (7.814 secs)\n",
      "banp:banp_periodic step 66600 lr 1.255e-04 [train_loss] ll_base 1.0834 ll 1.0941 loss -2.1775 (8.036 secs)\n",
      "banp:banp_periodic step 66800 lr 1.241e-04 [train_loss] ll_base 1.0268 ll 1.0330 loss -2.0598 (8.074 secs)\n",
      "banp:banp_periodic step 67000 lr 1.227e-04 [train_loss] ll_base 1.0547 ll 1.0627 loss -2.1174 (8.060 secs)\n",
      "banp:banp_periodic step 67200 lr 1.214e-04 [train_loss] ll_base 1.0426 ll 1.0501 loss -2.0927 (8.186 secs)\n",
      "banp:banp_periodic step 67400 lr 1.200e-04 [train_loss] ll_base 1.0641 ll 1.0750 loss -2.1392 (7.941 secs)\n",
      "banp:banp_periodic step 67600 lr 1.187e-04 [train_loss] ll_base 1.0335 ll 1.0455 loss -2.0790 (8.096 secs)\n",
      "banp:banp_periodic step 67800 lr 1.174e-04 [train_loss] ll_base 1.0158 ll 1.0262 loss -2.0420 (7.910 secs)\n",
      "banp:banp_periodic step 68000 lr 1.160e-04 [train_loss] ll_base 1.0033 ll 1.0101 loss -2.0134 (8.233 secs)\n",
      "banp:banp_periodic step 68200 lr 1.147e-04 [train_loss] ll_base 1.0944 ll 1.1040 loss -2.1984 (8.000 secs)\n",
      "banp:banp_periodic step 68400 lr 1.134e-04 [train_loss] ll_base 1.0704 ll 1.0788 loss -2.1492 (8.188 secs)\n",
      "banp:banp_periodic step 68600 lr 1.121e-04 [train_loss] ll_base 1.0507 ll 1.0593 loss -2.1100 (7.926 secs)\n",
      "banp:banp_periodic step 68800 lr 1.108e-04 [train_loss] ll_base 1.0577 ll 1.0661 loss -2.1237 (8.087 secs)\n",
      "banp:banp_periodic step 69000 lr 1.095e-04 [train_loss] ll_base 1.0833 ll 1.0902 loss -2.1736 (7.973 secs)\n",
      "banp:banp_periodic step 69200 lr 1.082e-04 [train_loss] ll_base 1.0040 ll 1.0122 loss -2.0162 (8.082 secs)\n",
      "banp:banp_periodic step 69400 lr 1.069e-04 [train_loss] ll_base 1.0692 ll 1.0782 loss -2.1475 (7.815 secs)\n",
      "banp:banp_periodic step 69600 lr 1.056e-04 [train_loss] ll_base 1.0345 ll 1.0416 loss -2.0761 (7.883 secs)\n",
      "banp:banp_periodic step 69800 lr 1.043e-04 [train_loss] ll_base 1.0349 ll 1.0428 loss -2.0777 (7.768 secs)\n",
      "banp:banp_periodic step 70000 lr 1.031e-04 [train_loss] ll_base 1.0200 ll 1.0278 loss -2.0478 (7.912 secs)\n",
      "100%|##########| 3000/3000 [01:25<00:00, 34.93it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.4167 tar_ll -2.9974 (85.900 secs)\n",
      "\n",
      "banp:banp_periodic step 70200 lr 1.018e-04 [train_loss] ll_base 1.0196 ll 1.0251 loss -2.0447 (10.344 secs)\n",
      "banp:banp_periodic step 70400 lr 1.005e-04 [train_loss] ll_base 1.0049 ll 1.0102 loss -2.0151 (9.234 secs)\n",
      "banp:banp_periodic step 70600 lr 9.927e-05 [train_loss] ll_base 1.0314 ll 1.0426 loss -2.0740 (7.931 secs)\n",
      "banp:banp_periodic step 70800 lr 9.802e-05 [train_loss] ll_base 1.0440 ll 1.0505 loss -2.0945 (8.246 secs)\n",
      "banp:banp_periodic step 71000 lr 9.677e-05 [train_loss] ll_base 1.0327 ll 1.0410 loss -2.0737 (7.964 secs)\n",
      "banp:banp_periodic step 71200 lr 9.554e-05 [train_loss] ll_base 1.0760 ll 1.0835 loss -2.1595 (7.981 secs)\n",
      "banp:banp_periodic step 71400 lr 9.430e-05 [train_loss] ll_base 1.0388 ll 1.0453 loss -2.0841 (7.907 secs)\n",
      "banp:banp_periodic step 71600 lr 9.308e-05 [train_loss] ll_base 1.0628 ll 1.0763 loss -2.1391 (7.949 secs)\n",
      "banp:banp_periodic step 71800 lr 9.186e-05 [train_loss] ll_base 1.0058 ll 1.0151 loss -2.0209 (7.720 secs)\n",
      "banp:banp_periodic step 72000 lr 9.064e-05 [train_loss] ll_base 1.0776 ll 1.0838 loss -2.1615 (8.015 secs)\n",
      "banp:banp_periodic step 72200 lr 8.944e-05 [train_loss] ll_base 1.0442 ll 1.0538 loss -2.0980 (7.799 secs)\n",
      "banp:banp_periodic step 72400 lr 8.824e-05 [train_loss] ll_base 1.0470 ll 1.0519 loss -2.0989 (8.014 secs)\n",
      "banp:banp_periodic step 72600 lr 8.704e-05 [train_loss] ll_base 1.0920 ll 1.1000 loss -2.1920 (7.975 secs)\n",
      "banp:banp_periodic step 72800 lr 8.585e-05 [train_loss] ll_base 1.0706 ll 1.0790 loss -2.1496 (7.931 secs)\n",
      "banp:banp_periodic step 73000 lr 8.467e-05 [train_loss] ll_base 1.0319 ll 1.0412 loss -2.0731 (7.795 secs)\n",
      "banp:banp_periodic step 73200 lr 8.350e-05 [train_loss] ll_base 1.0448 ll 1.0527 loss -2.0974 (7.881 secs)\n",
      "banp:banp_periodic step 73400 lr 8.233e-05 [train_loss] ll_base 1.0576 ll 1.0647 loss -2.1223 (7.933 secs)\n",
      "banp:banp_periodic step 73600 lr 8.117e-05 [train_loss] ll_base 1.0648 ll 1.0722 loss -2.1369 (8.059 secs)\n",
      "banp:banp_periodic step 73800 lr 8.001e-05 [train_loss] ll_base 1.0420 ll 1.0482 loss -2.0901 (7.898 secs)\n",
      "banp:banp_periodic step 74000 lr 7.886e-05 [train_loss] ll_base 1.0606 ll 1.0690 loss -2.1295 (7.789 secs)\n",
      "banp:banp_periodic step 74200 lr 7.772e-05 [train_loss] ll_base 1.1050 ll 1.1117 loss -2.2167 (7.927 secs)\n",
      "banp:banp_periodic step 74400 lr 7.659e-05 [train_loss] ll_base 1.0688 ll 1.0772 loss -2.1460 (7.835 secs)\n",
      "banp:banp_periodic step 74600 lr 7.546e-05 [train_loss] ll_base 1.0094 ll 1.0182 loss -2.0276 (7.964 secs)\n",
      "banp:banp_periodic step 74800 lr 7.434e-05 [train_loss] ll_base 1.1091 ll 1.1145 loss -2.2236 (7.628 secs)\n",
      "banp:banp_periodic step 75000 lr 7.322e-05 [train_loss] ll_base 1.0561 ll 1.0617 loss -2.1178 (7.966 secs)\n",
      "100%|##########| 3000/3000 [01:24<00:00, 35.44it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.3910 tar_ll -2.9388 (84.656 secs)\n",
      "\n",
      "banp:banp_periodic step 75200 lr 7.212e-05 [train_loss] ll_base 1.0851 ll 1.0930 loss -2.1781 (10.152 secs)\n",
      "banp:banp_periodic step 75400 lr 7.102e-05 [train_loss] ll_base 1.0052 ll 1.0147 loss -2.0199 (9.576 secs)\n",
      "banp:banp_periodic step 75600 lr 6.992e-05 [train_loss] ll_base 1.0492 ll 1.0585 loss -2.1077 (7.764 secs)\n",
      "banp:banp_periodic step 75800 lr 6.884e-05 [train_loss] ll_base 1.0781 ll 1.0848 loss -2.1629 (8.101 secs)\n",
      "banp:banp_periodic step 76000 lr 6.776e-05 [train_loss] ll_base 0.9551 ll 0.9616 loss -1.9167 (8.076 secs)\n",
      "banp:banp_periodic step 76200 lr 6.669e-05 [train_loss] ll_base 1.0636 ll 1.0725 loss -2.1362 (8.391 secs)\n",
      "banp:banp_periodic step 76400 lr 6.562e-05 [train_loss] ll_base 1.0840 ll 1.0930 loss -2.1771 (8.173 secs)\n",
      "banp:banp_periodic step 76600 lr 6.456e-05 [train_loss] ll_base 1.0651 ll 1.0718 loss -2.1369 (7.938 secs)\n",
      "banp:banp_periodic step 76800 lr 6.351e-05 [train_loss] ll_base 1.0444 ll 1.0543 loss -2.0987 (7.933 secs)\n",
      "banp:banp_periodic step 77000 lr 6.247e-05 [train_loss] ll_base 1.0694 ll 1.0758 loss -2.1452 (8.010 secs)\n",
      "banp:banp_periodic step 77200 lr 6.144e-05 [train_loss] ll_base 1.0697 ll 1.0756 loss -2.1453 (7.705 secs)\n",
      "banp:banp_periodic step 77400 lr 6.041e-05 [train_loss] ll_base 1.0295 ll 1.0381 loss -2.0675 (7.761 secs)\n",
      "banp:banp_periodic step 77600 lr 5.939e-05 [train_loss] ll_base 1.1020 ll 1.1080 loss -2.2100 (7.742 secs)\n",
      "banp:banp_periodic step 77800 lr 5.838e-05 [train_loss] ll_base 1.0420 ll 1.0505 loss -2.0926 (7.925 secs)\n",
      "banp:banp_periodic step 78000 lr 5.737e-05 [train_loss] ll_base 1.0879 ll 1.0946 loss -2.1825 (7.680 secs)\n",
      "banp:banp_periodic step 78200 lr 5.637e-05 [train_loss] ll_base 1.0586 ll 1.0645 loss -2.1231 (7.923 secs)\n",
      "banp:banp_periodic step 78400 lr 5.538e-05 [train_loss] ll_base 1.0541 ll 1.0612 loss -2.1153 (7.838 secs)\n",
      "banp:banp_periodic step 78600 lr 5.440e-05 [train_loss] ll_base 1.1024 ll 1.1108 loss -2.2132 (7.695 secs)\n",
      "banp:banp_periodic step 78800 lr 5.343e-05 [train_loss] ll_base 1.0276 ll 1.0371 loss -2.0647 (7.707 secs)\n",
      "banp:banp_periodic step 79000 lr 5.246e-05 [train_loss] ll_base 1.0817 ll 1.0879 loss -2.1696 (7.754 secs)\n",
      "banp:banp_periodic step 79200 lr 5.150e-05 [train_loss] ll_base 1.0292 ll 1.0368 loss -2.0660 (7.788 secs)\n",
      "banp:banp_periodic step 79400 lr 5.055e-05 [train_loss] ll_base 1.0290 ll 1.0360 loss -2.0650 (7.767 secs)\n",
      "banp:banp_periodic step 79600 lr 4.961e-05 [train_loss] ll_base 0.9842 ll 0.9933 loss -1.9775 (7.652 secs)\n",
      "banp:banp_periodic step 79800 lr 4.867e-05 [train_loss] ll_base 1.0602 ll 1.0677 loss -2.1279 (7.759 secs)\n",
      "banp:banp_periodic step 80000 lr 4.775e-05 [train_loss] ll_base 1.0962 ll 1.1049 loss -2.2011 (7.864 secs)\n",
      "100%|##########| 3000/3000 [01:26<00:00, 34.59it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.4331 tar_ll -2.8711 (86.723 secs)\n",
      "\n",
      "banp:banp_periodic step 80200 lr 4.683e-05 [train_loss] ll_base 1.0219 ll 1.0304 loss -2.0523 (10.449 secs)\n",
      "banp:banp_periodic step 80400 lr 4.592e-05 [train_loss] ll_base 1.0411 ll 1.0480 loss -2.0891 (9.616 secs)\n",
      "banp:banp_periodic step 80600 lr 4.501e-05 [train_loss] ll_base 1.0903 ll 1.0968 loss -2.1872 (8.128 secs)\n",
      "banp:banp_periodic step 80800 lr 4.412e-05 [train_loss] ll_base 1.0743 ll 1.0805 loss -2.1548 (8.049 secs)\n",
      "banp:banp_periodic step 81000 lr 4.323e-05 [train_loss] ll_base 1.1222 ll 1.1287 loss -2.2509 (7.919 secs)\n",
      "banp:banp_periodic step 81200 lr 4.235e-05 [train_loss] ll_base 1.0630 ll 1.0667 loss -2.1297 (7.877 secs)\n",
      "banp:banp_periodic step 81400 lr 4.148e-05 [train_loss] ll_base 1.0206 ll 1.0288 loss -2.0494 (7.826 secs)\n",
      "banp:banp_periodic step 81600 lr 4.062e-05 [train_loss] ll_base 1.1136 ll 1.1207 loss -2.2343 (7.732 secs)\n",
      "banp:banp_periodic step 81800 lr 3.976e-05 [train_loss] ll_base 1.0184 ll 1.0233 loss -2.0417 (7.844 secs)\n",
      "banp:banp_periodic step 82000 lr 3.892e-05 [train_loss] ll_base 1.0560 ll 1.0632 loss -2.1192 (7.865 secs)\n",
      "banp:banp_periodic step 82200 lr 3.808e-05 [train_loss] ll_base 1.0693 ll 1.0775 loss -2.1468 (7.729 secs)\n",
      "banp:banp_periodic step 82400 lr 3.725e-05 [train_loss] ll_base 1.0601 ll 1.0691 loss -2.1292 (7.965 secs)\n",
      "banp:banp_periodic step 82600 lr 3.643e-05 [train_loss] ll_base 1.0730 ll 1.0795 loss -2.1525 (7.619 secs)\n",
      "banp:banp_periodic step 82800 lr 3.562e-05 [train_loss] ll_base 1.0976 ll 1.1037 loss -2.2013 (8.006 secs)\n",
      "banp:banp_periodic step 83000 lr 3.481e-05 [train_loss] ll_base 1.0628 ll 1.0702 loss -2.1331 (7.736 secs)\n",
      "banp:banp_periodic step 83200 lr 3.402e-05 [train_loss] ll_base 1.0604 ll 1.0691 loss -2.1295 (7.841 secs)\n",
      "banp:banp_periodic step 83400 lr 3.323e-05 [train_loss] ll_base 1.0657 ll 1.0727 loss -2.1384 (7.518 secs)\n",
      "banp:banp_periodic step 83600 lr 3.245e-05 [train_loss] ll_base 1.0651 ll 1.0729 loss -2.1380 (7.685 secs)\n",
      "banp:banp_periodic step 83800 lr 3.168e-05 [train_loss] ll_base 1.0262 ll 1.0331 loss -2.0593 (7.895 secs)\n",
      "banp:banp_periodic step 84000 lr 3.092e-05 [train_loss] ll_base 1.0370 ll 1.0460 loss -2.0830 (7.798 secs)\n",
      "banp:banp_periodic step 84200 lr 3.017e-05 [train_loss] ll_base 1.1143 ll 1.1203 loss -2.2347 (8.038 secs)\n",
      "banp:banp_periodic step 84400 lr 2.943e-05 [train_loss] ll_base 1.0416 ll 1.0496 loss -2.0912 (7.898 secs)\n",
      "banp:banp_periodic step 84600 lr 2.869e-05 [train_loss] ll_base 1.0088 ll 1.0150 loss -2.0238 (7.982 secs)\n",
      "banp:banp_periodic step 84800 lr 2.797e-05 [train_loss] ll_base 1.0993 ll 1.1066 loss -2.2059 (7.832 secs)\n",
      "banp:banp_periodic step 85000 lr 2.725e-05 [train_loss] ll_base 1.0351 ll 1.0416 loss -2.0767 (8.259 secs)\n",
      "100%|##########| 3000/3000 [01:27<00:00, 34.30it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.4629 tar_ll -3.0261 (87.458 secs)\n",
      "\n",
      "banp:banp_periodic step 85200 lr 2.654e-05 [train_loss] ll_base 1.0644 ll 1.0716 loss -2.1361 (10.495 secs)\n",
      "banp:banp_periodic step 85400 lr 2.584e-05 [train_loss] ll_base 1.0631 ll 1.0701 loss -2.1331 (8.765 secs)\n",
      "banp:banp_periodic step 85600 lr 2.515e-05 [train_loss] ll_base 0.9897 ll 0.9941 loss -1.9838 (7.959 secs)\n",
      "banp:banp_periodic step 85800 lr 2.447e-05 [train_loss] ll_base 1.0621 ll 1.0705 loss -2.1325 (7.982 secs)\n",
      "banp:banp_periodic step 86000 lr 2.379e-05 [train_loss] ll_base 1.0376 ll 1.0438 loss -2.0814 (7.760 secs)\n",
      "banp:banp_periodic step 86200 lr 2.313e-05 [train_loss] ll_base 1.0525 ll 1.0589 loss -2.1115 (7.823 secs)\n",
      "banp:banp_periodic step 86400 lr 2.247e-05 [train_loss] ll_base 1.0047 ll 1.0120 loss -2.0167 (7.806 secs)\n",
      "banp:banp_periodic step 86600 lr 2.183e-05 [train_loss] ll_base 1.0267 ll 1.0358 loss -2.0625 (7.888 secs)\n",
      "banp:banp_periodic step 86800 lr 2.119e-05 [train_loss] ll_base 1.0804 ll 1.0863 loss -2.1666 (7.658 secs)\n",
      "banp:banp_periodic step 87000 lr 2.056e-05 [train_loss] ll_base 1.0673 ll 1.0739 loss -2.1412 (7.876 secs)\n",
      "banp:banp_periodic step 87200 lr 1.994e-05 [train_loss] ll_base 1.1095 ll 1.1156 loss -2.2251 (7.736 secs)\n",
      "banp:banp_periodic step 87400 lr 1.933e-05 [train_loss] ll_base 1.0739 ll 1.0806 loss -2.1545 (7.860 secs)\n",
      "banp:banp_periodic step 87600 lr 1.873e-05 [train_loss] ll_base 1.0722 ll 1.0783 loss -2.1505 (7.753 secs)\n",
      "banp:banp_periodic step 87800 lr 1.814e-05 [train_loss] ll_base 1.0591 ll 1.0655 loss -2.1246 (8.204 secs)\n",
      "banp:banp_periodic step 88000 lr 1.756e-05 [train_loss] ll_base 1.0499 ll 1.0549 loss -2.1048 (8.038 secs)\n",
      "banp:banp_periodic step 88200 lr 1.698e-05 [train_loss] ll_base 1.1065 ll 1.1131 loss -2.2196 (8.040 secs)\n",
      "banp:banp_periodic step 88400 lr 1.642e-05 [train_loss] ll_base 1.0519 ll 1.0591 loss -2.1110 (7.991 secs)\n",
      "banp:banp_periodic step 88600 lr 1.586e-05 [train_loss] ll_base 1.0403 ll 1.0457 loss -2.0860 (7.999 secs)\n",
      "banp:banp_periodic step 88800 lr 1.532e-05 [train_loss] ll_base 1.0713 ll 1.0782 loss -2.1495 (7.881 secs)\n",
      "banp:banp_periodic step 89000 lr 1.478e-05 [train_loss] ll_base 1.0525 ll 1.0584 loss -2.1110 (8.120 secs)\n",
      "banp:banp_periodic step 89200 lr 1.425e-05 [train_loss] ll_base 1.0356 ll 1.0409 loss -2.0765 (7.896 secs)\n",
      "banp:banp_periodic step 89400 lr 1.373e-05 [train_loss] ll_base 1.0789 ll 1.0868 loss -2.1657 (8.005 secs)\n",
      "banp:banp_periodic step 89600 lr 1.323e-05 [train_loss] ll_base 0.9924 ll 1.0016 loss -1.9939 (8.067 secs)\n",
      "banp:banp_periodic step 89800 lr 1.273e-05 [train_loss] ll_base 1.0569 ll 1.0641 loss -2.1209 (8.066 secs)\n",
      "banp:banp_periodic step 90000 lr 1.224e-05 [train_loss] ll_base 1.0522 ll 1.0603 loss -2.1125 (7.756 secs)\n",
      "100%|##########| 3000/3000 [01:28<00:00, 33.85it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.4735 tar_ll -2.9837 (88.622 secs)\n",
      "\n",
      "banp:banp_periodic step 90200 lr 1.176e-05 [train_loss] ll_base 1.0820 ll 1.0894 loss -2.1714 (9.438 secs)\n",
      "banp:banp_periodic step 90400 lr 1.128e-05 [train_loss] ll_base 1.0785 ll 1.0851 loss -2.1636 (9.341 secs)\n",
      "banp:banp_periodic step 90600 lr 1.082e-05 [train_loss] ll_base 1.0844 ll 1.0952 loss -2.1796 (7.715 secs)\n",
      "banp:banp_periodic step 90800 lr 1.037e-05 [train_loss] ll_base 1.0203 ll 1.0295 loss -2.0498 (7.877 secs)\n",
      "banp:banp_periodic step 91000 lr 9.927e-06 [train_loss] ll_base 1.0624 ll 1.0730 loss -2.1354 (7.740 secs)\n",
      "banp:banp_periodic step 91200 lr 9.493e-06 [train_loss] ll_base 1.0396 ll 1.0499 loss -2.0896 (7.782 secs)\n",
      "banp:banp_periodic step 91400 lr 9.069e-06 [train_loss] ll_base 1.0940 ll 1.1002 loss -2.1942 (7.831 secs)\n",
      "banp:banp_periodic step 91600 lr 8.655e-06 [train_loss] ll_base 1.0983 ll 1.1079 loss -2.2063 (8.049 secs)\n",
      "banp:banp_periodic step 91800 lr 8.250e-06 [train_loss] ll_base 1.0358 ll 1.0453 loss -2.0811 (7.788 secs)\n",
      "banp:banp_periodic step 92000 lr 7.854e-06 [train_loss] ll_base 1.0609 ll 1.0693 loss -2.1302 (7.912 secs)\n",
      "banp:banp_periodic step 92200 lr 7.468e-06 [train_loss] ll_base 1.0477 ll 1.0553 loss -2.1031 (7.838 secs)\n",
      "banp:banp_periodic step 92400 lr 7.092e-06 [train_loss] ll_base 1.0071 ll 1.0138 loss -2.0209 (8.004 secs)\n",
      "banp:banp_periodic step 92600 lr 6.725e-06 [train_loss] ll_base 1.1015 ll 1.1067 loss -2.2082 (7.903 secs)\n",
      "banp:banp_periodic step 92800 lr 6.368e-06 [train_loss] ll_base 1.0298 ll 1.0377 loss -2.0675 (7.861 secs)\n",
      "banp:banp_periodic step 93000 lr 6.021e-06 [train_loss] ll_base 1.0050 ll 1.0113 loss -2.0163 (7.800 secs)\n",
      "banp:banp_periodic step 93200 lr 5.683e-06 [train_loss] ll_base 1.0441 ll 1.0529 loss -2.0970 (7.954 secs)\n",
      "banp:banp_periodic step 93400 lr 5.355e-06 [train_loss] ll_base 1.0327 ll 1.0401 loss -2.0728 (7.804 secs)\n",
      "banp:banp_periodic step 93600 lr 5.036e-06 [train_loss] ll_base 0.9931 ll 1.0022 loss -1.9953 (8.109 secs)\n",
      "banp:banp_periodic step 93800 lr 4.727e-06 [train_loss] ll_base 1.0429 ll 1.0519 loss -2.0948 (7.938 secs)\n",
      "banp:banp_periodic step 94000 lr 4.428e-06 [train_loss] ll_base 1.0734 ll 1.0824 loss -2.1558 (7.964 secs)\n",
      "banp:banp_periodic step 94200 lr 4.139e-06 [train_loss] ll_base 1.0357 ll 1.0439 loss -2.0796 (7.843 secs)\n",
      "banp:banp_periodic step 94400 lr 3.859e-06 [train_loss] ll_base 1.0479 ll 1.0556 loss -2.1036 (8.002 secs)\n",
      "banp:banp_periodic step 94600 lr 3.589e-06 [train_loss] ll_base 1.1070 ll 1.1122 loss -2.2191 (7.842 secs)\n",
      "banp:banp_periodic step 94800 lr 3.329e-06 [train_loss] ll_base 1.0538 ll 1.0618 loss -2.1156 (7.794 secs)\n",
      "banp:banp_periodic step 95000 lr 3.078e-06 [train_loss] ll_base 1.0377 ll 1.0427 loss -2.0804 (7.748 secs)\n",
      "100%|##########| 3000/3000 [01:24<00:00, 35.70it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.4928 tar_ll -2.8741 (84.048 secs)\n",
      "\n",
      "banp:banp_periodic step 95200 lr 2.837e-06 [train_loss] ll_base 1.0463 ll 1.0521 loss -2.0984 (8.576 secs)\n",
      "banp:banp_periodic step 95400 lr 2.606e-06 [train_loss] ll_base 1.0159 ll 1.0252 loss -2.0410 (9.289 secs)\n",
      "banp:banp_periodic step 95600 lr 2.385e-06 [train_loss] ll_base 0.9995 ll 1.0052 loss -2.0048 (8.491 secs)\n",
      "banp:banp_periodic step 95800 lr 2.173e-06 [train_loss] ll_base 1.0961 ll 1.1037 loss -2.1999 (8.167 secs)\n",
      "banp:banp_periodic step 96000 lr 1.971e-06 [train_loss] ll_base 1.0670 ll 1.0734 loss -2.1404 (7.919 secs)\n",
      "banp:banp_periodic step 96200 lr 1.779e-06 [train_loss] ll_base 1.0429 ll 1.0470 loss -2.0899 (8.023 secs)\n",
      "banp:banp_periodic step 96400 lr 1.597e-06 [train_loss] ll_base 1.0277 ll 1.0360 loss -2.0637 (7.861 secs)\n",
      "banp:banp_periodic step 96600 lr 1.425e-06 [train_loss] ll_base 1.0248 ll 1.0323 loss -2.0571 (8.006 secs)\n",
      "banp:banp_periodic step 96800 lr 1.262e-06 [train_loss] ll_base 1.0678 ll 1.0776 loss -2.1453 (7.979 secs)\n",
      "banp:banp_periodic step 97000 lr 1.110e-06 [train_loss] ll_base 1.0575 ll 1.0647 loss -2.1222 (8.150 secs)\n",
      "banp:banp_periodic step 97200 lr 9.666e-07 [train_loss] ll_base 1.1163 ll 1.1228 loss -2.2391 (7.751 secs)\n",
      "banp:banp_periodic step 97400 lr 8.335e-07 [train_loss] ll_base 1.0964 ll 1.1009 loss -2.1973 (8.002 secs)\n",
      "banp:banp_periodic step 97600 lr 7.103e-07 [train_loss] ll_base 1.0701 ll 1.0781 loss -2.1482 (7.992 secs)\n",
      "banp:banp_periodic step 97800 lr 5.969e-07 [train_loss] ll_base 1.0774 ll 1.0851 loss -2.1625 (8.083 secs)\n",
      "banp:banp_periodic step 98000 lr 4.933e-07 [train_loss] ll_base 1.0651 ll 1.0729 loss -2.1379 (7.761 secs)\n",
      "banp:banp_periodic step 98200 lr 3.996e-07 [train_loss] ll_base 1.0253 ll 1.0353 loss -2.0606 (7.806 secs)\n",
      "banp:banp_periodic step 98400 lr 3.158e-07 [train_loss] ll_base 1.0328 ll 1.0392 loss -2.0720 (7.905 secs)\n",
      "banp:banp_periodic step 98600 lr 2.418e-07 [train_loss] ll_base 1.0372 ll 1.0454 loss -2.0826 (7.897 secs)\n",
      "banp:banp_periodic step 98800 lr 1.776e-07 [train_loss] ll_base 1.0397 ll 1.0460 loss -2.0857 (7.753 secs)\n",
      "banp:banp_periodic step 99000 lr 1.234e-07 [train_loss] ll_base 1.0809 ll 1.0853 loss -2.1662 (8.113 secs)\n",
      "banp:banp_periodic step 99200 lr 7.895e-08 [train_loss] ll_base 1.0225 ll 1.0318 loss -2.0542 (7.896 secs)\n",
      "banp:banp_periodic step 99400 lr 4.441e-08 [train_loss] ll_base 1.0730 ll 1.0791 loss -2.1521 (7.938 secs)\n",
      "banp:banp_periodic step 99600 lr 1.974e-08 [train_loss] ll_base 1.0505 ll 1.0540 loss -2.1046 (7.700 secs)\n",
      "banp:banp_periodic step 99800 lr 4.935e-09 [train_loss] ll_base 1.0410 ll 1.0473 loss -2.0883 (7.766 secs)\n",
      "banp:banp_periodic step 100000 lr 0.000e+00 [train_loss] ll_base 1.0685 ll 1.0756 loss -2.1442 (7.622 secs)\n",
      "100%|##########| 3000/3000 [01:25<00:00, 35.12it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.4921 tar_ll -2.8969 (85.429 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [01:31<00:00, 32.72it/s]\n",
      "banp:banp_periodic periodic ctx_ll 0.4931 tar_ll -2.8761 (91.678 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 5823822.0 miliseconds\n",
      "Execution time: 5823.822 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 396.29150390625 MB\n",
      "Memory Usage Change: 380.04150390625 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='banp', name='banp_periodic',val_seed=100, val_l=None,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a45eba-4185-4ce4-8b38-9ec4d7fea9ce",
   "metadata": {},
   "source": [
    "## TNP-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16c82ce5-8dd2-4897-a05b-ca3a3b29a3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: tnpd-tnpd_periodic\n",
      "Total number of parameters: 222082\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tnpd:tnpd_periodic step 200 lr 5.000e-04 [train_loss] tar_ll -0.6914 loss 0.6914 (7.055 secs)\n",
      "tnpd:tnpd_periodic step 400 lr 5.000e-04 [train_loss] tar_ll -0.5342 loss 0.5342 (5.807 secs)\n",
      "tnpd:tnpd_periodic step 600 lr 5.000e-04 [train_loss] tar_ll -0.1258 loss 0.1258 (5.545 secs)\n",
      "tnpd:tnpd_periodic step 800 lr 4.999e-04 [train_loss] tar_ll 0.0361 loss -0.0361 (5.292 secs)\n",
      "tnpd:tnpd_periodic step 1000 lr 4.999e-04 [train_loss] tar_ll 0.2194 loss -0.2194 (5.215 secs)\n",
      "tnpd:tnpd_periodic step 1200 lr 4.998e-04 [train_loss] tar_ll 0.2343 loss -0.2343 (5.361 secs)\n",
      "tnpd:tnpd_periodic step 1400 lr 4.998e-04 [train_loss] tar_ll 0.3162 loss -0.3162 (5.362 secs)\n",
      "tnpd:tnpd_periodic step 1600 lr 4.997e-04 [train_loss] tar_ll 0.3714 loss -0.3714 (5.278 secs)\n",
      "tnpd:tnpd_periodic step 1800 lr 4.996e-04 [train_loss] tar_ll 0.5019 loss -0.5019 (5.323 secs)\n",
      "tnpd:tnpd_periodic step 2000 lr 4.995e-04 [train_loss] tar_ll 0.4925 loss -0.4925 (5.416 secs)\n",
      "tnpd:tnpd_periodic step 2200 lr 4.994e-04 [train_loss] tar_ll 0.6634 loss -0.6634 (5.228 secs)\n",
      "tnpd:tnpd_periodic step 2400 lr 4.993e-04 [train_loss] tar_ll 0.4563 loss -0.4563 (5.356 secs)\n",
      "tnpd:tnpd_periodic step 2600 lr 4.992e-04 [train_loss] tar_ll 0.6434 loss -0.6434 (5.198 secs)\n",
      "tnpd:tnpd_periodic step 2800 lr 4.990e-04 [train_loss] tar_ll 0.6656 loss -0.6656 (5.197 secs)\n",
      "tnpd:tnpd_periodic step 3000 lr 4.989e-04 [train_loss] tar_ll 0.7120 loss -0.7120 (5.402 secs)\n",
      "tnpd:tnpd_periodic step 3200 lr 4.987e-04 [train_loss] tar_ll 0.6648 loss -0.6648 (5.065 secs)\n",
      "tnpd:tnpd_periodic step 3400 lr 4.986e-04 [train_loss] tar_ll 0.7653 loss -0.7653 (5.213 secs)\n",
      "tnpd:tnpd_periodic step 3600 lr 4.984e-04 [train_loss] tar_ll 0.7227 loss -0.7227 (5.279 secs)\n",
      "tnpd:tnpd_periodic step 3800 lr 4.982e-04 [train_loss] tar_ll 0.7664 loss -0.7664 (5.313 secs)\n",
      "tnpd:tnpd_periodic step 4000 lr 4.980e-04 [train_loss] tar_ll 0.7995 loss -0.7995 (5.330 secs)\n",
      "tnpd:tnpd_periodic step 4200 lr 4.978e-04 [train_loss] tar_ll 0.8818 loss -0.8818 (5.320 secs)\n",
      "tnpd:tnpd_periodic step 4400 lr 4.976e-04 [train_loss] tar_ll 0.8484 loss -0.8484 (5.522 secs)\n",
      "tnpd:tnpd_periodic step 4600 lr 4.974e-04 [train_loss] tar_ll 0.8561 loss -0.8561 (5.279 secs)\n",
      "tnpd:tnpd_periodic step 4800 lr 4.972e-04 [train_loss] tar_ll 0.9573 loss -0.9573 (5.327 secs)\n",
      "tnpd:tnpd_periodic step 5000 lr 4.969e-04 [train_loss] tar_ll 0.9277 loss -0.9277 (5.395 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 177.96it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -2.2655 loss 2.2655 (16.860 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 5200 lr 4.967e-04 [train_loss] tar_ll 0.8831 loss -0.8831 (5.519 secs)\n",
      "tnpd:tnpd_periodic step 5400 lr 4.964e-04 [train_loss] tar_ll 0.8851 loss -0.8851 (5.380 secs)\n",
      "tnpd:tnpd_periodic step 5600 lr 4.961e-04 [train_loss] tar_ll 0.8788 loss -0.8788 (5.312 secs)\n",
      "tnpd:tnpd_periodic step 5800 lr 4.959e-04 [train_loss] tar_ll 0.8441 loss -0.8441 (5.378 secs)\n",
      "tnpd:tnpd_periodic step 6000 lr 4.956e-04 [train_loss] tar_ll 0.9244 loss -0.9244 (5.348 secs)\n",
      "tnpd:tnpd_periodic step 6200 lr 4.953e-04 [train_loss] tar_ll 0.8966 loss -0.8966 (5.443 secs)\n",
      "tnpd:tnpd_periodic step 6400 lr 4.950e-04 [train_loss] tar_ll 0.8460 loss -0.8460 (5.347 secs)\n",
      "tnpd:tnpd_periodic step 6600 lr 4.946e-04 [train_loss] tar_ll 0.9460 loss -0.9460 (5.487 secs)\n",
      "tnpd:tnpd_periodic step 6800 lr 4.943e-04 [train_loss] tar_ll 0.9302 loss -0.9302 (5.456 secs)\n",
      "tnpd:tnpd_periodic step 7000 lr 4.940e-04 [train_loss] tar_ll 1.0315 loss -1.0315 (5.150 secs)\n",
      "tnpd:tnpd_periodic step 7200 lr 4.936e-04 [train_loss] tar_ll 0.9347 loss -0.9347 (5.597 secs)\n",
      "tnpd:tnpd_periodic step 7400 lr 4.933e-04 [train_loss] tar_ll 0.9449 loss -0.9449 (5.185 secs)\n",
      "tnpd:tnpd_periodic step 7600 lr 4.929e-04 [train_loss] tar_ll 0.9616 loss -0.9616 (5.346 secs)\n",
      "tnpd:tnpd_periodic step 7800 lr 4.925e-04 [train_loss] tar_ll 1.0003 loss -1.0003 (5.382 secs)\n",
      "tnpd:tnpd_periodic step 8000 lr 4.921e-04 [train_loss] tar_ll 0.9104 loss -0.9104 (5.178 secs)\n",
      "tnpd:tnpd_periodic step 8200 lr 4.918e-04 [train_loss] tar_ll 0.9479 loss -0.9479 (5.339 secs)\n",
      "tnpd:tnpd_periodic step 8400 lr 4.913e-04 [train_loss] tar_ll 0.8650 loss -0.8650 (5.244 secs)\n",
      "tnpd:tnpd_periodic step 8600 lr 4.909e-04 [train_loss] tar_ll 0.9851 loss -0.9851 (5.265 secs)\n",
      "tnpd:tnpd_periodic step 8800 lr 4.905e-04 [train_loss] tar_ll 1.0166 loss -1.0166 (5.149 secs)\n",
      "tnpd:tnpd_periodic step 9000 lr 4.901e-04 [train_loss] tar_ll 1.0494 loss -1.0494 (5.416 secs)\n",
      "tnpd:tnpd_periodic step 9200 lr 4.896e-04 [train_loss] tar_ll 0.9996 loss -0.9996 (5.440 secs)\n",
      "tnpd:tnpd_periodic step 9400 lr 4.892e-04 [train_loss] tar_ll 1.0412 loss -1.0412 (5.183 secs)\n",
      "tnpd:tnpd_periodic step 9600 lr 4.887e-04 [train_loss] tar_ll 0.8827 loss -0.8827 (5.421 secs)\n",
      "tnpd:tnpd_periodic step 9800 lr 4.882e-04 [train_loss] tar_ll 0.9618 loss -0.9618 (5.202 secs)\n",
      "tnpd:tnpd_periodic step 10000 lr 4.878e-04 [train_loss] tar_ll 0.9544 loss -0.9544 (5.205 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 181.21it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -2.1545 loss 2.1545 (16.559 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 10200 lr 4.873e-04 [train_loss] tar_ll 0.9770 loss -0.9770 (5.271 secs)\n",
      "tnpd:tnpd_periodic step 10400 lr 4.868e-04 [train_loss] tar_ll 0.9639 loss -0.9639 (5.110 secs)\n",
      "tnpd:tnpd_periodic step 10600 lr 4.863e-04 [train_loss] tar_ll 0.8819 loss -0.8819 (5.037 secs)\n",
      "tnpd:tnpd_periodic step 10800 lr 4.857e-04 [train_loss] tar_ll 1.0599 loss -1.0599 (5.258 secs)\n",
      "tnpd:tnpd_periodic step 11000 lr 4.852e-04 [train_loss] tar_ll 1.0010 loss -1.0010 (5.186 secs)\n",
      "tnpd:tnpd_periodic step 11200 lr 4.847e-04 [train_loss] tar_ll 1.0166 loss -1.0166 (5.119 secs)\n",
      "tnpd:tnpd_periodic step 11400 lr 4.841e-04 [train_loss] tar_ll 1.0110 loss -1.0110 (5.235 secs)\n",
      "tnpd:tnpd_periodic step 11600 lr 4.836e-04 [train_loss] tar_ll 1.0005 loss -1.0005 (5.103 secs)\n",
      "tnpd:tnpd_periodic step 11800 lr 4.830e-04 [train_loss] tar_ll 1.0225 loss -1.0225 (5.384 secs)\n",
      "tnpd:tnpd_periodic step 12000 lr 4.824e-04 [train_loss] tar_ll 1.0231 loss -1.0231 (5.448 secs)\n",
      "tnpd:tnpd_periodic step 12200 lr 4.819e-04 [train_loss] tar_ll 1.0356 loss -1.0356 (5.396 secs)\n",
      "tnpd:tnpd_periodic step 12400 lr 4.813e-04 [train_loss] tar_ll 1.1210 loss -1.1210 (5.263 secs)\n",
      "tnpd:tnpd_periodic step 12600 lr 4.807e-04 [train_loss] tar_ll 0.9909 loss -0.9909 (5.443 secs)\n",
      "tnpd:tnpd_periodic step 12800 lr 4.801e-04 [train_loss] tar_ll 1.0517 loss -1.0517 (5.365 secs)\n",
      "tnpd:tnpd_periodic step 13000 lr 4.794e-04 [train_loss] tar_ll 0.9984 loss -0.9984 (5.489 secs)\n",
      "tnpd:tnpd_periodic step 13200 lr 4.788e-04 [train_loss] tar_ll 1.0224 loss -1.0224 (5.579 secs)\n",
      "tnpd:tnpd_periodic step 13400 lr 4.782e-04 [train_loss] tar_ll 1.0516 loss -1.0516 (5.354 secs)\n",
      "tnpd:tnpd_periodic step 13600 lr 4.775e-04 [train_loss] tar_ll 1.0689 loss -1.0689 (5.497 secs)\n",
      "tnpd:tnpd_periodic step 13800 lr 4.769e-04 [train_loss] tar_ll 1.1049 loss -1.1049 (5.454 secs)\n",
      "tnpd:tnpd_periodic step 14000 lr 4.762e-04 [train_loss] tar_ll 1.0257 loss -1.0257 (5.539 secs)\n",
      "tnpd:tnpd_periodic step 14200 lr 4.755e-04 [train_loss] tar_ll 1.0897 loss -1.0897 (5.478 secs)\n",
      "tnpd:tnpd_periodic step 14400 lr 4.749e-04 [train_loss] tar_ll 0.9216 loss -0.9216 (5.646 secs)\n",
      "tnpd:tnpd_periodic step 14600 lr 4.742e-04 [train_loss] tar_ll 1.0652 loss -1.0652 (5.425 secs)\n",
      "tnpd:tnpd_periodic step 14800 lr 4.735e-04 [train_loss] tar_ll 1.0209 loss -1.0209 (5.631 secs)\n",
      "tnpd:tnpd_periodic step 15000 lr 4.728e-04 [train_loss] tar_ll 1.1256 loss -1.1256 (5.607 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 179.57it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -2.4931 loss 2.4931 (16.709 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 15200 lr 4.720e-04 [train_loss] tar_ll 1.0235 loss -1.0235 (5.198 secs)\n",
      "tnpd:tnpd_periodic step 15400 lr 4.713e-04 [train_loss] tar_ll 1.0074 loss -1.0074 (5.203 secs)\n",
      "tnpd:tnpd_periodic step 15600 lr 4.706e-04 [train_loss] tar_ll 1.1061 loss -1.1061 (5.174 secs)\n",
      "tnpd:tnpd_periodic step 15800 lr 4.698e-04 [train_loss] tar_ll 0.9537 loss -0.9537 (5.164 secs)\n",
      "tnpd:tnpd_periodic step 16000 lr 4.691e-04 [train_loss] tar_ll 1.0590 loss -1.0590 (5.193 secs)\n",
      "tnpd:tnpd_periodic step 16200 lr 4.683e-04 [train_loss] tar_ll 0.9462 loss -0.9462 (5.190 secs)\n",
      "tnpd:tnpd_periodic step 16400 lr 4.675e-04 [train_loss] tar_ll 1.0303 loss -1.0303 (5.075 secs)\n",
      "tnpd:tnpd_periodic step 16600 lr 4.668e-04 [train_loss] tar_ll 0.9103 loss -0.9103 (5.216 secs)\n",
      "tnpd:tnpd_periodic step 16800 lr 4.660e-04 [train_loss] tar_ll 0.9838 loss -0.9838 (5.278 secs)\n",
      "tnpd:tnpd_periodic step 17000 lr 4.652e-04 [train_loss] tar_ll 0.9842 loss -0.9842 (5.188 secs)\n",
      "tnpd:tnpd_periodic step 17200 lr 4.644e-04 [train_loss] tar_ll 1.0506 loss -1.0506 (5.122 secs)\n",
      "tnpd:tnpd_periodic step 17400 lr 4.636e-04 [train_loss] tar_ll 1.0729 loss -1.0729 (5.398 secs)\n",
      "tnpd:tnpd_periodic step 17600 lr 4.627e-04 [train_loss] tar_ll 1.0876 loss -1.0876 (5.230 secs)\n",
      "tnpd:tnpd_periodic step 17800 lr 4.619e-04 [train_loss] tar_ll 1.0912 loss -1.0912 (5.019 secs)\n",
      "tnpd:tnpd_periodic step 18000 lr 4.611e-04 [train_loss] tar_ll 1.1800 loss -1.1800 (5.183 secs)\n",
      "tnpd:tnpd_periodic step 18200 lr 4.602e-04 [train_loss] tar_ll 1.0533 loss -1.0533 (4.992 secs)\n",
      "tnpd:tnpd_periodic step 18400 lr 4.594e-04 [train_loss] tar_ll 1.0370 loss -1.0370 (4.987 secs)\n",
      "tnpd:tnpd_periodic step 18600 lr 4.585e-04 [train_loss] tar_ll 0.9910 loss -0.9910 (5.049 secs)\n",
      "tnpd:tnpd_periodic step 18800 lr 4.576e-04 [train_loss] tar_ll 1.1832 loss -1.1832 (5.070 secs)\n",
      "tnpd:tnpd_periodic step 19000 lr 4.568e-04 [train_loss] tar_ll 1.1479 loss -1.1479 (4.974 secs)\n",
      "tnpd:tnpd_periodic step 19200 lr 4.559e-04 [train_loss] tar_ll 1.1330 loss -1.1330 (5.284 secs)\n",
      "tnpd:tnpd_periodic step 19400 lr 4.550e-04 [train_loss] tar_ll 1.1056 loss -1.1056 (5.128 secs)\n",
      "tnpd:tnpd_periodic step 19600 lr 4.541e-04 [train_loss] tar_ll 1.1143 loss -1.1143 (5.201 secs)\n",
      "tnpd:tnpd_periodic step 19800 lr 4.532e-04 [train_loss] tar_ll 1.1231 loss -1.1231 (5.310 secs)\n",
      "tnpd:tnpd_periodic step 20000 lr 4.523e-04 [train_loss] tar_ll 1.1589 loss -1.1589 (5.045 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 179.01it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -3.3486 loss 3.3486 (16.761 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 20200 lr 4.513e-04 [train_loss] tar_ll 1.1194 loss -1.1194 (5.028 secs)\n",
      "tnpd:tnpd_periodic step 20400 lr 4.504e-04 [train_loss] tar_ll 1.1092 loss -1.1092 (5.219 secs)\n",
      "tnpd:tnpd_periodic step 20600 lr 4.494e-04 [train_loss] tar_ll 1.0845 loss -1.0845 (5.116 secs)\n",
      "tnpd:tnpd_periodic step 20800 lr 4.485e-04 [train_loss] tar_ll 1.1291 loss -1.1291 (5.117 secs)\n",
      "tnpd:tnpd_periodic step 21000 lr 4.475e-04 [train_loss] tar_ll 1.1598 loss -1.1598 (5.253 secs)\n",
      "tnpd:tnpd_periodic step 21200 lr 4.466e-04 [train_loss] tar_ll 1.1208 loss -1.1208 (5.093 secs)\n",
      "tnpd:tnpd_periodic step 21400 lr 4.456e-04 [train_loss] tar_ll 1.1315 loss -1.1315 (5.170 secs)\n",
      "tnpd:tnpd_periodic step 21600 lr 4.446e-04 [train_loss] tar_ll 1.1345 loss -1.1345 (5.369 secs)\n",
      "tnpd:tnpd_periodic step 21800 lr 4.436e-04 [train_loss] tar_ll 1.0889 loss -1.0889 (5.206 secs)\n",
      "tnpd:tnpd_periodic step 22000 lr 4.426e-04 [train_loss] tar_ll 1.1426 loss -1.1426 (5.162 secs)\n",
      "tnpd:tnpd_periodic step 22200 lr 4.416e-04 [train_loss] tar_ll 1.1648 loss -1.1648 (5.418 secs)\n",
      "tnpd:tnpd_periodic step 22400 lr 4.406e-04 [train_loss] tar_ll 1.1578 loss -1.1578 (5.244 secs)\n",
      "tnpd:tnpd_periodic step 22600 lr 4.396e-04 [train_loss] tar_ll 1.1052 loss -1.1052 (5.192 secs)\n",
      "tnpd:tnpd_periodic step 22800 lr 4.386e-04 [train_loss] tar_ll 1.1611 loss -1.1611 (5.291 secs)\n",
      "tnpd:tnpd_periodic step 23000 lr 4.375e-04 [train_loss] tar_ll 1.1007 loss -1.1007 (5.135 secs)\n",
      "tnpd:tnpd_periodic step 23200 lr 4.365e-04 [train_loss] tar_ll 1.0953 loss -1.0953 (5.150 secs)\n",
      "tnpd:tnpd_periodic step 23400 lr 4.354e-04 [train_loss] tar_ll 1.1944 loss -1.1944 (5.423 secs)\n",
      "tnpd:tnpd_periodic step 23600 lr 4.344e-04 [train_loss] tar_ll 1.1393 loss -1.1393 (5.754 secs)\n",
      "tnpd:tnpd_periodic step 23800 lr 4.333e-04 [train_loss] tar_ll 1.0561 loss -1.0561 (5.387 secs)\n",
      "tnpd:tnpd_periodic step 24000 lr 4.322e-04 [train_loss] tar_ll 1.2190 loss -1.2190 (5.296 secs)\n",
      "tnpd:tnpd_periodic step 24200 lr 4.312e-04 [train_loss] tar_ll 1.0719 loss -1.0719 (5.248 secs)\n",
      "tnpd:tnpd_periodic step 24400 lr 4.301e-04 [train_loss] tar_ll 1.0857 loss -1.0857 (5.135 secs)\n",
      "tnpd:tnpd_periodic step 24600 lr 4.290e-04 [train_loss] tar_ll 1.2424 loss -1.2424 (5.307 secs)\n",
      "tnpd:tnpd_periodic step 24800 lr 4.279e-04 [train_loss] tar_ll 1.1058 loss -1.1058 (5.166 secs)\n",
      "tnpd:tnpd_periodic step 25000 lr 4.268e-04 [train_loss] tar_ll 1.1900 loss -1.1900 (5.049 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 180.04it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -2.9135 loss 2.9135 (16.667 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 25200 lr 4.257e-04 [train_loss] tar_ll 1.0533 loss -1.0533 (5.161 secs)\n",
      "tnpd:tnpd_periodic step 25400 lr 4.245e-04 [train_loss] tar_ll 1.1109 loss -1.1109 (5.094 secs)\n",
      "tnpd:tnpd_periodic step 25600 lr 4.234e-04 [train_loss] tar_ll 1.1602 loss -1.1602 (5.526 secs)\n",
      "tnpd:tnpd_periodic step 25800 lr 4.223e-04 [train_loss] tar_ll 1.1719 loss -1.1719 (9.838 secs)\n",
      "tnpd:tnpd_periodic step 26000 lr 4.211e-04 [train_loss] tar_ll 1.2173 loss -1.2173 (8.877 secs)\n",
      "tnpd:tnpd_periodic step 26200 lr 4.200e-04 [train_loss] tar_ll 1.2606 loss -1.2606 (7.652 secs)\n",
      "tnpd:tnpd_periodic step 26400 lr 4.188e-04 [train_loss] tar_ll 1.1617 loss -1.1617 (7.146 secs)\n",
      "tnpd:tnpd_periodic step 26600 lr 4.177e-04 [train_loss] tar_ll 1.1161 loss -1.1161 (7.769 secs)\n",
      "tnpd:tnpd_periodic step 26800 lr 4.165e-04 [train_loss] tar_ll 1.0809 loss -1.0809 (6.058 secs)\n",
      "tnpd:tnpd_periodic step 27000 lr 4.153e-04 [train_loss] tar_ll 1.1045 loss -1.1045 (5.496 secs)\n",
      "tnpd:tnpd_periodic step 27200 lr 4.141e-04 [train_loss] tar_ll 1.1466 loss -1.1466 (5.286 secs)\n",
      "tnpd:tnpd_periodic step 27400 lr 4.130e-04 [train_loss] tar_ll 1.1699 loss -1.1699 (5.308 secs)\n",
      "tnpd:tnpd_periodic step 27600 lr 4.118e-04 [train_loss] tar_ll 1.0956 loss -1.0956 (5.507 secs)\n",
      "tnpd:tnpd_periodic step 27800 lr 4.106e-04 [train_loss] tar_ll 1.0739 loss -1.0739 (5.306 secs)\n",
      "tnpd:tnpd_periodic step 28000 lr 4.094e-04 [train_loss] tar_ll 1.1216 loss -1.1216 (5.453 secs)\n",
      "tnpd:tnpd_periodic step 28200 lr 4.081e-04 [train_loss] tar_ll 1.1194 loss -1.1194 (5.557 secs)\n",
      "tnpd:tnpd_periodic step 28400 lr 4.069e-04 [train_loss] tar_ll 1.1474 loss -1.1474 (5.419 secs)\n",
      "tnpd:tnpd_periodic step 28600 lr 4.057e-04 [train_loss] tar_ll 1.1129 loss -1.1129 (5.503 secs)\n",
      "tnpd:tnpd_periodic step 28800 lr 4.045e-04 [train_loss] tar_ll 1.1618 loss -1.1618 (5.580 secs)\n",
      "tnpd:tnpd_periodic step 29000 lr 4.032e-04 [train_loss] tar_ll 1.2662 loss -1.2662 (5.437 secs)\n",
      "tnpd:tnpd_periodic step 29200 lr 4.020e-04 [train_loss] tar_ll 1.1460 loss -1.1460 (5.396 secs)\n",
      "tnpd:tnpd_periodic step 29400 lr 4.007e-04 [train_loss] tar_ll 1.1873 loss -1.1873 (5.339 secs)\n",
      "tnpd:tnpd_periodic step 29600 lr 3.995e-04 [train_loss] tar_ll 1.2082 loss -1.2082 (5.157 secs)\n",
      "tnpd:tnpd_periodic step 29800 lr 3.982e-04 [train_loss] tar_ll 1.2613 loss -1.2613 (5.319 secs)\n",
      "tnpd:tnpd_periodic step 30000 lr 3.969e-04 [train_loss] tar_ll 1.2254 loss -1.2254 (5.430 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 181.92it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -3.6742 loss 3.6742 (16.493 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 30200 lr 3.957e-04 [train_loss] tar_ll 1.2016 loss -1.2016 (5.526 secs)\n",
      "tnpd:tnpd_periodic step 30400 lr 3.944e-04 [train_loss] tar_ll 1.1954 loss -1.1954 (5.695 secs)\n",
      "tnpd:tnpd_periodic step 30600 lr 3.931e-04 [train_loss] tar_ll 1.1807 loss -1.1807 (5.291 secs)\n",
      "tnpd:tnpd_periodic step 30800 lr 3.918e-04 [train_loss] tar_ll 0.9972 loss -0.9972 (5.412 secs)\n",
      "tnpd:tnpd_periodic step 31000 lr 3.905e-04 [train_loss] tar_ll 0.9406 loss -0.9406 (5.194 secs)\n",
      "tnpd:tnpd_periodic step 31200 lr 3.892e-04 [train_loss] tar_ll 1.0439 loss -1.0439 (5.434 secs)\n",
      "tnpd:tnpd_periodic step 31400 lr 3.879e-04 [train_loss] tar_ll 1.1127 loss -1.1127 (5.217 secs)\n",
      "tnpd:tnpd_periodic step 31600 lr 3.866e-04 [train_loss] tar_ll 1.1156 loss -1.1156 (5.332 secs)\n",
      "tnpd:tnpd_periodic step 31800 lr 3.853e-04 [train_loss] tar_ll 1.0095 loss -1.0095 (5.202 secs)\n",
      "tnpd:tnpd_periodic step 32000 lr 3.840e-04 [train_loss] tar_ll 1.0956 loss -1.0956 (5.329 secs)\n",
      "tnpd:tnpd_periodic step 32200 lr 3.826e-04 [train_loss] tar_ll 1.2366 loss -1.2366 (5.337 secs)\n",
      "tnpd:tnpd_periodic step 32400 lr 3.813e-04 [train_loss] tar_ll 1.0914 loss -1.0914 (5.191 secs)\n",
      "tnpd:tnpd_periodic step 32600 lr 3.800e-04 [train_loss] tar_ll 1.0965 loss -1.0965 (5.361 secs)\n",
      "tnpd:tnpd_periodic step 32800 lr 3.786e-04 [train_loss] tar_ll 1.2253 loss -1.2253 (5.205 secs)\n",
      "tnpd:tnpd_periodic step 33000 lr 3.773e-04 [train_loss] tar_ll 1.2487 loss -1.2487 (5.215 secs)\n",
      "tnpd:tnpd_periodic step 33200 lr 3.759e-04 [train_loss] tar_ll 1.1907 loss -1.1907 (5.249 secs)\n",
      "tnpd:tnpd_periodic step 33400 lr 3.745e-04 [train_loss] tar_ll 1.2280 loss -1.2280 (5.256 secs)\n",
      "tnpd:tnpd_periodic step 33600 lr 3.732e-04 [train_loss] tar_ll 1.2086 loss -1.2086 (5.212 secs)\n",
      "tnpd:tnpd_periodic step 33800 lr 3.718e-04 [train_loss] tar_ll 1.1491 loss -1.1491 (5.098 secs)\n",
      "tnpd:tnpd_periodic step 34000 lr 3.704e-04 [train_loss] tar_ll 1.2356 loss -1.2356 (5.238 secs)\n",
      "tnpd:tnpd_periodic step 34200 lr 3.691e-04 [train_loss] tar_ll 1.1159 loss -1.1159 (5.250 secs)\n",
      "tnpd:tnpd_periodic step 34400 lr 3.677e-04 [train_loss] tar_ll 1.2293 loss -1.2293 (5.144 secs)\n",
      "tnpd:tnpd_periodic step 34600 lr 3.663e-04 [train_loss] tar_ll 1.2459 loss -1.2459 (5.426 secs)\n",
      "tnpd:tnpd_periodic step 34800 lr 3.649e-04 [train_loss] tar_ll 1.1799 loss -1.1799 (5.354 secs)\n",
      "tnpd:tnpd_periodic step 35000 lr 3.635e-04 [train_loss] tar_ll 1.2045 loss -1.2045 (5.228 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 184.94it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -3.0528 loss 3.0528 (16.224 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 35200 lr 3.621e-04 [train_loss] tar_ll 1.1698 loss -1.1698 (5.592 secs)\n",
      "tnpd:tnpd_periodic step 35400 lr 3.607e-04 [train_loss] tar_ll 1.1813 loss -1.1813 (5.433 secs)\n",
      "tnpd:tnpd_periodic step 35600 lr 3.593e-04 [train_loss] tar_ll 1.2275 loss -1.2275 (5.222 secs)\n",
      "tnpd:tnpd_periodic step 35800 lr 3.579e-04 [train_loss] tar_ll 1.2046 loss -1.2046 (5.286 secs)\n",
      "tnpd:tnpd_periodic step 36000 lr 3.564e-04 [train_loss] tar_ll 1.1243 loss -1.1243 (5.420 secs)\n",
      "tnpd:tnpd_periodic step 36200 lr 3.550e-04 [train_loss] tar_ll 1.2509 loss -1.2509 (5.442 secs)\n",
      "tnpd:tnpd_periodic step 36400 lr 3.536e-04 [train_loss] tar_ll 1.1825 loss -1.1825 (5.468 secs)\n",
      "tnpd:tnpd_periodic step 36600 lr 3.522e-04 [train_loss] tar_ll 1.2373 loss -1.2373 (5.503 secs)\n",
      "tnpd:tnpd_periodic step 36800 lr 3.507e-04 [train_loss] tar_ll 1.1730 loss -1.1730 (5.331 secs)\n",
      "tnpd:tnpd_periodic step 37000 lr 3.493e-04 [train_loss] tar_ll 1.2349 loss -1.2349 (5.413 secs)\n",
      "tnpd:tnpd_periodic step 37200 lr 3.478e-04 [train_loss] tar_ll 1.1918 loss -1.1918 (5.405 secs)\n",
      "tnpd:tnpd_periodic step 37400 lr 3.464e-04 [train_loss] tar_ll 1.1243 loss -1.1243 (5.284 secs)\n",
      "tnpd:tnpd_periodic step 37600 lr 3.449e-04 [train_loss] tar_ll 1.2615 loss -1.2615 (5.464 secs)\n",
      "tnpd:tnpd_periodic step 37800 lr 3.435e-04 [train_loss] tar_ll 1.2202 loss -1.2202 (5.459 secs)\n",
      "tnpd:tnpd_periodic step 38000 lr 3.420e-04 [train_loss] tar_ll 1.2085 loss -1.2085 (5.272 secs)\n",
      "tnpd:tnpd_periodic step 38200 lr 3.406e-04 [train_loss] tar_ll 1.1511 loss -1.1511 (5.572 secs)\n",
      "tnpd:tnpd_periodic step 38400 lr 3.391e-04 [train_loss] tar_ll 1.2412 loss -1.2412 (5.210 secs)\n",
      "tnpd:tnpd_periodic step 38600 lr 3.376e-04 [train_loss] tar_ll 1.1414 loss -1.1414 (5.279 secs)\n",
      "tnpd:tnpd_periodic step 38800 lr 3.362e-04 [train_loss] tar_ll 1.1840 loss -1.1840 (5.365 secs)\n",
      "tnpd:tnpd_periodic step 39000 lr 3.347e-04 [train_loss] tar_ll 1.2094 loss -1.2094 (5.207 secs)\n",
      "tnpd:tnpd_periodic step 39200 lr 3.332e-04 [train_loss] tar_ll 1.2194 loss -1.2194 (5.210 secs)\n",
      "tnpd:tnpd_periodic step 39400 lr 3.317e-04 [train_loss] tar_ll 1.1795 loss -1.1795 (5.263 secs)\n",
      "tnpd:tnpd_periodic step 39600 lr 3.302e-04 [train_loss] tar_ll 1.2859 loss -1.2859 (5.382 secs)\n",
      "tnpd:tnpd_periodic step 39800 lr 3.287e-04 [train_loss] tar_ll 1.2139 loss -1.2139 (5.194 secs)\n",
      "tnpd:tnpd_periodic step 40000 lr 3.273e-04 [train_loss] tar_ll 1.2059 loss -1.2059 (5.312 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 181.44it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -2.7074 loss 2.7074 (16.540 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 40200 lr 3.258e-04 [train_loss] tar_ll 1.2611 loss -1.2611 (5.425 secs)\n",
      "tnpd:tnpd_periodic step 40400 lr 3.243e-04 [train_loss] tar_ll 1.3077 loss -1.3077 (5.352 secs)\n",
      "tnpd:tnpd_periodic step 40600 lr 3.228e-04 [train_loss] tar_ll 1.2034 loss -1.2034 (5.294 secs)\n",
      "tnpd:tnpd_periodic step 40800 lr 3.213e-04 [train_loss] tar_ll 1.2618 loss -1.2618 (5.163 secs)\n",
      "tnpd:tnpd_periodic step 41000 lr 3.197e-04 [train_loss] tar_ll 1.3237 loss -1.3237 (5.314 secs)\n",
      "tnpd:tnpd_periodic step 41200 lr 3.182e-04 [train_loss] tar_ll 1.1636 loss -1.1636 (5.326 secs)\n",
      "tnpd:tnpd_periodic step 41400 lr 3.167e-04 [train_loss] tar_ll 1.2813 loss -1.2813 (5.043 secs)\n",
      "tnpd:tnpd_periodic step 41600 lr 3.152e-04 [train_loss] tar_ll 1.2711 loss -1.2711 (5.175 secs)\n",
      "tnpd:tnpd_periodic step 41800 lr 3.137e-04 [train_loss] tar_ll 1.2717 loss -1.2717 (5.280 secs)\n",
      "tnpd:tnpd_periodic step 42000 lr 3.122e-04 [train_loss] tar_ll 1.1855 loss -1.1855 (5.139 secs)\n",
      "tnpd:tnpd_periodic step 42200 lr 3.106e-04 [train_loss] tar_ll 1.3444 loss -1.3444 (5.265 secs)\n",
      "tnpd:tnpd_periodic step 42400 lr 3.091e-04 [train_loss] tar_ll 1.1827 loss -1.1827 (5.364 secs)\n",
      "tnpd:tnpd_periodic step 42600 lr 3.076e-04 [train_loss] tar_ll 1.2620 loss -1.2620 (5.170 secs)\n",
      "tnpd:tnpd_periodic step 42800 lr 3.061e-04 [train_loss] tar_ll 1.2729 loss -1.2729 (5.171 secs)\n",
      "tnpd:tnpd_periodic step 43000 lr 3.045e-04 [train_loss] tar_ll 1.2498 loss -1.2498 (5.151 secs)\n",
      "tnpd:tnpd_periodic step 43200 lr 3.030e-04 [train_loss] tar_ll 1.2478 loss -1.2478 (5.463 secs)\n",
      "tnpd:tnpd_periodic step 43400 lr 3.015e-04 [train_loss] tar_ll 1.1832 loss -1.1832 (5.204 secs)\n",
      "tnpd:tnpd_periodic step 43600 lr 2.999e-04 [train_loss] tar_ll 1.1872 loss -1.1872 (5.445 secs)\n",
      "tnpd:tnpd_periodic step 43800 lr 2.984e-04 [train_loss] tar_ll 1.1803 loss -1.1803 (5.229 secs)\n",
      "tnpd:tnpd_periodic step 44000 lr 2.968e-04 [train_loss] tar_ll 1.2412 loss -1.2412 (5.165 secs)\n",
      "tnpd:tnpd_periodic step 44200 lr 2.953e-04 [train_loss] tar_ll 1.1769 loss -1.1769 (5.431 secs)\n",
      "tnpd:tnpd_periodic step 44400 lr 2.938e-04 [train_loss] tar_ll 1.2666 loss -1.2666 (5.176 secs)\n",
      "tnpd:tnpd_periodic step 44600 lr 2.922e-04 [train_loss] tar_ll 1.2587 loss -1.2587 (5.277 secs)\n",
      "tnpd:tnpd_periodic step 44800 lr 2.907e-04 [train_loss] tar_ll 1.2446 loss -1.2446 (5.444 secs)\n",
      "tnpd:tnpd_periodic step 45000 lr 2.891e-04 [train_loss] tar_ll 1.2654 loss -1.2654 (5.492 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 183.12it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -2.5520 loss 2.5520 (16.385 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 45200 lr 2.876e-04 [train_loss] tar_ll 1.2453 loss -1.2453 (5.251 secs)\n",
      "tnpd:tnpd_periodic step 45400 lr 2.860e-04 [train_loss] tar_ll 1.2942 loss -1.2942 (5.421 secs)\n",
      "tnpd:tnpd_periodic step 45600 lr 2.844e-04 [train_loss] tar_ll 1.2138 loss -1.2138 (5.215 secs)\n",
      "tnpd:tnpd_periodic step 45800 lr 2.829e-04 [train_loss] tar_ll 1.2176 loss -1.2176 (5.307 secs)\n",
      "tnpd:tnpd_periodic step 46000 lr 2.813e-04 [train_loss] tar_ll 1.3101 loss -1.3101 (5.419 secs)\n",
      "tnpd:tnpd_periodic step 46200 lr 2.798e-04 [train_loss] tar_ll 1.1684 loss -1.1684 (5.329 secs)\n",
      "tnpd:tnpd_periodic step 46400 lr 2.782e-04 [train_loss] tar_ll 1.2629 loss -1.2629 (5.388 secs)\n",
      "tnpd:tnpd_periodic step 46600 lr 2.767e-04 [train_loss] tar_ll 1.2489 loss -1.2489 (5.270 secs)\n",
      "tnpd:tnpd_periodic step 46800 lr 2.751e-04 [train_loss] tar_ll 1.2301 loss -1.2301 (5.382 secs)\n",
      "tnpd:tnpd_periodic step 47000 lr 2.735e-04 [train_loss] tar_ll 1.2572 loss -1.2572 (5.201 secs)\n",
      "tnpd:tnpd_periodic step 47200 lr 2.720e-04 [train_loss] tar_ll 1.2459 loss -1.2459 (5.303 secs)\n",
      "tnpd:tnpd_periodic step 47400 lr 2.704e-04 [train_loss] tar_ll 1.2227 loss -1.2227 (5.191 secs)\n",
      "tnpd:tnpd_periodic step 47600 lr 2.688e-04 [train_loss] tar_ll 1.2574 loss -1.2574 (5.140 secs)\n",
      "tnpd:tnpd_periodic step 47800 lr 2.673e-04 [train_loss] tar_ll 1.3061 loss -1.3061 (5.453 secs)\n",
      "tnpd:tnpd_periodic step 48000 lr 2.657e-04 [train_loss] tar_ll 1.2920 loss -1.2920 (5.289 secs)\n",
      "tnpd:tnpd_periodic step 48200 lr 2.641e-04 [train_loss] tar_ll 1.3424 loss -1.3424 (5.207 secs)\n",
      "tnpd:tnpd_periodic step 48400 lr 2.626e-04 [train_loss] tar_ll 1.2474 loss -1.2474 (5.345 secs)\n",
      "tnpd:tnpd_periodic step 48600 lr 2.610e-04 [train_loss] tar_ll 1.3335 loss -1.3335 (5.092 secs)\n",
      "tnpd:tnpd_periodic step 48800 lr 2.594e-04 [train_loss] tar_ll 1.3477 loss -1.3477 (5.365 secs)\n",
      "tnpd:tnpd_periodic step 49000 lr 2.579e-04 [train_loss] tar_ll 1.2264 loss -1.2264 (5.325 secs)\n",
      "tnpd:tnpd_periodic step 49200 lr 2.563e-04 [train_loss] tar_ll 1.2551 loss -1.2551 (5.210 secs)\n",
      "tnpd:tnpd_periodic step 49400 lr 2.547e-04 [train_loss] tar_ll 1.2305 loss -1.2305 (5.131 secs)\n",
      "tnpd:tnpd_periodic step 49600 lr 2.531e-04 [train_loss] tar_ll 1.3086 loss -1.3086 (5.152 secs)\n",
      "tnpd:tnpd_periodic step 49800 lr 2.516e-04 [train_loss] tar_ll 1.2557 loss -1.2557 (5.165 secs)\n",
      "tnpd:tnpd_periodic step 50000 lr 2.500e-04 [train_loss] tar_ll 1.3736 loss -1.3736 (5.324 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 185.34it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -3.0899 loss 3.0899 (16.190 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 50200 lr 2.484e-04 [train_loss] tar_ll 1.2873 loss -1.2873 (5.290 secs)\n",
      "tnpd:tnpd_periodic step 50400 lr 2.469e-04 [train_loss] tar_ll 1.2721 loss -1.2721 (5.125 secs)\n",
      "tnpd:tnpd_periodic step 50600 lr 2.453e-04 [train_loss] tar_ll 1.3375 loss -1.3375 (5.279 secs)\n",
      "tnpd:tnpd_periodic step 50800 lr 2.437e-04 [train_loss] tar_ll 1.2825 loss -1.2825 (5.160 secs)\n",
      "tnpd:tnpd_periodic step 51000 lr 2.421e-04 [train_loss] tar_ll 1.2619 loss -1.2619 (5.370 secs)\n",
      "tnpd:tnpd_periodic step 51200 lr 2.406e-04 [train_loss] tar_ll 1.2438 loss -1.2438 (5.322 secs)\n",
      "tnpd:tnpd_periodic step 51400 lr 2.390e-04 [train_loss] tar_ll 1.2476 loss -1.2476 (5.319 secs)\n",
      "tnpd:tnpd_periodic step 51600 lr 2.374e-04 [train_loss] tar_ll 1.3252 loss -1.3252 (5.195 secs)\n",
      "tnpd:tnpd_periodic step 51800 lr 2.359e-04 [train_loss] tar_ll 1.3091 loss -1.3091 (5.210 secs)\n",
      "tnpd:tnpd_periodic step 52000 lr 2.343e-04 [train_loss] tar_ll 1.3037 loss -1.3037 (5.288 secs)\n",
      "tnpd:tnpd_periodic step 52200 lr 2.327e-04 [train_loss] tar_ll 1.2054 loss -1.2054 (5.476 secs)\n",
      "tnpd:tnpd_periodic step 52400 lr 2.312e-04 [train_loss] tar_ll 1.3012 loss -1.3012 (5.394 secs)\n",
      "tnpd:tnpd_periodic step 52600 lr 2.296e-04 [train_loss] tar_ll 1.3242 loss -1.3242 (5.270 secs)\n",
      "tnpd:tnpd_periodic step 52800 lr 2.280e-04 [train_loss] tar_ll 1.3230 loss -1.3230 (5.371 secs)\n",
      "tnpd:tnpd_periodic step 53000 lr 2.265e-04 [train_loss] tar_ll 1.2784 loss -1.2784 (5.459 secs)\n",
      "tnpd:tnpd_periodic step 53200 lr 2.249e-04 [train_loss] tar_ll 1.2095 loss -1.2095 (5.600 secs)\n",
      "tnpd:tnpd_periodic step 53400 lr 2.233e-04 [train_loss] tar_ll 1.2749 loss -1.2749 (5.213 secs)\n",
      "tnpd:tnpd_periodic step 53600 lr 2.218e-04 [train_loss] tar_ll 1.2016 loss -1.2016 (5.333 secs)\n",
      "tnpd:tnpd_periodic step 53800 lr 2.202e-04 [train_loss] tar_ll 1.2585 loss -1.2585 (5.345 secs)\n",
      "tnpd:tnpd_periodic step 54000 lr 2.187e-04 [train_loss] tar_ll 1.3395 loss -1.3395 (5.386 secs)\n",
      "tnpd:tnpd_periodic step 54200 lr 2.171e-04 [train_loss] tar_ll 1.2182 loss -1.2182 (5.206 secs)\n",
      "tnpd:tnpd_periodic step 54400 lr 2.156e-04 [train_loss] tar_ll 1.3325 loss -1.3325 (5.346 secs)\n",
      "tnpd:tnpd_periodic step 54600 lr 2.140e-04 [train_loss] tar_ll 1.3019 loss -1.3019 (5.312 secs)\n",
      "tnpd:tnpd_periodic step 54800 lr 2.124e-04 [train_loss] tar_ll 1.2149 loss -1.2149 (5.258 secs)\n",
      "tnpd:tnpd_periodic step 55000 lr 2.109e-04 [train_loss] tar_ll 1.2550 loss -1.2550 (5.568 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 175.71it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -2.5534 loss 2.5534 (17.076 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 55200 lr 2.093e-04 [train_loss] tar_ll 1.3118 loss -1.3118 (5.896 secs)\n",
      "tnpd:tnpd_periodic step 55400 lr 2.078e-04 [train_loss] tar_ll 1.2855 loss -1.2855 (5.559 secs)\n",
      "tnpd:tnpd_periodic step 55600 lr 2.062e-04 [train_loss] tar_ll 1.2573 loss -1.2573 (5.402 secs)\n",
      "tnpd:tnpd_periodic step 55800 lr 2.047e-04 [train_loss] tar_ll 1.2667 loss -1.2667 (4.980 secs)\n",
      "tnpd:tnpd_periodic step 56000 lr 2.032e-04 [train_loss] tar_ll 1.2080 loss -1.2080 (5.300 secs)\n",
      "tnpd:tnpd_periodic step 56200 lr 2.016e-04 [train_loss] tar_ll 1.3573 loss -1.3573 (5.090 secs)\n",
      "tnpd:tnpd_periodic step 56400 lr 2.001e-04 [train_loss] tar_ll 1.2888 loss -1.2888 (5.429 secs)\n",
      "tnpd:tnpd_periodic step 56600 lr 1.985e-04 [train_loss] tar_ll 1.3218 loss -1.3218 (5.468 secs)\n",
      "tnpd:tnpd_periodic step 56800 lr 1.970e-04 [train_loss] tar_ll 1.3002 loss -1.3002 (5.761 secs)\n",
      "tnpd:tnpd_periodic step 57000 lr 1.955e-04 [train_loss] tar_ll 1.2605 loss -1.2605 (5.690 secs)\n",
      "tnpd:tnpd_periodic step 57200 lr 1.939e-04 [train_loss] tar_ll 1.2871 loss -1.2871 (7.085 secs)\n",
      "tnpd:tnpd_periodic step 57400 lr 1.924e-04 [train_loss] tar_ll 1.2515 loss -1.2515 (8.315 secs)\n",
      "tnpd:tnpd_periodic step 57600 lr 1.909e-04 [train_loss] tar_ll 1.2066 loss -1.2066 (8.027 secs)\n",
      "tnpd:tnpd_periodic step 57800 lr 1.894e-04 [train_loss] tar_ll 1.3244 loss -1.3244 (7.061 secs)\n",
      "tnpd:tnpd_periodic step 58000 lr 1.878e-04 [train_loss] tar_ll 1.2571 loss -1.2571 (7.178 secs)\n",
      "tnpd:tnpd_periodic step 58200 lr 1.863e-04 [train_loss] tar_ll 1.3776 loss -1.3776 (7.660 secs)\n",
      "tnpd:tnpd_periodic step 58400 lr 1.848e-04 [train_loss] tar_ll 1.2305 loss -1.2305 (7.356 secs)\n",
      "tnpd:tnpd_periodic step 58600 lr 1.833e-04 [train_loss] tar_ll 1.2926 loss -1.2926 (7.589 secs)\n",
      "tnpd:tnpd_periodic step 58800 lr 1.818e-04 [train_loss] tar_ll 1.2375 loss -1.2375 (7.520 secs)\n",
      "tnpd:tnpd_periodic step 59000 lr 1.803e-04 [train_loss] tar_ll 1.2517 loss -1.2517 (3.385 secs)\n",
      "tnpd:tnpd_periodic step 59200 lr 1.787e-04 [train_loss] tar_ll 1.3100 loss -1.3100 (2.708 secs)\n",
      "tnpd:tnpd_periodic step 59400 lr 1.772e-04 [train_loss] tar_ll 1.3039 loss -1.3039 (2.734 secs)\n",
      "tnpd:tnpd_periodic step 59600 lr 1.757e-04 [train_loss] tar_ll 1.2237 loss -1.2237 (2.894 secs)\n",
      "tnpd:tnpd_periodic step 59800 lr 1.742e-04 [train_loss] tar_ll 1.2723 loss -1.2723 (2.661 secs)\n",
      "tnpd:tnpd_periodic step 60000 lr 1.727e-04 [train_loss] tar_ll 1.3221 loss -1.3221 (2.696 secs)\n",
      "100%|##########| 3000/3000 [00:08<00:00, 336.63it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -2.3585 loss 2.3585 (8.914 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 60200 lr 1.713e-04 [train_loss] tar_ll 1.3026 loss -1.3026 (2.622 secs)\n",
      "tnpd:tnpd_periodic step 60400 lr 1.698e-04 [train_loss] tar_ll 1.3073 loss -1.3073 (2.644 secs)\n",
      "tnpd:tnpd_periodic step 60600 lr 1.683e-04 [train_loss] tar_ll 1.2856 loss -1.2856 (2.898 secs)\n",
      "tnpd:tnpd_periodic step 60800 lr 1.668e-04 [train_loss] tar_ll 1.3190 loss -1.3190 (2.703 secs)\n",
      "tnpd:tnpd_periodic step 61000 lr 1.653e-04 [train_loss] tar_ll 1.3087 loss -1.3087 (2.658 secs)\n",
      "tnpd:tnpd_periodic step 61200 lr 1.638e-04 [train_loss] tar_ll 1.2948 loss -1.2948 (2.670 secs)\n",
      "tnpd:tnpd_periodic step 61400 lr 1.624e-04 [train_loss] tar_ll 1.3597 loss -1.3597 (2.838 secs)\n",
      "tnpd:tnpd_periodic step 61600 lr 1.609e-04 [train_loss] tar_ll 1.3034 loss -1.3034 (8.156 secs)\n",
      "tnpd:tnpd_periodic step 61800 lr 1.594e-04 [train_loss] tar_ll 1.3311 loss -1.3311 (7.355 secs)\n",
      "tnpd:tnpd_periodic step 62000 lr 1.580e-04 [train_loss] tar_ll 1.2032 loss -1.2032 (8.085 secs)\n",
      "tnpd:tnpd_periodic step 62200 lr 1.565e-04 [train_loss] tar_ll 1.4066 loss -1.4066 (8.153 secs)\n",
      "tnpd:tnpd_periodic step 62400 lr 1.551e-04 [train_loss] tar_ll 1.3378 loss -1.3378 (7.828 secs)\n",
      "tnpd:tnpd_periodic step 62600 lr 1.536e-04 [train_loss] tar_ll 1.4630 loss -1.4630 (7.586 secs)\n",
      "tnpd:tnpd_periodic step 62800 lr 1.522e-04 [train_loss] tar_ll 1.3323 loss -1.3323 (7.982 secs)\n",
      "tnpd:tnpd_periodic step 63000 lr 1.507e-04 [train_loss] tar_ll 1.2591 loss -1.2591 (7.661 secs)\n",
      "tnpd:tnpd_periodic step 63200 lr 1.493e-04 [train_loss] tar_ll 1.3359 loss -1.3359 (6.275 secs)\n",
      "tnpd:tnpd_periodic step 63400 lr 1.478e-04 [train_loss] tar_ll 1.3256 loss -1.3256 (5.955 secs)\n",
      "tnpd:tnpd_periodic step 63600 lr 1.464e-04 [train_loss] tar_ll 1.3383 loss -1.3383 (6.442 secs)\n",
      "tnpd:tnpd_periodic step 63800 lr 1.450e-04 [train_loss] tar_ll 1.4194 loss -1.4194 (6.114 secs)\n",
      "tnpd:tnpd_periodic step 64000 lr 1.436e-04 [train_loss] tar_ll 1.2700 loss -1.2700 (6.158 secs)\n",
      "tnpd:tnpd_periodic step 64200 lr 1.421e-04 [train_loss] tar_ll 1.2918 loss -1.2918 (6.104 secs)\n",
      "tnpd:tnpd_periodic step 64400 lr 1.407e-04 [train_loss] tar_ll 1.4467 loss -1.4467 (6.161 secs)\n",
      "tnpd:tnpd_periodic step 64600 lr 1.393e-04 [train_loss] tar_ll 1.3160 loss -1.3160 (5.944 secs)\n",
      "tnpd:tnpd_periodic step 64800 lr 1.379e-04 [train_loss] tar_ll 1.3134 loss -1.3134 (6.177 secs)\n",
      "tnpd:tnpd_periodic step 65000 lr 1.365e-04 [train_loss] tar_ll 1.2758 loss -1.2758 (6.629 secs)\n",
      "100%|##########| 3000/3000 [00:24<00:00, 124.80it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -2.4835 loss 2.4835 (24.044 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 65200 lr 1.351e-04 [train_loss] tar_ll 1.3971 loss -1.3971 (6.459 secs)\n",
      "tnpd:tnpd_periodic step 65400 lr 1.337e-04 [train_loss] tar_ll 1.3382 loss -1.3382 (6.284 secs)\n",
      "tnpd:tnpd_periodic step 65600 lr 1.323e-04 [train_loss] tar_ll 1.3541 loss -1.3541 (6.267 secs)\n",
      "tnpd:tnpd_periodic step 65800 lr 1.309e-04 [train_loss] tar_ll 1.3343 loss -1.3343 (6.310 secs)\n",
      "tnpd:tnpd_periodic step 66000 lr 1.296e-04 [train_loss] tar_ll 1.2539 loss -1.2539 (6.130 secs)\n",
      "tnpd:tnpd_periodic step 66200 lr 1.282e-04 [train_loss] tar_ll 1.2919 loss -1.2919 (5.974 secs)\n",
      "tnpd:tnpd_periodic step 66400 lr 1.268e-04 [train_loss] tar_ll 1.3291 loss -1.3291 (5.647 secs)\n",
      "tnpd:tnpd_periodic step 66600 lr 1.255e-04 [train_loss] tar_ll 1.2862 loss -1.2862 (5.734 secs)\n",
      "tnpd:tnpd_periodic step 66800 lr 1.241e-04 [train_loss] tar_ll 1.4305 loss -1.4305 (5.499 secs)\n",
      "tnpd:tnpd_periodic step 67000 lr 1.227e-04 [train_loss] tar_ll 1.2909 loss -1.2909 (5.592 secs)\n",
      "tnpd:tnpd_periodic step 67200 lr 1.214e-04 [train_loss] tar_ll 1.3265 loss -1.3265 (5.895 secs)\n",
      "tnpd:tnpd_periodic step 67400 lr 1.200e-04 [train_loss] tar_ll 1.3349 loss -1.3349 (5.682 secs)\n",
      "tnpd:tnpd_periodic step 67600 lr 1.187e-04 [train_loss] tar_ll 1.3160 loss -1.3160 (5.853 secs)\n",
      "tnpd:tnpd_periodic step 67800 lr 1.174e-04 [train_loss] tar_ll 1.3105 loss -1.3105 (5.667 secs)\n",
      "tnpd:tnpd_periodic step 68000 lr 1.160e-04 [train_loss] tar_ll 1.3095 loss -1.3095 (5.662 secs)\n",
      "tnpd:tnpd_periodic step 68200 lr 1.147e-04 [train_loss] tar_ll 1.2920 loss -1.2920 (6.088 secs)\n",
      "tnpd:tnpd_periodic step 68400 lr 1.134e-04 [train_loss] tar_ll 1.3233 loss -1.3233 (5.890 secs)\n",
      "tnpd:tnpd_periodic step 68600 lr 1.121e-04 [train_loss] tar_ll 1.3307 loss -1.3307 (5.666 secs)\n",
      "tnpd:tnpd_periodic step 68800 lr 1.108e-04 [train_loss] tar_ll 1.3384 loss -1.3384 (5.822 secs)\n",
      "tnpd:tnpd_periodic step 69000 lr 1.095e-04 [train_loss] tar_ll 1.3824 loss -1.3824 (5.732 secs)\n",
      "tnpd:tnpd_periodic step 69200 lr 1.082e-04 [train_loss] tar_ll 1.3809 loss -1.3809 (5.795 secs)\n",
      "tnpd:tnpd_periodic step 69400 lr 1.069e-04 [train_loss] tar_ll 1.2259 loss -1.2259 (5.758 secs)\n",
      "tnpd:tnpd_periodic step 69600 lr 1.056e-04 [train_loss] tar_ll 1.3543 loss -1.3543 (5.650 secs)\n",
      "tnpd:tnpd_periodic step 69800 lr 1.043e-04 [train_loss] tar_ll 1.3744 loss -1.3744 (5.951 secs)\n",
      "tnpd:tnpd_periodic step 70000 lr 1.031e-04 [train_loss] tar_ll 1.2897 loss -1.2897 (5.895 secs)\n",
      "100%|##########| 3000/3000 [00:18<00:00, 164.71it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -3.0852 loss 3.0852 (18.214 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 70200 lr 1.018e-04 [train_loss] tar_ll 1.2457 loss -1.2457 (5.822 secs)\n",
      "tnpd:tnpd_periodic step 70400 lr 1.005e-04 [train_loss] tar_ll 1.2837 loss -1.2837 (5.680 secs)\n",
      "tnpd:tnpd_periodic step 70600 lr 9.927e-05 [train_loss] tar_ll 1.3342 loss -1.3342 (5.541 secs)\n",
      "tnpd:tnpd_periodic step 70800 lr 9.802e-05 [train_loss] tar_ll 1.3191 loss -1.3191 (5.653 secs)\n",
      "tnpd:tnpd_periodic step 71000 lr 9.677e-05 [train_loss] tar_ll 1.3786 loss -1.3786 (5.539 secs)\n",
      "tnpd:tnpd_periodic step 71200 lr 9.554e-05 [train_loss] tar_ll 1.3063 loss -1.3063 (5.647 secs)\n",
      "tnpd:tnpd_periodic step 71400 lr 9.430e-05 [train_loss] tar_ll 1.3247 loss -1.3247 (5.670 secs)\n",
      "tnpd:tnpd_periodic step 71600 lr 9.308e-05 [train_loss] tar_ll 1.3608 loss -1.3608 (5.672 secs)\n",
      "tnpd:tnpd_periodic step 71800 lr 9.186e-05 [train_loss] tar_ll 1.3338 loss -1.3338 (5.582 secs)\n",
      "tnpd:tnpd_periodic step 72000 lr 9.064e-05 [train_loss] tar_ll 1.2946 loss -1.2946 (5.720 secs)\n",
      "tnpd:tnpd_periodic step 72200 lr 8.944e-05 [train_loss] tar_ll 1.4152 loss -1.4152 (5.545 secs)\n",
      "tnpd:tnpd_periodic step 72400 lr 8.824e-05 [train_loss] tar_ll 1.2602 loss -1.2602 (5.635 secs)\n",
      "tnpd:tnpd_periodic step 72600 lr 8.704e-05 [train_loss] tar_ll 1.3585 loss -1.3585 (5.440 secs)\n",
      "tnpd:tnpd_periodic step 72800 lr 8.585e-05 [train_loss] tar_ll 1.3776 loss -1.3776 (5.380 secs)\n",
      "tnpd:tnpd_periodic step 73000 lr 8.467e-05 [train_loss] tar_ll 1.3116 loss -1.3116 (5.480 secs)\n",
      "tnpd:tnpd_periodic step 73200 lr 8.350e-05 [train_loss] tar_ll 1.3927 loss -1.3927 (5.545 secs)\n",
      "tnpd:tnpd_periodic step 73400 lr 8.233e-05 [train_loss] tar_ll 1.2968 loss -1.2968 (5.420 secs)\n",
      "tnpd:tnpd_periodic step 73600 lr 8.117e-05 [train_loss] tar_ll 1.4159 loss -1.4159 (5.470 secs)\n",
      "tnpd:tnpd_periodic step 73800 lr 8.001e-05 [train_loss] tar_ll 1.3881 loss -1.3881 (5.560 secs)\n",
      "tnpd:tnpd_periodic step 74000 lr 7.886e-05 [train_loss] tar_ll 1.3051 loss -1.3051 (5.571 secs)\n",
      "tnpd:tnpd_periodic step 74200 lr 7.772e-05 [train_loss] tar_ll 1.2202 loss -1.2202 (5.559 secs)\n",
      "tnpd:tnpd_periodic step 74400 lr 7.659e-05 [train_loss] tar_ll 1.3630 loss -1.3630 (5.649 secs)\n",
      "tnpd:tnpd_periodic step 74600 lr 7.546e-05 [train_loss] tar_ll 1.3977 loss -1.3977 (5.645 secs)\n",
      "tnpd:tnpd_periodic step 74800 lr 7.434e-05 [train_loss] tar_ll 1.4027 loss -1.4027 (5.690 secs)\n",
      "tnpd:tnpd_periodic step 75000 lr 7.322e-05 [train_loss] tar_ll 1.3449 loss -1.3449 (5.742 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 166.80it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -3.0003 loss 3.0003 (17.990 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 75200 lr 7.212e-05 [train_loss] tar_ll 1.3693 loss -1.3693 (5.850 secs)\n",
      "tnpd:tnpd_periodic step 75400 lr 7.102e-05 [train_loss] tar_ll 1.4007 loss -1.4007 (5.605 secs)\n",
      "tnpd:tnpd_periodic step 75600 lr 6.992e-05 [train_loss] tar_ll 1.3665 loss -1.3665 (5.851 secs)\n",
      "tnpd:tnpd_periodic step 75800 lr 6.884e-05 [train_loss] tar_ll 1.3461 loss -1.3461 (5.724 secs)\n",
      "tnpd:tnpd_periodic step 76000 lr 6.776e-05 [train_loss] tar_ll 1.2913 loss -1.2913 (5.770 secs)\n",
      "tnpd:tnpd_periodic step 76200 lr 6.669e-05 [train_loss] tar_ll 1.3356 loss -1.3356 (5.751 secs)\n",
      "tnpd:tnpd_periodic step 76400 lr 6.562e-05 [train_loss] tar_ll 1.3510 loss -1.3510 (5.644 secs)\n",
      "tnpd:tnpd_periodic step 76600 lr 6.456e-05 [train_loss] tar_ll 1.3961 loss -1.3961 (5.625 secs)\n",
      "tnpd:tnpd_periodic step 76800 lr 6.351e-05 [train_loss] tar_ll 1.4010 loss -1.4010 (5.772 secs)\n",
      "tnpd:tnpd_periodic step 77000 lr 6.247e-05 [train_loss] tar_ll 1.3011 loss -1.3011 (5.820 secs)\n",
      "tnpd:tnpd_periodic step 77200 lr 6.144e-05 [train_loss] tar_ll 1.2759 loss -1.2759 (5.735 secs)\n",
      "tnpd:tnpd_periodic step 77400 lr 6.041e-05 [train_loss] tar_ll 1.2956 loss -1.2956 (5.655 secs)\n",
      "tnpd:tnpd_periodic step 77600 lr 5.939e-05 [train_loss] tar_ll 1.3685 loss -1.3685 (5.575 secs)\n",
      "tnpd:tnpd_periodic step 77800 lr 5.838e-05 [train_loss] tar_ll 1.3268 loss -1.3268 (5.585 secs)\n",
      "tnpd:tnpd_periodic step 78000 lr 5.737e-05 [train_loss] tar_ll 1.3249 loss -1.3249 (5.720 secs)\n",
      "tnpd:tnpd_periodic step 78200 lr 5.637e-05 [train_loss] tar_ll 1.4075 loss -1.4075 (5.475 secs)\n",
      "tnpd:tnpd_periodic step 78400 lr 5.538e-05 [train_loss] tar_ll 1.3660 loss -1.3660 (5.470 secs)\n",
      "tnpd:tnpd_periodic step 78600 lr 5.440e-05 [train_loss] tar_ll 1.4101 loss -1.4101 (5.555 secs)\n",
      "tnpd:tnpd_periodic step 78800 lr 5.343e-05 [train_loss] tar_ll 1.3850 loss -1.3850 (5.395 secs)\n",
      "tnpd:tnpd_periodic step 79000 lr 5.246e-05 [train_loss] tar_ll 1.4165 loss -1.4165 (5.393 secs)\n",
      "tnpd:tnpd_periodic step 79200 lr 5.150e-05 [train_loss] tar_ll 1.4449 loss -1.4449 (5.707 secs)\n",
      "tnpd:tnpd_periodic step 79400 lr 5.055e-05 [train_loss] tar_ll 1.3512 loss -1.3512 (5.585 secs)\n",
      "tnpd:tnpd_periodic step 79600 lr 4.961e-05 [train_loss] tar_ll 1.3172 loss -1.3172 (5.645 secs)\n",
      "tnpd:tnpd_periodic step 79800 lr 4.867e-05 [train_loss] tar_ll 1.3997 loss -1.3997 (5.662 secs)\n",
      "tnpd:tnpd_periodic step 80000 lr 4.775e-05 [train_loss] tar_ll 1.3445 loss -1.3445 (5.410 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 173.14it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -2.8613 loss 2.8613 (17.328 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 80200 lr 4.683e-05 [train_loss] tar_ll 1.3779 loss -1.3779 (5.524 secs)\n",
      "tnpd:tnpd_periodic step 80400 lr 4.592e-05 [train_loss] tar_ll 1.3597 loss -1.3597 (5.430 secs)\n",
      "tnpd:tnpd_periodic step 80600 lr 4.501e-05 [train_loss] tar_ll 1.3191 loss -1.3191 (5.433 secs)\n",
      "tnpd:tnpd_periodic step 80800 lr 4.412e-05 [train_loss] tar_ll 1.4779 loss -1.4779 (5.547 secs)\n",
      "tnpd:tnpd_periodic step 81000 lr 4.323e-05 [train_loss] tar_ll 1.3474 loss -1.3474 (5.577 secs)\n",
      "tnpd:tnpd_periodic step 81200 lr 4.235e-05 [train_loss] tar_ll 1.4184 loss -1.4184 (5.579 secs)\n",
      "tnpd:tnpd_periodic step 81400 lr 4.148e-05 [train_loss] tar_ll 1.3452 loss -1.3452 (5.690 secs)\n",
      "tnpd:tnpd_periodic step 81600 lr 4.062e-05 [train_loss] tar_ll 1.3934 loss -1.3934 (5.615 secs)\n",
      "tnpd:tnpd_periodic step 81800 lr 3.976e-05 [train_loss] tar_ll 1.4088 loss -1.4088 (5.660 secs)\n",
      "tnpd:tnpd_periodic step 82000 lr 3.892e-05 [train_loss] tar_ll 1.3927 loss -1.3927 (5.685 secs)\n",
      "tnpd:tnpd_periodic step 82200 lr 3.808e-05 [train_loss] tar_ll 1.4742 loss -1.4742 (5.765 secs)\n",
      "tnpd:tnpd_periodic step 82400 lr 3.725e-05 [train_loss] tar_ll 1.3473 loss -1.3473 (5.585 secs)\n",
      "tnpd:tnpd_periodic step 82600 lr 3.643e-05 [train_loss] tar_ll 1.3673 loss -1.3673 (5.690 secs)\n",
      "tnpd:tnpd_periodic step 82800 lr 3.562e-05 [train_loss] tar_ll 1.4227 loss -1.4227 (5.646 secs)\n",
      "tnpd:tnpd_periodic step 83000 lr 3.481e-05 [train_loss] tar_ll 1.3871 loss -1.3871 (5.801 secs)\n",
      "tnpd:tnpd_periodic step 83200 lr 3.402e-05 [train_loss] tar_ll 1.4638 loss -1.4638 (5.785 secs)\n",
      "tnpd:tnpd_periodic step 83400 lr 3.323e-05 [train_loss] tar_ll 1.4343 loss -1.4343 (5.580 secs)\n",
      "tnpd:tnpd_periodic step 83600 lr 3.245e-05 [train_loss] tar_ll 1.3950 loss -1.3950 (5.820 secs)\n",
      "tnpd:tnpd_periodic step 83800 lr 3.168e-05 [train_loss] tar_ll 1.3924 loss -1.3924 (5.750 secs)\n",
      "tnpd:tnpd_periodic step 84000 lr 3.092e-05 [train_loss] tar_ll 1.3906 loss -1.3906 (5.580 secs)\n",
      "tnpd:tnpd_periodic step 84200 lr 3.017e-05 [train_loss] tar_ll 1.3647 loss -1.3647 (5.630 secs)\n",
      "tnpd:tnpd_periodic step 84400 lr 2.943e-05 [train_loss] tar_ll 1.3717 loss -1.3717 (5.590 secs)\n",
      "tnpd:tnpd_periodic step 84600 lr 2.869e-05 [train_loss] tar_ll 1.2720 loss -1.2720 (5.776 secs)\n",
      "tnpd:tnpd_periodic step 84800 lr 2.797e-05 [train_loss] tar_ll 1.4196 loss -1.4196 (5.698 secs)\n",
      "tnpd:tnpd_periodic step 85000 lr 2.725e-05 [train_loss] tar_ll 1.3708 loss -1.3708 (5.810 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 167.37it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -3.0256 loss 3.0256 (17.930 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 85200 lr 2.654e-05 [train_loss] tar_ll 1.4068 loss -1.4068 (5.619 secs)\n",
      "tnpd:tnpd_periodic step 85400 lr 2.584e-05 [train_loss] tar_ll 1.4661 loss -1.4661 (5.391 secs)\n",
      "tnpd:tnpd_periodic step 85600 lr 2.515e-05 [train_loss] tar_ll 1.4617 loss -1.4617 (5.686 secs)\n",
      "tnpd:tnpd_periodic step 85800 lr 2.447e-05 [train_loss] tar_ll 1.3410 loss -1.3410 (5.647 secs)\n",
      "tnpd:tnpd_periodic step 86000 lr 2.379e-05 [train_loss] tar_ll 1.4812 loss -1.4812 (5.605 secs)\n",
      "tnpd:tnpd_periodic step 86200 lr 2.313e-05 [train_loss] tar_ll 1.3684 loss -1.3684 (5.635 secs)\n",
      "tnpd:tnpd_periodic step 86400 lr 2.247e-05 [train_loss] tar_ll 1.3761 loss -1.3761 (5.575 secs)\n",
      "tnpd:tnpd_periodic step 86600 lr 2.183e-05 [train_loss] tar_ll 1.4108 loss -1.4108 (5.685 secs)\n",
      "tnpd:tnpd_periodic step 86800 lr 2.119e-05 [train_loss] tar_ll 1.4591 loss -1.4591 (5.590 secs)\n",
      "tnpd:tnpd_periodic step 87000 lr 2.056e-05 [train_loss] tar_ll 1.3270 loss -1.3270 (5.687 secs)\n",
      "tnpd:tnpd_periodic step 87200 lr 1.994e-05 [train_loss] tar_ll 1.4527 loss -1.4527 (5.619 secs)\n",
      "tnpd:tnpd_periodic step 87400 lr 1.933e-05 [train_loss] tar_ll 1.3880 loss -1.3880 (5.534 secs)\n",
      "tnpd:tnpd_periodic step 87600 lr 1.873e-05 [train_loss] tar_ll 1.4012 loss -1.4012 (5.606 secs)\n",
      "tnpd:tnpd_periodic step 87800 lr 1.814e-05 [train_loss] tar_ll 1.3683 loss -1.3683 (5.560 secs)\n",
      "tnpd:tnpd_periodic step 88000 lr 1.756e-05 [train_loss] tar_ll 1.4945 loss -1.4945 (5.455 secs)\n",
      "tnpd:tnpd_periodic step 88200 lr 1.698e-05 [train_loss] tar_ll 1.3373 loss -1.3373 (5.765 secs)\n",
      "tnpd:tnpd_periodic step 88400 lr 1.642e-05 [train_loss] tar_ll 1.3018 loss -1.3018 (5.599 secs)\n",
      "tnpd:tnpd_periodic step 88600 lr 1.586e-05 [train_loss] tar_ll 1.3896 loss -1.3896 (5.576 secs)\n",
      "tnpd:tnpd_periodic step 88800 lr 1.532e-05 [train_loss] tar_ll 1.4006 loss -1.4006 (5.670 secs)\n",
      "tnpd:tnpd_periodic step 89000 lr 1.478e-05 [train_loss] tar_ll 1.3410 loss -1.3410 (5.779 secs)\n",
      "tnpd:tnpd_periodic step 89200 lr 1.425e-05 [train_loss] tar_ll 1.3973 loss -1.3973 (5.860 secs)\n",
      "tnpd:tnpd_periodic step 89400 lr 1.373e-05 [train_loss] tar_ll 1.4267 loss -1.4267 (5.760 secs)\n",
      "tnpd:tnpd_periodic step 89600 lr 1.323e-05 [train_loss] tar_ll 1.3864 loss -1.3864 (5.609 secs)\n",
      "tnpd:tnpd_periodic step 89800 lr 1.273e-05 [train_loss] tar_ll 1.4271 loss -1.4271 (5.615 secs)\n",
      "tnpd:tnpd_periodic step 90000 lr 1.224e-05 [train_loss] tar_ll 1.3399 loss -1.3399 (5.556 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 172.24it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -2.7635 loss 2.7635 (17.418 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 90200 lr 1.176e-05 [train_loss] tar_ll 1.2994 loss -1.2994 (5.802 secs)\n",
      "tnpd:tnpd_periodic step 90400 lr 1.128e-05 [train_loss] tar_ll 1.3910 loss -1.3910 (5.715 secs)\n",
      "tnpd:tnpd_periodic step 90600 lr 1.082e-05 [train_loss] tar_ll 1.3544 loss -1.3544 (5.705 secs)\n",
      "tnpd:tnpd_periodic step 90800 lr 1.037e-05 [train_loss] tar_ll 1.3740 loss -1.3740 (5.965 secs)\n",
      "tnpd:tnpd_periodic step 91000 lr 9.927e-06 [train_loss] tar_ll 1.3571 loss -1.3571 (5.595 secs)\n",
      "tnpd:tnpd_periodic step 91200 lr 9.493e-06 [train_loss] tar_ll 1.4671 loss -1.4671 (5.615 secs)\n",
      "tnpd:tnpd_periodic step 91400 lr 9.069e-06 [train_loss] tar_ll 1.4557 loss -1.4557 (5.725 secs)\n",
      "tnpd:tnpd_periodic step 91600 lr 8.655e-06 [train_loss] tar_ll 1.3784 loss -1.3784 (5.585 secs)\n",
      "tnpd:tnpd_periodic step 91800 lr 8.250e-06 [train_loss] tar_ll 1.3854 loss -1.3854 (5.738 secs)\n",
      "tnpd:tnpd_periodic step 92000 lr 7.854e-06 [train_loss] tar_ll 1.4204 loss -1.4204 (5.721 secs)\n",
      "tnpd:tnpd_periodic step 92200 lr 7.468e-06 [train_loss] tar_ll 1.4437 loss -1.4437 (5.719 secs)\n",
      "tnpd:tnpd_periodic step 92400 lr 7.092e-06 [train_loss] tar_ll 1.3297 loss -1.3297 (5.570 secs)\n",
      "tnpd:tnpd_periodic step 92600 lr 6.725e-06 [train_loss] tar_ll 1.2589 loss -1.2589 (5.761 secs)\n",
      "tnpd:tnpd_periodic step 92800 lr 6.368e-06 [train_loss] tar_ll 1.4102 loss -1.4102 (5.619 secs)\n",
      "tnpd:tnpd_periodic step 93000 lr 6.021e-06 [train_loss] tar_ll 1.3298 loss -1.3298 (5.680 secs)\n",
      "tnpd:tnpd_periodic step 93200 lr 5.683e-06 [train_loss] tar_ll 1.3592 loss -1.3592 (5.440 secs)\n",
      "tnpd:tnpd_periodic step 93400 lr 5.355e-06 [train_loss] tar_ll 1.3096 loss -1.3096 (5.590 secs)\n",
      "tnpd:tnpd_periodic step 93600 lr 5.036e-06 [train_loss] tar_ll 1.4096 loss -1.4096 (5.690 secs)\n",
      "tnpd:tnpd_periodic step 93800 lr 4.727e-06 [train_loss] tar_ll 1.3748 loss -1.3748 (5.610 secs)\n",
      "tnpd:tnpd_periodic step 94000 lr 4.428e-06 [train_loss] tar_ll 1.3440 loss -1.3440 (5.725 secs)\n",
      "tnpd:tnpd_periodic step 94200 lr 4.139e-06 [train_loss] tar_ll 1.4085 loss -1.4085 (5.675 secs)\n",
      "tnpd:tnpd_periodic step 94400 lr 3.859e-06 [train_loss] tar_ll 1.4127 loss -1.4127 (5.631 secs)\n",
      "tnpd:tnpd_periodic step 94600 lr 3.589e-06 [train_loss] tar_ll 1.4043 loss -1.4043 (5.625 secs)\n",
      "tnpd:tnpd_periodic step 94800 lr 3.329e-06 [train_loss] tar_ll 1.2864 loss -1.2864 (5.701 secs)\n",
      "tnpd:tnpd_periodic step 95000 lr 3.078e-06 [train_loss] tar_ll 1.3833 loss -1.3833 (5.854 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 172.64it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -3.1015 loss 3.1015 (17.380 secs)\n",
      "\n",
      "tnpd:tnpd_periodic step 95200 lr 2.837e-06 [train_loss] tar_ll 1.4449 loss -1.4449 (5.591 secs)\n",
      "tnpd:tnpd_periodic step 95400 lr 2.606e-06 [train_loss] tar_ll 1.2822 loss -1.2822 (5.455 secs)\n",
      "tnpd:tnpd_periodic step 95600 lr 2.385e-06 [train_loss] tar_ll 1.3798 loss -1.3798 (5.510 secs)\n",
      "tnpd:tnpd_periodic step 95800 lr 2.173e-06 [train_loss] tar_ll 1.4208 loss -1.4208 (5.565 secs)\n",
      "tnpd:tnpd_periodic step 96000 lr 1.971e-06 [train_loss] tar_ll 1.3462 loss -1.3462 (5.720 secs)\n",
      "tnpd:tnpd_periodic step 96200 lr 1.779e-06 [train_loss] tar_ll 1.4207 loss -1.4207 (5.685 secs)\n",
      "tnpd:tnpd_periodic step 96400 lr 1.597e-06 [train_loss] tar_ll 1.5005 loss -1.5005 (5.875 secs)\n",
      "tnpd:tnpd_periodic step 96600 lr 1.425e-06 [train_loss] tar_ll 1.3956 loss -1.3956 (5.880 secs)\n",
      "tnpd:tnpd_periodic step 96800 lr 1.262e-06 [train_loss] tar_ll 1.4539 loss -1.4539 (5.606 secs)\n",
      "tnpd:tnpd_periodic step 97000 lr 1.110e-06 [train_loss] tar_ll 1.3539 loss -1.3539 (5.833 secs)\n",
      "tnpd:tnpd_periodic step 97200 lr 9.666e-07 [train_loss] tar_ll 1.3835 loss -1.3835 (5.732 secs)\n",
      "tnpd:tnpd_periodic step 97400 lr 8.335e-07 [train_loss] tar_ll 1.4092 loss -1.4092 (5.928 secs)\n",
      "tnpd:tnpd_periodic step 97600 lr 7.103e-07 [train_loss] tar_ll 1.3799 loss -1.3799 (5.717 secs)\n",
      "tnpd:tnpd_periodic step 97800 lr 5.969e-07 [train_loss] tar_ll 1.4344 loss -1.4344 (5.725 secs)\n",
      "tnpd:tnpd_periodic step 98000 lr 4.933e-07 [train_loss] tar_ll 1.4091 loss -1.4091 (5.905 secs)\n",
      "tnpd:tnpd_periodic step 98200 lr 3.996e-07 [train_loss] tar_ll 1.3613 loss -1.3613 (5.845 secs)\n",
      "tnpd:tnpd_periodic step 98400 lr 3.158e-07 [train_loss] tar_ll 1.4033 loss -1.4033 (5.870 secs)\n",
      "tnpd:tnpd_periodic step 98600 lr 2.418e-07 [train_loss] tar_ll 1.4092 loss -1.4092 (5.725 secs)\n",
      "tnpd:tnpd_periodic step 98800 lr 1.776e-07 [train_loss] tar_ll 1.4569 loss -1.4569 (5.920 secs)\n",
      "tnpd:tnpd_periodic step 99000 lr 1.234e-07 [train_loss] tar_ll 1.4212 loss -1.4212 (5.973 secs)\n",
      "tnpd:tnpd_periodic step 99200 lr 7.895e-08 [train_loss] tar_ll 1.4171 loss -1.4171 (5.739 secs)\n",
      "tnpd:tnpd_periodic step 99400 lr 4.441e-08 [train_loss] tar_ll 1.3451 loss -1.3451 (5.722 secs)\n",
      "tnpd:tnpd_periodic step 99600 lr 1.974e-08 [train_loss] tar_ll 1.4072 loss -1.4072 (5.750 secs)\n",
      "tnpd:tnpd_periodic step 99800 lr 4.935e-09 [train_loss] tar_ll 1.3085 loss -1.3085 (5.770 secs)\n",
      "tnpd:tnpd_periodic step 100000 lr 0.000e+00 [train_loss] tar_ll 1.3137 loss -1.3137 (5.780 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 166.95it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -3.1397 loss 3.1397 (17.970 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:18<00:00, 165.94it/s]\n",
      "tnpd:tnpd_periodic periodic tar_ll -3.1397 loss 3.1397 (18.081 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3142758.75 miliseconds\n",
      "Execution time: 3142.75875 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 39.81005859375 MB\n",
      "Memory Usage Change: 23.56005859375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='tnpd', name='tnpd_periodic',val_seed=100, val_l=None,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc44daa-cd39-4aa0-ac89-2ae857916c77",
   "metadata": {},
   "source": [
    "## EQTNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d5035-7886-4514-9042-639a18366169",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mod(modelo='eqtnp', name='eqtnp_periodic',val_seed=100, val_l=None,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223d9b7-3781-4ee4-b29b-d847df7fad2c",
   "metadata": {},
   "source": [
    "## LBANP (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf42a793-8df1-468f-8e2e-b6d21be65be9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: lbanp-lbanp-num_latents-8_periodic\n",
      "Total number of parameters: 784834\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8_periodic step 200 lr 5.000e-04 [train_loss] tar_ll -0.7218 loss 0.7218 (24.323 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 400 lr 5.000e-04 [train_loss] tar_ll -0.6963 loss 0.6963 (23.493 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 600 lr 5.000e-04 [train_loss] tar_ll -0.5981 loss 0.5981 (25.625 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 800 lr 4.999e-04 [train_loss] tar_ll -0.5605 loss 0.5605 (24.491 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 1000 lr 4.999e-04 [train_loss] tar_ll -0.4754 loss 0.4754 (25.000 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 1200 lr 4.998e-04 [train_loss] tar_ll -0.4742 loss 0.4742 (25.487 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 1400 lr 4.998e-04 [train_loss] tar_ll -0.3470 loss 0.3470 (23.938 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 1600 lr 4.997e-04 [train_loss] tar_ll -0.2740 loss 0.2740 (25.631 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 1800 lr 4.996e-04 [train_loss] tar_ll -0.2173 loss 0.2173 (23.947 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 2000 lr 4.995e-04 [train_loss] tar_ll -0.1992 loss 0.1992 (22.412 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 2200 lr 4.994e-04 [train_loss] tar_ll -0.1165 loss 0.1165 (24.661 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 2400 lr 4.993e-04 [train_loss] tar_ll -0.0567 loss 0.0567 (14.931 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 2600 lr 4.992e-04 [train_loss] tar_ll 0.0018 loss -0.0018 (13.855 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 2800 lr 4.990e-04 [train_loss] tar_ll 0.0626 loss -0.0626 (14.132 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 3000 lr 4.989e-04 [train_loss] tar_ll 0.1037 loss -0.1037 (13.868 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 3200 lr 4.987e-04 [train_loss] tar_ll 0.0829 loss -0.0829 (13.802 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 3400 lr 4.986e-04 [train_loss] tar_ll 0.1342 loss -0.1342 (13.567 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 3600 lr 4.984e-04 [train_loss] tar_ll 0.0802 loss -0.0802 (13.658 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 3800 lr 4.982e-04 [train_loss] tar_ll 0.0510 loss -0.0510 (14.056 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 4000 lr 4.980e-04 [train_loss] tar_ll 0.1294 loss -0.1294 (13.893 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 4200 lr 4.978e-04 [train_loss] tar_ll 0.1581 loss -0.1581 (13.763 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 4400 lr 4.976e-04 [train_loss] tar_ll 0.0131 loss -0.0131 (13.927 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 4600 lr 4.974e-04 [train_loss] tar_ll 0.1776 loss -0.1776 (13.864 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 4800 lr 4.972e-04 [train_loss] tar_ll 0.1659 loss -0.1659 (14.763 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 5000 lr 4.969e-04 [train_loss] tar_ll 0.2155 loss -0.2155 (14.380 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.48it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -1.1632 loss 1.1632 (67.463 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 5200 lr 4.967e-04 [train_loss] tar_ll 0.2336 loss -0.2336 (14.398 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 5400 lr 4.964e-04 [train_loss] tar_ll 0.3636 loss -0.3636 (13.518 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 5600 lr 4.961e-04 [train_loss] tar_ll 0.3159 loss -0.3159 (13.579 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 5800 lr 4.959e-04 [train_loss] tar_ll 0.3712 loss -0.3712 (13.460 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 6000 lr 4.956e-04 [train_loss] tar_ll 0.3312 loss -0.3312 (13.463 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 6200 lr 4.953e-04 [train_loss] tar_ll 0.2809 loss -0.2809 (13.851 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 6400 lr 4.950e-04 [train_loss] tar_ll 0.3109 loss -0.3109 (13.583 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 6600 lr 4.946e-04 [train_loss] tar_ll 0.3896 loss -0.3896 (13.475 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 6800 lr 4.943e-04 [train_loss] tar_ll 0.3752 loss -0.3752 (13.426 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 7000 lr 4.940e-04 [train_loss] tar_ll 0.3333 loss -0.3333 (13.293 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 7200 lr 4.936e-04 [train_loss] tar_ll 0.3898 loss -0.3898 (13.555 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 7400 lr 4.933e-04 [train_loss] tar_ll 0.3938 loss -0.3938 (13.509 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 7600 lr 4.929e-04 [train_loss] tar_ll 0.4491 loss -0.4491 (14.229 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 7800 lr 4.925e-04 [train_loss] tar_ll 0.3674 loss -0.3674 (13.906 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 8000 lr 4.921e-04 [train_loss] tar_ll 0.4225 loss -0.4225 (14.018 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 8200 lr 4.918e-04 [train_loss] tar_ll 0.4554 loss -0.4554 (14.259 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 8400 lr 4.913e-04 [train_loss] tar_ll 0.4550 loss -0.4550 (14.139 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 8600 lr 4.909e-04 [train_loss] tar_ll 0.4343 loss -0.4343 (14.035 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 8800 lr 4.905e-04 [train_loss] tar_ll 0.4359 loss -0.4359 (14.085 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 9000 lr 4.901e-04 [train_loss] tar_ll 0.4364 loss -0.4364 (13.962 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 9200 lr 4.896e-04 [train_loss] tar_ll 0.4716 loss -0.4716 (13.820 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 9400 lr 4.892e-04 [train_loss] tar_ll 0.4923 loss -0.4923 (13.835 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 9600 lr 4.887e-04 [train_loss] tar_ll 0.3944 loss -0.3944 (13.977 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 9800 lr 4.882e-04 [train_loss] tar_ll 0.4504 loss -0.4504 (14.180 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 10000 lr 4.878e-04 [train_loss] tar_ll 0.5455 loss -0.5455 (13.794 secs)\n",
      "100%|##########| 3000/3000 [01:05<00:00, 46.08it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -1.0987 loss 1.0987 (65.101 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 10200 lr 4.873e-04 [train_loss] tar_ll 0.4116 loss -0.4116 (14.369 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 10400 lr 4.868e-04 [train_loss] tar_ll 0.2191 loss -0.2191 (13.786 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 10600 lr 4.863e-04 [train_loss] tar_ll 0.4265 loss -0.4265 (13.968 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 10800 lr 4.857e-04 [train_loss] tar_ll 0.4315 loss -0.4315 (14.349 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 11000 lr 4.852e-04 [train_loss] tar_ll 0.4635 loss -0.4635 (13.866 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 11200 lr 4.847e-04 [train_loss] tar_ll 0.3931 loss -0.3931 (13.922 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 11400 lr 4.841e-04 [train_loss] tar_ll 0.4333 loss -0.4333 (13.703 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 11600 lr 4.836e-04 [train_loss] tar_ll 0.4556 loss -0.4556 (14.089 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 11800 lr 4.830e-04 [train_loss] tar_ll 0.5159 loss -0.5159 (13.794 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 12000 lr 4.824e-04 [train_loss] tar_ll 0.5085 loss -0.5085 (13.852 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 12200 lr 4.819e-04 [train_loss] tar_ll 0.5530 loss -0.5530 (13.933 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 12400 lr 4.813e-04 [train_loss] tar_ll 0.5683 loss -0.5683 (14.090 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 12600 lr 4.807e-04 [train_loss] tar_ll 0.5262 loss -0.5262 (14.707 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 12800 lr 4.801e-04 [train_loss] tar_ll 0.5193 loss -0.5193 (13.960 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 13000 lr 4.794e-04 [train_loss] tar_ll 0.5502 loss -0.5502 (13.857 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 13200 lr 4.788e-04 [train_loss] tar_ll 0.5754 loss -0.5754 (13.953 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 13400 lr 4.782e-04 [train_loss] tar_ll 0.4919 loss -0.4919 (14.078 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 13600 lr 4.775e-04 [train_loss] tar_ll 0.5695 loss -0.5695 (13.995 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 13800 lr 4.769e-04 [train_loss] tar_ll 0.4846 loss -0.4846 (14.078 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 14000 lr 4.762e-04 [train_loss] tar_ll 0.5419 loss -0.5419 (14.161 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 14200 lr 4.755e-04 [train_loss] tar_ll 0.5559 loss -0.5559 (14.194 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 14400 lr 4.749e-04 [train_loss] tar_ll 0.5503 loss -0.5503 (13.991 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 14600 lr 4.742e-04 [train_loss] tar_ll 0.5941 loss -0.5941 (13.895 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 14800 lr 4.735e-04 [train_loss] tar_ll 0.6170 loss -0.6170 (13.940 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 15000 lr 4.728e-04 [train_loss] tar_ll 0.5002 loss -0.5002 (13.985 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.10it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -1.5298 loss 1.5298 (66.521 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 15200 lr 4.720e-04 [train_loss] tar_ll 0.5124 loss -0.5124 (13.825 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 15400 lr 4.713e-04 [train_loss] tar_ll 0.6757 loss -0.6757 (13.808 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 15600 lr 4.706e-04 [train_loss] tar_ll 0.5969 loss -0.5969 (13.514 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 15800 lr 4.698e-04 [train_loss] tar_ll 0.5115 loss -0.5115 (14.091 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 16000 lr 4.691e-04 [train_loss] tar_ll 0.5845 loss -0.5845 (14.009 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 16200 lr 4.683e-04 [train_loss] tar_ll 0.5640 loss -0.5640 (14.052 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 16400 lr 4.675e-04 [train_loss] tar_ll 0.6519 loss -0.6519 (13.968 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 16600 lr 4.668e-04 [train_loss] tar_ll 0.6132 loss -0.6132 (14.309 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 16800 lr 4.660e-04 [train_loss] tar_ll 0.5200 loss -0.5200 (14.330 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 17000 lr 4.652e-04 [train_loss] tar_ll 0.5592 loss -0.5592 (13.823 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 17200 lr 4.644e-04 [train_loss] tar_ll 0.6314 loss -0.6314 (13.825 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 17400 lr 4.636e-04 [train_loss] tar_ll 0.6043 loss -0.6043 (13.975 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 17600 lr 4.627e-04 [train_loss] tar_ll 0.6816 loss -0.6816 (13.883 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 17800 lr 4.619e-04 [train_loss] tar_ll 0.6561 loss -0.6561 (13.924 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 18000 lr 4.611e-04 [train_loss] tar_ll 0.5484 loss -0.5484 (13.657 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 18200 lr 4.602e-04 [train_loss] tar_ll 0.6127 loss -0.6127 (13.682 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 18400 lr 4.594e-04 [train_loss] tar_ll 0.5419 loss -0.5419 (13.650 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 18600 lr 4.585e-04 [train_loss] tar_ll 0.5642 loss -0.5642 (13.592 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 18800 lr 4.576e-04 [train_loss] tar_ll 0.6323 loss -0.6323 (13.719 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 19000 lr 4.568e-04 [train_loss] tar_ll 0.6026 loss -0.6026 (13.765 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 19200 lr 4.559e-04 [train_loss] tar_ll 0.6723 loss -0.6723 (13.841 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 19400 lr 4.550e-04 [train_loss] tar_ll 0.6630 loss -0.6630 (14.017 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 19600 lr 4.541e-04 [train_loss] tar_ll 0.6984 loss -0.6984 (13.816 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 19800 lr 4.532e-04 [train_loss] tar_ll 0.6391 loss -0.6391 (14.281 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 20000 lr 4.523e-04 [train_loss] tar_ll 0.6284 loss -0.6284 (14.208 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 44.94it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -1.6690 loss 1.6690 (66.763 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 20200 lr 4.513e-04 [train_loss] tar_ll 0.6489 loss -0.6489 (14.337 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 20400 lr 4.504e-04 [train_loss] tar_ll 0.6739 loss -0.6739 (14.406 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 20600 lr 4.494e-04 [train_loss] tar_ll 0.6455 loss -0.6455 (13.806 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 20800 lr 4.485e-04 [train_loss] tar_ll 0.6427 loss -0.6427 (13.898 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 21000 lr 4.475e-04 [train_loss] tar_ll 0.5988 loss -0.5988 (13.743 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 21200 lr 4.466e-04 [train_loss] tar_ll 0.6908 loss -0.6908 (13.827 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 21400 lr 4.456e-04 [train_loss] tar_ll 0.6928 loss -0.6928 (13.726 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 21600 lr 4.446e-04 [train_loss] tar_ll 0.6623 loss -0.6623 (14.037 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 21800 lr 4.436e-04 [train_loss] tar_ll 0.7208 loss -0.7208 (14.003 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 22000 lr 4.426e-04 [train_loss] tar_ll 0.7097 loss -0.7097 (14.195 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 22200 lr 4.416e-04 [train_loss] tar_ll 0.6202 loss -0.6202 (14.690 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 22400 lr 4.406e-04 [train_loss] tar_ll 0.6537 loss -0.6537 (14.148 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 22600 lr 4.396e-04 [train_loss] tar_ll 0.6918 loss -0.6918 (14.047 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 22800 lr 4.386e-04 [train_loss] tar_ll 0.6889 loss -0.6889 (13.928 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 23000 lr 4.375e-04 [train_loss] tar_ll 0.7253 loss -0.7253 (14.452 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 23200 lr 4.365e-04 [train_loss] tar_ll 0.7002 loss -0.7002 (14.259 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 23400 lr 4.354e-04 [train_loss] tar_ll 0.6045 loss -0.6045 (13.922 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 23600 lr 4.344e-04 [train_loss] tar_ll 0.7045 loss -0.7045 (13.937 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 23800 lr 4.333e-04 [train_loss] tar_ll 0.6510 loss -0.6510 (13.950 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 24000 lr 4.322e-04 [train_loss] tar_ll 0.6372 loss -0.6372 (13.851 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 24200 lr 4.312e-04 [train_loss] tar_ll 0.6483 loss -0.6483 (13.955 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 24400 lr 4.301e-04 [train_loss] tar_ll 0.7373 loss -0.7373 (13.984 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 24600 lr 4.290e-04 [train_loss] tar_ll 0.6215 loss -0.6215 (13.853 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 24800 lr 4.279e-04 [train_loss] tar_ll 0.7556 loss -0.7556 (14.005 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 25000 lr 4.268e-04 [train_loss] tar_ll 0.6529 loss -0.6529 (14.216 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.62it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -1.8377 loss 1.8377 (67.234 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 25200 lr 4.257e-04 [train_loss] tar_ll 0.7058 loss -0.7058 (14.256 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 25400 lr 4.245e-04 [train_loss] tar_ll 0.7191 loss -0.7191 (13.985 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 25600 lr 4.234e-04 [train_loss] tar_ll 0.7433 loss -0.7433 (14.005 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 25800 lr 4.223e-04 [train_loss] tar_ll 0.6951 loss -0.6951 (14.006 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 26000 lr 4.211e-04 [train_loss] tar_ll 0.6423 loss -0.6423 (13.682 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 26200 lr 4.200e-04 [train_loss] tar_ll 0.7465 loss -0.7465 (14.074 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 26400 lr 4.188e-04 [train_loss] tar_ll 0.7282 loss -0.7282 (13.863 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 26600 lr 4.177e-04 [train_loss] tar_ll 0.6637 loss -0.6637 (14.112 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 26800 lr 4.165e-04 [train_loss] tar_ll 0.6901 loss -0.6901 (13.797 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 27000 lr 4.153e-04 [train_loss] tar_ll 0.7196 loss -0.7196 (13.617 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 27200 lr 4.141e-04 [train_loss] tar_ll 0.6635 loss -0.6635 (13.602 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 27400 lr 4.130e-04 [train_loss] tar_ll 0.6596 loss -0.6596 (13.647 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 27600 lr 4.118e-04 [train_loss] tar_ll 0.7462 loss -0.7462 (14.242 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 27800 lr 4.106e-04 [train_loss] tar_ll 0.7106 loss -0.7106 (14.088 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 28000 lr 4.094e-04 [train_loss] tar_ll 0.6489 loss -0.6489 (14.599 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 28200 lr 4.081e-04 [train_loss] tar_ll 0.7414 loss -0.7414 (14.127 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 28400 lr 4.069e-04 [train_loss] tar_ll 0.5825 loss -0.5825 (14.030 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 28600 lr 4.057e-04 [train_loss] tar_ll 0.8091 loss -0.8091 (13.977 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 28800 lr 4.045e-04 [train_loss] tar_ll 0.8804 loss -0.8804 (14.172 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 29000 lr 4.032e-04 [train_loss] tar_ll 0.7362 loss -0.7362 (13.916 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 29200 lr 4.020e-04 [train_loss] tar_ll 0.6801 loss -0.6801 (13.961 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 29400 lr 4.007e-04 [train_loss] tar_ll 0.7437 loss -0.7437 (13.723 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 29600 lr 3.995e-04 [train_loss] tar_ll 0.7698 loss -0.7698 (13.774 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 29800 lr 3.982e-04 [train_loss] tar_ll 0.8115 loss -0.8115 (13.960 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 30000 lr 3.969e-04 [train_loss] tar_ll 0.7970 loss -0.7970 (13.786 secs)\n",
      "100%|##########| 3000/3000 [01:05<00:00, 45.74it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -1.4803 loss 1.4803 (65.587 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 30200 lr 3.957e-04 [train_loss] tar_ll 0.7037 loss -0.7037 (13.819 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 30400 lr 3.944e-04 [train_loss] tar_ll 0.7649 loss -0.7649 (13.947 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 30600 lr 3.931e-04 [train_loss] tar_ll 0.7720 loss -0.7720 (14.194 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 30800 lr 3.918e-04 [train_loss] tar_ll 0.7869 loss -0.7869 (14.085 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 31000 lr 3.905e-04 [train_loss] tar_ll 0.7802 loss -0.7802 (14.175 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 31200 lr 3.892e-04 [train_loss] tar_ll 0.7494 loss -0.7494 (14.177 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 31400 lr 3.879e-04 [train_loss] tar_ll 0.7352 loss -0.7352 (14.129 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 31600 lr 3.866e-04 [train_loss] tar_ll 0.7032 loss -0.7032 (13.897 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 31800 lr 3.853e-04 [train_loss] tar_ll 0.7038 loss -0.7038 (13.862 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 32000 lr 3.840e-04 [train_loss] tar_ll 0.7393 loss -0.7393 (13.773 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 32200 lr 3.826e-04 [train_loss] tar_ll 0.7105 loss -0.7105 (13.732 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 32400 lr 3.813e-04 [train_loss] tar_ll 0.7655 loss -0.7655 (13.930 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 32600 lr 3.800e-04 [train_loss] tar_ll 0.7042 loss -0.7042 (13.986 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 32800 lr 3.786e-04 [train_loss] tar_ll 0.7770 loss -0.7770 (13.975 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 33000 lr 3.773e-04 [train_loss] tar_ll 0.8487 loss -0.8487 (13.766 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 33200 lr 3.759e-04 [train_loss] tar_ll 0.7884 loss -0.7884 (13.977 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 33400 lr 3.745e-04 [train_loss] tar_ll 0.8063 loss -0.8063 (14.063 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 33600 lr 3.732e-04 [train_loss] tar_ll 0.7746 loss -0.7746 (14.003 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 33800 lr 3.718e-04 [train_loss] tar_ll 0.7780 loss -0.7780 (14.196 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 34000 lr 3.704e-04 [train_loss] tar_ll 0.7432 loss -0.7432 (14.266 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 34200 lr 3.691e-04 [train_loss] tar_ll 0.7732 loss -0.7732 (14.371 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 34400 lr 3.677e-04 [train_loss] tar_ll 0.7807 loss -0.7807 (14.211 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 34600 lr 3.663e-04 [train_loss] tar_ll 0.7599 loss -0.7599 (14.609 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 34800 lr 3.649e-04 [train_loss] tar_ll 0.7975 loss -0.7975 (14.041 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 35000 lr 3.635e-04 [train_loss] tar_ll 0.8446 loss -0.8446 (14.030 secs)\n",
      "100%|##########| 3000/3000 [01:05<00:00, 45.52it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -1.5533 loss 1.5533 (65.904 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 35200 lr 3.621e-04 [train_loss] tar_ll 0.8050 loss -0.8050 (13.803 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 35400 lr 3.607e-04 [train_loss] tar_ll 0.7721 loss -0.7721 (13.893 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 35600 lr 3.593e-04 [train_loss] tar_ll 0.8184 loss -0.8184 (13.866 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 35800 lr 3.579e-04 [train_loss] tar_ll 0.8430 loss -0.8430 (13.615 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 36000 lr 3.564e-04 [train_loss] tar_ll 0.8506 loss -0.8506 (14.071 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 36200 lr 3.550e-04 [train_loss] tar_ll 0.8558 loss -0.8558 (14.308 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 36400 lr 3.536e-04 [train_loss] tar_ll 0.7054 loss -0.7054 (14.126 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 36600 lr 3.522e-04 [train_loss] tar_ll 0.7408 loss -0.7408 (14.099 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 36800 lr 3.507e-04 [train_loss] tar_ll 0.6412 loss -0.6412 (14.184 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 37000 lr 3.493e-04 [train_loss] tar_ll 0.7486 loss -0.7486 (14.044 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 37200 lr 3.478e-04 [train_loss] tar_ll 0.7610 loss -0.7610 (14.102 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 37400 lr 3.464e-04 [train_loss] tar_ll 0.8392 loss -0.8392 (13.981 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 37600 lr 3.449e-04 [train_loss] tar_ll 0.7993 loss -0.7993 (14.090 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 37800 lr 3.435e-04 [train_loss] tar_ll 0.7958 loss -0.7958 (13.818 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 38000 lr 3.420e-04 [train_loss] tar_ll 0.8384 loss -0.8384 (13.857 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 38200 lr 3.406e-04 [train_loss] tar_ll 0.8268 loss -0.8268 (13.910 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 38400 lr 3.391e-04 [train_loss] tar_ll 0.9227 loss -0.9227 (13.857 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 38600 lr 3.376e-04 [train_loss] tar_ll 0.8126 loss -0.8126 (13.930 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 38800 lr 3.362e-04 [train_loss] tar_ll 0.8179 loss -0.8179 (13.949 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 39000 lr 3.347e-04 [train_loss] tar_ll 0.5444 loss -0.5444 (13.707 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 39200 lr 3.332e-04 [train_loss] tar_ll 0.7667 loss -0.7667 (14.015 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 39400 lr 3.317e-04 [train_loss] tar_ll 0.7029 loss -0.7029 (14.174 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 39600 lr 3.302e-04 [train_loss] tar_ll 0.7551 loss -0.7551 (13.781 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 39800 lr 3.287e-04 [train_loss] tar_ll 0.7600 loss -0.7600 (14.297 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 40000 lr 3.273e-04 [train_loss] tar_ll 0.7799 loss -0.7799 (14.281 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.16it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -1.8874 loss 1.8874 (66.431 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 40200 lr 3.258e-04 [train_loss] tar_ll 0.7235 loss -0.7235 (14.047 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 40400 lr 3.243e-04 [train_loss] tar_ll 0.7803 loss -0.7803 (13.838 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 40600 lr 3.228e-04 [train_loss] tar_ll 0.7147 loss -0.7147 (13.661 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 40800 lr 3.213e-04 [train_loss] tar_ll 0.7611 loss -0.7611 (13.695 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 41000 lr 3.197e-04 [train_loss] tar_ll 0.8053 loss -0.8053 (13.283 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 41200 lr 3.182e-04 [train_loss] tar_ll 0.7472 loss -0.7472 (13.633 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 41400 lr 3.167e-04 [train_loss] tar_ll 0.7449 loss -0.7449 (13.374 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 41600 lr 3.152e-04 [train_loss] tar_ll 0.8114 loss -0.8114 (13.810 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 41800 lr 3.137e-04 [train_loss] tar_ll 0.7626 loss -0.7626 (13.502 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 42000 lr 3.122e-04 [train_loss] tar_ll 0.8406 loss -0.8406 (13.694 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 42200 lr 3.106e-04 [train_loss] tar_ll 0.8179 loss -0.8179 (13.980 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 42400 lr 3.091e-04 [train_loss] tar_ll 0.7879 loss -0.7879 (13.997 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 42600 lr 3.076e-04 [train_loss] tar_ll 0.7492 loss -0.7492 (14.235 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 42800 lr 3.061e-04 [train_loss] tar_ll 0.8407 loss -0.8407 (14.235 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 43000 lr 3.045e-04 [train_loss] tar_ll 0.7670 loss -0.7670 (13.924 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 43200 lr 3.030e-04 [train_loss] tar_ll 0.8429 loss -0.8429 (13.975 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 43400 lr 3.015e-04 [train_loss] tar_ll 0.8748 loss -0.8748 (13.683 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 43600 lr 2.999e-04 [train_loss] tar_ll 0.8252 loss -0.8252 (13.805 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 43800 lr 2.984e-04 [train_loss] tar_ll 0.7810 loss -0.7810 (14.714 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 44000 lr 2.968e-04 [train_loss] tar_ll 0.7421 loss -0.7421 (13.336 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 44200 lr 2.953e-04 [train_loss] tar_ll 0.8530 loss -0.8530 (13.231 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 44400 lr 2.938e-04 [train_loss] tar_ll 0.8163 loss -0.8163 (13.015 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 44600 lr 2.922e-04 [train_loss] tar_ll 0.8381 loss -0.8381 (13.118 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 44800 lr 2.907e-04 [train_loss] tar_ll 0.8080 loss -0.8080 (13.286 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 45000 lr 2.891e-04 [train_loss] tar_ll 0.8802 loss -0.8802 (18.916 secs)\n",
      "100%|##########| 3000/3000 [02:10<00:00, 23.07it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -1.7027 loss 1.7027 (130.075 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 45200 lr 2.876e-04 [train_loss] tar_ll 0.8390 loss -0.8390 (22.063 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 45400 lr 2.860e-04 [train_loss] tar_ll 0.7530 loss -0.7530 (22.943 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 45600 lr 2.844e-04 [train_loss] tar_ll 0.7938 loss -0.7938 (21.540 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 45800 lr 2.829e-04 [train_loss] tar_ll 0.8450 loss -0.8450 (43.052 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 46000 lr 2.813e-04 [train_loss] tar_ll 0.8219 loss -0.8219 (40.573 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 46200 lr 2.798e-04 [train_loss] tar_ll 0.8455 loss -0.8455 (40.060 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 46400 lr 2.782e-04 [train_loss] tar_ll 0.8423 loss -0.8423 (33.836 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 46600 lr 2.767e-04 [train_loss] tar_ll 0.8592 loss -0.8592 (25.229 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 46800 lr 2.751e-04 [train_loss] tar_ll 0.8678 loss -0.8678 (25.867 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 47000 lr 2.735e-04 [train_loss] tar_ll 0.8236 loss -0.8236 (18.569 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 47200 lr 2.720e-04 [train_loss] tar_ll 0.8922 loss -0.8922 (14.204 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 47400 lr 2.704e-04 [train_loss] tar_ll 0.8757 loss -0.8757 (14.175 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 47600 lr 2.688e-04 [train_loss] tar_ll 0.8588 loss -0.8588 (13.434 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 47800 lr 2.673e-04 [train_loss] tar_ll 0.8163 loss -0.8163 (13.225 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 48000 lr 2.657e-04 [train_loss] tar_ll 0.7673 loss -0.7673 (13.371 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 48200 lr 2.641e-04 [train_loss] tar_ll 0.8967 loss -0.8967 (13.630 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 48400 lr 2.626e-04 [train_loss] tar_ll 0.9112 loss -0.9112 (13.998 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 48600 lr 2.610e-04 [train_loss] tar_ll 0.7708 loss -0.7708 (13.682 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 48800 lr 2.594e-04 [train_loss] tar_ll 0.7936 loss -0.7936 (14.063 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 49000 lr 2.579e-04 [train_loss] tar_ll 0.9260 loss -0.9260 (14.140 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 49200 lr 2.563e-04 [train_loss] tar_ll 0.8576 loss -0.8576 (14.529 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 49400 lr 2.547e-04 [train_loss] tar_ll 0.8187 loss -0.8187 (13.809 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 49600 lr 2.531e-04 [train_loss] tar_ll 0.8590 loss -0.8590 (13.728 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 49800 lr 2.516e-04 [train_loss] tar_ll 0.8448 loss -0.8448 (13.167 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 50000 lr 2.500e-04 [train_loss] tar_ll 0.9234 loss -0.9234 (12.870 secs)\n",
      "100%|##########| 3000/3000 [01:05<00:00, 46.05it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -2.0240 loss 2.0240 (65.156 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 50200 lr 2.484e-04 [train_loss] tar_ll 0.8870 loss -0.8870 (12.881 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 50400 lr 2.469e-04 [train_loss] tar_ll 0.8852 loss -0.8852 (12.724 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 50600 lr 2.453e-04 [train_loss] tar_ll 0.9409 loss -0.9409 (12.956 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 50800 lr 2.437e-04 [train_loss] tar_ll 0.8879 loss -0.8879 (13.815 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 51000 lr 2.421e-04 [train_loss] tar_ll 0.8812 loss -0.8812 (13.837 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 51200 lr 2.406e-04 [train_loss] tar_ll 0.8250 loss -0.8250 (14.121 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 51400 lr 2.390e-04 [train_loss] tar_ll 0.8838 loss -0.8838 (14.171 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 51600 lr 2.374e-04 [train_loss] tar_ll 0.9068 loss -0.9068 (13.920 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 51800 lr 2.359e-04 [train_loss] tar_ll 0.8819 loss -0.8819 (14.412 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 52000 lr 2.343e-04 [train_loss] tar_ll 0.9500 loss -0.9500 (13.976 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 52200 lr 2.327e-04 [train_loss] tar_ll 0.9189 loss -0.9189 (13.985 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 52400 lr 2.312e-04 [train_loss] tar_ll 0.8846 loss -0.8846 (13.777 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 52600 lr 2.296e-04 [train_loss] tar_ll 0.8773 loss -0.8773 (13.739 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 52800 lr 2.280e-04 [train_loss] tar_ll 0.8987 loss -0.8987 (14.102 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 53000 lr 2.265e-04 [train_loss] tar_ll 0.8987 loss -0.8987 (13.630 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 53200 lr 2.249e-04 [train_loss] tar_ll 0.8712 loss -0.8712 (13.995 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 53400 lr 2.233e-04 [train_loss] tar_ll 0.9785 loss -0.9785 (13.888 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 53600 lr 2.218e-04 [train_loss] tar_ll 0.9676 loss -0.9676 (13.511 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 53800 lr 2.202e-04 [train_loss] tar_ll 0.9974 loss -0.9974 (13.874 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 54000 lr 2.187e-04 [train_loss] tar_ll 0.9432 loss -0.9432 (13.564 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 54200 lr 2.171e-04 [train_loss] tar_ll 0.9374 loss -0.9374 (14.026 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 54400 lr 2.156e-04 [train_loss] tar_ll 0.9531 loss -0.9531 (13.907 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 54600 lr 2.140e-04 [train_loss] tar_ll 0.9423 loss -0.9423 (13.770 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 54800 lr 2.124e-04 [train_loss] tar_ll 1.0041 loss -1.0041 (14.002 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 55000 lr 2.109e-04 [train_loss] tar_ll 0.9415 loss -0.9415 (14.138 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.40it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -1.9045 loss 1.9045 (66.076 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 55200 lr 2.093e-04 [train_loss] tar_ll 0.9448 loss -0.9448 (14.054 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 55400 lr 2.078e-04 [train_loss] tar_ll 0.9622 loss -0.9622 (13.757 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 55600 lr 2.062e-04 [train_loss] tar_ll 0.9231 loss -0.9231 (13.747 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 55800 lr 2.047e-04 [train_loss] tar_ll 0.9382 loss -0.9382 (13.701 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 56000 lr 2.032e-04 [train_loss] tar_ll 0.9460 loss -0.9460 (13.823 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 56200 lr 2.016e-04 [train_loss] tar_ll 0.9497 loss -0.9497 (14.088 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 56400 lr 2.001e-04 [train_loss] tar_ll 1.0178 loss -1.0178 (13.817 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 56600 lr 1.985e-04 [train_loss] tar_ll 0.9284 loss -0.9284 (14.055 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0334 loss -1.0334 (14.046 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 57000 lr 1.955e-04 [train_loss] tar_ll 0.9268 loss -0.9268 (13.937 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 57200 lr 1.939e-04 [train_loss] tar_ll 0.8775 loss -0.8775 (14.246 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 57400 lr 1.924e-04 [train_loss] tar_ll 0.9275 loss -0.9275 (13.858 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 57600 lr 1.909e-04 [train_loss] tar_ll 1.0032 loss -1.0032 (14.132 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 57800 lr 1.894e-04 [train_loss] tar_ll 0.9009 loss -0.9009 (14.155 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 58000 lr 1.878e-04 [train_loss] tar_ll 1.0135 loss -1.0135 (13.916 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 58200 lr 1.863e-04 [train_loss] tar_ll 0.9494 loss -0.9494 (14.192 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 58400 lr 1.848e-04 [train_loss] tar_ll 1.0054 loss -1.0054 (13.701 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 58600 lr 1.833e-04 [train_loss] tar_ll 0.9903 loss -0.9903 (13.677 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 58800 lr 1.818e-04 [train_loss] tar_ll 1.0251 loss -1.0251 (13.838 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 59000 lr 1.803e-04 [train_loss] tar_ll 0.9561 loss -0.9561 (13.801 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 59200 lr 1.787e-04 [train_loss] tar_ll 1.0394 loss -1.0394 (13.518 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 59400 lr 1.772e-04 [train_loss] tar_ll 0.9357 loss -0.9357 (13.620 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 59600 lr 1.757e-04 [train_loss] tar_ll 1.0004 loss -1.0004 (13.582 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 59800 lr 1.742e-04 [train_loss] tar_ll 1.0512 loss -1.0512 (13.638 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 60000 lr 1.727e-04 [train_loss] tar_ll 1.0156 loss -1.0156 (14.031 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.02it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -1.8765 loss 1.8765 (66.639 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 60200 lr 1.713e-04 [train_loss] tar_ll 1.0503 loss -1.0503 (14.195 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 60400 lr 1.698e-04 [train_loss] tar_ll 0.9663 loss -0.9663 (14.103 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 60600 lr 1.683e-04 [train_loss] tar_ll 1.0011 loss -1.0011 (14.005 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 60800 lr 1.668e-04 [train_loss] tar_ll 0.9921 loss -0.9921 (13.781 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 61000 lr 1.653e-04 [train_loss] tar_ll 0.9679 loss -0.9679 (13.739 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 61200 lr 1.638e-04 [train_loss] tar_ll 0.9947 loss -0.9947 (14.091 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 61400 lr 1.624e-04 [train_loss] tar_ll 1.0192 loss -1.0192 (13.827 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 61600 lr 1.609e-04 [train_loss] tar_ll 0.9898 loss -0.9898 (13.681 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 61800 lr 1.594e-04 [train_loss] tar_ll 0.9694 loss -0.9694 (13.524 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 62000 lr 1.580e-04 [train_loss] tar_ll 0.9410 loss -0.9410 (13.765 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 62200 lr 1.565e-04 [train_loss] tar_ll 0.9799 loss -0.9799 (13.814 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 62400 lr 1.551e-04 [train_loss] tar_ll 1.0355 loss -1.0355 (14.014 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 62600 lr 1.536e-04 [train_loss] tar_ll 0.9798 loss -0.9798 (14.123 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 62800 lr 1.522e-04 [train_loss] tar_ll 1.0469 loss -1.0469 (14.007 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 63000 lr 1.507e-04 [train_loss] tar_ll 1.0793 loss -1.0793 (13.961 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 63200 lr 1.493e-04 [train_loss] tar_ll 0.9668 loss -0.9668 (14.074 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 63400 lr 1.478e-04 [train_loss] tar_ll 1.0564 loss -1.0564 (14.084 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 63600 lr 1.464e-04 [train_loss] tar_ll 1.0258 loss -1.0258 (13.732 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 63800 lr 1.450e-04 [train_loss] tar_ll 1.0249 loss -1.0249 (13.987 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 64000 lr 1.436e-04 [train_loss] tar_ll 1.0374 loss -1.0374 (13.859 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 64200 lr 1.421e-04 [train_loss] tar_ll 1.0102 loss -1.0102 (13.938 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 64400 lr 1.407e-04 [train_loss] tar_ll 0.9680 loss -0.9680 (13.896 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 64600 lr 1.393e-04 [train_loss] tar_ll 0.9841 loss -0.9841 (14.000 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 64800 lr 1.379e-04 [train_loss] tar_ll 1.0099 loss -1.0099 (13.910 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 65000 lr 1.365e-04 [train_loss] tar_ll 0.9621 loss -0.9621 (13.796 secs)\n",
      "100%|##########| 3000/3000 [01:05<00:00, 45.97it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -2.0359 loss 2.0359 (65.268 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 65200 lr 1.351e-04 [train_loss] tar_ll 0.9960 loss -0.9960 (13.993 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 65400 lr 1.337e-04 [train_loss] tar_ll 1.0875 loss -1.0875 (14.169 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 65600 lr 1.323e-04 [train_loss] tar_ll 0.9993 loss -0.9993 (14.126 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 65800 lr 1.309e-04 [train_loss] tar_ll 1.0653 loss -1.0653 (14.393 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 66000 lr 1.296e-04 [train_loss] tar_ll 1.0440 loss -1.0440 (13.763 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 66200 lr 1.282e-04 [train_loss] tar_ll 1.0046 loss -1.0046 (14.074 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 66400 lr 1.268e-04 [train_loss] tar_ll 1.0378 loss -1.0378 (13.969 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 66600 lr 1.255e-04 [train_loss] tar_ll 1.1462 loss -1.1462 (13.807 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 66800 lr 1.241e-04 [train_loss] tar_ll 1.0561 loss -1.0561 (13.800 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 67000 lr 1.227e-04 [train_loss] tar_ll 1.1006 loss -1.1006 (13.951 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 67200 lr 1.214e-04 [train_loss] tar_ll 1.0374 loss -1.0374 (13.699 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 67400 lr 1.200e-04 [train_loss] tar_ll 1.0228 loss -1.0228 (13.766 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 67600 lr 1.187e-04 [train_loss] tar_ll 0.9059 loss -0.9059 (13.513 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 67800 lr 1.174e-04 [train_loss] tar_ll 0.9581 loss -0.9581 (13.654 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 68000 lr 1.160e-04 [train_loss] tar_ll 1.0768 loss -1.0768 (13.674 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 68200 lr 1.147e-04 [train_loss] tar_ll 1.0425 loss -1.0425 (13.921 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 68400 lr 1.134e-04 [train_loss] tar_ll 1.0424 loss -1.0424 (13.695 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 68600 lr 1.121e-04 [train_loss] tar_ll 1.0270 loss -1.0270 (13.940 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 68800 lr 1.108e-04 [train_loss] tar_ll 1.0805 loss -1.0805 (13.959 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 69000 lr 1.095e-04 [train_loss] tar_ll 1.0209 loss -1.0209 (14.237 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 69200 lr 1.082e-04 [train_loss] tar_ll 1.0029 loss -1.0029 (14.250 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 69400 lr 1.069e-04 [train_loss] tar_ll 1.0854 loss -1.0854 (14.035 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 69600 lr 1.056e-04 [train_loss] tar_ll 1.0010 loss -1.0010 (13.813 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 69800 lr 1.043e-04 [train_loss] tar_ll 1.1079 loss -1.1079 (14.062 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 70000 lr 1.031e-04 [train_loss] tar_ll 1.1003 loss -1.1003 (14.105 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.29it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -2.5854 loss 2.5854 (66.237 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 70200 lr 1.018e-04 [train_loss] tar_ll 1.0258 loss -1.0258 (14.037 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 70400 lr 1.005e-04 [train_loss] tar_ll 1.0509 loss -1.0509 (13.736 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 70600 lr 9.927e-05 [train_loss] tar_ll 1.0832 loss -1.0832 (13.600 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 70800 lr 9.802e-05 [train_loss] tar_ll 1.0692 loss -1.0692 (13.642 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 71000 lr 9.677e-05 [train_loss] tar_ll 1.1055 loss -1.1055 (14.066 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 71200 lr 9.554e-05 [train_loss] tar_ll 1.0603 loss -1.0603 (14.056 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 71400 lr 9.430e-05 [train_loss] tar_ll 1.0725 loss -1.0725 (14.301 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 71600 lr 9.308e-05 [train_loss] tar_ll 1.0919 loss -1.0919 (14.173 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 71800 lr 9.186e-05 [train_loss] tar_ll 0.9981 loss -0.9981 (13.945 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 72000 lr 9.064e-05 [train_loss] tar_ll 1.0738 loss -1.0738 (13.869 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 72200 lr 8.944e-05 [train_loss] tar_ll 1.0451 loss -1.0451 (14.188 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 72400 lr 8.824e-05 [train_loss] tar_ll 1.0844 loss -1.0844 (13.885 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 72600 lr 8.704e-05 [train_loss] tar_ll 1.0677 loss -1.0677 (13.987 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 72800 lr 8.585e-05 [train_loss] tar_ll 1.1067 loss -1.1067 (13.816 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 73000 lr 8.467e-05 [train_loss] tar_ll 1.0768 loss -1.0768 (13.950 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 73200 lr 8.350e-05 [train_loss] tar_ll 1.1202 loss -1.1202 (14.165 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 73400 lr 8.233e-05 [train_loss] tar_ll 1.1479 loss -1.1479 (13.838 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 73600 lr 8.117e-05 [train_loss] tar_ll 1.0913 loss -1.0913 (14.175 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 73800 lr 8.001e-05 [train_loss] tar_ll 1.1631 loss -1.1631 (13.922 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 74000 lr 7.886e-05 [train_loss] tar_ll 1.0638 loss -1.0638 (13.867 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 74200 lr 7.772e-05 [train_loss] tar_ll 1.1105 loss -1.1105 (13.917 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 74400 lr 7.659e-05 [train_loss] tar_ll 1.0852 loss -1.0852 (14.203 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 74600 lr 7.546e-05 [train_loss] tar_ll 1.1888 loss -1.1888 (14.017 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 74800 lr 7.434e-05 [train_loss] tar_ll 1.1524 loss -1.1524 (14.366 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1129 loss -1.1129 (14.248 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.76it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -2.4581 loss 2.4581 (67.027 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1012 loss -1.1012 (13.809 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 75400 lr 7.102e-05 [train_loss] tar_ll 1.1220 loss -1.1220 (13.886 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 75600 lr 6.992e-05 [train_loss] tar_ll 1.0496 loss -1.0496 (14.051 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 75800 lr 6.884e-05 [train_loss] tar_ll 1.1084 loss -1.1084 (14.001 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 76000 lr 6.776e-05 [train_loss] tar_ll 1.1307 loss -1.1307 (13.784 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 76200 lr 6.669e-05 [train_loss] tar_ll 1.0508 loss -1.0508 (14.020 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 76400 lr 6.562e-05 [train_loss] tar_ll 1.0992 loss -1.0992 (14.344 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 76600 lr 6.456e-05 [train_loss] tar_ll 1.0841 loss -1.0841 (13.482 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 76800 lr 6.351e-05 [train_loss] tar_ll 1.0718 loss -1.0718 (13.943 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 77000 lr 6.247e-05 [train_loss] tar_ll 1.0719 loss -1.0719 (14.211 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 77200 lr 6.144e-05 [train_loss] tar_ll 1.1016 loss -1.1016 (14.228 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 77400 lr 6.041e-05 [train_loss] tar_ll 1.1897 loss -1.1897 (14.220 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 77600 lr 5.939e-05 [train_loss] tar_ll 1.1375 loss -1.1375 (13.951 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 77800 lr 5.838e-05 [train_loss] tar_ll 1.1050 loss -1.1050 (14.124 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 78000 lr 5.737e-05 [train_loss] tar_ll 1.0679 loss -1.0679 (14.229 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 78200 lr 5.637e-05 [train_loss] tar_ll 1.1542 loss -1.1542 (14.494 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 78400 lr 5.538e-05 [train_loss] tar_ll 1.0771 loss -1.0771 (13.972 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 78600 lr 5.440e-05 [train_loss] tar_ll 1.1719 loss -1.1719 (13.831 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 78800 lr 5.343e-05 [train_loss] tar_ll 1.1285 loss -1.1285 (13.927 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 79000 lr 5.246e-05 [train_loss] tar_ll 1.1430 loss -1.1430 (13.997 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 79200 lr 5.150e-05 [train_loss] tar_ll 1.0966 loss -1.0966 (14.020 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 79400 lr 5.055e-05 [train_loss] tar_ll 1.0346 loss -1.0346 (14.074 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 79600 lr 4.961e-05 [train_loss] tar_ll 1.1455 loss -1.1455 (13.753 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 79800 lr 4.867e-05 [train_loss] tar_ll 1.1395 loss -1.1395 (14.043 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 80000 lr 4.775e-05 [train_loss] tar_ll 1.2060 loss -1.2060 (13.862 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.71it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -2.5337 loss 2.5337 (67.101 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 80200 lr 4.683e-05 [train_loss] tar_ll 1.0348 loss -1.0348 (14.222 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 80400 lr 4.592e-05 [train_loss] tar_ll 1.0707 loss -1.0707 (14.318 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1330 loss -1.1330 (14.064 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 80800 lr 4.412e-05 [train_loss] tar_ll 1.1237 loss -1.1237 (13.939 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 81000 lr 4.323e-05 [train_loss] tar_ll 1.1252 loss -1.1252 (14.232 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 81200 lr 4.235e-05 [train_loss] tar_ll 1.1838 loss -1.1838 (13.982 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 81400 lr 4.148e-05 [train_loss] tar_ll 1.1018 loss -1.1018 (14.138 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 81600 lr 4.062e-05 [train_loss] tar_ll 1.2046 loss -1.2046 (13.978 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 81800 lr 3.976e-05 [train_loss] tar_ll 1.1514 loss -1.1514 (14.607 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 82000 lr 3.892e-05 [train_loss] tar_ll 1.1254 loss -1.1254 (15.211 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 82200 lr 3.808e-05 [train_loss] tar_ll 1.1627 loss -1.1627 (14.646 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 82400 lr 3.725e-05 [train_loss] tar_ll 1.1522 loss -1.1522 (13.957 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 82600 lr 3.643e-05 [train_loss] tar_ll 1.1761 loss -1.1761 (14.101 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 82800 lr 3.562e-05 [train_loss] tar_ll 1.0676 loss -1.0676 (13.964 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 83000 lr 3.481e-05 [train_loss] tar_ll 1.1337 loss -1.1337 (13.821 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 83200 lr 3.402e-05 [train_loss] tar_ll 1.1413 loss -1.1413 (14.218 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 83400 lr 3.323e-05 [train_loss] tar_ll 1.1126 loss -1.1126 (14.009 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 83600 lr 3.245e-05 [train_loss] tar_ll 1.1152 loss -1.1152 (14.056 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 83800 lr 3.168e-05 [train_loss] tar_ll 1.1257 loss -1.1257 (14.106 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 84000 lr 3.092e-05 [train_loss] tar_ll 1.1354 loss -1.1354 (14.005 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 84200 lr 3.017e-05 [train_loss] tar_ll 1.0432 loss -1.0432 (14.327 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 84400 lr 2.943e-05 [train_loss] tar_ll 1.0330 loss -1.0330 (13.707 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 84600 lr 2.869e-05 [train_loss] tar_ll 1.1140 loss -1.1140 (13.767 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 84800 lr 2.797e-05 [train_loss] tar_ll 1.0987 loss -1.0987 (14.244 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 85000 lr 2.725e-05 [train_loss] tar_ll 1.1837 loss -1.1837 (14.012 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 44.82it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -2.5449 loss 2.5449 (66.937 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 85200 lr 2.654e-05 [train_loss] tar_ll 1.1394 loss -1.1394 (14.057 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 85400 lr 2.584e-05 [train_loss] tar_ll 1.1479 loss -1.1479 (14.162 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 85600 lr 2.515e-05 [train_loss] tar_ll 1.2104 loss -1.2104 (14.263 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 85800 lr 2.447e-05 [train_loss] tar_ll 1.1816 loss -1.1816 (14.413 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 86000 lr 2.379e-05 [train_loss] tar_ll 1.1682 loss -1.1682 (14.414 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 86200 lr 2.313e-05 [train_loss] tar_ll 1.1269 loss -1.1269 (14.479 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 86400 lr 2.247e-05 [train_loss] tar_ll 1.2968 loss -1.2968 (14.290 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 86600 lr 2.183e-05 [train_loss] tar_ll 1.2072 loss -1.2072 (14.075 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 86800 lr 2.119e-05 [train_loss] tar_ll 1.1697 loss -1.1697 (13.949 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 87000 lr 2.056e-05 [train_loss] tar_ll 1.1621 loss -1.1621 (13.921 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 87200 lr 1.994e-05 [train_loss] tar_ll 1.0911 loss -1.0911 (13.940 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 87400 lr 1.933e-05 [train_loss] tar_ll 1.1633 loss -1.1633 (13.866 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 87600 lr 1.873e-05 [train_loss] tar_ll 1.2819 loss -1.2819 (14.189 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 87800 lr 1.814e-05 [train_loss] tar_ll 1.1422 loss -1.1422 (14.297 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 88000 lr 1.756e-05 [train_loss] tar_ll 1.1125 loss -1.1125 (13.721 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 88200 lr 1.698e-05 [train_loss] tar_ll 1.1668 loss -1.1668 (13.797 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 88400 lr 1.642e-05 [train_loss] tar_ll 1.1430 loss -1.1430 (13.989 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 88600 lr 1.586e-05 [train_loss] tar_ll 1.1865 loss -1.1865 (14.000 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 88800 lr 1.532e-05 [train_loss] tar_ll 1.1418 loss -1.1418 (14.352 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 89000 lr 1.478e-05 [train_loss] tar_ll 1.2049 loss -1.2049 (14.375 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 89200 lr 1.425e-05 [train_loss] tar_ll 1.1688 loss -1.1688 (14.291 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 89400 lr 1.373e-05 [train_loss] tar_ll 1.2214 loss -1.2214 (14.137 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 89600 lr 1.323e-05 [train_loss] tar_ll 1.0999 loss -1.0999 (13.981 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 89800 lr 1.273e-05 [train_loss] tar_ll 1.1387 loss -1.1387 (13.993 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 90000 lr 1.224e-05 [train_loss] tar_ll 1.1754 loss -1.1754 (14.370 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 44.88it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -2.6914 loss 2.6914 (66.852 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 90200 lr 1.176e-05 [train_loss] tar_ll 1.1917 loss -1.1917 (13.753 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 90400 lr 1.128e-05 [train_loss] tar_ll 1.1611 loss -1.1611 (13.866 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 90600 lr 1.082e-05 [train_loss] tar_ll 1.1525 loss -1.1525 (13.774 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 90800 lr 1.037e-05 [train_loss] tar_ll 1.1990 loss -1.1990 (13.896 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 91000 lr 9.927e-06 [train_loss] tar_ll 1.2085 loss -1.2085 (14.035 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 91200 lr 9.493e-06 [train_loss] tar_ll 1.1147 loss -1.1147 (14.261 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 91400 lr 9.069e-06 [train_loss] tar_ll 1.1007 loss -1.1007 (14.060 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 91600 lr 8.655e-06 [train_loss] tar_ll 1.1718 loss -1.1718 (14.034 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 91800 lr 8.250e-06 [train_loss] tar_ll 1.1236 loss -1.1236 (14.384 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2828 loss -1.2828 (14.265 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 92200 lr 7.468e-06 [train_loss] tar_ll 1.0889 loss -1.0889 (14.502 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 92400 lr 7.092e-06 [train_loss] tar_ll 1.1786 loss -1.1786 (13.980 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 92600 lr 6.725e-06 [train_loss] tar_ll 1.2260 loss -1.2260 (14.226 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 92800 lr 6.368e-06 [train_loss] tar_ll 1.1362 loss -1.1362 (14.142 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 93000 lr 6.021e-06 [train_loss] tar_ll 1.1777 loss -1.1777 (14.108 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 93200 lr 5.683e-06 [train_loss] tar_ll 1.1337 loss -1.1337 (13.987 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 93400 lr 5.355e-06 [train_loss] tar_ll 1.1905 loss -1.1905 (14.004 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 93600 lr 5.036e-06 [train_loss] tar_ll 1.2030 loss -1.2030 (13.812 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 93800 lr 4.727e-06 [train_loss] tar_ll 1.1395 loss -1.1395 (13.600 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 94000 lr 4.428e-06 [train_loss] tar_ll 1.2135 loss -1.2135 (13.683 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 94200 lr 4.139e-06 [train_loss] tar_ll 1.2159 loss -1.2159 (13.701 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 94400 lr 3.859e-06 [train_loss] tar_ll 1.2360 loss -1.2360 (14.383 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 94600 lr 3.589e-06 [train_loss] tar_ll 1.2269 loss -1.2269 (14.157 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 94800 lr 3.329e-06 [train_loss] tar_ll 1.1112 loss -1.1112 (14.097 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 95000 lr 3.078e-06 [train_loss] tar_ll 1.2690 loss -1.2690 (14.166 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.37it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -2.6862 loss 2.6862 (66.132 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8_periodic step 95200 lr 2.837e-06 [train_loss] tar_ll 1.2152 loss -1.2152 (13.925 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 95400 lr 2.606e-06 [train_loss] tar_ll 1.1951 loss -1.1951 (13.937 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 95600 lr 2.385e-06 [train_loss] tar_ll 1.1077 loss -1.1077 (14.015 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 95800 lr 2.173e-06 [train_loss] tar_ll 1.1584 loss -1.1584 (13.822 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 96000 lr 1.971e-06 [train_loss] tar_ll 1.1123 loss -1.1123 (13.626 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 96200 lr 1.779e-06 [train_loss] tar_ll 1.1927 loss -1.1927 (13.856 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 96400 lr 1.597e-06 [train_loss] tar_ll 1.2031 loss -1.2031 (14.151 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 96600 lr 1.425e-06 [train_loss] tar_ll 1.2021 loss -1.2021 (13.859 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 96800 lr 1.262e-06 [train_loss] tar_ll 1.2145 loss -1.2145 (14.159 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 97000 lr 1.110e-06 [train_loss] tar_ll 1.1418 loss -1.1418 (14.272 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 97200 lr 9.666e-07 [train_loss] tar_ll 1.2336 loss -1.2336 (14.388 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 97400 lr 8.335e-07 [train_loss] tar_ll 1.1726 loss -1.1726 (14.229 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 97600 lr 7.103e-07 [train_loss] tar_ll 1.1364 loss -1.1364 (13.998 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 97800 lr 5.969e-07 [train_loss] tar_ll 1.1697 loss -1.1697 (13.823 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 98000 lr 4.933e-07 [train_loss] tar_ll 1.0969 loss -1.0969 (13.821 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 98200 lr 3.996e-07 [train_loss] tar_ll 1.1342 loss -1.1342 (13.958 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 98400 lr 3.158e-07 [train_loss] tar_ll 1.1183 loss -1.1183 (13.921 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 98600 lr 2.418e-07 [train_loss] tar_ll 1.1851 loss -1.1851 (13.654 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 98800 lr 1.776e-07 [train_loss] tar_ll 1.1671 loss -1.1671 (13.916 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 99000 lr 1.234e-07 [train_loss] tar_ll 1.2029 loss -1.2029 (14.168 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 99200 lr 7.895e-08 [train_loss] tar_ll 1.2172 loss -1.2172 (14.220 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 99400 lr 4.441e-08 [train_loss] tar_ll 1.1402 loss -1.1402 (13.899 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 99600 lr 1.974e-08 [train_loss] tar_ll 1.1857 loss -1.1857 (13.937 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 99800 lr 4.935e-09 [train_loss] tar_ll 1.1755 loss -1.1755 (13.774 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic step 100000 lr 0.000e+00 [train_loss] tar_ll 1.2069 loss -1.2069 (13.728 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.44it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -2.6590 loss 2.6590 (67.506 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.21it/s]\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -2.6590 loss 2.6590 (66.363 secs)\n",
      "lbanp:lbanp-num_latents-8_periodic periodic tar_ll -2.6590 loss 2.6590 (66.363 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 8742994.0 miliseconds\n",
      "Execution time: 8742.994 seconds\n",
      "Initial Memory Usage: 0.0 MB\n",
      "Final Memory Usage: 55.13037109375 MB\n",
      "Memory Usage Change: 55.13037109375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='lbanp', name='lbanp-num_latents-8_periodic',val_seed=100, val_l=8,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b62aa55-c8b1-4aa3-aa0d-bac070c174a4",
   "metadata": {},
   "source": [
    "## LBANP (128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "038f86cd-22d1-4190-81d8-9494e7c8028f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: lbanp-lbanp-num_latents-128_periodic\n",
      "Total number of parameters: 792514\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-128_periodic step 200 lr 5.000e-04 [train_loss] tar_ll -0.6829 loss 0.6829 (14.200 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 400 lr 5.000e-04 [train_loss] tar_ll -0.6637 loss 0.6637 (14.191 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 600 lr 5.000e-04 [train_loss] tar_ll -0.5277 loss 0.5277 (14.014 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 800 lr 4.999e-04 [train_loss] tar_ll -0.4684 loss 0.4684 (14.141 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 1000 lr 4.999e-04 [train_loss] tar_ll -0.4012 loss 0.4012 (14.004 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 1200 lr 4.998e-04 [train_loss] tar_ll -0.3501 loss 0.3501 (13.887 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 1400 lr 4.998e-04 [train_loss] tar_ll -0.2364 loss 0.2364 (14.480 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 1600 lr 4.997e-04 [train_loss] tar_ll -0.1301 loss 0.1301 (14.299 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 1800 lr 4.996e-04 [train_loss] tar_ll -0.0920 loss 0.0920 (14.225 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 2000 lr 4.995e-04 [train_loss] tar_ll -0.0937 loss 0.0937 (14.350 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 2200 lr 4.994e-04 [train_loss] tar_ll 0.0484 loss -0.0484 (14.404 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 2400 lr 4.993e-04 [train_loss] tar_ll 0.1064 loss -0.1064 (14.414 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1245 loss -0.1245 (14.231 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 2800 lr 4.990e-04 [train_loss] tar_ll 0.1424 loss -0.1424 (14.327 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 3000 lr 4.989e-04 [train_loss] tar_ll 0.1680 loss -0.1680 (14.328 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 3200 lr 4.987e-04 [train_loss] tar_ll 0.1674 loss -0.1674 (14.598 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 3400 lr 4.986e-04 [train_loss] tar_ll 0.2587 loss -0.2587 (14.244 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2543 loss -0.2543 (13.988 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 3800 lr 4.982e-04 [train_loss] tar_ll 0.2699 loss -0.2699 (13.922 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 4000 lr 4.980e-04 [train_loss] tar_ll 0.3069 loss -0.3069 (14.018 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 4200 lr 4.978e-04 [train_loss] tar_ll 0.1177 loss -0.1177 (14.529 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 4400 lr 4.976e-04 [train_loss] tar_ll 0.2638 loss -0.2638 (14.328 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 4600 lr 4.974e-04 [train_loss] tar_ll 0.3593 loss -0.3593 (13.982 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4003 loss -0.4003 (14.000 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 5000 lr 4.969e-04 [train_loss] tar_ll 0.3810 loss -0.3810 (14.566 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.25it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.4866 loss 1.4866 (67.799 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 5200 lr 4.967e-04 [train_loss] tar_ll 0.2169 loss -0.2169 (14.429 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 5400 lr 4.964e-04 [train_loss] tar_ll 0.2248 loss -0.2248 (14.898 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 5600 lr 4.961e-04 [train_loss] tar_ll 0.1831 loss -0.1831 (14.372 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 5800 lr 4.959e-04 [train_loss] tar_ll 0.3211 loss -0.3211 (14.173 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 6000 lr 4.956e-04 [train_loss] tar_ll 0.1224 loss -0.1224 (13.958 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 6200 lr 4.953e-04 [train_loss] tar_ll 0.1059 loss -0.1059 (14.128 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 6400 lr 4.950e-04 [train_loss] tar_ll 0.2977 loss -0.2977 (14.226 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 6600 lr 4.946e-04 [train_loss] tar_ll 0.4121 loss -0.4121 (13.936 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 6800 lr 4.943e-04 [train_loss] tar_ll 0.4012 loss -0.4012 (13.956 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 7000 lr 4.940e-04 [train_loss] tar_ll 0.2570 loss -0.2570 (14.173 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 7200 lr 4.936e-04 [train_loss] tar_ll 0.2845 loss -0.2845 (13.910 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 7400 lr 4.933e-04 [train_loss] tar_ll 0.2987 loss -0.2987 (14.025 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 7600 lr 4.929e-04 [train_loss] tar_ll 0.3682 loss -0.3682 (14.546 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 7800 lr 4.925e-04 [train_loss] tar_ll 0.3675 loss -0.3675 (14.349 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 8000 lr 4.921e-04 [train_loss] tar_ll 0.3138 loss -0.3138 (14.234 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 8200 lr 4.918e-04 [train_loss] tar_ll 0.3464 loss -0.3464 (14.438 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 8400 lr 4.913e-04 [train_loss] tar_ll 0.3465 loss -0.3465 (14.447 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 8600 lr 4.909e-04 [train_loss] tar_ll 0.2177 loss -0.2177 (14.164 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 8800 lr 4.905e-04 [train_loss] tar_ll 0.4365 loss -0.4365 (14.510 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 9000 lr 4.901e-04 [train_loss] tar_ll 0.4251 loss -0.4251 (14.467 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 9200 lr 4.896e-04 [train_loss] tar_ll 0.5052 loss -0.5052 (14.080 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 9400 lr 4.892e-04 [train_loss] tar_ll 0.5166 loss -0.5166 (14.036 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 9600 lr 4.887e-04 [train_loss] tar_ll 0.4170 loss -0.4170 (14.114 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 9800 lr 4.882e-04 [train_loss] tar_ll 0.4326 loss -0.4326 (14.218 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 10000 lr 4.878e-04 [train_loss] tar_ll 0.4246 loss -0.4246 (14.260 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.30it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.9370 loss 1.9370 (66.227 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 10200 lr 4.873e-04 [train_loss] tar_ll 0.3951 loss -0.3951 (14.318 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 10400 lr 4.868e-04 [train_loss] tar_ll 0.3384 loss -0.3384 (14.330 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 10600 lr 4.863e-04 [train_loss] tar_ll 0.5191 loss -0.5191 (14.461 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 10800 lr 4.857e-04 [train_loss] tar_ll 0.4461 loss -0.4461 (14.300 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 11000 lr 4.852e-04 [train_loss] tar_ll 0.5130 loss -0.5130 (14.162 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 11200 lr 4.847e-04 [train_loss] tar_ll 0.5578 loss -0.5578 (14.293 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 11400 lr 4.841e-04 [train_loss] tar_ll 0.6068 loss -0.6068 (14.353 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 11600 lr 4.836e-04 [train_loss] tar_ll 0.4833 loss -0.4833 (13.878 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 11800 lr 4.830e-04 [train_loss] tar_ll 0.4558 loss -0.4558 (13.999 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 12000 lr 4.824e-04 [train_loss] tar_ll 0.4436 loss -0.4436 (14.151 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 12200 lr 4.819e-04 [train_loss] tar_ll 0.2175 loss -0.2175 (14.368 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 12400 lr 4.813e-04 [train_loss] tar_ll 0.2905 loss -0.2905 (14.020 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 12600 lr 4.807e-04 [train_loss] tar_ll 0.4280 loss -0.4280 (14.095 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 12800 lr 4.801e-04 [train_loss] tar_ll 0.3079 loss -0.3079 (14.038 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 13000 lr 4.794e-04 [train_loss] tar_ll 0.3939 loss -0.3939 (14.118 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 13200 lr 4.788e-04 [train_loss] tar_ll 0.3797 loss -0.3797 (14.327 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 13400 lr 4.782e-04 [train_loss] tar_ll 0.4759 loss -0.4759 (14.464 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 13600 lr 4.775e-04 [train_loss] tar_ll 0.4860 loss -0.4860 (14.287 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 13800 lr 4.769e-04 [train_loss] tar_ll 0.4524 loss -0.4524 (14.585 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 14000 lr 4.762e-04 [train_loss] tar_ll 0.3044 loss -0.3044 (14.438 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 14200 lr 4.755e-04 [train_loss] tar_ll 0.3732 loss -0.3732 (14.334 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 14400 lr 4.749e-04 [train_loss] tar_ll 0.2474 loss -0.2474 (14.336 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 14600 lr 4.742e-04 [train_loss] tar_ll 0.2895 loss -0.2895 (14.717 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 14800 lr 4.735e-04 [train_loss] tar_ll 0.2143 loss -0.2143 (14.373 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 15000 lr 4.728e-04 [train_loss] tar_ll 0.3577 loss -0.3577 (14.088 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 44.86it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.6249 loss 1.6249 (66.875 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 15200 lr 4.720e-04 [train_loss] tar_ll 0.4429 loss -0.4429 (13.993 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 15400 lr 4.713e-04 [train_loss] tar_ll 0.5004 loss -0.5004 (13.713 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 15600 lr 4.706e-04 [train_loss] tar_ll 0.5348 loss -0.5348 (14.060 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 15800 lr 4.698e-04 [train_loss] tar_ll 0.4206 loss -0.4206 (14.258 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 16000 lr 4.691e-04 [train_loss] tar_ll 0.4855 loss -0.4855 (14.085 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 16200 lr 4.683e-04 [train_loss] tar_ll 0.4814 loss -0.4814 (14.725 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 16400 lr 4.675e-04 [train_loss] tar_ll 0.5270 loss -0.5270 (14.609 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 16600 lr 4.668e-04 [train_loss] tar_ll 0.1504 loss -0.1504 (14.394 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 16800 lr 4.660e-04 [train_loss] tar_ll -0.0885 loss 0.0885 (14.503 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 17000 lr 4.652e-04 [train_loss] tar_ll 0.0879 loss -0.0879 (14.505 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 17200 lr 4.644e-04 [train_loss] tar_ll 0.2432 loss -0.2432 (14.439 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 17400 lr 4.636e-04 [train_loss] tar_ll 0.2426 loss -0.2426 (14.083 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 17600 lr 4.627e-04 [train_loss] tar_ll 0.2128 loss -0.2128 (14.242 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 17800 lr 4.619e-04 [train_loss] tar_ll 0.1518 loss -0.1518 (14.193 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 18000 lr 4.611e-04 [train_loss] tar_ll -0.1036 loss 0.1036 (14.284 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 18200 lr 4.602e-04 [train_loss] tar_ll -0.0478 loss 0.0478 (14.115 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 18400 lr 4.594e-04 [train_loss] tar_ll -0.2058 loss 0.2058 (14.130 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 18600 lr 4.585e-04 [train_loss] tar_ll -0.1576 loss 0.1576 (14.700 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 18800 lr 4.576e-04 [train_loss] tar_ll 0.0003 loss -0.0003 (14.416 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 19000 lr 4.568e-04 [train_loss] tar_ll 0.1762 loss -0.1762 (14.339 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 19200 lr 4.559e-04 [train_loss] tar_ll 0.1304 loss -0.1304 (14.397 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 19400 lr 4.550e-04 [train_loss] tar_ll 0.1518 loss -0.1518 (14.188 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 19600 lr 4.541e-04 [train_loss] tar_ll 0.0048 loss -0.0048 (14.335 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 19800 lr 4.532e-04 [train_loss] tar_ll -0.2079 loss 0.2079 (14.141 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 20000 lr 4.523e-04 [train_loss] tar_ll -0.0451 loss 0.0451 (14.197 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.30it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.4710 loss 1.4710 (67.720 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 20200 lr 4.513e-04 [train_loss] tar_ll 0.1748 loss -0.1748 (13.829 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 20400 lr 4.504e-04 [train_loss] tar_ll -0.1868 loss 0.1868 (14.173 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 20600 lr 4.494e-04 [train_loss] tar_ll -0.2688 loss 0.2688 (14.125 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 20800 lr 4.485e-04 [train_loss] tar_ll -0.2385 loss 0.2385 (13.858 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 21000 lr 4.475e-04 [train_loss] tar_ll -0.1338 loss 0.1338 (13.867 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 21200 lr 4.466e-04 [train_loss] tar_ll -0.0804 loss 0.0804 (13.765 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 21400 lr 4.456e-04 [train_loss] tar_ll -0.1555 loss 0.1555 (14.406 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 21600 lr 4.446e-04 [train_loss] tar_ll -0.0967 loss 0.0967 (14.504 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 21800 lr 4.436e-04 [train_loss] tar_ll 0.0406 loss -0.0406 (14.484 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 22000 lr 4.426e-04 [train_loss] tar_ll 0.1013 loss -0.1013 (14.440 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 22200 lr 4.416e-04 [train_loss] tar_ll -0.0419 loss 0.0419 (14.444 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 22400 lr 4.406e-04 [train_loss] tar_ll -0.0319 loss 0.0319 (14.594 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 22600 lr 4.396e-04 [train_loss] tar_ll 0.0918 loss -0.0918 (14.699 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 22800 lr 4.386e-04 [train_loss] tar_ll 0.1568 loss -0.1568 (14.569 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 23000 lr 4.375e-04 [train_loss] tar_ll 0.1950 loss -0.1950 (14.077 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 23200 lr 4.365e-04 [train_loss] tar_ll 0.2470 loss -0.2470 (14.204 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 23400 lr 4.354e-04 [train_loss] tar_ll 0.1927 loss -0.1927 (14.090 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 23600 lr 4.344e-04 [train_loss] tar_ll 0.0160 loss -0.0160 (14.253 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 23800 lr 4.333e-04 [train_loss] tar_ll 0.0998 loss -0.0998 (14.260 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 24000 lr 4.322e-04 [train_loss] tar_ll 0.0991 loss -0.0991 (14.077 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 24200 lr 4.312e-04 [train_loss] tar_ll -0.0037 loss 0.0037 (14.131 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 24400 lr 4.301e-04 [train_loss] tar_ll -0.3912 loss 0.3912 (14.001 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 24600 lr 4.290e-04 [train_loss] tar_ll -0.2996 loss 0.2996 (13.973 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 24800 lr 4.279e-04 [train_loss] tar_ll -0.2655 loss 0.2655 (14.589 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 25000 lr 4.268e-04 [train_loss] tar_ll -0.1920 loss 0.1920 (14.390 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.38it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.0912 loss 1.0912 (67.594 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 25200 lr 4.257e-04 [train_loss] tar_ll -0.1600 loss 0.1600 (14.691 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 25400 lr 4.245e-04 [train_loss] tar_ll -0.1610 loss 0.1610 (14.470 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 25600 lr 4.234e-04 [train_loss] tar_ll -0.1490 loss 0.1490 (14.011 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 25800 lr 4.223e-04 [train_loss] tar_ll -0.0907 loss 0.0907 (14.194 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 26000 lr 4.211e-04 [train_loss] tar_ll -0.1015 loss 0.1015 (14.205 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 26200 lr 4.200e-04 [train_loss] tar_ll -0.0766 loss 0.0766 (14.284 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 26400 lr 4.188e-04 [train_loss] tar_ll -0.1239 loss 0.1239 (14.142 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 26600 lr 4.177e-04 [train_loss] tar_ll -0.0755 loss 0.0755 (13.871 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 26800 lr 4.165e-04 [train_loss] tar_ll -0.1065 loss 0.1065 (14.180 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 27000 lr 4.153e-04 [train_loss] tar_ll -0.0867 loss 0.0867 (14.183 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 27200 lr 4.141e-04 [train_loss] tar_ll -0.0523 loss 0.0523 (14.550 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 27400 lr 4.130e-04 [train_loss] tar_ll -0.0716 loss 0.0716 (14.518 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 27600 lr 4.118e-04 [train_loss] tar_ll -0.1680 loss 0.1680 (14.273 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 27800 lr 4.106e-04 [train_loss] tar_ll -0.1649 loss 0.1649 (14.220 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 28000 lr 4.094e-04 [train_loss] tar_ll -0.1074 loss 0.1074 (14.212 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 28200 lr 4.081e-04 [train_loss] tar_ll -0.0961 loss 0.0961 (14.372 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 28400 lr 4.069e-04 [train_loss] tar_ll -0.1088 loss 0.1088 (14.599 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 28600 lr 4.057e-04 [train_loss] tar_ll -0.1551 loss 0.1551 (14.752 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 28800 lr 4.045e-04 [train_loss] tar_ll -0.1088 loss 0.1088 (14.283 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 29000 lr 4.032e-04 [train_loss] tar_ll -0.1001 loss 0.1001 (13.779 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 29200 lr 4.020e-04 [train_loss] tar_ll -0.0468 loss 0.0468 (14.310 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 29400 lr 4.007e-04 [train_loss] tar_ll -0.0292 loss 0.0292 (14.376 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 29600 lr 3.995e-04 [train_loss] tar_ll -0.0055 loss 0.0055 (13.942 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 29800 lr 3.982e-04 [train_loss] tar_ll -0.0108 loss 0.0108 (14.079 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 30000 lr 3.969e-04 [train_loss] tar_ll -0.0588 loss 0.0588 (13.911 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.25it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.1249 loss 1.1249 (67.806 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 30200 lr 3.957e-04 [train_loss] tar_ll -0.0306 loss 0.0306 (14.679 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 30400 lr 3.944e-04 [train_loss] tar_ll -0.0135 loss 0.0135 (14.397 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 30600 lr 3.931e-04 [train_loss] tar_ll 0.0364 loss -0.0364 (14.254 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 30800 lr 3.918e-04 [train_loss] tar_ll 0.0303 loss -0.0303 (14.835 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 31000 lr 3.905e-04 [train_loss] tar_ll -0.0757 loss 0.0757 (14.616 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 31200 lr 3.892e-04 [train_loss] tar_ll 0.0001 loss -0.0001 (14.266 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 31400 lr 3.879e-04 [train_loss] tar_ll 0.0499 loss -0.0499 (14.071 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 31600 lr 3.866e-04 [train_loss] tar_ll 0.0372 loss -0.0372 (14.253 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 31800 lr 3.853e-04 [train_loss] tar_ll 0.0747 loss -0.0747 (13.972 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 32000 lr 3.840e-04 [train_loss] tar_ll 0.0524 loss -0.0524 (13.959 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 32200 lr 3.826e-04 [train_loss] tar_ll 0.0706 loss -0.0706 (14.016 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 32400 lr 3.813e-04 [train_loss] tar_ll 0.0740 loss -0.0740 (14.023 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 32600 lr 3.800e-04 [train_loss] tar_ll 0.0105 loss -0.0105 (13.793 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 32800 lr 3.786e-04 [train_loss] tar_ll -0.0728 loss 0.0728 (14.012 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 33000 lr 3.773e-04 [train_loss] tar_ll -0.0945 loss 0.0945 (14.237 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 33200 lr 3.759e-04 [train_loss] tar_ll -0.0657 loss 0.0657 (14.337 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 33400 lr 3.745e-04 [train_loss] tar_ll -0.0913 loss 0.0913 (14.435 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 33600 lr 3.732e-04 [train_loss] tar_ll -0.0566 loss 0.0566 (14.663 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 33800 lr 3.718e-04 [train_loss] tar_ll -0.0521 loss 0.0521 (14.275 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 34000 lr 3.704e-04 [train_loss] tar_ll -0.0166 loss 0.0166 (14.514 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 34200 lr 3.691e-04 [train_loss] tar_ll -0.0331 loss 0.0331 (14.542 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 34400 lr 3.677e-04 [train_loss] tar_ll 0.0166 loss -0.0166 (14.586 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 34600 lr 3.663e-04 [train_loss] tar_ll -0.0041 loss 0.0041 (14.180 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 34800 lr 3.649e-04 [train_loss] tar_ll 0.0541 loss -0.0541 (14.018 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 35000 lr 3.635e-04 [train_loss] tar_ll -0.0129 loss 0.0129 (14.070 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 44.82it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -0.9934 loss 0.9934 (66.945 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 35200 lr 3.621e-04 [train_loss] tar_ll 0.0715 loss -0.0715 (14.585 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 35400 lr 3.607e-04 [train_loss] tar_ll 0.0497 loss -0.0497 (14.115 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 35600 lr 3.593e-04 [train_loss] tar_ll 0.0123 loss -0.0123 (14.484 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 35800 lr 3.579e-04 [train_loss] tar_ll 0.0092 loss -0.0092 (14.399 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 36000 lr 3.564e-04 [train_loss] tar_ll 0.0361 loss -0.0361 (14.494 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 36200 lr 3.550e-04 [train_loss] tar_ll -0.0051 loss 0.0051 (14.421 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 36400 lr 3.536e-04 [train_loss] tar_ll 0.0326 loss -0.0326 (14.458 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 36600 lr 3.522e-04 [train_loss] tar_ll 0.0505 loss -0.0505 (14.127 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 36800 lr 3.507e-04 [train_loss] tar_ll 0.0775 loss -0.0775 (14.189 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 37000 lr 3.493e-04 [train_loss] tar_ll 0.0994 loss -0.0994 (14.165 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 37200 lr 3.478e-04 [train_loss] tar_ll 0.1183 loss -0.1183 (14.148 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 37400 lr 3.464e-04 [train_loss] tar_ll 0.1274 loss -0.1274 (14.429 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 37600 lr 3.449e-04 [train_loss] tar_ll 0.1102 loss -0.1102 (14.121 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 37800 lr 3.435e-04 [train_loss] tar_ll 0.0547 loss -0.0547 (14.017 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 38000 lr 3.420e-04 [train_loss] tar_ll 0.0288 loss -0.0288 (14.097 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 38200 lr 3.406e-04 [train_loss] tar_ll 0.1157 loss -0.1157 (14.073 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 38400 lr 3.391e-04 [train_loss] tar_ll 0.1425 loss -0.1425 (13.868 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 38600 lr 3.376e-04 [train_loss] tar_ll 0.1571 loss -0.1571 (13.904 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 38800 lr 3.362e-04 [train_loss] tar_ll 0.0986 loss -0.0986 (14.352 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 39000 lr 3.347e-04 [train_loss] tar_ll 0.1700 loss -0.1700 (14.802 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 39200 lr 3.332e-04 [train_loss] tar_ll 0.1471 loss -0.1471 (14.716 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 39400 lr 3.317e-04 [train_loss] tar_ll 0.1697 loss -0.1697 (14.378 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 39600 lr 3.302e-04 [train_loss] tar_ll 0.1940 loss -0.1940 (14.268 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 39800 lr 3.287e-04 [train_loss] tar_ll 0.1730 loss -0.1730 (14.585 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 40000 lr 3.273e-04 [train_loss] tar_ll 0.2025 loss -0.2025 (14.481 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 44.82it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.3077 loss 1.3077 (66.941 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 40200 lr 3.258e-04 [train_loss] tar_ll 0.1888 loss -0.1888 (13.955 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 40400 lr 3.243e-04 [train_loss] tar_ll 0.2249 loss -0.2249 (13.918 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 40600 lr 3.228e-04 [train_loss] tar_ll 0.2187 loss -0.2187 (14.118 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 40800 lr 3.213e-04 [train_loss] tar_ll 0.2258 loss -0.2258 (14.144 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 41000 lr 3.197e-04 [train_loss] tar_ll 0.2506 loss -0.2506 (14.269 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 41200 lr 3.182e-04 [train_loss] tar_ll 0.2060 loss -0.2060 (14.493 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 41400 lr 3.167e-04 [train_loss] tar_ll 0.2378 loss -0.2378 (14.515 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 41600 lr 3.152e-04 [train_loss] tar_ll 0.2655 loss -0.2655 (14.371 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 41800 lr 3.137e-04 [train_loss] tar_ll 0.1794 loss -0.1794 (14.407 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 42000 lr 3.122e-04 [train_loss] tar_ll 0.1762 loss -0.1762 (14.273 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 42200 lr 3.106e-04 [train_loss] tar_ll 0.2025 loss -0.2025 (14.422 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 42400 lr 3.091e-04 [train_loss] tar_ll 0.2250 loss -0.2250 (14.110 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 42600 lr 3.076e-04 [train_loss] tar_ll 0.2887 loss -0.2887 (14.147 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 42800 lr 3.061e-04 [train_loss] tar_ll 0.1961 loss -0.1961 (14.200 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 43000 lr 3.045e-04 [train_loss] tar_ll 0.2703 loss -0.2703 (14.233 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 43200 lr 3.030e-04 [train_loss] tar_ll 0.2523 loss -0.2523 (14.344 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 43400 lr 3.015e-04 [train_loss] tar_ll 0.2520 loss -0.2520 (14.181 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 43600 lr 2.999e-04 [train_loss] tar_ll 0.1916 loss -0.1916 (13.844 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 43800 lr 2.984e-04 [train_loss] tar_ll 0.2275 loss -0.2275 (14.361 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 44000 lr 2.968e-04 [train_loss] tar_ll 0.2770 loss -0.2770 (14.028 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 44200 lr 2.953e-04 [train_loss] tar_ll 0.2646 loss -0.2646 (14.340 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 44400 lr 2.938e-04 [train_loss] tar_ll 0.3052 loss -0.3052 (14.527 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 44600 lr 2.922e-04 [train_loss] tar_ll 0.2882 loss -0.2882 (14.381 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 44800 lr 2.907e-04 [train_loss] tar_ll 0.2678 loss -0.2678 (14.530 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 45000 lr 2.891e-04 [train_loss] tar_ll 0.3499 loss -0.3499 (14.348 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.34it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.3353 loss 1.3353 (67.663 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 45200 lr 2.876e-04 [train_loss] tar_ll 0.2366 loss -0.2366 (13.876 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 45400 lr 2.860e-04 [train_loss] tar_ll 0.3015 loss -0.3015 (13.893 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 45600 lr 2.844e-04 [train_loss] tar_ll 0.2540 loss -0.2540 (14.227 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 45800 lr 2.829e-04 [train_loss] tar_ll 0.2538 loss -0.2538 (13.976 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 46000 lr 2.813e-04 [train_loss] tar_ll 0.2625 loss -0.2625 (14.209 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 46200 lr 2.798e-04 [train_loss] tar_ll 0.4027 loss -0.4027 (14.270 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 46400 lr 2.782e-04 [train_loss] tar_ll 0.3348 loss -0.3348 (14.325 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 46600 lr 2.767e-04 [train_loss] tar_ll 0.2740 loss -0.2740 (14.389 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 46800 lr 2.751e-04 [train_loss] tar_ll 0.3808 loss -0.3808 (14.442 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 47000 lr 2.735e-04 [train_loss] tar_ll 0.2994 loss -0.2994 (14.271 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 47200 lr 2.720e-04 [train_loss] tar_ll 0.3444 loss -0.3444 (14.457 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 47400 lr 2.704e-04 [train_loss] tar_ll 0.3255 loss -0.3255 (14.683 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 47600 lr 2.688e-04 [train_loss] tar_ll 0.3220 loss -0.3220 (14.375 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 47800 lr 2.673e-04 [train_loss] tar_ll 0.4364 loss -0.4364 (14.221 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 48000 lr 2.657e-04 [train_loss] tar_ll 0.3355 loss -0.3355 (14.569 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 48200 lr 2.641e-04 [train_loss] tar_ll 0.3618 loss -0.3618 (14.762 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 48400 lr 2.626e-04 [train_loss] tar_ll 0.3418 loss -0.3418 (14.384 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 48600 lr 2.610e-04 [train_loss] tar_ll 0.4304 loss -0.4304 (13.950 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 48800 lr 2.594e-04 [train_loss] tar_ll 0.4015 loss -0.4015 (13.908 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 49000 lr 2.579e-04 [train_loss] tar_ll 0.3450 loss -0.3450 (14.150 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 49200 lr 2.563e-04 [train_loss] tar_ll 0.3495 loss -0.3495 (14.367 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 49400 lr 2.547e-04 [train_loss] tar_ll 0.3423 loss -0.3423 (14.051 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 49600 lr 2.531e-04 [train_loss] tar_ll 0.3454 loss -0.3454 (14.140 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 49800 lr 2.516e-04 [train_loss] tar_ll 0.3789 loss -0.3789 (13.775 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 50000 lr 2.500e-04 [train_loss] tar_ll 0.3623 loss -0.3623 (13.746 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.68it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.2821 loss 1.2821 (67.147 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 50200 lr 2.484e-04 [train_loss] tar_ll 0.4096 loss -0.4096 (14.424 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 50400 lr 2.469e-04 [train_loss] tar_ll 0.3434 loss -0.3434 (14.383 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 50600 lr 2.453e-04 [train_loss] tar_ll 0.4094 loss -0.4094 (14.407 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 50800 lr 2.437e-04 [train_loss] tar_ll 0.3938 loss -0.3938 (14.157 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 51000 lr 2.421e-04 [train_loss] tar_ll 0.4220 loss -0.4220 (14.065 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 51200 lr 2.406e-04 [train_loss] tar_ll 0.3891 loss -0.3891 (14.193 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 51400 lr 2.390e-04 [train_loss] tar_ll 0.3912 loss -0.3912 (14.010 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 51600 lr 2.374e-04 [train_loss] tar_ll 0.4279 loss -0.4279 (14.278 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 51800 lr 2.359e-04 [train_loss] tar_ll 0.4743 loss -0.4743 (13.891 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 52000 lr 2.343e-04 [train_loss] tar_ll 0.4230 loss -0.4230 (14.117 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 52200 lr 2.327e-04 [train_loss] tar_ll 0.4255 loss -0.4255 (14.407 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 52400 lr 2.312e-04 [train_loss] tar_ll 0.4485 loss -0.4485 (14.213 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 52600 lr 2.296e-04 [train_loss] tar_ll 0.3787 loss -0.3787 (14.112 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 52800 lr 2.280e-04 [train_loss] tar_ll 0.4223 loss -0.4223 (14.187 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 53000 lr 2.265e-04 [train_loss] tar_ll 0.4561 loss -0.4561 (14.258 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 53200 lr 2.249e-04 [train_loss] tar_ll 0.4420 loss -0.4420 (14.342 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 53400 lr 2.233e-04 [train_loss] tar_ll 0.4298 loss -0.4298 (14.664 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 53600 lr 2.218e-04 [train_loss] tar_ll 0.4696 loss -0.4696 (14.485 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 53800 lr 2.202e-04 [train_loss] tar_ll 0.4429 loss -0.4429 (14.183 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 54000 lr 2.187e-04 [train_loss] tar_ll 0.4198 loss -0.4198 (14.113 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 54200 lr 2.171e-04 [train_loss] tar_ll 0.4303 loss -0.4303 (14.250 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 54400 lr 2.156e-04 [train_loss] tar_ll 0.3774 loss -0.3774 (13.958 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 54600 lr 2.140e-04 [train_loss] tar_ll 0.3867 loss -0.3867 (14.169 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 54800 lr 2.124e-04 [train_loss] tar_ll 0.4544 loss -0.4544 (14.267 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 55000 lr 2.109e-04 [train_loss] tar_ll 0.4707 loss -0.4707 (14.535 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 44.79it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.4384 loss 1.4384 (66.984 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 55200 lr 2.093e-04 [train_loss] tar_ll 0.3900 loss -0.3900 (14.202 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 55400 lr 2.078e-04 [train_loss] tar_ll 0.4796 loss -0.4796 (14.097 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 55600 lr 2.062e-04 [train_loss] tar_ll 0.4967 loss -0.4967 (14.228 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 55800 lr 2.047e-04 [train_loss] tar_ll 0.5009 loss -0.5009 (14.331 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 56000 lr 2.032e-04 [train_loss] tar_ll 0.4440 loss -0.4440 (14.275 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 56200 lr 2.016e-04 [train_loss] tar_ll 0.4513 loss -0.4513 (14.546 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 56400 lr 2.001e-04 [train_loss] tar_ll 0.4496 loss -0.4496 (14.542 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 56600 lr 1.985e-04 [train_loss] tar_ll 0.4114 loss -0.4114 (14.378 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 56800 lr 1.970e-04 [train_loss] tar_ll 0.4682 loss -0.4682 (14.141 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 57000 lr 1.955e-04 [train_loss] tar_ll 0.5278 loss -0.5278 (14.049 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 57200 lr 1.939e-04 [train_loss] tar_ll 0.4410 loss -0.4410 (14.139 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 57400 lr 1.924e-04 [train_loss] tar_ll 0.5244 loss -0.5244 (14.068 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 57600 lr 1.909e-04 [train_loss] tar_ll 0.4288 loss -0.4288 (13.876 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 57800 lr 1.894e-04 [train_loss] tar_ll 0.4954 loss -0.4954 (14.012 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 58000 lr 1.878e-04 [train_loss] tar_ll 0.5140 loss -0.5140 (14.223 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 58200 lr 1.863e-04 [train_loss] tar_ll 0.5200 loss -0.5200 (14.215 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 58400 lr 1.848e-04 [train_loss] tar_ll 0.4749 loss -0.4749 (14.211 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 58600 lr 1.833e-04 [train_loss] tar_ll 0.5697 loss -0.5697 (14.154 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 58800 lr 1.818e-04 [train_loss] tar_ll 0.5341 loss -0.5341 (14.306 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 59000 lr 1.803e-04 [train_loss] tar_ll 0.5058 loss -0.5058 (14.373 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 59200 lr 1.787e-04 [train_loss] tar_ll 0.4985 loss -0.4985 (14.689 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 59400 lr 1.772e-04 [train_loss] tar_ll 0.4379 loss -0.4379 (14.276 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 59600 lr 1.757e-04 [train_loss] tar_ll 0.6052 loss -0.6052 (14.458 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 59800 lr 1.742e-04 [train_loss] tar_ll 0.4802 loss -0.4802 (14.400 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 60000 lr 1.727e-04 [train_loss] tar_ll 0.4821 loss -0.4821 (14.434 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.54it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.6473 loss 1.6473 (67.363 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 60200 lr 1.713e-04 [train_loss] tar_ll 0.5055 loss -0.5055 (14.021 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 60400 lr 1.698e-04 [train_loss] tar_ll 0.5258 loss -0.5258 (14.624 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 60600 lr 1.683e-04 [train_loss] tar_ll 0.5587 loss -0.5587 (14.264 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 60800 lr 1.668e-04 [train_loss] tar_ll 0.5352 loss -0.5352 (14.331 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 61000 lr 1.653e-04 [train_loss] tar_ll 0.4746 loss -0.4746 (14.485 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 61200 lr 1.638e-04 [train_loss] tar_ll 0.5044 loss -0.5044 (14.564 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 61400 lr 1.624e-04 [train_loss] tar_ll 0.5591 loss -0.5591 (14.630 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 61600 lr 1.609e-04 [train_loss] tar_ll 0.6076 loss -0.6076 (14.319 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 61800 lr 1.594e-04 [train_loss] tar_ll 0.5541 loss -0.5541 (14.289 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 62000 lr 1.580e-04 [train_loss] tar_ll 0.5466 loss -0.5466 (14.589 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 62200 lr 1.565e-04 [train_loss] tar_ll 0.5322 loss -0.5322 (14.372 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 62400 lr 1.551e-04 [train_loss] tar_ll 0.5031 loss -0.5031 (14.124 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 62600 lr 1.536e-04 [train_loss] tar_ll 0.5871 loss -0.5871 (13.924 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 62800 lr 1.522e-04 [train_loss] tar_ll 0.5962 loss -0.5962 (14.188 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 63000 lr 1.507e-04 [train_loss] tar_ll 0.6307 loss -0.6307 (13.968 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 63200 lr 1.493e-04 [train_loss] tar_ll 0.5098 loss -0.5098 (14.308 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 63400 lr 1.478e-04 [train_loss] tar_ll 0.4813 loss -0.4813 (13.776 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 63600 lr 1.464e-04 [train_loss] tar_ll 0.5244 loss -0.5244 (14.149 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 63800 lr 1.450e-04 [train_loss] tar_ll 0.5542 loss -0.5542 (13.833 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 64000 lr 1.436e-04 [train_loss] tar_ll 0.4970 loss -0.4970 (14.102 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 64200 lr 1.421e-04 [train_loss] tar_ll 0.5551 loss -0.5551 (14.768 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 64400 lr 1.407e-04 [train_loss] tar_ll 0.4734 loss -0.4734 (14.356 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 64600 lr 1.393e-04 [train_loss] tar_ll 0.5525 loss -0.5525 (14.445 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 64800 lr 1.379e-04 [train_loss] tar_ll 0.5764 loss -0.5764 (14.559 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 65000 lr 1.365e-04 [train_loss] tar_ll 0.5832 loss -0.5832 (14.452 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.63it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.8055 loss 1.8055 (67.223 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 65200 lr 1.351e-04 [train_loss] tar_ll 0.5435 loss -0.5435 (14.087 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 65400 lr 1.337e-04 [train_loss] tar_ll 0.6191 loss -0.6191 (14.161 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 65600 lr 1.323e-04 [train_loss] tar_ll 0.4936 loss -0.4936 (14.058 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 65800 lr 1.309e-04 [train_loss] tar_ll 0.6060 loss -0.6060 (14.144 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 66000 lr 1.296e-04 [train_loss] tar_ll 0.5681 loss -0.5681 (14.151 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 66200 lr 1.282e-04 [train_loss] tar_ll 0.6137 loss -0.6137 (13.877 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 66400 lr 1.268e-04 [train_loss] tar_ll 0.5224 loss -0.5224 (13.911 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 66600 lr 1.255e-04 [train_loss] tar_ll 0.5743 loss -0.5743 (14.319 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 66800 lr 1.241e-04 [train_loss] tar_ll 0.5487 loss -0.5487 (14.366 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 67000 lr 1.227e-04 [train_loss] tar_ll 0.5598 loss -0.5598 (14.586 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 67200 lr 1.214e-04 [train_loss] tar_ll 0.6209 loss -0.6209 (14.674 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 67400 lr 1.200e-04 [train_loss] tar_ll 0.6385 loss -0.6385 (14.174 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 67600 lr 1.187e-04 [train_loss] tar_ll 0.6306 loss -0.6306 (14.551 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 67800 lr 1.174e-04 [train_loss] tar_ll 0.5957 loss -0.5957 (14.389 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 68000 lr 1.160e-04 [train_loss] tar_ll 0.6417 loss -0.6417 (14.325 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 68200 lr 1.147e-04 [train_loss] tar_ll 0.6282 loss -0.6282 (14.344 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 68400 lr 1.134e-04 [train_loss] tar_ll 0.5494 loss -0.5494 (14.080 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 68600 lr 1.121e-04 [train_loss] tar_ll 0.5965 loss -0.5965 (14.083 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 68800 lr 1.108e-04 [train_loss] tar_ll 0.6242 loss -0.6242 (14.273 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 69000 lr 1.095e-04 [train_loss] tar_ll 0.6736 loss -0.6736 (14.279 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 69200 lr 1.082e-04 [train_loss] tar_ll 0.6193 loss -0.6193 (14.215 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 69400 lr 1.069e-04 [train_loss] tar_ll 0.6632 loss -0.6632 (14.043 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 69600 lr 1.056e-04 [train_loss] tar_ll 0.6252 loss -0.6252 (14.287 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 69800 lr 1.043e-04 [train_loss] tar_ll 0.6253 loss -0.6253 (14.409 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 70000 lr 1.031e-04 [train_loss] tar_ll 0.6401 loss -0.6401 (14.340 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.58it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.5285 loss 1.5285 (67.305 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 70200 lr 1.018e-04 [train_loss] tar_ll 0.6123 loss -0.6123 (14.408 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 70400 lr 1.005e-04 [train_loss] tar_ll 0.5751 loss -0.5751 (14.494 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 70600 lr 9.927e-05 [train_loss] tar_ll 0.6234 loss -0.6234 (14.148 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 70800 lr 9.802e-05 [train_loss] tar_ll 0.5888 loss -0.5888 (13.962 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 71000 lr 9.677e-05 [train_loss] tar_ll 0.6142 loss -0.6142 (14.040 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 71200 lr 9.554e-05 [train_loss] tar_ll 0.6499 loss -0.6499 (14.378 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 71400 lr 9.430e-05 [train_loss] tar_ll 0.6223 loss -0.6223 (14.208 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 71600 lr 9.308e-05 [train_loss] tar_ll 0.6677 loss -0.6677 (14.163 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 71800 lr 9.186e-05 [train_loss] tar_ll 0.6165 loss -0.6165 (13.988 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 72000 lr 9.064e-05 [train_loss] tar_ll 0.6402 loss -0.6402 (13.960 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 72200 lr 8.944e-05 [train_loss] tar_ll 0.6727 loss -0.6727 (13.939 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 72400 lr 8.824e-05 [train_loss] tar_ll 0.6094 loss -0.6094 (14.529 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 72600 lr 8.704e-05 [train_loss] tar_ll 0.6009 loss -0.6009 (14.448 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 72800 lr 8.585e-05 [train_loss] tar_ll 0.6430 loss -0.6430 (14.865 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 73000 lr 8.467e-05 [train_loss] tar_ll 0.6101 loss -0.6101 (14.448 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 73200 lr 8.350e-05 [train_loss] tar_ll 0.6523 loss -0.6523 (14.634 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 73400 lr 8.233e-05 [train_loss] tar_ll 0.6401 loss -0.6401 (14.249 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 73600 lr 8.117e-05 [train_loss] tar_ll 0.6292 loss -0.6292 (14.843 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 73800 lr 8.001e-05 [train_loss] tar_ll 0.6496 loss -0.6496 (14.693 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 74000 lr 7.886e-05 [train_loss] tar_ll 0.6211 loss -0.6211 (13.878 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 74200 lr 7.772e-05 [train_loss] tar_ll 0.6051 loss -0.6051 (14.300 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 74400 lr 7.659e-05 [train_loss] tar_ll 0.6324 loss -0.6324 (13.854 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 74600 lr 7.546e-05 [train_loss] tar_ll 0.6291 loss -0.6291 (14.142 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 74800 lr 7.434e-05 [train_loss] tar_ll 0.6540 loss -0.6540 (14.309 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 75000 lr 7.322e-05 [train_loss] tar_ll 0.6352 loss -0.6352 (14.028 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.69it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.6790 loss 1.6790 (67.133 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 75200 lr 7.212e-05 [train_loss] tar_ll 0.6351 loss -0.6351 (14.345 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 75400 lr 7.102e-05 [train_loss] tar_ll 0.6655 loss -0.6655 (14.221 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 75600 lr 6.992e-05 [train_loss] tar_ll 0.6169 loss -0.6169 (14.448 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 75800 lr 6.884e-05 [train_loss] tar_ll 0.6505 loss -0.6505 (14.170 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 76000 lr 6.776e-05 [train_loss] tar_ll 0.6462 loss -0.6462 (14.463 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 76200 lr 6.669e-05 [train_loss] tar_ll 0.6885 loss -0.6885 (14.484 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 76400 lr 6.562e-05 [train_loss] tar_ll 0.6758 loss -0.6758 (14.381 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 76600 lr 6.456e-05 [train_loss] tar_ll 0.6701 loss -0.6701 (14.210 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 76800 lr 6.351e-05 [train_loss] tar_ll 0.6819 loss -0.6819 (14.207 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 77000 lr 6.247e-05 [train_loss] tar_ll 0.6180 loss -0.6180 (14.282 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 77200 lr 6.144e-05 [train_loss] tar_ll 0.7036 loss -0.7036 (14.404 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 77400 lr 6.041e-05 [train_loss] tar_ll 0.6769 loss -0.6769 (14.141 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 77600 lr 5.939e-05 [train_loss] tar_ll 0.6488 loss -0.6488 (13.998 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 77800 lr 5.838e-05 [train_loss] tar_ll 0.6013 loss -0.6013 (14.054 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 78000 lr 5.737e-05 [train_loss] tar_ll 0.7128 loss -0.7128 (13.952 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 78200 lr 5.637e-05 [train_loss] tar_ll 0.6625 loss -0.6625 (14.509 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 78400 lr 5.538e-05 [train_loss] tar_ll 0.6830 loss -0.6830 (14.384 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 78600 lr 5.440e-05 [train_loss] tar_ll 0.7569 loss -0.7569 (14.460 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 78800 lr 5.343e-05 [train_loss] tar_ll 0.6442 loss -0.6442 (14.583 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 79000 lr 5.246e-05 [train_loss] tar_ll 0.6369 loss -0.6369 (14.272 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 79200 lr 5.150e-05 [train_loss] tar_ll 0.7101 loss -0.7101 (14.561 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 79400 lr 5.055e-05 [train_loss] tar_ll 0.7024 loss -0.7024 (14.243 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 79600 lr 4.961e-05 [train_loss] tar_ll 0.6349 loss -0.6349 (14.309 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 79800 lr 4.867e-05 [train_loss] tar_ll 0.7013 loss -0.7013 (13.933 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 80000 lr 4.775e-05 [train_loss] tar_ll 0.6812 loss -0.6812 (13.951 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.26it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.8722 loss 1.8722 (67.791 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 80200 lr 4.683e-05 [train_loss] tar_ll 0.6935 loss -0.6935 (14.325 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 80400 lr 4.592e-05 [train_loss] tar_ll 0.6932 loss -0.6932 (14.427 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 80600 lr 4.501e-05 [train_loss] tar_ll 0.7232 loss -0.7232 (14.474 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 80800 lr 4.412e-05 [train_loss] tar_ll 0.7231 loss -0.7231 (14.405 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 81000 lr 4.323e-05 [train_loss] tar_ll 0.6709 loss -0.6709 (14.644 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 81200 lr 4.235e-05 [train_loss] tar_ll 0.7465 loss -0.7465 (14.460 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 81400 lr 4.148e-05 [train_loss] tar_ll 0.6654 loss -0.6654 (14.230 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 81600 lr 4.062e-05 [train_loss] tar_ll 0.6046 loss -0.6046 (14.564 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 81800 lr 3.976e-05 [train_loss] tar_ll 0.6378 loss -0.6378 (14.774 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 82000 lr 3.892e-05 [train_loss] tar_ll 0.6693 loss -0.6693 (14.675 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 82200 lr 3.808e-05 [train_loss] tar_ll 0.7639 loss -0.7639 (14.040 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 82400 lr 3.725e-05 [train_loss] tar_ll 0.6947 loss -0.6947 (14.135 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 82600 lr 3.643e-05 [train_loss] tar_ll 0.7357 loss -0.7357 (14.117 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 82800 lr 3.562e-05 [train_loss] tar_ll 0.7064 loss -0.7064 (14.038 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 83000 lr 3.481e-05 [train_loss] tar_ll 0.6906 loss -0.6906 (13.965 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 83200 lr 3.402e-05 [train_loss] tar_ll 0.6970 loss -0.6970 (14.226 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 83400 lr 3.323e-05 [train_loss] tar_ll 0.7419 loss -0.7419 (14.128 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 83600 lr 3.245e-05 [train_loss] tar_ll 0.7564 loss -0.7564 (13.690 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 83800 lr 3.168e-05 [train_loss] tar_ll 0.7192 loss -0.7192 (14.246 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 84000 lr 3.092e-05 [train_loss] tar_ll 0.6863 loss -0.6863 (14.435 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 84200 lr 3.017e-05 [train_loss] tar_ll 0.7952 loss -0.7952 (14.420 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 84400 lr 2.943e-05 [train_loss] tar_ll 0.6855 loss -0.6855 (14.645 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 84600 lr 2.869e-05 [train_loss] tar_ll 0.7019 loss -0.7019 (14.571 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 84800 lr 2.797e-05 [train_loss] tar_ll 0.7165 loss -0.7165 (14.330 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 85000 lr 2.725e-05 [train_loss] tar_ll 0.7236 loss -0.7236 (14.751 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.29it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.8000 loss 1.8000 (67.736 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 85200 lr 2.654e-05 [train_loss] tar_ll 0.7094 loss -0.7094 (14.405 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 85400 lr 2.584e-05 [train_loss] tar_ll 0.7580 loss -0.7580 (14.163 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 85600 lr 2.515e-05 [train_loss] tar_ll 0.6985 loss -0.6985 (13.894 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 85800 lr 2.447e-05 [train_loss] tar_ll 0.7069 loss -0.7069 (13.979 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 86000 lr 2.379e-05 [train_loss] tar_ll 0.6935 loss -0.6935 (14.298 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 86200 lr 2.313e-05 [train_loss] tar_ll 0.7591 loss -0.7591 (14.660 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 86400 lr 2.247e-05 [train_loss] tar_ll 0.6550 loss -0.6550 (14.220 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 86600 lr 2.183e-05 [train_loss] tar_ll 0.6946 loss -0.6946 (14.090 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 86800 lr 2.119e-05 [train_loss] tar_ll 0.6304 loss -0.6304 (14.214 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 87000 lr 2.056e-05 [train_loss] tar_ll 0.7378 loss -0.7378 (14.251 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 87200 lr 1.994e-05 [train_loss] tar_ll 0.7304 loss -0.7304 (14.185 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 87400 lr 1.933e-05 [train_loss] tar_ll 0.7000 loss -0.7000 (14.541 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 87600 lr 1.873e-05 [train_loss] tar_ll 0.8451 loss -0.8451 (14.222 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 87800 lr 1.814e-05 [train_loss] tar_ll 0.6988 loss -0.6988 (14.283 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 88000 lr 1.756e-05 [train_loss] tar_ll 0.7495 loss -0.7495 (14.328 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 88200 lr 1.698e-05 [train_loss] tar_ll 0.6867 loss -0.6867 (14.305 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 88400 lr 1.642e-05 [train_loss] tar_ll 0.7181 loss -0.7181 (14.181 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 88600 lr 1.586e-05 [train_loss] tar_ll 0.7938 loss -0.7938 (14.062 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 88800 lr 1.532e-05 [train_loss] tar_ll 0.7612 loss -0.7612 (14.225 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 89000 lr 1.478e-05 [train_loss] tar_ll 0.6808 loss -0.6808 (14.205 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 89200 lr 1.425e-05 [train_loss] tar_ll 0.6917 loss -0.6917 (14.092 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 89400 lr 1.373e-05 [train_loss] tar_ll 0.7183 loss -0.7183 (14.122 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 89600 lr 1.323e-05 [train_loss] tar_ll 0.7291 loss -0.7291 (14.247 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 89800 lr 1.273e-05 [train_loss] tar_ll 0.7400 loss -0.7400 (14.412 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 90000 lr 1.224e-05 [train_loss] tar_ll 0.7354 loss -0.7354 (14.434 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.20it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.8454 loss 1.8454 (67.884 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 90200 lr 1.176e-05 [train_loss] tar_ll 0.7179 loss -0.7179 (14.881 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 90400 lr 1.128e-05 [train_loss] tar_ll 0.7514 loss -0.7514 (13.807 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 90600 lr 1.082e-05 [train_loss] tar_ll 0.7306 loss -0.7306 (14.123 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 90800 lr 1.037e-05 [train_loss] tar_ll 0.7914 loss -0.7914 (14.017 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 91000 lr 9.927e-06 [train_loss] tar_ll 0.7829 loss -0.7829 (14.090 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 91200 lr 9.493e-06 [train_loss] tar_ll 0.7592 loss -0.7592 (14.308 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 91400 lr 9.069e-06 [train_loss] tar_ll 0.7362 loss -0.7362 (14.088 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 91600 lr 8.655e-06 [train_loss] tar_ll 0.6733 loss -0.6733 (13.877 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 91800 lr 8.250e-06 [train_loss] tar_ll 0.6671 loss -0.6671 (14.081 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 92000 lr 7.854e-06 [train_loss] tar_ll 0.8122 loss -0.8122 (14.269 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 92200 lr 7.468e-06 [train_loss] tar_ll 0.7195 loss -0.7195 (14.385 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 92400 lr 7.092e-06 [train_loss] tar_ll 0.7244 loss -0.7244 (14.156 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 92600 lr 6.725e-06 [train_loss] tar_ll 0.7358 loss -0.7358 (14.096 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 92800 lr 6.368e-06 [train_loss] tar_ll 0.7213 loss -0.7213 (14.055 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 93000 lr 6.021e-06 [train_loss] tar_ll 0.7075 loss -0.7075 (14.423 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 93200 lr 5.683e-06 [train_loss] tar_ll 0.7296 loss -0.7296 (14.367 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 93400 lr 5.355e-06 [train_loss] tar_ll 0.7258 loss -0.7258 (14.400 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 93600 lr 5.036e-06 [train_loss] tar_ll 0.7794 loss -0.7794 (14.018 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 93800 lr 4.727e-06 [train_loss] tar_ll 0.7035 loss -0.7035 (14.565 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 94000 lr 4.428e-06 [train_loss] tar_ll 0.7202 loss -0.7202 (13.962 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 94200 lr 4.139e-06 [train_loss] tar_ll 0.7206 loss -0.7206 (14.619 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 94400 lr 3.859e-06 [train_loss] tar_ll 0.6782 loss -0.6782 (14.087 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 94600 lr 3.589e-06 [train_loss] tar_ll 0.7674 loss -0.7674 (14.259 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 94800 lr 3.329e-06 [train_loss] tar_ll 0.7547 loss -0.7547 (14.289 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 95000 lr 3.078e-06 [train_loss] tar_ll 0.7146 loss -0.7146 (13.984 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.65it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.9284 loss 1.9284 (67.195 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-128_periodic step 95200 lr 2.837e-06 [train_loss] tar_ll 0.7464 loss -0.7464 (14.756 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 95400 lr 2.606e-06 [train_loss] tar_ll 0.7033 loss -0.7033 (14.341 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 95600 lr 2.385e-06 [train_loss] tar_ll 0.7662 loss -0.7662 (14.344 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 95800 lr 2.173e-06 [train_loss] tar_ll 0.7974 loss -0.7974 (14.253 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 96000 lr 1.971e-06 [train_loss] tar_ll 0.7561 loss -0.7561 (14.060 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 96200 lr 1.779e-06 [train_loss] tar_ll 0.7138 loss -0.7138 (14.079 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 96400 lr 1.597e-06 [train_loss] tar_ll 0.7698 loss -0.7698 (14.046 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 96600 lr 1.425e-06 [train_loss] tar_ll 0.6763 loss -0.6763 (14.205 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 96800 lr 1.262e-06 [train_loss] tar_ll 0.7000 loss -0.7000 (14.009 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 97000 lr 1.110e-06 [train_loss] tar_ll 0.6787 loss -0.6787 (14.327 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 97200 lr 9.666e-07 [train_loss] tar_ll 0.7398 loss -0.7398 (14.208 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 97400 lr 8.335e-07 [train_loss] tar_ll 0.7374 loss -0.7374 (14.016 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 97600 lr 7.103e-07 [train_loss] tar_ll 0.7493 loss -0.7493 (13.980 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 97800 lr 5.969e-07 [train_loss] tar_ll 0.6882 loss -0.6882 (14.560 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 98000 lr 4.933e-07 [train_loss] tar_ll 0.7688 loss -0.7688 (14.323 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 98200 lr 3.996e-07 [train_loss] tar_ll 0.7596 loss -0.7596 (14.551 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 98400 lr 3.158e-07 [train_loss] tar_ll 0.7193 loss -0.7193 (14.478 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 98600 lr 2.418e-07 [train_loss] tar_ll 0.7047 loss -0.7047 (14.038 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 98800 lr 1.776e-07 [train_loss] tar_ll 0.7385 loss -0.7385 (14.399 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 99000 lr 1.234e-07 [train_loss] tar_ll 0.7583 loss -0.7583 (14.333 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 99200 lr 7.895e-08 [train_loss] tar_ll 0.7727 loss -0.7727 (14.585 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 99400 lr 4.441e-08 [train_loss] tar_ll 0.7411 loss -0.7411 (14.525 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 99600 lr 1.974e-08 [train_loss] tar_ll 0.7046 loss -0.7046 (14.023 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 99800 lr 4.935e-09 [train_loss] tar_ll 0.7796 loss -0.7796 (13.959 secs)\n",
      "lbanp:lbanp-num_latents-128_periodic step 100000 lr 0.000e+00 [train_loss] tar_ll 0.7277 loss -0.7277 (14.064 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.23it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.9169 loss 1.9169 (66.326 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [01:08<00:00, 44.06it/s]\n",
      "lbanp:lbanp-num_latents-128_periodic periodic tar_ll -1.9169 loss 1.9169 (68.092 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 8574216.0 miliseconds\n",
      "Execution time: 8574.216 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 177.02978515625 MB\n",
      "Memory Usage Change: 160.77978515625 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='lbanp', name='lbanp-num_latents-128_periodic',val_seed=100, val_l=128,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cb0ccc-0288-4e8d-a3e5-1bf0794f9732",
   "metadata": {},
   "source": [
    "## TNP-ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "034f7ef7-ec02-45a4-8a17-559cf3c3bfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: tnpnd-tnpnd_periodic\n",
      "Total number of parameters: 332821\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tnpnd:tnpnd_periodic step 200 lr 5.000e-04 [train_loss] tar_ll -20.8756 loss 20.8756 mean_std 3.1837 (8.524 secs)\n",
      "tnpnd:tnpnd_periodic step 400 lr 5.000e-04 [train_loss] tar_ll -16.5903 loss 16.5903 mean_std 3.5612 (7.979 secs)\n",
      "tnpnd:tnpnd_periodic step 600 lr 5.000e-04 [train_loss] tar_ll -14.3470 loss 14.3470 mean_std 3.0885 (7.886 secs)\n",
      "tnpnd:tnpnd_periodic step 800 lr 4.999e-04 [train_loss] tar_ll -12.5762 loss 12.5762 mean_std 2.9195 (7.940 secs)\n",
      "tnpnd:tnpnd_periodic step 1000 lr 4.999e-04 [train_loss] tar_ll -11.2604 loss 11.2604 mean_std 2.7407 (7.933 secs)\n",
      "tnpnd:tnpnd_periodic step 1200 lr 4.998e-04 [train_loss] tar_ll -11.3345 loss 11.3345 mean_std 2.7441 (7.867 secs)\n",
      "tnpnd:tnpnd_periodic step 1400 lr 4.998e-04 [train_loss] tar_ll -12.2090 loss 12.2090 mean_std 2.7617 (8.061 secs)\n",
      "tnpnd:tnpnd_periodic step 1600 lr 4.997e-04 [train_loss] tar_ll -12.1235 loss 12.1235 mean_std 2.6791 (7.775 secs)\n",
      "tnpnd:tnpnd_periodic step 1800 lr 4.996e-04 [train_loss] tar_ll -11.6298 loss 11.6298 mean_std 2.4872 (8.230 secs)\n",
      "tnpnd:tnpnd_periodic step 2000 lr 4.995e-04 [train_loss] tar_ll -11.5981 loss 11.5981 mean_std 2.3837 (7.873 secs)\n",
      "tnpnd:tnpnd_periodic step 2200 lr 4.994e-04 [train_loss] tar_ll -12.5276 loss 12.5276 mean_std 2.2121 (8.180 secs)\n",
      "tnpnd:tnpnd_periodic step 2400 lr 4.993e-04 [train_loss] tar_ll -9.4913 loss 9.4913 mean_std 1.6777 (7.670 secs)\n",
      "tnpnd:tnpnd_periodic step 2600 lr 4.992e-04 [train_loss] tar_ll -10.0260 loss 10.0260 mean_std 1.9691 (8.035 secs)\n",
      "tnpnd:tnpnd_periodic step 2800 lr 4.990e-04 [train_loss] tar_ll -8.0029 loss 8.0029 mean_std 1.3612 (7.731 secs)\n",
      "tnpnd:tnpnd_periodic step 3000 lr 4.989e-04 [train_loss] tar_ll -6.2302 loss 6.2302 mean_std 1.0652 (7.794 secs)\n",
      "tnpnd:tnpnd_periodic step 3200 lr 4.987e-04 [train_loss] tar_ll -8.2919 loss 8.2919 mean_std 1.5104 (8.286 secs)\n",
      "tnpnd:tnpnd_periodic step 3400 lr 4.986e-04 [train_loss] tar_ll -6.5505 loss 6.5505 mean_std 0.9117 (7.007 secs)\n",
      "tnpnd:tnpnd_periodic step 3600 lr 4.984e-04 [train_loss] tar_ll -6.0348 loss 6.0348 mean_std 0.6766 (6.752 secs)\n",
      "tnpnd:tnpnd_periodic step 3800 lr 4.982e-04 [train_loss] tar_ll -3.0925 loss 3.0925 mean_std 0.3007 (7.304 secs)\n",
      "tnpnd:tnpnd_periodic step 4000 lr 4.980e-04 [train_loss] tar_ll -2.1523 loss 2.1523 mean_std 0.2093 (6.993 secs)\n",
      "tnpnd:tnpnd_periodic step 4200 lr 4.978e-04 [train_loss] tar_ll -1.9782 loss 1.9782 mean_std 0.2154 (7.513 secs)\n",
      "tnpnd:tnpnd_periodic step 4400 lr 4.976e-04 [train_loss] tar_ll -2.1388 loss 2.1388 mean_std 0.1728 (7.648 secs)\n",
      "tnpnd:tnpnd_periodic step 4600 lr 4.974e-04 [train_loss] tar_ll -0.0621 loss 0.0621 mean_std 0.1140 (7.556 secs)\n",
      "tnpnd:tnpnd_periodic step 4800 lr 4.972e-04 [train_loss] tar_ll 1.4902 loss -1.4902 mean_std 0.0771 (7.567 secs)\n",
      "tnpnd:tnpnd_periodic step 5000 lr 4.969e-04 [train_loss] tar_ll 1.3567 loss -1.3567 mean_std 0.0757 (7.503 secs)\n",
      "100%|##########| 3000/3000 [00:26<00:00, 113.70it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -1.3873 loss 1.3873 mean_std 0.2750 (26.389 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 5200 lr 4.967e-04 [train_loss] tar_ll 3.3610 loss -3.3610 mean_std 0.0540 (8.134 secs)\n",
      "tnpnd:tnpnd_periodic step 5400 lr 4.964e-04 [train_loss] tar_ll 1.7242 loss -1.7242 mean_std 0.0745 (7.586 secs)\n",
      "tnpnd:tnpnd_periodic step 5600 lr 4.961e-04 [train_loss] tar_ll 1.1609 loss -1.1609 mean_std 0.0634 (8.098 secs)\n",
      "tnpnd:tnpnd_periodic step 5800 lr 4.959e-04 [train_loss] tar_ll 3.1454 loss -3.1454 mean_std 0.0436 (7.745 secs)\n",
      "tnpnd:tnpnd_periodic step 6000 lr 4.956e-04 [train_loss] tar_ll 2.9923 loss -2.9923 mean_std 0.0419 (7.959 secs)\n",
      "tnpnd:tnpnd_periodic step 6200 lr 4.953e-04 [train_loss] tar_ll 0.7151 loss -0.7151 mean_std 0.0755 (7.924 secs)\n",
      "tnpnd:tnpnd_periodic step 6400 lr 4.950e-04 [train_loss] tar_ll 4.4947 loss -4.4947 mean_std 0.0377 (7.941 secs)\n",
      "tnpnd:tnpnd_periodic step 6600 lr 4.946e-04 [train_loss] tar_ll 6.1140 loss -6.1140 mean_std 0.0270 (7.876 secs)\n",
      "tnpnd:tnpnd_periodic step 6800 lr 4.943e-04 [train_loss] tar_ll 6.9231 loss -6.9231 mean_std 0.0245 (8.135 secs)\n",
      "tnpnd:tnpnd_periodic step 7000 lr 4.940e-04 [train_loss] tar_ll 7.3870 loss -7.3870 mean_std 0.0251 (7.802 secs)\n",
      "tnpnd:tnpnd_periodic step 7200 lr 4.936e-04 [train_loss] tar_ll 7.8294 loss -7.8294 mean_std 0.0220 (7.908 secs)\n",
      "tnpnd:tnpnd_periodic step 7400 lr 4.933e-04 [train_loss] tar_ll 7.4578 loss -7.4578 mean_std 0.0240 (7.791 secs)\n",
      "tnpnd:tnpnd_periodic step 7600 lr 4.929e-04 [train_loss] tar_ll 6.9365 loss -6.9365 mean_std 0.0382 (7.707 secs)\n",
      "tnpnd:tnpnd_periodic step 7800 lr 4.925e-04 [train_loss] tar_ll 6.1949 loss -6.1949 mean_std 0.0421 (8.019 secs)\n",
      "tnpnd:tnpnd_periodic step 8000 lr 4.921e-04 [train_loss] tar_ll 8.5811 loss -8.5811 mean_std 0.0202 (7.982 secs)\n",
      "tnpnd:tnpnd_periodic step 8200 lr 4.918e-04 [train_loss] tar_ll 8.7332 loss -8.7332 mean_std 0.0236 (7.706 secs)\n",
      "tnpnd:tnpnd_periodic step 8400 lr 4.913e-04 [train_loss] tar_ll 7.1677 loss -7.1677 mean_std 0.0275 (7.740 secs)\n",
      "tnpnd:tnpnd_periodic step 8600 lr 4.909e-04 [train_loss] tar_ll 9.3845 loss -9.3845 mean_std 0.0195 (7.638 secs)\n",
      "tnpnd:tnpnd_periodic step 8800 lr 4.905e-04 [train_loss] tar_ll 7.6520 loss -7.6520 mean_std 0.0263 (7.630 secs)\n",
      "tnpnd:tnpnd_periodic step 9000 lr 4.901e-04 [train_loss] tar_ll 10.7613 loss -10.7613 mean_std 0.0164 (7.760 secs)\n",
      "tnpnd:tnpnd_periodic step 9200 lr 4.896e-04 [train_loss] tar_ll 9.9829 loss -9.9829 mean_std 0.0165 (7.832 secs)\n",
      "tnpnd:tnpnd_periodic step 9400 lr 4.892e-04 [train_loss] tar_ll 10.0660 loss -10.0660 mean_std 0.0174 (8.464 secs)\n",
      "tnpnd:tnpnd_periodic step 9600 lr 4.887e-04 [train_loss] tar_ll 9.9709 loss -9.9709 mean_std 0.0197 (7.120 secs)\n",
      "tnpnd:tnpnd_periodic step 9800 lr 4.882e-04 [train_loss] tar_ll 8.5546 loss -8.5546 mean_std 0.0219 (6.873 secs)\n",
      "tnpnd:tnpnd_periodic step 10000 lr 4.878e-04 [train_loss] tar_ll 9.9518 loss -9.9518 mean_std 0.0212 (6.775 secs)\n",
      "100%|##########| 3000/3000 [00:24<00:00, 121.55it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -1.1997 loss 1.1997 mean_std 0.1218 (24.685 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 10200 lr 4.873e-04 [train_loss] tar_ll 8.6111 loss -8.6111 mean_std 0.0177 (7.713 secs)\n",
      "tnpnd:tnpnd_periodic step 10400 lr 4.868e-04 [train_loss] tar_ll 10.7726 loss -10.7726 mean_std 0.0171 (7.681 secs)\n",
      "tnpnd:tnpnd_periodic step 10600 lr 4.863e-04 [train_loss] tar_ll 10.6301 loss -10.6301 mean_std 0.0174 (7.648 secs)\n",
      "tnpnd:tnpnd_periodic step 10800 lr 4.857e-04 [train_loss] tar_ll 9.3215 loss -9.3215 mean_std 0.0184 (7.703 secs)\n",
      "tnpnd:tnpnd_periodic step 11000 lr 4.852e-04 [train_loss] tar_ll 10.0716 loss -10.0716 mean_std 0.0167 (7.733 secs)\n",
      "tnpnd:tnpnd_periodic step 11200 lr 4.847e-04 [train_loss] tar_ll 10.3503 loss -10.3503 mean_std 0.0166 (8.212 secs)\n",
      "tnpnd:tnpnd_periodic step 11400 lr 4.841e-04 [train_loss] tar_ll 9.1781 loss -9.1781 mean_std 0.0203 (7.811 secs)\n",
      "tnpnd:tnpnd_periodic step 11600 lr 4.836e-04 [train_loss] tar_ll 9.9813 loss -9.9813 mean_std 0.0185 (8.204 secs)\n",
      "tnpnd:tnpnd_periodic step 11800 lr 4.830e-04 [train_loss] tar_ll 10.6035 loss -10.6035 mean_std 0.0185 (8.126 secs)\n",
      "tnpnd:tnpnd_periodic step 12000 lr 4.824e-04 [train_loss] tar_ll 11.1720 loss -11.1720 mean_std 0.0148 (8.077 secs)\n",
      "tnpnd:tnpnd_periodic step 12200 lr 4.819e-04 [train_loss] tar_ll 11.4819 loss -11.4819 mean_std 0.0168 (7.663 secs)\n",
      "tnpnd:tnpnd_periodic step 12400 lr 4.813e-04 [train_loss] tar_ll 8.1250 loss -8.1250 mean_std 0.0222 (8.239 secs)\n",
      "tnpnd:tnpnd_periodic step 12600 lr 4.807e-04 [train_loss] tar_ll 9.6371 loss -9.6371 mean_std 0.0196 (7.965 secs)\n",
      "tnpnd:tnpnd_periodic step 12800 lr 4.801e-04 [train_loss] tar_ll 10.9737 loss -10.9737 mean_std 0.0164 (8.012 secs)\n",
      "tnpnd:tnpnd_periodic step 13000 lr 4.794e-04 [train_loss] tar_ll 10.2773 loss -10.2773 mean_std 0.0168 (7.585 secs)\n",
      "tnpnd:tnpnd_periodic step 13200 lr 4.788e-04 [train_loss] tar_ll 11.2615 loss -11.2615 mean_std 0.0138 (8.277 secs)\n",
      "tnpnd:tnpnd_periodic step 13400 lr 4.782e-04 [train_loss] tar_ll 10.5619 loss -10.5619 mean_std 0.0149 (7.798 secs)\n",
      "tnpnd:tnpnd_periodic step 13600 lr 4.775e-04 [train_loss] tar_ll 11.6714 loss -11.6714 mean_std 0.0129 (7.886 secs)\n",
      "tnpnd:tnpnd_periodic step 13800 lr 4.769e-04 [train_loss] tar_ll 9.0885 loss -9.0885 mean_std 0.0205 (7.601 secs)\n",
      "tnpnd:tnpnd_periodic step 14000 lr 4.762e-04 [train_loss] tar_ll 10.8616 loss -10.8616 mean_std 0.0164 (8.181 secs)\n",
      "tnpnd:tnpnd_periodic step 14200 lr 4.755e-04 [train_loss] tar_ll 9.8559 loss -9.8559 mean_std 0.0198 (7.594 secs)\n",
      "tnpnd:tnpnd_periodic step 14400 lr 4.749e-04 [train_loss] tar_ll 11.7590 loss -11.7590 mean_std 0.0129 (8.140 secs)\n",
      "tnpnd:tnpnd_periodic step 14600 lr 4.742e-04 [train_loss] tar_ll 11.1453 loss -11.1453 mean_std 0.0199 (7.904 secs)\n",
      "tnpnd:tnpnd_periodic step 14800 lr 4.735e-04 [train_loss] tar_ll 12.5359 loss -12.5359 mean_std 0.0134 (8.257 secs)\n",
      "tnpnd:tnpnd_periodic step 15000 lr 4.728e-04 [train_loss] tar_ll 8.8719 loss -8.8719 mean_std 0.0188 (6.996 secs)\n",
      "100%|##########| 3000/3000 [00:24<00:00, 124.89it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -2.3789 loss 2.3789 mean_std 0.0676 (24.025 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 15200 lr 4.720e-04 [train_loss] tar_ll 11.2312 loss -11.2312 mean_std 0.0130 (8.248 secs)\n",
      "tnpnd:tnpnd_periodic step 15400 lr 4.713e-04 [train_loss] tar_ll 11.5078 loss -11.5078 mean_std 0.0145 (8.267 secs)\n",
      "tnpnd:tnpnd_periodic step 15600 lr 4.706e-04 [train_loss] tar_ll 11.4642 loss -11.4642 mean_std 0.0134 (8.395 secs)\n",
      "tnpnd:tnpnd_periodic step 15800 lr 4.698e-04 [train_loss] tar_ll 11.7411 loss -11.7411 mean_std 0.0142 (7.955 secs)\n",
      "tnpnd:tnpnd_periodic step 16000 lr 4.691e-04 [train_loss] tar_ll 10.0446 loss -10.0446 mean_std 0.0149 (7.589 secs)\n",
      "tnpnd:tnpnd_periodic step 16200 lr 4.683e-04 [train_loss] tar_ll 11.6477 loss -11.6477 mean_std 0.0130 (7.769 secs)\n",
      "tnpnd:tnpnd_periodic step 16400 lr 4.675e-04 [train_loss] tar_ll 12.7017 loss -12.7017 mean_std 0.0139 (7.892 secs)\n",
      "tnpnd:tnpnd_periodic step 16600 lr 4.668e-04 [train_loss] tar_ll 11.2974 loss -11.2974 mean_std 0.0123 (8.195 secs)\n",
      "tnpnd:tnpnd_periodic step 16800 lr 4.660e-04 [train_loss] tar_ll 11.9699 loss -11.9699 mean_std 0.0137 (8.034 secs)\n",
      "tnpnd:tnpnd_periodic step 17000 lr 4.652e-04 [train_loss] tar_ll 11.5238 loss -11.5238 mean_std 0.0126 (8.232 secs)\n",
      "tnpnd:tnpnd_periodic step 17200 lr 4.644e-04 [train_loss] tar_ll 12.3985 loss -12.3985 mean_std 0.0127 (8.116 secs)\n",
      "tnpnd:tnpnd_periodic step 17400 lr 4.636e-04 [train_loss] tar_ll 12.0599 loss -12.0599 mean_std 0.0156 (8.330 secs)\n",
      "tnpnd:tnpnd_periodic step 17600 lr 4.627e-04 [train_loss] tar_ll 11.6490 loss -11.6490 mean_std 0.0143 (7.904 secs)\n",
      "tnpnd:tnpnd_periodic step 17800 lr 4.619e-04 [train_loss] tar_ll 11.4983 loss -11.4983 mean_std 0.0138 (7.896 secs)\n",
      "tnpnd:tnpnd_periodic step 18000 lr 4.611e-04 [train_loss] tar_ll 13.0798 loss -13.0798 mean_std 0.0123 (7.943 secs)\n",
      "tnpnd:tnpnd_periodic step 18200 lr 4.602e-04 [train_loss] tar_ll 12.5019 loss -12.5019 mean_std 0.0161 (8.039 secs)\n",
      "tnpnd:tnpnd_periodic step 18400 lr 4.594e-04 [train_loss] tar_ll 12.6782 loss -12.6782 mean_std 0.0122 (7.996 secs)\n",
      "tnpnd:tnpnd_periodic step 18600 lr 4.585e-04 [train_loss] tar_ll 13.0072 loss -13.0072 mean_std 0.0107 (7.936 secs)\n",
      "tnpnd:tnpnd_periodic step 18800 lr 4.576e-04 [train_loss] tar_ll 11.4648 loss -11.4648 mean_std 0.0144 (7.713 secs)\n",
      "tnpnd:tnpnd_periodic step 19000 lr 4.568e-04 [train_loss] tar_ll 13.4667 loss -13.4667 mean_std 0.0112 (7.990 secs)\n",
      "tnpnd:tnpnd_periodic step 19200 lr 4.559e-04 [train_loss] tar_ll 12.7219 loss -12.7219 mean_std 0.0118 (7.947 secs)\n",
      "tnpnd:tnpnd_periodic step 19400 lr 4.550e-04 [train_loss] tar_ll 11.3749 loss -11.3749 mean_std 0.0156 (7.870 secs)\n",
      "tnpnd:tnpnd_periodic step 19600 lr 4.541e-04 [train_loss] tar_ll 12.6251 loss -12.6251 mean_std 0.0112 (7.635 secs)\n",
      "tnpnd:tnpnd_periodic step 19800 lr 4.532e-04 [train_loss] tar_ll 13.4741 loss -13.4741 mean_std 0.0102 (7.854 secs)\n",
      "tnpnd:tnpnd_periodic step 20000 lr 4.523e-04 [train_loss] tar_ll 12.2459 loss -12.2459 mean_std 0.0106 (7.577 secs)\n",
      "100%|##########| 3000/3000 [00:25<00:00, 118.48it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -1.9684 loss 1.9684 mean_std 0.0668 (25.328 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 20200 lr 4.513e-04 [train_loss] tar_ll 12.0698 loss -12.0698 mean_std 0.0114 (7.151 secs)\n",
      "tnpnd:tnpnd_periodic step 20400 lr 4.504e-04 [train_loss] tar_ll 13.1462 loss -13.1462 mean_std 0.0115 (7.826 secs)\n",
      "tnpnd:tnpnd_periodic step 20600 lr 4.494e-04 [train_loss] tar_ll 11.0955 loss -11.0955 mean_std 0.0157 (7.447 secs)\n",
      "tnpnd:tnpnd_periodic step 20800 lr 4.485e-04 [train_loss] tar_ll 12.1894 loss -12.1894 mean_std 0.0145 (7.827 secs)\n",
      "tnpnd:tnpnd_periodic step 21000 lr 4.475e-04 [train_loss] tar_ll 12.1958 loss -12.1958 mean_std 0.0117 (7.959 secs)\n",
      "tnpnd:tnpnd_periodic step 21200 lr 4.466e-04 [train_loss] tar_ll 11.5096 loss -11.5096 mean_std 0.0127 (7.976 secs)\n",
      "tnpnd:tnpnd_periodic step 21400 lr 4.456e-04 [train_loss] tar_ll 12.7841 loss -12.7841 mean_std 0.0117 (7.427 secs)\n",
      "tnpnd:tnpnd_periodic step 21600 lr 4.446e-04 [train_loss] tar_ll 13.6361 loss -13.6361 mean_std 0.0104 (7.742 secs)\n",
      "tnpnd:tnpnd_periodic step 21800 lr 4.436e-04 [train_loss] tar_ll 13.7427 loss -13.7427 mean_std 0.0107 (7.491 secs)\n",
      "tnpnd:tnpnd_periodic step 22000 lr 4.426e-04 [train_loss] tar_ll 13.0179 loss -13.0179 mean_std 0.0137 (7.852 secs)\n",
      "tnpnd:tnpnd_periodic step 22200 lr 4.416e-04 [train_loss] tar_ll 12.7772 loss -12.7772 mean_std 0.0116 (7.733 secs)\n",
      "tnpnd:tnpnd_periodic step 22400 lr 4.406e-04 [train_loss] tar_ll 13.4858 loss -13.4858 mean_std 0.0129 (7.918 secs)\n",
      "tnpnd:tnpnd_periodic step 22600 lr 4.396e-04 [train_loss] tar_ll 13.2897 loss -13.2897 mean_std 0.0111 (7.856 secs)\n",
      "tnpnd:tnpnd_periodic step 22800 lr 4.386e-04 [train_loss] tar_ll 13.2245 loss -13.2245 mean_std 0.0103 (8.277 secs)\n",
      "tnpnd:tnpnd_periodic step 23000 lr 4.375e-04 [train_loss] tar_ll 13.2986 loss -13.2986 mean_std 0.0119 (8.000 secs)\n",
      "tnpnd:tnpnd_periodic step 23200 lr 4.365e-04 [train_loss] tar_ll 12.0552 loss -12.0552 mean_std 0.0122 (8.182 secs)\n",
      "tnpnd:tnpnd_periodic step 23400 lr 4.354e-04 [train_loss] tar_ll 12.1465 loss -12.1465 mean_std 0.0133 (7.896 secs)\n",
      "tnpnd:tnpnd_periodic step 23600 lr 4.344e-04 [train_loss] tar_ll 12.4871 loss -12.4871 mean_std 0.0144 (8.006 secs)\n",
      "tnpnd:tnpnd_periodic step 23800 lr 4.333e-04 [train_loss] tar_ll 12.6742 loss -12.6742 mean_std 0.0117 (7.663 secs)\n",
      "tnpnd:tnpnd_periodic step 24000 lr 4.322e-04 [train_loss] tar_ll 13.0171 loss -13.0171 mean_std 0.0118 (7.981 secs)\n",
      "tnpnd:tnpnd_periodic step 24200 lr 4.312e-04 [train_loss] tar_ll 13.6008 loss -13.6008 mean_std 0.0111 (7.819 secs)\n",
      "tnpnd:tnpnd_periodic step 24400 lr 4.301e-04 [train_loss] tar_ll 13.0222 loss -13.0222 mean_std 0.0121 (8.034 secs)\n",
      "tnpnd:tnpnd_periodic step 24600 lr 4.290e-04 [train_loss] tar_ll 10.5039 loss -10.5039 mean_std 0.0194 (7.782 secs)\n",
      "tnpnd:tnpnd_periodic step 24800 lr 4.279e-04 [train_loss] tar_ll 9.8342 loss -9.8342 mean_std 0.0152 (8.021 secs)\n",
      "tnpnd:tnpnd_periodic step 25000 lr 4.268e-04 [train_loss] tar_ll 12.4026 loss -12.4026 mean_std 0.0103 (7.634 secs)\n",
      "100%|##########| 3000/3000 [00:26<00:00, 114.35it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -2.4216 loss 2.4216 mean_std 0.0451 (26.239 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 25200 lr 4.257e-04 [train_loss] tar_ll 12.9339 loss -12.9339 mean_std 0.0111 (7.915 secs)\n",
      "tnpnd:tnpnd_periodic step 25400 lr 4.245e-04 [train_loss] tar_ll 12.4281 loss -12.4281 mean_std 0.0129 (7.855 secs)\n",
      "tnpnd:tnpnd_periodic step 25600 lr 4.234e-04 [train_loss] tar_ll 13.2359 loss -13.2359 mean_std 0.0094 (7.611 secs)\n",
      "tnpnd:tnpnd_periodic step 25800 lr 4.223e-04 [train_loss] tar_ll 14.1288 loss -14.1288 mean_std 0.0086 (7.902 secs)\n",
      "tnpnd:tnpnd_periodic step 26000 lr 4.211e-04 [train_loss] tar_ll 13.5426 loss -13.5426 mean_std 0.0108 (7.657 secs)\n",
      "tnpnd:tnpnd_periodic step 26200 lr 4.200e-04 [train_loss] tar_ll 13.1823 loss -13.1823 mean_std 0.0091 (8.044 secs)\n",
      "tnpnd:tnpnd_periodic step 26400 lr 4.188e-04 [train_loss] tar_ll 13.9140 loss -13.9140 mean_std 0.0085 (7.558 secs)\n",
      "tnpnd:tnpnd_periodic step 26600 lr 4.177e-04 [train_loss] tar_ll 13.2601 loss -13.2601 mean_std 0.0096 (7.931 secs)\n",
      "tnpnd:tnpnd_periodic step 26800 lr 4.165e-04 [train_loss] tar_ll 12.9490 loss -12.9490 mean_std 0.0119 (7.521 secs)\n",
      "tnpnd:tnpnd_periodic step 27000 lr 4.153e-04 [train_loss] tar_ll 14.9385 loss -14.9385 mean_std 0.0113 (7.866 secs)\n",
      "tnpnd:tnpnd_periodic step 27200 lr 4.141e-04 [train_loss] tar_ll 13.2298 loss -13.2298 mean_std 0.0101 (7.877 secs)\n",
      "tnpnd:tnpnd_periodic step 27400 lr 4.130e-04 [train_loss] tar_ll 13.7122 loss -13.7122 mean_std 0.0102 (8.039 secs)\n",
      "tnpnd:tnpnd_periodic step 27600 lr 4.118e-04 [train_loss] tar_ll 14.4666 loss -14.4666 mean_std 0.0103 (7.734 secs)\n",
      "tnpnd:tnpnd_periodic step 27800 lr 4.106e-04 [train_loss] tar_ll 12.8252 loss -12.8252 mean_std 0.0120 (8.030 secs)\n",
      "tnpnd:tnpnd_periodic step 28000 lr 4.094e-04 [train_loss] tar_ll 13.5483 loss -13.5483 mean_std 0.0115 (7.880 secs)\n",
      "tnpnd:tnpnd_periodic step 28200 lr 4.081e-04 [train_loss] tar_ll 13.3114 loss -13.3114 mean_std 0.0103 (8.356 secs)\n",
      "tnpnd:tnpnd_periodic step 28400 lr 4.069e-04 [train_loss] tar_ll 14.1420 loss -14.1420 mean_std 0.0112 (7.839 secs)\n",
      "tnpnd:tnpnd_periodic step 28600 lr 4.057e-04 [train_loss] tar_ll 15.3322 loss -15.3322 mean_std 0.0081 (7.935 secs)\n",
      "tnpnd:tnpnd_periodic step 28800 lr 4.045e-04 [train_loss] tar_ll 13.7644 loss -13.7644 mean_std 0.0093 (7.644 secs)\n",
      "tnpnd:tnpnd_periodic step 29000 lr 4.032e-04 [train_loss] tar_ll 14.2079 loss -14.2079 mean_std 0.0088 (8.129 secs)\n",
      "tnpnd:tnpnd_periodic step 29200 lr 4.020e-04 [train_loss] tar_ll 14.1593 loss -14.1593 mean_std 0.0089 (7.908 secs)\n",
      "tnpnd:tnpnd_periodic step 29400 lr 4.007e-04 [train_loss] tar_ll 13.5977 loss -13.5977 mean_std 0.0110 (8.050 secs)\n",
      "tnpnd:tnpnd_periodic step 29600 lr 3.995e-04 [train_loss] tar_ll 13.2600 loss -13.2600 mean_std 0.0103 (8.008 secs)\n",
      "tnpnd:tnpnd_periodic step 29800 lr 3.982e-04 [train_loss] tar_ll 14.2382 loss -14.2382 mean_std 0.0099 (8.119 secs)\n",
      "tnpnd:tnpnd_periodic step 30000 lr 3.969e-04 [train_loss] tar_ll 14.1338 loss -14.1338 mean_std 0.0090 (7.751 secs)\n",
      "100%|##########| 3000/3000 [00:26<00:00, 112.36it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -3.4812 loss 3.4812 mean_std 0.0535 (26.704 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 30200 lr 3.957e-04 [train_loss] tar_ll 13.0961 loss -13.0961 mean_std 0.0138 (7.778 secs)\n",
      "tnpnd:tnpnd_periodic step 30400 lr 3.944e-04 [train_loss] tar_ll 13.0871 loss -13.0871 mean_std 0.0113 (8.080 secs)\n",
      "tnpnd:tnpnd_periodic step 30600 lr 3.931e-04 [train_loss] tar_ll 14.5475 loss -14.5475 mean_std 0.0093 (7.806 secs)\n",
      "tnpnd:tnpnd_periodic step 30800 lr 3.918e-04 [train_loss] tar_ll 14.4333 loss -14.4333 mean_std 0.0100 (7.673 secs)\n",
      "tnpnd:tnpnd_periodic step 31000 lr 3.905e-04 [train_loss] tar_ll 12.8821 loss -12.8821 mean_std 0.0093 (7.513 secs)\n",
      "tnpnd:tnpnd_periodic step 31200 lr 3.892e-04 [train_loss] tar_ll 13.3432 loss -13.3432 mean_std 0.0107 (7.824 secs)\n",
      "tnpnd:tnpnd_periodic step 31400 lr 3.879e-04 [train_loss] tar_ll 12.7926 loss -12.7926 mean_std 0.0110 (7.551 secs)\n",
      "tnpnd:tnpnd_periodic step 31600 lr 3.866e-04 [train_loss] tar_ll 14.5209 loss -14.5209 mean_std 0.0100 (7.603 secs)\n",
      "tnpnd:tnpnd_periodic step 31800 lr 3.853e-04 [train_loss] tar_ll 13.4706 loss -13.4706 mean_std 0.0101 (7.503 secs)\n",
      "tnpnd:tnpnd_periodic step 32000 lr 3.840e-04 [train_loss] tar_ll 14.3977 loss -14.3977 mean_std 0.0076 (7.628 secs)\n",
      "tnpnd:tnpnd_periodic step 32200 lr 3.826e-04 [train_loss] tar_ll 13.4467 loss -13.4467 mean_std 0.0092 (7.503 secs)\n",
      "tnpnd:tnpnd_periodic step 32400 lr 3.813e-04 [train_loss] tar_ll 12.9782 loss -12.9782 mean_std 0.0105 (7.787 secs)\n",
      "tnpnd:tnpnd_periodic step 32600 lr 3.800e-04 [train_loss] tar_ll 12.8969 loss -12.8969 mean_std 0.0106 (7.739 secs)\n",
      "tnpnd:tnpnd_periodic step 32800 lr 3.786e-04 [train_loss] tar_ll 14.3298 loss -14.3298 mean_std 0.0105 (7.811 secs)\n",
      "tnpnd:tnpnd_periodic step 33000 lr 3.773e-04 [train_loss] tar_ll 13.7289 loss -13.7289 mean_std 0.0094 (6.859 secs)\n",
      "tnpnd:tnpnd_periodic step 33200 lr 3.759e-04 [train_loss] tar_ll 16.0721 loss -16.0721 mean_std 0.0090 (6.715 secs)\n",
      "tnpnd:tnpnd_periodic step 33400 lr 3.745e-04 [train_loss] tar_ll 12.0598 loss -12.0598 mean_std 0.0125 (7.774 secs)\n",
      "tnpnd:tnpnd_periodic step 33600 lr 3.732e-04 [train_loss] tar_ll 13.3262 loss -13.3262 mean_std 0.0093 (7.627 secs)\n",
      "tnpnd:tnpnd_periodic step 33800 lr 3.718e-04 [train_loss] tar_ll 14.8335 loss -14.8335 mean_std 0.0089 (7.713 secs)\n",
      "tnpnd:tnpnd_periodic step 34000 lr 3.704e-04 [train_loss] tar_ll 15.5867 loss -15.5867 mean_std 0.0067 (7.836 secs)\n",
      "tnpnd:tnpnd_periodic step 34200 lr 3.691e-04 [train_loss] tar_ll 14.0888 loss -14.0888 mean_std 0.0087 (8.359 secs)\n",
      "tnpnd:tnpnd_periodic step 34400 lr 3.677e-04 [train_loss] tar_ll 14.5049 loss -14.5049 mean_std 0.0084 (8.052 secs)\n",
      "tnpnd:tnpnd_periodic step 34600 lr 3.663e-04 [train_loss] tar_ll 15.3020 loss -15.3020 mean_std 0.0087 (7.866 secs)\n",
      "tnpnd:tnpnd_periodic step 34800 lr 3.649e-04 [train_loss] tar_ll 14.6239 loss -14.6239 mean_std 0.0082 (7.580 secs)\n",
      "tnpnd:tnpnd_periodic step 35000 lr 3.635e-04 [train_loss] tar_ll 14.7433 loss -14.7433 mean_std 0.0091 (8.238 secs)\n",
      "100%|##########| 3000/3000 [00:26<00:00, 113.57it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -2.6932 loss 2.6932 mean_std 0.0489 (26.418 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 35200 lr 3.621e-04 [train_loss] tar_ll 12.2997 loss -12.2997 mean_std 0.0101 (7.689 secs)\n",
      "tnpnd:tnpnd_periodic step 35400 lr 3.607e-04 [train_loss] tar_ll 14.2232 loss -14.2232 mean_std 0.0102 (7.821 secs)\n",
      "tnpnd:tnpnd_periodic step 35600 lr 3.593e-04 [train_loss] tar_ll 14.3267 loss -14.3267 mean_std 0.0083 (7.890 secs)\n",
      "tnpnd:tnpnd_periodic step 35800 lr 3.579e-04 [train_loss] tar_ll 14.1879 loss -14.1879 mean_std 0.0091 (7.960 secs)\n",
      "tnpnd:tnpnd_periodic step 36000 lr 3.564e-04 [train_loss] tar_ll 14.2650 loss -14.2650 mean_std 0.0101 (7.878 secs)\n",
      "tnpnd:tnpnd_periodic step 36200 lr 3.550e-04 [train_loss] tar_ll 15.3839 loss -15.3839 mean_std 0.0085 (7.970 secs)\n",
      "tnpnd:tnpnd_periodic step 36400 lr 3.536e-04 [train_loss] tar_ll 14.0617 loss -14.0617 mean_std 0.0104 (7.675 secs)\n",
      "tnpnd:tnpnd_periodic step 36600 lr 3.522e-04 [train_loss] tar_ll 13.6372 loss -13.6372 mean_std 0.0096 (7.808 secs)\n",
      "tnpnd:tnpnd_periodic step 36800 lr 3.507e-04 [train_loss] tar_ll 15.0381 loss -15.0381 mean_std 0.0090 (7.589 secs)\n",
      "tnpnd:tnpnd_periodic step 37000 lr 3.493e-04 [train_loss] tar_ll 15.3706 loss -15.3706 mean_std 0.0096 (7.817 secs)\n",
      "tnpnd:tnpnd_periodic step 37200 lr 3.478e-04 [train_loss] tar_ll 15.5853 loss -15.5853 mean_std 0.0086 (7.812 secs)\n",
      "tnpnd:tnpnd_periodic step 37400 lr 3.464e-04 [train_loss] tar_ll 14.1841 loss -14.1841 mean_std 0.0097 (7.835 secs)\n",
      "tnpnd:tnpnd_periodic step 37600 lr 3.449e-04 [train_loss] tar_ll 14.9154 loss -14.9154 mean_std 0.0087 (7.633 secs)\n",
      "tnpnd:tnpnd_periodic step 37800 lr 3.435e-04 [train_loss] tar_ll 14.6042 loss -14.6042 mean_std 0.0082 (7.922 secs)\n",
      "tnpnd:tnpnd_periodic step 38000 lr 3.420e-04 [train_loss] tar_ll 13.3427 loss -13.3427 mean_std 0.0114 (7.770 secs)\n",
      "tnpnd:tnpnd_periodic step 38200 lr 3.406e-04 [train_loss] tar_ll 14.1950 loss -14.1950 mean_std 0.0096 (8.025 secs)\n",
      "tnpnd:tnpnd_periodic step 38400 lr 3.391e-04 [train_loss] tar_ll 15.7164 loss -15.7164 mean_std 0.0074 (7.722 secs)\n",
      "tnpnd:tnpnd_periodic step 38600 lr 3.376e-04 [train_loss] tar_ll 15.2672 loss -15.2672 mean_std 0.0068 (7.850 secs)\n",
      "tnpnd:tnpnd_periodic step 38800 lr 3.362e-04 [train_loss] tar_ll 16.4791 loss -16.4791 mean_std 0.0099 (7.848 secs)\n",
      "tnpnd:tnpnd_periodic step 39000 lr 3.347e-04 [train_loss] tar_ll 15.0292 loss -15.0292 mean_std 0.0070 (8.231 secs)\n",
      "tnpnd:tnpnd_periodic step 39200 lr 3.332e-04 [train_loss] tar_ll 15.6554 loss -15.6554 mean_std 0.0089 (7.761 secs)\n",
      "tnpnd:tnpnd_periodic step 39400 lr 3.317e-04 [train_loss] tar_ll 13.7373 loss -13.7373 mean_std 0.0087 (6.744 secs)\n",
      "tnpnd:tnpnd_periodic step 39600 lr 3.302e-04 [train_loss] tar_ll 15.1055 loss -15.1055 mean_std 0.0086 (7.127 secs)\n",
      "tnpnd:tnpnd_periodic step 39800 lr 3.287e-04 [train_loss] tar_ll 16.3421 loss -16.3421 mean_std 0.0092 (7.725 secs)\n",
      "tnpnd:tnpnd_periodic step 40000 lr 3.273e-04 [train_loss] tar_ll 15.3875 loss -15.3875 mean_std 0.0090 (7.832 secs)\n",
      "100%|##########| 3000/3000 [00:25<00:00, 115.73it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -2.6192 loss 2.6192 mean_std 0.0426 (25.925 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 40200 lr 3.258e-04 [train_loss] tar_ll 14.9199 loss -14.9199 mean_std 0.0089 (8.025 secs)\n",
      "tnpnd:tnpnd_periodic step 40400 lr 3.243e-04 [train_loss] tar_ll 14.4842 loss -14.4842 mean_std 0.0085 (8.009 secs)\n",
      "tnpnd:tnpnd_periodic step 40600 lr 3.228e-04 [train_loss] tar_ll 16.1964 loss -16.1964 mean_std 0.0077 (7.694 secs)\n",
      "tnpnd:tnpnd_periodic step 40800 lr 3.213e-04 [train_loss] tar_ll 14.7935 loss -14.7935 mean_std 0.0092 (7.757 secs)\n",
      "tnpnd:tnpnd_periodic step 41000 lr 3.197e-04 [train_loss] tar_ll 15.1402 loss -15.1402 mean_std 0.0085 (7.812 secs)\n",
      "tnpnd:tnpnd_periodic step 41200 lr 3.182e-04 [train_loss] tar_ll 15.8116 loss -15.8116 mean_std 0.0078 (7.700 secs)\n",
      "tnpnd:tnpnd_periodic step 41400 lr 3.167e-04 [train_loss] tar_ll 13.7262 loss -13.7262 mean_std 0.0086 (7.833 secs)\n",
      "tnpnd:tnpnd_periodic step 41600 lr 3.152e-04 [train_loss] tar_ll 14.7948 loss -14.7948 mean_std 0.0080 (7.828 secs)\n",
      "tnpnd:tnpnd_periodic step 41800 lr 3.137e-04 [train_loss] tar_ll 15.5428 loss -15.5428 mean_std 0.0082 (7.933 secs)\n",
      "tnpnd:tnpnd_periodic step 42000 lr 3.122e-04 [train_loss] tar_ll 14.6996 loss -14.6996 mean_std 0.0080 (7.679 secs)\n",
      "tnpnd:tnpnd_periodic step 42200 lr 3.106e-04 [train_loss] tar_ll 14.8790 loss -14.8790 mean_std 0.0082 (7.825 secs)\n",
      "tnpnd:tnpnd_periodic step 42400 lr 3.091e-04 [train_loss] tar_ll 14.7469 loss -14.7469 mean_std 0.0090 (7.672 secs)\n",
      "tnpnd:tnpnd_periodic step 42600 lr 3.076e-04 [train_loss] tar_ll 14.1559 loss -14.1559 mean_std 0.0095 (7.638 secs)\n",
      "tnpnd:tnpnd_periodic step 42800 lr 3.061e-04 [train_loss] tar_ll 14.0253 loss -14.0253 mean_std 0.0101 (7.811 secs)\n",
      "tnpnd:tnpnd_periodic step 43000 lr 3.045e-04 [train_loss] tar_ll 16.4200 loss -16.4200 mean_std 0.0087 (7.325 secs)\n",
      "tnpnd:tnpnd_periodic step 43200 lr 3.030e-04 [train_loss] tar_ll 15.4824 loss -15.4824 mean_std 0.0077 (7.468 secs)\n",
      "tnpnd:tnpnd_periodic step 43400 lr 3.015e-04 [train_loss] tar_ll 13.2646 loss -13.2646 mean_std 0.0113 (7.527 secs)\n",
      "tnpnd:tnpnd_periodic step 43600 lr 2.999e-04 [train_loss] tar_ll 15.5002 loss -15.5002 mean_std 0.0069 (7.402 secs)\n",
      "tnpnd:tnpnd_periodic step 43800 lr 2.984e-04 [train_loss] tar_ll 15.3016 loss -15.3016 mean_std 0.0071 (7.931 secs)\n",
      "tnpnd:tnpnd_periodic step 44000 lr 2.968e-04 [train_loss] tar_ll 15.6895 loss -15.6895 mean_std 0.0067 (7.740 secs)\n",
      "tnpnd:tnpnd_periodic step 44200 lr 2.953e-04 [train_loss] tar_ll 14.7619 loss -14.7619 mean_std 0.0090 (8.067 secs)\n",
      "tnpnd:tnpnd_periodic step 44400 lr 2.938e-04 [train_loss] tar_ll 15.6060 loss -15.6060 mean_std 0.0076 (7.760 secs)\n",
      "tnpnd:tnpnd_periodic step 44600 lr 2.922e-04 [train_loss] tar_ll 14.9391 loss -14.9391 mean_std 0.0085 (8.092 secs)\n",
      "tnpnd:tnpnd_periodic step 44800 lr 2.907e-04 [train_loss] tar_ll 14.4303 loss -14.4303 mean_std 0.0087 (7.929 secs)\n",
      "tnpnd:tnpnd_periodic step 45000 lr 2.891e-04 [train_loss] tar_ll 15.4802 loss -15.4802 mean_std 0.0066 (7.760 secs)\n",
      "100%|##########| 3000/3000 [00:26<00:00, 114.38it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -3.4647 loss 3.4647 mean_std 0.0432 (26.231 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 45200 lr 2.876e-04 [train_loss] tar_ll 16.0446 loss -16.0446 mean_std 0.0069 (7.291 secs)\n",
      "tnpnd:tnpnd_periodic step 45400 lr 2.860e-04 [train_loss] tar_ll 15.1730 loss -15.1730 mean_std 0.0096 (7.521 secs)\n",
      "tnpnd:tnpnd_periodic step 45600 lr 2.844e-04 [train_loss] tar_ll 15.6289 loss -15.6289 mean_std 0.0079 (7.965 secs)\n",
      "tnpnd:tnpnd_periodic step 45800 lr 2.829e-04 [train_loss] tar_ll 15.5128 loss -15.5128 mean_std 0.0071 (7.608 secs)\n",
      "tnpnd:tnpnd_periodic step 46000 lr 2.813e-04 [train_loss] tar_ll 14.3793 loss -14.3793 mean_std 0.0100 (7.773 secs)\n",
      "tnpnd:tnpnd_periodic step 46200 lr 2.798e-04 [train_loss] tar_ll 15.3049 loss -15.3049 mean_std 0.0097 (7.814 secs)\n",
      "tnpnd:tnpnd_periodic step 46400 lr 2.782e-04 [train_loss] tar_ll 13.3614 loss -13.3614 mean_std 0.0087 (8.028 secs)\n",
      "tnpnd:tnpnd_periodic step 46600 lr 2.767e-04 [train_loss] tar_ll 15.7800 loss -15.7800 mean_std 0.0063 (7.629 secs)\n",
      "tnpnd:tnpnd_periodic step 46800 lr 2.751e-04 [train_loss] tar_ll 16.4901 loss -16.4901 mean_std 0.0079 (7.890 secs)\n",
      "tnpnd:tnpnd_periodic step 47000 lr 2.735e-04 [train_loss] tar_ll 14.6447 loss -14.6447 mean_std 0.0078 (7.672 secs)\n",
      "tnpnd:tnpnd_periodic step 47200 lr 2.720e-04 [train_loss] tar_ll 16.0080 loss -16.0080 mean_std 0.0084 (8.034 secs)\n",
      "tnpnd:tnpnd_periodic step 47400 lr 2.704e-04 [train_loss] tar_ll 14.6662 loss -14.6662 mean_std 0.0075 (7.679 secs)\n",
      "tnpnd:tnpnd_periodic step 47600 lr 2.688e-04 [train_loss] tar_ll 16.1367 loss -16.1367 mean_std 0.0085 (7.830 secs)\n",
      "tnpnd:tnpnd_periodic step 47800 lr 2.673e-04 [train_loss] tar_ll 15.2366 loss -15.2366 mean_std 0.0075 (7.663 secs)\n",
      "tnpnd:tnpnd_periodic step 48000 lr 2.657e-04 [train_loss] tar_ll 14.2994 loss -14.2994 mean_std 0.0089 (7.670 secs)\n",
      "tnpnd:tnpnd_periodic step 48200 lr 2.641e-04 [train_loss] tar_ll 15.6285 loss -15.6285 mean_std 0.0084 (7.595 secs)\n",
      "tnpnd:tnpnd_periodic step 48400 lr 2.626e-04 [train_loss] tar_ll 14.9421 loss -14.9421 mean_std 0.0090 (7.747 secs)\n",
      "tnpnd:tnpnd_periodic step 48600 lr 2.610e-04 [train_loss] tar_ll 16.2405 loss -16.2405 mean_std 0.0079 (7.503 secs)\n",
      "tnpnd:tnpnd_periodic step 48800 lr 2.594e-04 [train_loss] tar_ll 15.2932 loss -15.2932 mean_std 0.0084 (7.748 secs)\n",
      "tnpnd:tnpnd_periodic step 49000 lr 2.579e-04 [train_loss] tar_ll 15.8144 loss -15.8144 mean_std 0.0072 (7.554 secs)\n",
      "tnpnd:tnpnd_periodic step 49200 lr 2.563e-04 [train_loss] tar_ll 16.3503 loss -16.3503 mean_std 0.0078 (8.116 secs)\n",
      "tnpnd:tnpnd_periodic step 49400 lr 2.547e-04 [train_loss] tar_ll 14.8442 loss -14.8442 mean_std 0.0077 (7.889 secs)\n",
      "tnpnd:tnpnd_periodic step 49600 lr 2.531e-04 [train_loss] tar_ll 16.2714 loss -16.2714 mean_std 0.0086 (7.769 secs)\n",
      "tnpnd:tnpnd_periodic step 49800 lr 2.516e-04 [train_loss] tar_ll 15.3433 loss -15.3433 mean_std 0.0077 (7.710 secs)\n",
      "tnpnd:tnpnd_periodic step 50000 lr 2.500e-04 [train_loss] tar_ll 15.8602 loss -15.8602 mean_std 0.0072 (8.053 secs)\n",
      "100%|##########| 3000/3000 [00:26<00:00, 112.66it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -3.1231 loss 3.1231 mean_std 0.0358 (26.635 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 50200 lr 2.484e-04 [train_loss] tar_ll 15.0875 loss -15.0875 mean_std 0.0082 (8.067 secs)\n",
      "tnpnd:tnpnd_periodic step 50400 lr 2.469e-04 [train_loss] tar_ll 13.5677 loss -13.5677 mean_std 0.0083 (7.774 secs)\n",
      "tnpnd:tnpnd_periodic step 50600 lr 2.453e-04 [train_loss] tar_ll 17.0653 loss -17.0653 mean_std 0.0071 (7.972 secs)\n",
      "tnpnd:tnpnd_periodic step 50800 lr 2.437e-04 [train_loss] tar_ll 15.0324 loss -15.0324 mean_std 0.0078 (7.574 secs)\n",
      "tnpnd:tnpnd_periodic step 51000 lr 2.421e-04 [train_loss] tar_ll 17.3724 loss -17.3724 mean_std 0.0067 (7.882 secs)\n",
      "tnpnd:tnpnd_periodic step 51200 lr 2.406e-04 [train_loss] tar_ll 14.5209 loss -14.5209 mean_std 0.0088 (7.877 secs)\n",
      "tnpnd:tnpnd_periodic step 51400 lr 2.390e-04 [train_loss] tar_ll 15.4302 loss -15.4302 mean_std 0.0073 (8.026 secs)\n",
      "tnpnd:tnpnd_periodic step 51600 lr 2.374e-04 [train_loss] tar_ll 14.5241 loss -14.5241 mean_std 0.0073 (7.553 secs)\n",
      "tnpnd:tnpnd_periodic step 51800 lr 2.359e-04 [train_loss] tar_ll 14.2875 loss -14.2875 mean_std 0.0079 (7.881 secs)\n",
      "tnpnd:tnpnd_periodic step 52000 lr 2.343e-04 [train_loss] tar_ll 15.1620 loss -15.1620 mean_std 0.0082 (6.940 secs)\n",
      "tnpnd:tnpnd_periodic step 52200 lr 2.327e-04 [train_loss] tar_ll 15.3633 loss -15.3633 mean_std 0.0075 (6.832 secs)\n",
      "tnpnd:tnpnd_periodic step 52400 lr 2.312e-04 [train_loss] tar_ll 16.0301 loss -16.0301 mean_std 0.0073 (7.247 secs)\n",
      "tnpnd:tnpnd_periodic step 52600 lr 2.296e-04 [train_loss] tar_ll 16.0503 loss -16.0503 mean_std 0.0080 (7.930 secs)\n",
      "tnpnd:tnpnd_periodic step 52800 lr 2.280e-04 [train_loss] tar_ll 15.9002 loss -15.9002 mean_std 0.0067 (8.104 secs)\n",
      "tnpnd:tnpnd_periodic step 53000 lr 2.265e-04 [train_loss] tar_ll 15.9051 loss -15.9051 mean_std 0.0069 (7.747 secs)\n",
      "tnpnd:tnpnd_periodic step 53200 lr 2.249e-04 [train_loss] tar_ll 15.4946 loss -15.4946 mean_std 0.0080 (7.723 secs)\n",
      "tnpnd:tnpnd_periodic step 53400 lr 2.233e-04 [train_loss] tar_ll 15.9539 loss -15.9539 mean_std 0.0072 (7.964 secs)\n",
      "tnpnd:tnpnd_periodic step 53600 lr 2.218e-04 [train_loss] tar_ll 16.1134 loss -16.1134 mean_std 0.0088 (7.787 secs)\n",
      "tnpnd:tnpnd_periodic step 53800 lr 2.202e-04 [train_loss] tar_ll 15.1188 loss -15.1188 mean_std 0.0075 (7.503 secs)\n",
      "tnpnd:tnpnd_periodic step 54000 lr 2.187e-04 [train_loss] tar_ll 16.9437 loss -16.9437 mean_std 0.0060 (7.825 secs)\n",
      "tnpnd:tnpnd_periodic step 54200 lr 2.171e-04 [train_loss] tar_ll 17.4079 loss -17.4079 mean_std 0.0065 (7.606 secs)\n",
      "tnpnd:tnpnd_periodic step 54400 lr 2.156e-04 [train_loss] tar_ll 16.1087 loss -16.1087 mean_std 0.0074 (7.516 secs)\n",
      "tnpnd:tnpnd_periodic step 54600 lr 2.140e-04 [train_loss] tar_ll 15.6551 loss -15.6551 mean_std 0.0073 (7.665 secs)\n",
      "tnpnd:tnpnd_periodic step 54800 lr 2.124e-04 [train_loss] tar_ll 16.4887 loss -16.4887 mean_std 0.0071 (8.088 secs)\n",
      "tnpnd:tnpnd_periodic step 55000 lr 2.109e-04 [train_loss] tar_ll 15.1224 loss -15.1224 mean_std 0.0077 (8.131 secs)\n",
      "100%|##########| 3000/3000 [00:27<00:00, 109.59it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -4.8241 loss 4.8241 mean_std 0.0347 (27.379 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 55200 lr 2.093e-04 [train_loss] tar_ll 16.0928 loss -16.0928 mean_std 0.0068 (8.215 secs)\n",
      "tnpnd:tnpnd_periodic step 55400 lr 2.078e-04 [train_loss] tar_ll 15.3391 loss -15.3391 mean_std 0.0082 (7.868 secs)\n",
      "tnpnd:tnpnd_periodic step 55600 lr 2.062e-04 [train_loss] tar_ll 16.9312 loss -16.9312 mean_std 0.0072 (7.832 secs)\n",
      "tnpnd:tnpnd_periodic step 55800 lr 2.047e-04 [train_loss] tar_ll 15.4140 loss -15.4140 mean_std 0.0081 (7.708 secs)\n",
      "tnpnd:tnpnd_periodic step 56000 lr 2.032e-04 [train_loss] tar_ll 16.6858 loss -16.6858 mean_std 0.0075 (7.880 secs)\n",
      "tnpnd:tnpnd_periodic step 56200 lr 2.016e-04 [train_loss] tar_ll 16.5369 loss -16.5369 mean_std 0.0077 (7.848 secs)\n",
      "tnpnd:tnpnd_periodic step 56400 lr 2.001e-04 [train_loss] tar_ll 15.6041 loss -15.6041 mean_std 0.0064 (7.973 secs)\n",
      "tnpnd:tnpnd_periodic step 56600 lr 1.985e-04 [train_loss] tar_ll 15.4179 loss -15.4179 mean_std 0.0073 (7.497 secs)\n",
      "tnpnd:tnpnd_periodic step 56800 lr 1.970e-04 [train_loss] tar_ll 16.7135 loss -16.7135 mean_std 0.0067 (8.069 secs)\n",
      "tnpnd:tnpnd_periodic step 57000 lr 1.955e-04 [train_loss] tar_ll 14.8083 loss -14.8083 mean_std 0.0078 (7.436 secs)\n",
      "tnpnd:tnpnd_periodic step 57200 lr 1.939e-04 [train_loss] tar_ll 15.7333 loss -15.7333 mean_std 0.0066 (8.031 secs)\n",
      "tnpnd:tnpnd_periodic step 57400 lr 1.924e-04 [train_loss] tar_ll 16.3022 loss -16.3022 mean_std 0.0084 (7.664 secs)\n",
      "tnpnd:tnpnd_periodic step 57600 lr 1.909e-04 [train_loss] tar_ll 14.9202 loss -14.9202 mean_std 0.0072 (7.749 secs)\n",
      "tnpnd:tnpnd_periodic step 57800 lr 1.894e-04 [train_loss] tar_ll 15.1264 loss -15.1264 mean_std 0.0073 (7.879 secs)\n",
      "tnpnd:tnpnd_periodic step 58000 lr 1.878e-04 [train_loss] tar_ll 15.4387 loss -15.4387 mean_std 0.0075 (7.663 secs)\n",
      "tnpnd:tnpnd_periodic step 58200 lr 1.863e-04 [train_loss] tar_ll 15.5932 loss -15.5932 mean_std 0.0077 (7.935 secs)\n",
      "tnpnd:tnpnd_periodic step 58400 lr 1.848e-04 [train_loss] tar_ll 14.4246 loss -14.4246 mean_std 0.0076 (7.587 secs)\n",
      "tnpnd:tnpnd_periodic step 58600 lr 1.833e-04 [train_loss] tar_ll 16.6713 loss -16.6713 mean_std 0.0076 (7.830 secs)\n",
      "tnpnd:tnpnd_periodic step 58800 lr 1.818e-04 [train_loss] tar_ll 15.9214 loss -15.9214 mean_std 0.0064 (7.536 secs)\n",
      "tnpnd:tnpnd_periodic step 59000 lr 1.803e-04 [train_loss] tar_ll 15.2075 loss -15.2075 mean_std 0.0068 (7.736 secs)\n",
      "tnpnd:tnpnd_periodic step 59200 lr 1.787e-04 [train_loss] tar_ll 16.4945 loss -16.4945 mean_std 0.0076 (7.668 secs)\n",
      "tnpnd:tnpnd_periodic step 59400 lr 1.772e-04 [train_loss] tar_ll 15.5029 loss -15.5029 mean_std 0.0069 (6.918 secs)\n",
      "tnpnd:tnpnd_periodic step 59600 lr 1.757e-04 [train_loss] tar_ll 16.4579 loss -16.4579 mean_std 0.0076 (6.785 secs)\n",
      "tnpnd:tnpnd_periodic step 59800 lr 1.742e-04 [train_loss] tar_ll 15.8337 loss -15.8337 mean_std 0.0076 (7.879 secs)\n",
      "tnpnd:tnpnd_periodic step 60000 lr 1.727e-04 [train_loss] tar_ll 15.2974 loss -15.2974 mean_std 0.0069 (7.472 secs)\n",
      "100%|##########| 3000/3000 [00:26<00:00, 115.35it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -3.6920 loss 3.6920 mean_std 0.0408 (26.011 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 60200 lr 1.713e-04 [train_loss] tar_ll 15.3308 loss -15.3308 mean_std 0.0077 (8.101 secs)\n",
      "tnpnd:tnpnd_periodic step 60400 lr 1.698e-04 [train_loss] tar_ll 15.4303 loss -15.4303 mean_std 0.0065 (7.957 secs)\n",
      "tnpnd:tnpnd_periodic step 60600 lr 1.683e-04 [train_loss] tar_ll 16.3811 loss -16.3811 mean_std 0.0057 (7.889 secs)\n",
      "tnpnd:tnpnd_periodic step 60800 lr 1.668e-04 [train_loss] tar_ll 15.0161 loss -15.0161 mean_std 0.0074 (8.242 secs)\n",
      "tnpnd:tnpnd_periodic step 61000 lr 1.653e-04 [train_loss] tar_ll 15.1248 loss -15.1248 mean_std 0.0083 (8.073 secs)\n",
      "tnpnd:tnpnd_periodic step 61200 lr 1.638e-04 [train_loss] tar_ll 17.3225 loss -17.3225 mean_std 0.0069 (8.112 secs)\n",
      "tnpnd:tnpnd_periodic step 61400 lr 1.624e-04 [train_loss] tar_ll 16.9850 loss -16.9850 mean_std 0.0070 (7.881 secs)\n",
      "tnpnd:tnpnd_periodic step 61600 lr 1.609e-04 [train_loss] tar_ll 14.1614 loss -14.1614 mean_std 0.0096 (8.068 secs)\n",
      "tnpnd:tnpnd_periodic step 61800 lr 1.594e-04 [train_loss] tar_ll 15.7474 loss -15.7474 mean_std 0.0060 (8.014 secs)\n",
      "tnpnd:tnpnd_periodic step 62000 lr 1.580e-04 [train_loss] tar_ll 16.7566 loss -16.7566 mean_std 0.0052 (7.940 secs)\n",
      "tnpnd:tnpnd_periodic step 62200 lr 1.565e-04 [train_loss] tar_ll 15.6823 loss -15.6823 mean_std 0.0083 (7.891 secs)\n",
      "tnpnd:tnpnd_periodic step 62400 lr 1.551e-04 [train_loss] tar_ll 15.6832 loss -15.6832 mean_std 0.0080 (7.796 secs)\n",
      "tnpnd:tnpnd_periodic step 62600 lr 1.536e-04 [train_loss] tar_ll 15.7225 loss -15.7225 mean_std 0.0069 (7.442 secs)\n",
      "tnpnd:tnpnd_periodic step 62800 lr 1.522e-04 [train_loss] tar_ll 17.0450 loss -17.0450 mean_std 0.0065 (7.971 secs)\n",
      "tnpnd:tnpnd_periodic step 63000 lr 1.507e-04 [train_loss] tar_ll 16.2689 loss -16.2689 mean_std 0.0060 (7.643 secs)\n",
      "tnpnd:tnpnd_periodic step 63200 lr 1.493e-04 [train_loss] tar_ll 16.5602 loss -16.5602 mean_std 0.0080 (7.829 secs)\n",
      "tnpnd:tnpnd_periodic step 63400 lr 1.478e-04 [train_loss] tar_ll 16.8565 loss -16.8565 mean_std 0.0063 (7.685 secs)\n",
      "tnpnd:tnpnd_periodic step 63600 lr 1.464e-04 [train_loss] tar_ll 15.6077 loss -15.6077 mean_std 0.0070 (8.190 secs)\n",
      "tnpnd:tnpnd_periodic step 63800 lr 1.450e-04 [train_loss] tar_ll 15.5887 loss -15.5887 mean_std 0.0076 (7.624 secs)\n",
      "tnpnd:tnpnd_periodic step 64000 lr 1.436e-04 [train_loss] tar_ll 16.3326 loss -16.3326 mean_std 0.0074 (7.754 secs)\n",
      "tnpnd:tnpnd_periodic step 64200 lr 1.421e-04 [train_loss] tar_ll 16.2652 loss -16.2652 mean_std 0.0060 (7.688 secs)\n",
      "tnpnd:tnpnd_periodic step 64400 lr 1.407e-04 [train_loss] tar_ll 15.3667 loss -15.3667 mean_std 0.0060 (7.890 secs)\n",
      "tnpnd:tnpnd_periodic step 64600 lr 1.393e-04 [train_loss] tar_ll 14.7231 loss -14.7231 mean_std 0.0083 (7.479 secs)\n",
      "tnpnd:tnpnd_periodic step 64800 lr 1.379e-04 [train_loss] tar_ll 16.2033 loss -16.2033 mean_std 0.0075 (7.811 secs)\n",
      "tnpnd:tnpnd_periodic step 65000 lr 1.365e-04 [train_loss] tar_ll 16.0500 loss -16.0500 mean_std 0.0076 (7.575 secs)\n",
      "100%|##########| 3000/3000 [00:25<00:00, 117.30it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -2.7552 loss 2.7552 mean_std 0.0381 (25.577 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 65200 lr 1.351e-04 [train_loss] tar_ll 14.5987 loss -14.5987 mean_std 0.0069 (6.955 secs)\n",
      "tnpnd:tnpnd_periodic step 65400 lr 1.337e-04 [train_loss] tar_ll 17.6355 loss -17.6355 mean_std 0.0069 (7.122 secs)\n",
      "tnpnd:tnpnd_periodic step 65600 lr 1.323e-04 [train_loss] tar_ll 17.2340 loss -17.2340 mean_std 0.0066 (8.047 secs)\n",
      "tnpnd:tnpnd_periodic step 65800 lr 1.309e-04 [train_loss] tar_ll 17.0394 loss -17.0394 mean_std 0.0064 (8.167 secs)\n",
      "tnpnd:tnpnd_periodic step 66000 lr 1.296e-04 [train_loss] tar_ll 17.3184 loss -17.3184 mean_std 0.0064 (7.684 secs)\n",
      "tnpnd:tnpnd_periodic step 66200 lr 1.282e-04 [train_loss] tar_ll 18.0485 loss -18.0485 mean_std 0.0061 (8.269 secs)\n",
      "tnpnd:tnpnd_periodic step 66400 lr 1.268e-04 [train_loss] tar_ll 15.6581 loss -15.6581 mean_std 0.0065 (8.109 secs)\n",
      "tnpnd:tnpnd_periodic step 66600 lr 1.255e-04 [train_loss] tar_ll 15.9699 loss -15.9699 mean_std 0.0062 (8.081 secs)\n",
      "tnpnd:tnpnd_periodic step 66800 lr 1.241e-04 [train_loss] tar_ll 16.4922 loss -16.4922 mean_std 0.0068 (7.852 secs)\n",
      "tnpnd:tnpnd_periodic step 67000 lr 1.227e-04 [train_loss] tar_ll 17.2773 loss -17.2773 mean_std 0.0061 (8.093 secs)\n",
      "tnpnd:tnpnd_periodic step 67200 lr 1.214e-04 [train_loss] tar_ll 16.4751 loss -16.4751 mean_std 0.0070 (7.780 secs)\n",
      "tnpnd:tnpnd_periodic step 67400 lr 1.200e-04 [train_loss] tar_ll 16.1790 loss -16.1790 mean_std 0.0064 (8.161 secs)\n",
      "tnpnd:tnpnd_periodic step 67600 lr 1.187e-04 [train_loss] tar_ll 15.7680 loss -15.7680 mean_std 0.0069 (7.668 secs)\n",
      "tnpnd:tnpnd_periodic step 67800 lr 1.174e-04 [train_loss] tar_ll 17.4436 loss -17.4436 mean_std 0.0060 (7.829 secs)\n",
      "tnpnd:tnpnd_periodic step 68000 lr 1.160e-04 [train_loss] tar_ll 15.1443 loss -15.1443 mean_std 0.0066 (7.582 secs)\n",
      "tnpnd:tnpnd_periodic step 68200 lr 1.147e-04 [train_loss] tar_ll 17.8410 loss -17.8410 mean_std 0.0064 (8.127 secs)\n",
      "tnpnd:tnpnd_periodic step 68400 lr 1.134e-04 [train_loss] tar_ll 17.1130 loss -17.1130 mean_std 0.0072 (7.625 secs)\n",
      "tnpnd:tnpnd_periodic step 68600 lr 1.121e-04 [train_loss] tar_ll 16.7737 loss -16.7737 mean_std 0.0064 (8.050 secs)\n",
      "tnpnd:tnpnd_periodic step 68800 lr 1.108e-04 [train_loss] tar_ll 16.6137 loss -16.6137 mean_std 0.0079 (7.633 secs)\n",
      "tnpnd:tnpnd_periodic step 69000 lr 1.095e-04 [train_loss] tar_ll 15.9134 loss -15.9134 mean_std 0.0075 (8.116 secs)\n",
      "tnpnd:tnpnd_periodic step 69200 lr 1.082e-04 [train_loss] tar_ll 16.9501 loss -16.9501 mean_std 0.0065 (8.106 secs)\n",
      "tnpnd:tnpnd_periodic step 69400 lr 1.069e-04 [train_loss] tar_ll 16.4023 loss -16.4023 mean_std 0.0079 (7.906 secs)\n",
      "tnpnd:tnpnd_periodic step 69600 lr 1.056e-04 [train_loss] tar_ll 16.8621 loss -16.8621 mean_std 0.0073 (7.792 secs)\n",
      "tnpnd:tnpnd_periodic step 69800 lr 1.043e-04 [train_loss] tar_ll 18.2346 loss -18.2346 mean_std 0.0066 (7.866 secs)\n",
      "tnpnd:tnpnd_periodic step 70000 lr 1.031e-04 [train_loss] tar_ll 16.2738 loss -16.2738 mean_std 0.0073 (7.598 secs)\n",
      "100%|##########| 3000/3000 [00:26<00:00, 115.02it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -3.0906 loss 3.0906 mean_std 0.0365 (26.086 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 70200 lr 1.018e-04 [train_loss] tar_ll 15.3679 loss -15.3679 mean_std 0.0082 (7.890 secs)\n",
      "tnpnd:tnpnd_periodic step 70400 lr 1.005e-04 [train_loss] tar_ll 16.4524 loss -16.4524 mean_std 0.0076 (7.972 secs)\n",
      "tnpnd:tnpnd_periodic step 70600 lr 9.927e-05 [train_loss] tar_ll 15.2139 loss -15.2139 mean_std 0.0062 (7.619 secs)\n",
      "tnpnd:tnpnd_periodic step 70800 lr 9.802e-05 [train_loss] tar_ll 17.0517 loss -17.0517 mean_std 0.0073 (7.788 secs)\n",
      "tnpnd:tnpnd_periodic step 71000 lr 9.677e-05 [train_loss] tar_ll 17.5781 loss -17.5781 mean_std 0.0061 (7.767 secs)\n",
      "tnpnd:tnpnd_periodic step 71200 lr 9.554e-05 [train_loss] tar_ll 16.6514 loss -16.6514 mean_std 0.0067 (8.058 secs)\n",
      "tnpnd:tnpnd_periodic step 71400 lr 9.430e-05 [train_loss] tar_ll 15.8125 loss -15.8125 mean_std 0.0075 (7.591 secs)\n",
      "tnpnd:tnpnd_periodic step 71600 lr 9.308e-05 [train_loss] tar_ll 16.0548 loss -16.0548 mean_std 0.0067 (8.034 secs)\n",
      "tnpnd:tnpnd_periodic step 71800 lr 9.186e-05 [train_loss] tar_ll 16.0827 loss -16.0827 mean_std 0.0066 (7.615 secs)\n",
      "tnpnd:tnpnd_periodic step 72000 lr 9.064e-05 [train_loss] tar_ll 15.3609 loss -15.3609 mean_std 0.0073 (7.911 secs)\n",
      "tnpnd:tnpnd_periodic step 72200 lr 8.944e-05 [train_loss] tar_ll 17.5066 loss -17.5066 mean_std 0.0057 (8.067 secs)\n",
      "tnpnd:tnpnd_periodic step 72400 lr 8.824e-05 [train_loss] tar_ll 17.2525 loss -17.2525 mean_std 0.0058 (8.079 secs)\n",
      "tnpnd:tnpnd_periodic step 72600 lr 8.704e-05 [train_loss] tar_ll 17.3265 loss -17.3265 mean_std 0.0065 (6.992 secs)\n",
      "tnpnd:tnpnd_periodic step 72800 lr 8.585e-05 [train_loss] tar_ll 16.4460 loss -16.4460 mean_std 0.0066 (6.998 secs)\n",
      "tnpnd:tnpnd_periodic step 73000 lr 8.467e-05 [train_loss] tar_ll 15.7732 loss -15.7732 mean_std 0.0076 (7.210 secs)\n",
      "tnpnd:tnpnd_periodic step 73200 lr 8.350e-05 [train_loss] tar_ll 16.1223 loss -16.1223 mean_std 0.0070 (7.912 secs)\n",
      "tnpnd:tnpnd_periodic step 73400 lr 8.233e-05 [train_loss] tar_ll 16.2291 loss -16.2291 mean_std 0.0074 (7.289 secs)\n",
      "tnpnd:tnpnd_periodic step 73600 lr 8.117e-05 [train_loss] tar_ll 17.1714 loss -17.1714 mean_std 0.0060 (7.770 secs)\n",
      "tnpnd:tnpnd_periodic step 73800 lr 8.001e-05 [train_loss] tar_ll 17.1506 loss -17.1506 mean_std 0.0064 (7.764 secs)\n",
      "tnpnd:tnpnd_periodic step 74000 lr 7.886e-05 [train_loss] tar_ll 17.5478 loss -17.5478 mean_std 0.0064 (7.773 secs)\n",
      "tnpnd:tnpnd_periodic step 74200 lr 7.772e-05 [train_loss] tar_ll 17.6449 loss -17.6449 mean_std 0.0071 (7.793 secs)\n",
      "tnpnd:tnpnd_periodic step 74400 lr 7.659e-05 [train_loss] tar_ll 18.0978 loss -18.0978 mean_std 0.0061 (7.931 secs)\n",
      "tnpnd:tnpnd_periodic step 74600 lr 7.546e-05 [train_loss] tar_ll 17.0427 loss -17.0427 mean_std 0.0063 (7.893 secs)\n",
      "tnpnd:tnpnd_periodic step 74800 lr 7.434e-05 [train_loss] tar_ll 16.7775 loss -16.7775 mean_std 0.0053 (8.013 secs)\n",
      "tnpnd:tnpnd_periodic step 75000 lr 7.322e-05 [train_loss] tar_ll 16.4870 loss -16.4870 mean_std 0.0061 (7.908 secs)\n",
      "100%|##########| 3000/3000 [00:26<00:00, 112.26it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -3.2418 loss 3.2418 mean_std 0.0401 (26.725 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 75200 lr 7.212e-05 [train_loss] tar_ll 17.9320 loss -17.9320 mean_std 0.0068 (8.145 secs)\n",
      "tnpnd:tnpnd_periodic step 75400 lr 7.102e-05 [train_loss] tar_ll 16.2501 loss -16.2501 mean_std 0.0063 (8.199 secs)\n",
      "tnpnd:tnpnd_periodic step 75600 lr 6.992e-05 [train_loss] tar_ll 18.9779 loss -18.9779 mean_std 0.0065 (8.050 secs)\n",
      "tnpnd:tnpnd_periodic step 75800 lr 6.884e-05 [train_loss] tar_ll 16.3916 loss -16.3916 mean_std 0.0068 (7.961 secs)\n",
      "tnpnd:tnpnd_periodic step 76000 lr 6.776e-05 [train_loss] tar_ll 16.5857 loss -16.5857 mean_std 0.0070 (7.827 secs)\n",
      "tnpnd:tnpnd_periodic step 76200 lr 6.669e-05 [train_loss] tar_ll 15.7258 loss -15.7258 mean_std 0.0061 (8.110 secs)\n",
      "tnpnd:tnpnd_periodic step 76400 lr 6.562e-05 [train_loss] tar_ll 16.2267 loss -16.2267 mean_std 0.0059 (7.903 secs)\n",
      "tnpnd:tnpnd_periodic step 76600 lr 6.456e-05 [train_loss] tar_ll 16.2570 loss -16.2570 mean_std 0.0064 (8.191 secs)\n",
      "tnpnd:tnpnd_periodic step 76800 lr 6.351e-05 [train_loss] tar_ll 16.5547 loss -16.5547 mean_std 0.0067 (7.737 secs)\n",
      "tnpnd:tnpnd_periodic step 77000 lr 6.247e-05 [train_loss] tar_ll 17.3985 loss -17.3985 mean_std 0.0057 (8.500 secs)\n",
      "tnpnd:tnpnd_periodic step 77200 lr 6.144e-05 [train_loss] tar_ll 16.3334 loss -16.3334 mean_std 0.0062 (8.234 secs)\n",
      "tnpnd:tnpnd_periodic step 77400 lr 6.041e-05 [train_loss] tar_ll 16.3314 loss -16.3314 mean_std 0.0067 (8.147 secs)\n",
      "tnpnd:tnpnd_periodic step 77600 lr 5.939e-05 [train_loss] tar_ll 16.6949 loss -16.6949 mean_std 0.0062 (7.722 secs)\n",
      "tnpnd:tnpnd_periodic step 77800 lr 5.838e-05 [train_loss] tar_ll 17.9010 loss -17.9010 mean_std 0.0058 (7.968 secs)\n",
      "tnpnd:tnpnd_periodic step 78000 lr 5.737e-05 [train_loss] tar_ll 15.8552 loss -15.8552 mean_std 0.0067 (7.853 secs)\n",
      "tnpnd:tnpnd_periodic step 78200 lr 5.637e-05 [train_loss] tar_ll 16.7026 loss -16.7026 mean_std 0.0067 (8.125 secs)\n",
      "tnpnd:tnpnd_periodic step 78400 lr 5.538e-05 [train_loss] tar_ll 15.2581 loss -15.2581 mean_std 0.0069 (7.935 secs)\n",
      "tnpnd:tnpnd_periodic step 78600 lr 5.440e-05 [train_loss] tar_ll 16.7906 loss -16.7906 mean_std 0.0063 (7.974 secs)\n",
      "tnpnd:tnpnd_periodic step 78800 lr 5.343e-05 [train_loss] tar_ll 17.3253 loss -17.3253 mean_std 0.0057 (7.383 secs)\n",
      "tnpnd:tnpnd_periodic step 79000 lr 5.246e-05 [train_loss] tar_ll 16.9277 loss -16.9277 mean_std 0.0055 (7.564 secs)\n",
      "tnpnd:tnpnd_periodic step 79200 lr 5.150e-05 [train_loss] tar_ll 17.5513 loss -17.5513 mean_std 0.0066 (7.293 secs)\n",
      "tnpnd:tnpnd_periodic step 79400 lr 5.055e-05 [train_loss] tar_ll 16.8981 loss -16.8981 mean_std 0.0061 (7.740 secs)\n",
      "tnpnd:tnpnd_periodic step 79600 lr 4.961e-05 [train_loss] tar_ll 17.9805 loss -17.9805 mean_std 0.0066 (7.805 secs)\n",
      "tnpnd:tnpnd_periodic step 79800 lr 4.867e-05 [train_loss] tar_ll 16.7726 loss -16.7726 mean_std 0.0067 (7.853 secs)\n",
      "tnpnd:tnpnd_periodic step 80000 lr 4.775e-05 [train_loss] tar_ll 17.2243 loss -17.2243 mean_std 0.0070 (8.016 secs)\n",
      "100%|##########| 3000/3000 [00:23<00:00, 129.40it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -3.4656 loss 3.4656 mean_std 0.0382 (23.187 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 80200 lr 4.683e-05 [train_loss] tar_ll 16.3431 loss -16.3431 mean_std 0.0060 (7.368 secs)\n",
      "tnpnd:tnpnd_periodic step 80400 lr 4.592e-05 [train_loss] tar_ll 17.2005 loss -17.2005 mean_std 0.0065 (7.911 secs)\n",
      "tnpnd:tnpnd_periodic step 80600 lr 4.501e-05 [train_loss] tar_ll 17.9914 loss -17.9914 mean_std 0.0060 (7.605 secs)\n",
      "tnpnd:tnpnd_periodic step 80800 lr 4.412e-05 [train_loss] tar_ll 18.4051 loss -18.4051 mean_std 0.0070 (7.823 secs)\n",
      "tnpnd:tnpnd_periodic step 81000 lr 4.323e-05 [train_loss] tar_ll 17.3651 loss -17.3651 mean_std 0.0051 (7.709 secs)\n",
      "tnpnd:tnpnd_periodic step 81200 lr 4.235e-05 [train_loss] tar_ll 17.3105 loss -17.3105 mean_std 0.0046 (8.047 secs)\n",
      "tnpnd:tnpnd_periodic step 81400 lr 4.148e-05 [train_loss] tar_ll 17.8786 loss -17.8786 mean_std 0.0064 (7.584 secs)\n",
      "tnpnd:tnpnd_periodic step 81600 lr 4.062e-05 [train_loss] tar_ll 18.1043 loss -18.1043 mean_std 0.0060 (7.921 secs)\n",
      "tnpnd:tnpnd_periodic step 81800 lr 3.976e-05 [train_loss] tar_ll 17.2854 loss -17.2854 mean_std 0.0054 (7.598 secs)\n",
      "tnpnd:tnpnd_periodic step 82000 lr 3.892e-05 [train_loss] tar_ll 17.0435 loss -17.0435 mean_std 0.0068 (8.047 secs)\n",
      "tnpnd:tnpnd_periodic step 82200 lr 3.808e-05 [train_loss] tar_ll 17.3550 loss -17.3550 mean_std 0.0066 (7.905 secs)\n",
      "tnpnd:tnpnd_periodic step 82400 lr 3.725e-05 [train_loss] tar_ll 17.8916 loss -17.8916 mean_std 0.0063 (7.877 secs)\n",
      "tnpnd:tnpnd_periodic step 82600 lr 3.643e-05 [train_loss] tar_ll 16.0095 loss -16.0095 mean_std 0.0067 (7.853 secs)\n",
      "tnpnd:tnpnd_periodic step 82800 lr 3.562e-05 [train_loss] tar_ll 15.9146 loss -15.9146 mean_std 0.0061 (7.734 secs)\n",
      "tnpnd:tnpnd_periodic step 83000 lr 3.481e-05 [train_loss] tar_ll 15.6603 loss -15.6603 mean_std 0.0067 (7.746 secs)\n",
      "tnpnd:tnpnd_periodic step 83200 lr 3.402e-05 [train_loss] tar_ll 16.4969 loss -16.4969 mean_std 0.0069 (7.897 secs)\n",
      "tnpnd:tnpnd_periodic step 83400 lr 3.323e-05 [train_loss] tar_ll 17.3328 loss -17.3328 mean_std 0.0063 (8.048 secs)\n",
      "tnpnd:tnpnd_periodic step 83600 lr 3.245e-05 [train_loss] tar_ll 17.0669 loss -17.0669 mean_std 0.0062 (7.843 secs)\n",
      "tnpnd:tnpnd_periodic step 83800 lr 3.168e-05 [train_loss] tar_ll 16.8587 loss -16.8587 mean_std 0.0071 (7.932 secs)\n",
      "tnpnd:tnpnd_periodic step 84000 lr 3.092e-05 [train_loss] tar_ll 18.2330 loss -18.2330 mean_std 0.0061 (7.766 secs)\n",
      "tnpnd:tnpnd_periodic step 84200 lr 3.017e-05 [train_loss] tar_ll 16.7385 loss -16.7385 mean_std 0.0064 (7.938 secs)\n",
      "tnpnd:tnpnd_periodic step 84400 lr 2.943e-05 [train_loss] tar_ll 17.7210 loss -17.7210 mean_std 0.0057 (7.394 secs)\n",
      "tnpnd:tnpnd_periodic step 84600 lr 2.869e-05 [train_loss] tar_ll 16.9040 loss -16.9040 mean_std 0.0067 (8.081 secs)\n",
      "tnpnd:tnpnd_periodic step 84800 lr 2.797e-05 [train_loss] tar_ll 17.3392 loss -17.3392 mean_std 0.0065 (7.729 secs)\n",
      "tnpnd:tnpnd_periodic step 85000 lr 2.725e-05 [train_loss] tar_ll 18.1003 loss -18.1003 mean_std 0.0055 (7.855 secs)\n",
      "100%|##########| 3000/3000 [00:26<00:00, 113.57it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -3.0533 loss 3.0533 mean_std 0.0397 (26.416 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 85200 lr 2.654e-05 [train_loss] tar_ll 16.5165 loss -16.5165 mean_std 0.0078 (7.824 secs)\n",
      "tnpnd:tnpnd_periodic step 85400 lr 2.584e-05 [train_loss] tar_ll 16.9681 loss -16.9681 mean_std 0.0067 (7.990 secs)\n",
      "tnpnd:tnpnd_periodic step 85600 lr 2.515e-05 [train_loss] tar_ll 17.1022 loss -17.1022 mean_std 0.0057 (7.848 secs)\n",
      "tnpnd:tnpnd_periodic step 85800 lr 2.447e-05 [train_loss] tar_ll 17.8878 loss -17.8878 mean_std 0.0059 (7.701 secs)\n",
      "tnpnd:tnpnd_periodic step 86000 lr 2.379e-05 [train_loss] tar_ll 16.6713 loss -16.6713 mean_std 0.0057 (7.495 secs)\n",
      "tnpnd:tnpnd_periodic step 86200 lr 2.313e-05 [train_loss] tar_ll 17.6391 loss -17.6391 mean_std 0.0055 (7.657 secs)\n",
      "tnpnd:tnpnd_periodic step 86400 lr 2.247e-05 [train_loss] tar_ll 16.4459 loss -16.4459 mean_std 0.0072 (7.570 secs)\n",
      "tnpnd:tnpnd_periodic step 86600 lr 2.183e-05 [train_loss] tar_ll 17.6862 loss -17.6862 mean_std 0.0062 (7.776 secs)\n",
      "tnpnd:tnpnd_periodic step 86800 lr 2.119e-05 [train_loss] tar_ll 18.1560 loss -18.1560 mean_std 0.0055 (7.812 secs)\n",
      "tnpnd:tnpnd_periodic step 87000 lr 2.056e-05 [train_loss] tar_ll 17.4820 loss -17.4820 mean_std 0.0064 (7.647 secs)\n",
      "tnpnd:tnpnd_periodic step 87200 lr 1.994e-05 [train_loss] tar_ll 16.9283 loss -16.9283 mean_std 0.0067 (8.055 secs)\n",
      "tnpnd:tnpnd_periodic step 87400 lr 1.933e-05 [train_loss] tar_ll 17.3543 loss -17.3543 mean_std 0.0071 (7.782 secs)\n",
      "tnpnd:tnpnd_periodic step 87600 lr 1.873e-05 [train_loss] tar_ll 17.9029 loss -17.9029 mean_std 0.0067 (8.246 secs)\n",
      "tnpnd:tnpnd_periodic step 87800 lr 1.814e-05 [train_loss] tar_ll 16.3345 loss -16.3345 mean_std 0.0068 (8.209 secs)\n",
      "tnpnd:tnpnd_periodic step 88000 lr 1.756e-05 [train_loss] tar_ll 18.1175 loss -18.1175 mean_std 0.0053 (8.334 secs)\n",
      "tnpnd:tnpnd_periodic step 88200 lr 1.698e-05 [train_loss] tar_ll 16.1692 loss -16.1692 mean_std 0.0066 (8.254 secs)\n",
      "tnpnd:tnpnd_periodic step 88400 lr 1.642e-05 [train_loss] tar_ll 16.2285 loss -16.2285 mean_std 0.0069 (7.510 secs)\n",
      "tnpnd:tnpnd_periodic step 88600 lr 1.586e-05 [train_loss] tar_ll 16.3481 loss -16.3481 mean_std 0.0073 (6.764 secs)\n",
      "tnpnd:tnpnd_periodic step 88800 lr 1.532e-05 [train_loss] tar_ll 16.4425 loss -16.4425 mean_std 0.0067 (6.833 secs)\n",
      "tnpnd:tnpnd_periodic step 89000 lr 1.478e-05 [train_loss] tar_ll 16.7928 loss -16.7928 mean_std 0.0060 (7.025 secs)\n",
      "tnpnd:tnpnd_periodic step 89200 lr 1.425e-05 [train_loss] tar_ll 17.8206 loss -17.8206 mean_std 0.0057 (7.010 secs)\n",
      "tnpnd:tnpnd_periodic step 89400 lr 1.373e-05 [train_loss] tar_ll 16.3033 loss -16.3033 mean_std 0.0066 (7.337 secs)\n",
      "tnpnd:tnpnd_periodic step 89600 lr 1.323e-05 [train_loss] tar_ll 17.9570 loss -17.9570 mean_std 0.0053 (7.958 secs)\n",
      "tnpnd:tnpnd_periodic step 89800 lr 1.273e-05 [train_loss] tar_ll 17.5758 loss -17.5758 mean_std 0.0054 (7.648 secs)\n",
      "tnpnd:tnpnd_periodic step 90000 lr 1.224e-05 [train_loss] tar_ll 17.7136 loss -17.7136 mean_std 0.0063 (7.660 secs)\n",
      "100%|##########| 3000/3000 [00:26<00:00, 113.56it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -3.5709 loss 3.5709 mean_std 0.0351 (26.423 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 90200 lr 1.176e-05 [train_loss] tar_ll 17.7475 loss -17.7475 mean_std 0.0063 (8.044 secs)\n",
      "tnpnd:tnpnd_periodic step 90400 lr 1.128e-05 [train_loss] tar_ll 17.3896 loss -17.3896 mean_std 0.0066 (7.666 secs)\n",
      "tnpnd:tnpnd_periodic step 90600 lr 1.082e-05 [train_loss] tar_ll 17.5344 loss -17.5344 mean_std 0.0059 (7.996 secs)\n",
      "tnpnd:tnpnd_periodic step 90800 lr 1.037e-05 [train_loss] tar_ll 17.3145 loss -17.3145 mean_std 0.0062 (7.779 secs)\n",
      "tnpnd:tnpnd_periodic step 91000 lr 9.927e-06 [train_loss] tar_ll 16.9334 loss -16.9334 mean_std 0.0049 (7.762 secs)\n",
      "tnpnd:tnpnd_periodic step 91200 lr 9.493e-06 [train_loss] tar_ll 17.4820 loss -17.4820 mean_std 0.0053 (7.623 secs)\n",
      "tnpnd:tnpnd_periodic step 91400 lr 9.069e-06 [train_loss] tar_ll 15.5887 loss -15.5887 mean_std 0.0064 (8.015 secs)\n",
      "tnpnd:tnpnd_periodic step 91600 lr 8.655e-06 [train_loss] tar_ll 18.1805 loss -18.1805 mean_std 0.0057 (7.863 secs)\n",
      "tnpnd:tnpnd_periodic step 91800 lr 8.250e-06 [train_loss] tar_ll 16.0350 loss -16.0350 mean_std 0.0079 (7.705 secs)\n",
      "tnpnd:tnpnd_periodic step 92000 lr 7.854e-06 [train_loss] tar_ll 17.4632 loss -17.4632 mean_std 0.0056 (7.877 secs)\n",
      "tnpnd:tnpnd_periodic step 92200 lr 7.468e-06 [train_loss] tar_ll 16.8730 loss -16.8730 mean_std 0.0072 (8.137 secs)\n",
      "tnpnd:tnpnd_periodic step 92400 lr 7.092e-06 [train_loss] tar_ll 17.1559 loss -17.1559 mean_std 0.0059 (8.181 secs)\n",
      "tnpnd:tnpnd_periodic step 92600 lr 6.725e-06 [train_loss] tar_ll 15.3539 loss -15.3539 mean_std 0.0070 (8.226 secs)\n",
      "tnpnd:tnpnd_periodic step 92800 lr 6.368e-06 [train_loss] tar_ll 17.7556 loss -17.7556 mean_std 0.0062 (7.993 secs)\n",
      "tnpnd:tnpnd_periodic step 93000 lr 6.021e-06 [train_loss] tar_ll 16.5203 loss -16.5203 mean_std 0.0064 (8.037 secs)\n",
      "tnpnd:tnpnd_periodic step 93200 lr 5.683e-06 [train_loss] tar_ll 16.7789 loss -16.7789 mean_std 0.0071 (7.901 secs)\n",
      "tnpnd:tnpnd_periodic step 93400 lr 5.355e-06 [train_loss] tar_ll 17.0472 loss -17.0472 mean_std 0.0062 (8.098 secs)\n",
      "tnpnd:tnpnd_periodic step 93600 lr 5.036e-06 [train_loss] tar_ll 15.8754 loss -15.8754 mean_std 0.0060 (8.064 secs)\n",
      "tnpnd:tnpnd_periodic step 93800 lr 4.727e-06 [train_loss] tar_ll 16.4590 loss -16.4590 mean_std 0.0063 (8.246 secs)\n",
      "tnpnd:tnpnd_periodic step 94000 lr 4.428e-06 [train_loss] tar_ll 17.5977 loss -17.5977 mean_std 0.0068 (7.600 secs)\n",
      "tnpnd:tnpnd_periodic step 94200 lr 4.139e-06 [train_loss] tar_ll 17.5159 loss -17.5159 mean_std 0.0061 (8.244 secs)\n",
      "tnpnd:tnpnd_periodic step 94400 lr 3.859e-06 [train_loss] tar_ll 17.1383 loss -17.1383 mean_std 0.0067 (7.624 secs)\n",
      "tnpnd:tnpnd_periodic step 94600 lr 3.589e-06 [train_loss] tar_ll 16.7966 loss -16.7966 mean_std 0.0051 (7.858 secs)\n",
      "tnpnd:tnpnd_periodic step 94800 lr 3.329e-06 [train_loss] tar_ll 16.7438 loss -16.7438 mean_std 0.0061 (7.552 secs)\n",
      "tnpnd:tnpnd_periodic step 95000 lr 3.078e-06 [train_loss] tar_ll 16.4762 loss -16.4762 mean_std 0.0067 (8.162 secs)\n",
      "100%|##########| 3000/3000 [00:26<00:00, 112.89it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -3.3463 loss 3.3463 mean_std 0.0387 (26.578 secs)\n",
      "\n",
      "tnpnd:tnpnd_periodic step 95200 lr 2.837e-06 [train_loss] tar_ll 15.8453 loss -15.8453 mean_std 0.0069 (7.859 secs)\n",
      "tnpnd:tnpnd_periodic step 95400 lr 2.606e-06 [train_loss] tar_ll 16.3290 loss -16.3290 mean_std 0.0064 (7.567 secs)\n",
      "tnpnd:tnpnd_periodic step 95600 lr 2.385e-06 [train_loss] tar_ll 16.8474 loss -16.8474 mean_std 0.0065 (7.806 secs)\n",
      "tnpnd:tnpnd_periodic step 95800 lr 2.173e-06 [train_loss] tar_ll 17.3659 loss -17.3659 mean_std 0.0069 (7.793 secs)\n",
      "tnpnd:tnpnd_periodic step 96000 lr 1.971e-06 [train_loss] tar_ll 19.1351 loss -19.1351 mean_std 0.0059 (7.818 secs)\n",
      "tnpnd:tnpnd_periodic step 96200 lr 1.779e-06 [train_loss] tar_ll 17.6644 loss -17.6644 mean_std 0.0056 (7.711 secs)\n",
      "tnpnd:tnpnd_periodic step 96400 lr 1.597e-06 [train_loss] tar_ll 17.4252 loss -17.4252 mean_std 0.0059 (7.693 secs)\n",
      "tnpnd:tnpnd_periodic step 96600 lr 1.425e-06 [train_loss] tar_ll 17.9350 loss -17.9350 mean_std 0.0055 (7.917 secs)\n",
      "tnpnd:tnpnd_periodic step 96800 lr 1.262e-06 [train_loss] tar_ll 17.2222 loss -17.2222 mean_std 0.0065 (7.847 secs)\n",
      "tnpnd:tnpnd_periodic step 97000 lr 1.110e-06 [train_loss] tar_ll 18.0388 loss -18.0388 mean_std 0.0053 (7.485 secs)\n",
      "tnpnd:tnpnd_periodic step 97200 lr 9.666e-07 [train_loss] tar_ll 15.6067 loss -15.6067 mean_std 0.0061 (7.832 secs)\n",
      "tnpnd:tnpnd_periodic step 97400 lr 8.335e-07 [train_loss] tar_ll 16.8029 loss -16.8029 mean_std 0.0052 (8.104 secs)\n",
      "tnpnd:tnpnd_periodic step 97600 lr 7.103e-07 [train_loss] tar_ll 18.3788 loss -18.3788 mean_std 0.0061 (8.270 secs)\n",
      "tnpnd:tnpnd_periodic step 97800 lr 5.969e-07 [train_loss] tar_ll 16.6015 loss -16.6015 mean_std 0.0064 (7.593 secs)\n",
      "tnpnd:tnpnd_periodic step 98000 lr 4.933e-07 [train_loss] tar_ll 17.2362 loss -17.2362 mean_std 0.0070 (7.146 secs)\n",
      "tnpnd:tnpnd_periodic step 98200 lr 3.996e-07 [train_loss] tar_ll 18.7154 loss -18.7154 mean_std 0.0055 (7.039 secs)\n",
      "tnpnd:tnpnd_periodic step 98400 lr 3.158e-07 [train_loss] tar_ll 18.0018 loss -18.0018 mean_std 0.0063 (7.919 secs)\n",
      "tnpnd:tnpnd_periodic step 98600 lr 2.418e-07 [train_loss] tar_ll 17.9524 loss -17.9524 mean_std 0.0063 (7.757 secs)\n",
      "tnpnd:tnpnd_periodic step 98800 lr 1.776e-07 [train_loss] tar_ll 17.4237 loss -17.4237 mean_std 0.0060 (7.759 secs)\n",
      "tnpnd:tnpnd_periodic step 99000 lr 1.234e-07 [train_loss] tar_ll 16.7185 loss -16.7185 mean_std 0.0064 (8.124 secs)\n",
      "tnpnd:tnpnd_periodic step 99200 lr 7.895e-08 [train_loss] tar_ll 16.8886 loss -16.8886 mean_std 0.0059 (8.485 secs)\n",
      "tnpnd:tnpnd_periodic step 99400 lr 4.441e-08 [train_loss] tar_ll 17.4025 loss -17.4025 mean_std 0.0054 (8.120 secs)\n",
      "tnpnd:tnpnd_periodic step 99600 lr 1.974e-08 [train_loss] tar_ll 16.3724 loss -16.3724 mean_std 0.0056 (8.048 secs)\n",
      "tnpnd:tnpnd_periodic step 99800 lr 4.935e-09 [train_loss] tar_ll 17.3005 loss -17.3005 mean_std 0.0053 (7.951 secs)\n",
      "tnpnd:tnpnd_periodic step 100000 lr 0.000e+00 [train_loss] tar_ll 17.5620 loss -17.5620 mean_std 0.0065 (8.141 secs)\n",
      "100%|##########| 3000/3000 [00:26<00:00, 112.33it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -3.3841 loss 3.3841 mean_std 0.0383 (26.710 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:26<00:00, 113.77it/s]\n",
      "tnpnd:tnpnd_periodic periodic tar_ll -3.3841 loss 3.3841 mean_std 0.0383 (26.373 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4479724.0 miliseconds\n",
      "Execution time: 4479.724 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 45.77783203125 MB\n",
      "Memory Usage Change: 29.52783203125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='tnpnd', name='tnpnd_periodic',val_seed=100, val_l=None,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ace92-1e42-43a4-8718-54a62b475c3c",
   "metadata": {},
   "source": [
    "## TNP-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "745912ca-5861-45f9-a74f-3b510ae5af3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: tnpa-tnpa_periodic\n",
      "Total number of parameters: 222082\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument pretrain: False\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tnpa:tnpa_periodic step 200 lr 5.000e-04 [train_loss] tar_ll -0.5877 loss 0.5877 (5.713 secs)\n",
      "tnpa:tnpa_periodic step 400 lr 5.000e-04 [train_loss] tar_ll -0.1061 loss 0.1061 (5.640 secs)\n",
      "tnpa:tnpa_periodic step 600 lr 5.000e-04 [train_loss] tar_ll 0.2479 loss -0.2479 (5.394 secs)\n",
      "tnpa:tnpa_periodic step 800 lr 4.999e-04 [train_loss] tar_ll 0.2940 loss -0.2940 (5.636 secs)\n",
      "tnpa:tnpa_periodic step 1000 lr 4.999e-04 [train_loss] tar_ll 0.5845 loss -0.5845 (5.471 secs)\n",
      "tnpa:tnpa_periodic step 1200 lr 4.998e-04 [train_loss] tar_ll 0.6004 loss -0.6004 (5.309 secs)\n",
      "tnpa:tnpa_periodic step 1400 lr 4.998e-04 [train_loss] tar_ll 0.6713 loss -0.6713 (5.119 secs)\n",
      "tnpa:tnpa_periodic step 1600 lr 4.997e-04 [train_loss] tar_ll 0.7451 loss -0.7451 (5.140 secs)\n",
      "tnpa:tnpa_periodic step 1800 lr 4.996e-04 [train_loss] tar_ll 0.8317 loss -0.8317 (5.309 secs)\n",
      "tnpa:tnpa_periodic step 2000 lr 4.995e-04 [train_loss] tar_ll 0.8526 loss -0.8526 (5.266 secs)\n",
      "tnpa:tnpa_periodic step 2200 lr 4.994e-04 [train_loss] tar_ll 0.9758 loss -0.9758 (5.132 secs)\n",
      "tnpa:tnpa_periodic step 2400 lr 4.993e-04 [train_loss] tar_ll 0.9720 loss -0.9720 (5.099 secs)\n",
      "tnpa:tnpa_periodic step 2600 lr 4.992e-04 [train_loss] tar_ll 0.9782 loss -0.9782 (5.233 secs)\n",
      "tnpa:tnpa_periodic step 2800 lr 4.990e-04 [train_loss] tar_ll 0.9681 loss -0.9681 (5.074 secs)\n",
      "tnpa:tnpa_periodic step 3000 lr 4.989e-04 [train_loss] tar_ll 0.7473 loss -0.7473 (4.942 secs)\n",
      "tnpa:tnpa_periodic step 3200 lr 4.987e-04 [train_loss] tar_ll 0.8695 loss -0.8695 (5.167 secs)\n",
      "tnpa:tnpa_periodic step 3400 lr 4.986e-04 [train_loss] tar_ll 1.0174 loss -1.0174 (5.047 secs)\n",
      "tnpa:tnpa_periodic step 3600 lr 4.984e-04 [train_loss] tar_ll 1.0422 loss -1.0422 (5.251 secs)\n",
      "tnpa:tnpa_periodic step 3800 lr 4.982e-04 [train_loss] tar_ll 1.0973 loss -1.0973 (5.673 secs)\n",
      "tnpa:tnpa_periodic step 4000 lr 4.980e-04 [train_loss] tar_ll 1.0438 loss -1.0438 (3.995 secs)\n",
      "tnpa:tnpa_periodic step 4200 lr 4.978e-04 [train_loss] tar_ll 1.0299 loss -1.0299 (2.515 secs)\n",
      "tnpa:tnpa_periodic step 4400 lr 4.976e-04 [train_loss] tar_ll 0.8382 loss -0.8382 (2.445 secs)\n",
      "tnpa:tnpa_periodic step 4600 lr 4.974e-04 [train_loss] tar_ll 1.0938 loss -1.0938 (2.445 secs)\n",
      "tnpa:tnpa_periodic step 4800 lr 4.972e-04 [train_loss] tar_ll 1.1262 loss -1.1262 (2.490 secs)\n",
      "tnpa:tnpa_periodic step 5000 lr 4.969e-04 [train_loss] tar_ll 1.1376 loss -1.1376 (2.480 secs)\n",
      "  0%|          | 0/3000 [00:00<?, ?it/s]C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n",
      "100%|##########| 3000/3000 [00:08<00:00, 357.47it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.7819 loss 2.7819 (8.392 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 5200 lr 4.967e-04 [train_loss] tar_ll 1.1264 loss -1.1264 (2.425 secs)\n",
      "tnpa:tnpa_periodic step 5400 lr 4.964e-04 [train_loss] tar_ll 1.1056 loss -1.1056 (2.500 secs)\n",
      "tnpa:tnpa_periodic step 5600 lr 4.961e-04 [train_loss] tar_ll 1.1446 loss -1.1446 (2.580 secs)\n",
      "tnpa:tnpa_periodic step 5800 lr 4.959e-04 [train_loss] tar_ll 1.1735 loss -1.1735 (2.680 secs)\n",
      "tnpa:tnpa_periodic step 6000 lr 4.956e-04 [train_loss] tar_ll 1.0833 loss -1.0833 (2.555 secs)\n",
      "tnpa:tnpa_periodic step 6200 lr 4.953e-04 [train_loss] tar_ll 1.1240 loss -1.1240 (2.665 secs)\n",
      "tnpa:tnpa_periodic step 6400 lr 4.950e-04 [train_loss] tar_ll 1.2028 loss -1.2028 (2.570 secs)\n",
      "tnpa:tnpa_periodic step 6600 lr 4.946e-04 [train_loss] tar_ll 1.0209 loss -1.0209 (2.615 secs)\n",
      "tnpa:tnpa_periodic step 6800 lr 4.943e-04 [train_loss] tar_ll 1.1609 loss -1.1609 (2.670 secs)\n",
      "tnpa:tnpa_periodic step 7000 lr 4.940e-04 [train_loss] tar_ll 1.2041 loss -1.2041 (2.405 secs)\n",
      "tnpa:tnpa_periodic step 7200 lr 4.936e-04 [train_loss] tar_ll 1.2545 loss -1.2545 (2.455 secs)\n",
      "tnpa:tnpa_periodic step 7400 lr 4.933e-04 [train_loss] tar_ll 1.1842 loss -1.1842 (2.495 secs)\n",
      "tnpa:tnpa_periodic step 7600 lr 4.929e-04 [train_loss] tar_ll 1.2352 loss -1.2352 (2.540 secs)\n",
      "tnpa:tnpa_periodic step 7800 lr 4.925e-04 [train_loss] tar_ll 1.1700 loss -1.1700 (2.530 secs)\n",
      "tnpa:tnpa_periodic step 8000 lr 4.921e-04 [train_loss] tar_ll 1.2408 loss -1.2408 (2.665 secs)\n",
      "tnpa:tnpa_periodic step 8200 lr 4.918e-04 [train_loss] tar_ll 1.2254 loss -1.2254 (2.550 secs)\n",
      "tnpa:tnpa_periodic step 8400 lr 4.913e-04 [train_loss] tar_ll 1.2174 loss -1.2174 (2.490 secs)\n",
      "tnpa:tnpa_periodic step 8600 lr 4.909e-04 [train_loss] tar_ll 1.2867 loss -1.2867 (2.560 secs)\n",
      "tnpa:tnpa_periodic step 8800 lr 4.905e-04 [train_loss] tar_ll 1.3073 loss -1.3073 (2.505 secs)\n",
      "tnpa:tnpa_periodic step 9000 lr 4.901e-04 [train_loss] tar_ll 1.1717 loss -1.1717 (2.540 secs)\n",
      "tnpa:tnpa_periodic step 9200 lr 4.896e-04 [train_loss] tar_ll 1.2636 loss -1.2636 (2.620 secs)\n",
      "tnpa:tnpa_periodic step 9400 lr 4.892e-04 [train_loss] tar_ll 1.2421 loss -1.2421 (2.595 secs)\n",
      "tnpa:tnpa_periodic step 9600 lr 4.887e-04 [train_loss] tar_ll 1.2991 loss -1.2991 (2.505 secs)\n",
      "tnpa:tnpa_periodic step 9800 lr 4.882e-04 [train_loss] tar_ll 1.2906 loss -1.2906 (2.490 secs)\n",
      "tnpa:tnpa_periodic step 10000 lr 4.878e-04 [train_loss] tar_ll 1.2789 loss -1.2789 (2.420 secs)\n",
      "100%|##########| 3000/3000 [00:08<00:00, 357.80it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -1.8492 loss 1.8492 (8.385 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 10200 lr 4.873e-04 [train_loss] tar_ll 1.3084 loss -1.3084 (2.575 secs)\n",
      "tnpa:tnpa_periodic step 10400 lr 4.868e-04 [train_loss] tar_ll 1.2889 loss -1.2889 (2.665 secs)\n",
      "tnpa:tnpa_periodic step 10600 lr 4.863e-04 [train_loss] tar_ll 1.2883 loss -1.2883 (2.750 secs)\n",
      "tnpa:tnpa_periodic step 10800 lr 4.857e-04 [train_loss] tar_ll 1.2156 loss -1.2156 (2.685 secs)\n",
      "tnpa:tnpa_periodic step 11000 lr 4.852e-04 [train_loss] tar_ll 1.2782 loss -1.2782 (2.770 secs)\n",
      "tnpa:tnpa_periodic step 11200 lr 4.847e-04 [train_loss] tar_ll 1.2792 loss -1.2792 (2.655 secs)\n",
      "tnpa:tnpa_periodic step 11400 lr 4.841e-04 [train_loss] tar_ll 1.3509 loss -1.3509 (2.595 secs)\n",
      "tnpa:tnpa_periodic step 11600 lr 4.836e-04 [train_loss] tar_ll 1.2362 loss -1.2362 (2.605 secs)\n",
      "tnpa:tnpa_periodic step 11800 lr 4.830e-04 [train_loss] tar_ll 1.2870 loss -1.2870 (2.664 secs)\n",
      "tnpa:tnpa_periodic step 12000 lr 4.824e-04 [train_loss] tar_ll 1.0470 loss -1.0470 (2.586 secs)\n",
      "tnpa:tnpa_periodic step 12200 lr 4.819e-04 [train_loss] tar_ll 1.2539 loss -1.2539 (2.720 secs)\n",
      "tnpa:tnpa_periodic step 12400 lr 4.813e-04 [train_loss] tar_ll 1.2833 loss -1.2833 (2.665 secs)\n",
      "tnpa:tnpa_periodic step 12600 lr 4.807e-04 [train_loss] tar_ll 1.2797 loss -1.2797 (2.717 secs)\n",
      "tnpa:tnpa_periodic step 12800 lr 4.801e-04 [train_loss] tar_ll 1.1297 loss -1.1297 (4.523 secs)\n",
      "tnpa:tnpa_periodic step 13000 lr 4.794e-04 [train_loss] tar_ll 1.2889 loss -1.2889 (6.087 secs)\n",
      "tnpa:tnpa_periodic step 13200 lr 4.788e-04 [train_loss] tar_ll 1.3017 loss -1.3017 (5.233 secs)\n",
      "tnpa:tnpa_periodic step 13400 lr 4.782e-04 [train_loss] tar_ll 1.3242 loss -1.3242 (5.839 secs)\n",
      "tnpa:tnpa_periodic step 13600 lr 4.775e-04 [train_loss] tar_ll 1.3045 loss -1.3045 (5.864 secs)\n",
      "tnpa:tnpa_periodic step 13800 lr 4.769e-04 [train_loss] tar_ll 1.2527 loss -1.2527 (7.262 secs)\n",
      "tnpa:tnpa_periodic step 14000 lr 4.762e-04 [train_loss] tar_ll 1.2759 loss -1.2759 (8.117 secs)\n",
      "tnpa:tnpa_periodic step 14200 lr 4.755e-04 [train_loss] tar_ll 1.3096 loss -1.3096 (8.274 secs)\n",
      "tnpa:tnpa_periodic step 14400 lr 4.749e-04 [train_loss] tar_ll 1.3055 loss -1.3055 (8.311 secs)\n",
      "tnpa:tnpa_periodic step 14600 lr 4.742e-04 [train_loss] tar_ll 1.3533 loss -1.3533 (8.962 secs)\n",
      "tnpa:tnpa_periodic step 14800 lr 4.735e-04 [train_loss] tar_ll 1.3907 loss -1.3907 (9.234 secs)\n",
      "tnpa:tnpa_periodic step 15000 lr 4.728e-04 [train_loss] tar_ll 1.3099 loss -1.3099 (8.706 secs)\n",
      "100%|##########| 3000/3000 [00:24<00:00, 120.13it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.4502 loss 2.4502 (24.976 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 15200 lr 4.720e-04 [train_loss] tar_ll 1.2978 loss -1.2978 (7.516 secs)\n",
      "tnpa:tnpa_periodic step 15400 lr 4.713e-04 [train_loss] tar_ll 1.3215 loss -1.3215 (7.108 secs)\n",
      "tnpa:tnpa_periodic step 15600 lr 4.706e-04 [train_loss] tar_ll 1.3192 loss -1.3192 (7.808 secs)\n",
      "tnpa:tnpa_periodic step 15800 lr 4.698e-04 [train_loss] tar_ll 1.2843 loss -1.2843 (7.792 secs)\n",
      "tnpa:tnpa_periodic step 16000 lr 4.691e-04 [train_loss] tar_ll 1.3483 loss -1.3483 (8.082 secs)\n",
      "tnpa:tnpa_periodic step 16200 lr 4.683e-04 [train_loss] tar_ll 1.3268 loss -1.3268 (7.709 secs)\n",
      "tnpa:tnpa_periodic step 16400 lr 4.675e-04 [train_loss] tar_ll 1.3417 loss -1.3417 (8.188 secs)\n",
      "tnpa:tnpa_periodic step 16600 lr 4.668e-04 [train_loss] tar_ll 1.3029 loss -1.3029 (8.660 secs)\n",
      "tnpa:tnpa_periodic step 16800 lr 4.660e-04 [train_loss] tar_ll 1.3064 loss -1.3064 (9.314 secs)\n",
      "tnpa:tnpa_periodic step 17000 lr 4.652e-04 [train_loss] tar_ll 1.3099 loss -1.3099 (9.331 secs)\n",
      "tnpa:tnpa_periodic step 17200 lr 4.644e-04 [train_loss] tar_ll 1.3063 loss -1.3063 (8.921 secs)\n",
      "tnpa:tnpa_periodic step 17400 lr 4.636e-04 [train_loss] tar_ll 1.3418 loss -1.3418 (8.706 secs)\n",
      "tnpa:tnpa_periodic step 17600 lr 4.627e-04 [train_loss] tar_ll 1.2923 loss -1.2923 (9.311 secs)\n",
      "tnpa:tnpa_periodic step 17800 lr 4.619e-04 [train_loss] tar_ll 1.3124 loss -1.3124 (9.128 secs)\n",
      "tnpa:tnpa_periodic step 18000 lr 4.611e-04 [train_loss] tar_ll 1.2938 loss -1.2938 (8.927 secs)\n",
      "tnpa:tnpa_periodic step 18200 lr 4.602e-04 [train_loss] tar_ll 1.3447 loss -1.3447 (8.825 secs)\n",
      "tnpa:tnpa_periodic step 18400 lr 4.594e-04 [train_loss] tar_ll 1.4060 loss -1.4060 (8.604 secs)\n",
      "tnpa:tnpa_periodic step 18600 lr 4.585e-04 [train_loss] tar_ll 1.3744 loss -1.3744 (8.638 secs)\n",
      "tnpa:tnpa_periodic step 18800 lr 4.576e-04 [train_loss] tar_ll 1.3706 loss -1.3706 (6.352 secs)\n",
      "tnpa:tnpa_periodic step 19000 lr 4.568e-04 [train_loss] tar_ll 1.3439 loss -1.3439 (6.130 secs)\n",
      "tnpa:tnpa_periodic step 19200 lr 4.559e-04 [train_loss] tar_ll 1.4496 loss -1.4496 (6.216 secs)\n",
      "tnpa:tnpa_periodic step 19400 lr 4.550e-04 [train_loss] tar_ll 1.4030 loss -1.4030 (6.062 secs)\n",
      "tnpa:tnpa_periodic step 19600 lr 4.541e-04 [train_loss] tar_ll 1.4200 loss -1.4200 (5.748 secs)\n",
      "tnpa:tnpa_periodic step 19800 lr 4.532e-04 [train_loss] tar_ll 1.3189 loss -1.3189 (6.538 secs)\n",
      "tnpa:tnpa_periodic step 20000 lr 4.523e-04 [train_loss] tar_ll 1.3738 loss -1.3738 (6.456 secs)\n",
      "100%|##########| 3000/3000 [00:19<00:00, 152.56it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.7470 loss 2.7470 (19.667 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 20200 lr 4.513e-04 [train_loss] tar_ll 1.3660 loss -1.3660 (6.250 secs)\n",
      "tnpa:tnpa_periodic step 20400 lr 4.504e-04 [train_loss] tar_ll 1.4201 loss -1.4201 (6.203 secs)\n",
      "tnpa:tnpa_periodic step 20600 lr 4.494e-04 [train_loss] tar_ll 1.3889 loss -1.3889 (6.802 secs)\n",
      "tnpa:tnpa_periodic step 20800 lr 4.485e-04 [train_loss] tar_ll 1.4146 loss -1.4146 (6.286 secs)\n",
      "tnpa:tnpa_periodic step 21000 lr 4.475e-04 [train_loss] tar_ll 1.4044 loss -1.4044 (7.072 secs)\n",
      "tnpa:tnpa_periodic step 21200 lr 4.466e-04 [train_loss] tar_ll 1.3613 loss -1.3613 (7.114 secs)\n",
      "tnpa:tnpa_periodic step 21400 lr 4.456e-04 [train_loss] tar_ll 1.2923 loss -1.2923 (7.135 secs)\n",
      "tnpa:tnpa_periodic step 21600 lr 4.446e-04 [train_loss] tar_ll 1.2109 loss -1.2109 (7.584 secs)\n",
      "tnpa:tnpa_periodic step 21800 lr 4.436e-04 [train_loss] tar_ll 1.3311 loss -1.3311 (6.209 secs)\n",
      "tnpa:tnpa_periodic step 22000 lr 4.426e-04 [train_loss] tar_ll 1.3637 loss -1.3637 (5.151 secs)\n",
      "tnpa:tnpa_periodic step 22200 lr 4.416e-04 [train_loss] tar_ll 1.4763 loss -1.4763 (5.246 secs)\n",
      "tnpa:tnpa_periodic step 22400 lr 4.406e-04 [train_loss] tar_ll 1.4205 loss -1.4205 (5.360 secs)\n",
      "tnpa:tnpa_periodic step 22600 lr 4.396e-04 [train_loss] tar_ll 1.3436 loss -1.3436 (5.923 secs)\n",
      "tnpa:tnpa_periodic step 22800 lr 4.386e-04 [train_loss] tar_ll 1.3653 loss -1.3653 (6.309 secs)\n",
      "tnpa:tnpa_periodic step 23000 lr 4.375e-04 [train_loss] tar_ll 1.4011 loss -1.4011 (5.872 secs)\n",
      "tnpa:tnpa_periodic step 23200 lr 4.365e-04 [train_loss] tar_ll 1.3945 loss -1.3945 (5.595 secs)\n",
      "tnpa:tnpa_periodic step 23400 lr 4.354e-04 [train_loss] tar_ll 1.4241 loss -1.4241 (5.569 secs)\n",
      "tnpa:tnpa_periodic step 23600 lr 4.344e-04 [train_loss] tar_ll 1.3977 loss -1.3977 (5.564 secs)\n",
      "tnpa:tnpa_periodic step 23800 lr 4.333e-04 [train_loss] tar_ll 1.4176 loss -1.4176 (5.929 secs)\n",
      "tnpa:tnpa_periodic step 24000 lr 4.322e-04 [train_loss] tar_ll 1.3284 loss -1.3284 (5.457 secs)\n",
      "tnpa:tnpa_periodic step 24200 lr 4.312e-04 [train_loss] tar_ll 1.4169 loss -1.4169 (5.629 secs)\n",
      "tnpa:tnpa_periodic step 24400 lr 4.301e-04 [train_loss] tar_ll 1.4949 loss -1.4949 (5.325 secs)\n",
      "tnpa:tnpa_periodic step 24600 lr 4.290e-04 [train_loss] tar_ll 1.4097 loss -1.4097 (5.623 secs)\n",
      "tnpa:tnpa_periodic step 24800 lr 4.279e-04 [train_loss] tar_ll 1.4493 loss -1.4493 (5.342 secs)\n",
      "tnpa:tnpa_periodic step 25000 lr 4.268e-04 [train_loss] tar_ll 1.4181 loss -1.4181 (5.319 secs)\n",
      "100%|##########| 3000/3000 [00:17<00:00, 174.40it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.0980 loss 2.0980 (17.204 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 25200 lr 4.257e-04 [train_loss] tar_ll 1.4326 loss -1.4326 (6.273 secs)\n",
      "tnpa:tnpa_periodic step 25400 lr 4.245e-04 [train_loss] tar_ll 1.3321 loss -1.3321 (7.504 secs)\n",
      "tnpa:tnpa_periodic step 25600 lr 4.234e-04 [train_loss] tar_ll 1.4219 loss -1.4219 (6.764 secs)\n",
      "tnpa:tnpa_periodic step 25800 lr 4.223e-04 [train_loss] tar_ll 1.4499 loss -1.4499 (7.650 secs)\n",
      "tnpa:tnpa_periodic step 26000 lr 4.211e-04 [train_loss] tar_ll 1.4024 loss -1.4024 (7.435 secs)\n",
      "tnpa:tnpa_periodic step 26200 lr 4.200e-04 [train_loss] tar_ll 1.3502 loss -1.3502 (8.057 secs)\n",
      "tnpa:tnpa_periodic step 26400 lr 4.188e-04 [train_loss] tar_ll 1.4050 loss -1.4050 (8.144 secs)\n",
      "tnpa:tnpa_periodic step 26600 lr 4.177e-04 [train_loss] tar_ll 1.3864 loss -1.3864 (8.456 secs)\n",
      "tnpa:tnpa_periodic step 26800 lr 4.165e-04 [train_loss] tar_ll 1.4037 loss -1.4037 (8.483 secs)\n",
      "tnpa:tnpa_periodic step 27000 lr 4.153e-04 [train_loss] tar_ll 1.4593 loss -1.4593 (7.917 secs)\n",
      "tnpa:tnpa_periodic step 27200 lr 4.141e-04 [train_loss] tar_ll 1.4403 loss -1.4403 (8.631 secs)\n",
      "tnpa:tnpa_periodic step 27400 lr 4.130e-04 [train_loss] tar_ll 1.3853 loss -1.3853 (8.174 secs)\n",
      "tnpa:tnpa_periodic step 27600 lr 4.118e-04 [train_loss] tar_ll 1.4390 loss -1.4390 (7.707 secs)\n",
      "tnpa:tnpa_periodic step 27800 lr 4.106e-04 [train_loss] tar_ll 1.4073 loss -1.4073 (6.566 secs)\n",
      "tnpa:tnpa_periodic step 28000 lr 4.094e-04 [train_loss] tar_ll 1.4287 loss -1.4287 (5.733 secs)\n",
      "tnpa:tnpa_periodic step 28200 lr 4.081e-04 [train_loss] tar_ll 1.4522 loss -1.4522 (5.259 secs)\n",
      "tnpa:tnpa_periodic step 28400 lr 4.069e-04 [train_loss] tar_ll 1.4563 loss -1.4563 (5.113 secs)\n",
      "tnpa:tnpa_periodic step 28600 lr 4.057e-04 [train_loss] tar_ll 1.3927 loss -1.3927 (5.222 secs)\n",
      "tnpa:tnpa_periodic step 28800 lr 4.045e-04 [train_loss] tar_ll 1.3750 loss -1.3750 (5.561 secs)\n",
      "tnpa:tnpa_periodic step 29000 lr 4.032e-04 [train_loss] tar_ll 1.4317 loss -1.4317 (5.540 secs)\n",
      "tnpa:tnpa_periodic step 29200 lr 4.020e-04 [train_loss] tar_ll 1.4276 loss -1.4276 (5.463 secs)\n",
      "tnpa:tnpa_periodic step 29400 lr 4.007e-04 [train_loss] tar_ll 1.4360 loss -1.4360 (5.641 secs)\n",
      "tnpa:tnpa_periodic step 29600 lr 3.995e-04 [train_loss] tar_ll 1.4702 loss -1.4702 (5.357 secs)\n",
      "tnpa:tnpa_periodic step 29800 lr 3.982e-04 [train_loss] tar_ll 1.4301 loss -1.4301 (5.333 secs)\n",
      "tnpa:tnpa_periodic step 30000 lr 3.969e-04 [train_loss] tar_ll 1.4143 loss -1.4143 (5.367 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 177.28it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -1.9876 loss 1.9876 (16.926 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 30200 lr 3.957e-04 [train_loss] tar_ll 1.4722 loss -1.4722 (5.194 secs)\n",
      "tnpa:tnpa_periodic step 30400 lr 3.944e-04 [train_loss] tar_ll 1.4356 loss -1.4356 (5.529 secs)\n",
      "tnpa:tnpa_periodic step 30600 lr 3.931e-04 [train_loss] tar_ll 1.4675 loss -1.4675 (5.707 secs)\n",
      "tnpa:tnpa_periodic step 30800 lr 3.918e-04 [train_loss] tar_ll 1.4375 loss -1.4375 (5.263 secs)\n",
      "tnpa:tnpa_periodic step 31000 lr 3.905e-04 [train_loss] tar_ll 1.3547 loss -1.3547 (5.374 secs)\n",
      "tnpa:tnpa_periodic step 31200 lr 3.892e-04 [train_loss] tar_ll 1.4400 loss -1.4400 (5.522 secs)\n",
      "tnpa:tnpa_periodic step 31400 lr 3.879e-04 [train_loss] tar_ll 1.4784 loss -1.4784 (5.340 secs)\n",
      "tnpa:tnpa_periodic step 31600 lr 3.866e-04 [train_loss] tar_ll 1.4621 loss -1.4621 (5.140 secs)\n",
      "tnpa:tnpa_periodic step 31800 lr 3.853e-04 [train_loss] tar_ll 1.4119 loss -1.4119 (5.307 secs)\n",
      "tnpa:tnpa_periodic step 32000 lr 3.840e-04 [train_loss] tar_ll 1.4095 loss -1.4095 (5.143 secs)\n",
      "tnpa:tnpa_periodic step 32200 lr 3.826e-04 [train_loss] tar_ll 1.4318 loss -1.4318 (5.155 secs)\n",
      "tnpa:tnpa_periodic step 32400 lr 3.813e-04 [train_loss] tar_ll 1.5034 loss -1.5034 (5.350 secs)\n",
      "tnpa:tnpa_periodic step 32600 lr 3.800e-04 [train_loss] tar_ll 1.4933 loss -1.4933 (5.065 secs)\n",
      "tnpa:tnpa_periodic step 32800 lr 3.786e-04 [train_loss] tar_ll 1.4474 loss -1.4474 (5.178 secs)\n",
      "tnpa:tnpa_periodic step 33000 lr 3.773e-04 [train_loss] tar_ll 1.4421 loss -1.4421 (5.062 secs)\n",
      "tnpa:tnpa_periodic step 33200 lr 3.759e-04 [train_loss] tar_ll 1.4499 loss -1.4499 (5.267 secs)\n",
      "tnpa:tnpa_periodic step 33400 lr 3.745e-04 [train_loss] tar_ll 1.5025 loss -1.5025 (5.125 secs)\n",
      "tnpa:tnpa_periodic step 33600 lr 3.732e-04 [train_loss] tar_ll 1.4616 loss -1.4616 (5.316 secs)\n",
      "tnpa:tnpa_periodic step 33800 lr 3.718e-04 [train_loss] tar_ll 1.5016 loss -1.5016 (5.298 secs)\n",
      "tnpa:tnpa_periodic step 34000 lr 3.704e-04 [train_loss] tar_ll 1.4862 loss -1.4862 (5.239 secs)\n",
      "tnpa:tnpa_periodic step 34200 lr 3.691e-04 [train_loss] tar_ll 1.4924 loss -1.4924 (5.507 secs)\n",
      "tnpa:tnpa_periodic step 34400 lr 3.677e-04 [train_loss] tar_ll 1.3388 loss -1.3388 (5.101 secs)\n",
      "tnpa:tnpa_periodic step 34600 lr 3.663e-04 [train_loss] tar_ll 1.4532 loss -1.4532 (5.244 secs)\n",
      "tnpa:tnpa_periodic step 34800 lr 3.649e-04 [train_loss] tar_ll 1.3948 loss -1.3948 (5.138 secs)\n",
      "tnpa:tnpa_periodic step 35000 lr 3.635e-04 [train_loss] tar_ll 1.4097 loss -1.4097 (5.017 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 184.71it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -3.4768 loss 3.4768 (16.244 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 35200 lr 3.621e-04 [train_loss] tar_ll 1.4453 loss -1.4453 (5.022 secs)\n",
      "tnpa:tnpa_periodic step 35400 lr 3.607e-04 [train_loss] tar_ll 1.4878 loss -1.4878 (4.975 secs)\n",
      "tnpa:tnpa_periodic step 35600 lr 3.593e-04 [train_loss] tar_ll 1.4812 loss -1.4812 (5.100 secs)\n",
      "tnpa:tnpa_periodic step 35800 lr 3.579e-04 [train_loss] tar_ll 1.4479 loss -1.4479 (4.892 secs)\n",
      "tnpa:tnpa_periodic step 36000 lr 3.564e-04 [train_loss] tar_ll 1.4961 loss -1.4961 (5.117 secs)\n",
      "tnpa:tnpa_periodic step 36200 lr 3.550e-04 [train_loss] tar_ll 1.4564 loss -1.4564 (5.036 secs)\n",
      "tnpa:tnpa_periodic step 36400 lr 3.536e-04 [train_loss] tar_ll 1.4956 loss -1.4956 (5.047 secs)\n",
      "tnpa:tnpa_periodic step 36600 lr 3.522e-04 [train_loss] tar_ll 1.4292 loss -1.4292 (5.177 secs)\n",
      "tnpa:tnpa_periodic step 36800 lr 3.507e-04 [train_loss] tar_ll 1.4673 loss -1.4673 (5.069 secs)\n",
      "tnpa:tnpa_periodic step 37000 lr 3.493e-04 [train_loss] tar_ll 1.4667 loss -1.4667 (5.056 secs)\n",
      "tnpa:tnpa_periodic step 37200 lr 3.478e-04 [train_loss] tar_ll 1.4977 loss -1.4977 (5.200 secs)\n",
      "tnpa:tnpa_periodic step 37400 lr 3.464e-04 [train_loss] tar_ll 1.4807 loss -1.4807 (5.137 secs)\n",
      "tnpa:tnpa_periodic step 37600 lr 3.449e-04 [train_loss] tar_ll 1.4319 loss -1.4319 (5.249 secs)\n",
      "tnpa:tnpa_periodic step 37800 lr 3.435e-04 [train_loss] tar_ll 1.5234 loss -1.5234 (5.429 secs)\n",
      "tnpa:tnpa_periodic step 38000 lr 3.420e-04 [train_loss] tar_ll 1.4868 loss -1.4868 (5.439 secs)\n",
      "tnpa:tnpa_periodic step 38200 lr 3.406e-04 [train_loss] tar_ll 1.4894 loss -1.4894 (5.466 secs)\n",
      "tnpa:tnpa_periodic step 38400 lr 3.391e-04 [train_loss] tar_ll 1.4568 loss -1.4568 (5.254 secs)\n",
      "tnpa:tnpa_periodic step 38600 lr 3.376e-04 [train_loss] tar_ll 1.4886 loss -1.4886 (5.340 secs)\n",
      "tnpa:tnpa_periodic step 38800 lr 3.362e-04 [train_loss] tar_ll 1.4996 loss -1.4996 (5.340 secs)\n",
      "tnpa:tnpa_periodic step 39000 lr 3.347e-04 [train_loss] tar_ll 1.5456 loss -1.5456 (5.510 secs)\n",
      "tnpa:tnpa_periodic step 39200 lr 3.332e-04 [train_loss] tar_ll 1.5231 loss -1.5231 (5.292 secs)\n",
      "tnpa:tnpa_periodic step 39400 lr 3.317e-04 [train_loss] tar_ll 1.5553 loss -1.5553 (5.060 secs)\n",
      "tnpa:tnpa_periodic step 39600 lr 3.302e-04 [train_loss] tar_ll 1.4782 loss -1.4782 (5.154 secs)\n",
      "tnpa:tnpa_periodic step 39800 lr 3.287e-04 [train_loss] tar_ll 1.5234 loss -1.5234 (5.219 secs)\n",
      "tnpa:tnpa_periodic step 40000 lr 3.273e-04 [train_loss] tar_ll 1.4892 loss -1.4892 (5.040 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 181.20it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.7209 loss 2.7209 (16.561 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 40200 lr 3.258e-04 [train_loss] tar_ll 1.4677 loss -1.4677 (5.295 secs)\n",
      "tnpa:tnpa_periodic step 40400 lr 3.243e-04 [train_loss] tar_ll 1.5536 loss -1.5536 (4.935 secs)\n",
      "tnpa:tnpa_periodic step 40600 lr 3.228e-04 [train_loss] tar_ll 1.5075 loss -1.5075 (4.984 secs)\n",
      "tnpa:tnpa_periodic step 40800 lr 3.213e-04 [train_loss] tar_ll 1.4492 loss -1.4492 (5.330 secs)\n",
      "tnpa:tnpa_periodic step 41000 lr 3.197e-04 [train_loss] tar_ll 1.4075 loss -1.4075 (5.260 secs)\n",
      "tnpa:tnpa_periodic step 41200 lr 3.182e-04 [train_loss] tar_ll 1.4929 loss -1.4929 (5.044 secs)\n",
      "tnpa:tnpa_periodic step 41400 lr 3.167e-04 [train_loss] tar_ll 1.4773 loss -1.4773 (5.030 secs)\n",
      "tnpa:tnpa_periodic step 41600 lr 3.152e-04 [train_loss] tar_ll 1.4574 loss -1.4574 (5.213 secs)\n",
      "tnpa:tnpa_periodic step 41800 lr 3.137e-04 [train_loss] tar_ll 1.5756 loss -1.5756 (5.265 secs)\n",
      "tnpa:tnpa_periodic step 42000 lr 3.122e-04 [train_loss] tar_ll 1.5626 loss -1.5626 (5.472 secs)\n",
      "tnpa:tnpa_periodic step 42200 lr 3.106e-04 [train_loss] tar_ll 1.4858 loss -1.4858 (5.604 secs)\n",
      "tnpa:tnpa_periodic step 42400 lr 3.091e-04 [train_loss] tar_ll 1.4846 loss -1.4846 (5.303 secs)\n",
      "tnpa:tnpa_periodic step 42600 lr 3.076e-04 [train_loss] tar_ll 1.5417 loss -1.5417 (5.154 secs)\n",
      "tnpa:tnpa_periodic step 42800 lr 3.061e-04 [train_loss] tar_ll 1.5767 loss -1.5767 (5.124 secs)\n",
      "tnpa:tnpa_periodic step 43000 lr 3.045e-04 [train_loss] tar_ll 1.5190 loss -1.5190 (5.113 secs)\n",
      "tnpa:tnpa_periodic step 43200 lr 3.030e-04 [train_loss] tar_ll 1.4916 loss -1.4916 (5.100 secs)\n",
      "tnpa:tnpa_periodic step 43400 lr 3.015e-04 [train_loss] tar_ll 1.5085 loss -1.5085 (5.232 secs)\n",
      "tnpa:tnpa_periodic step 43600 lr 2.999e-04 [train_loss] tar_ll 1.4674 loss -1.4674 (5.932 secs)\n",
      "tnpa:tnpa_periodic step 43800 lr 2.984e-04 [train_loss] tar_ll 1.4910 loss -1.4910 (5.668 secs)\n",
      "tnpa:tnpa_periodic step 44000 lr 2.968e-04 [train_loss] tar_ll 1.5217 loss -1.5217 (6.029 secs)\n",
      "tnpa:tnpa_periodic step 44200 lr 2.953e-04 [train_loss] tar_ll 1.4382 loss -1.4382 (5.644 secs)\n",
      "tnpa:tnpa_periodic step 44400 lr 2.938e-04 [train_loss] tar_ll 1.4985 loss -1.4985 (5.653 secs)\n",
      "tnpa:tnpa_periodic step 44600 lr 2.922e-04 [train_loss] tar_ll 1.5024 loss -1.5024 (5.693 secs)\n",
      "tnpa:tnpa_periodic step 44800 lr 2.907e-04 [train_loss] tar_ll 1.5612 loss -1.5612 (5.747 secs)\n",
      "tnpa:tnpa_periodic step 45000 lr 2.891e-04 [train_loss] tar_ll 1.5363 loss -1.5363 (6.045 secs)\n",
      "100%|##########| 3000/3000 [00:18<00:00, 164.72it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.4514 loss 2.4514 (18.217 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 45200 lr 2.876e-04 [train_loss] tar_ll 1.5355 loss -1.5355 (5.965 secs)\n",
      "tnpa:tnpa_periodic step 45400 lr 2.860e-04 [train_loss] tar_ll 1.5897 loss -1.5897 (5.974 secs)\n",
      "tnpa:tnpa_periodic step 45600 lr 2.844e-04 [train_loss] tar_ll 1.5564 loss -1.5564 (6.117 secs)\n",
      "tnpa:tnpa_periodic step 45800 lr 2.829e-04 [train_loss] tar_ll 1.4838 loss -1.4838 (5.604 secs)\n",
      "tnpa:tnpa_periodic step 46000 lr 2.813e-04 [train_loss] tar_ll 1.5561 loss -1.5561 (5.863 secs)\n",
      "tnpa:tnpa_periodic step 46200 lr 2.798e-04 [train_loss] tar_ll 1.5031 loss -1.5031 (6.020 secs)\n",
      "tnpa:tnpa_periodic step 46400 lr 2.782e-04 [train_loss] tar_ll 1.5289 loss -1.5289 (5.868 secs)\n",
      "tnpa:tnpa_periodic step 46600 lr 2.767e-04 [train_loss] tar_ll 1.5052 loss -1.5052 (5.979 secs)\n",
      "tnpa:tnpa_periodic step 46800 lr 2.751e-04 [train_loss] tar_ll 1.4144 loss -1.4144 (5.901 secs)\n",
      "tnpa:tnpa_periodic step 47000 lr 2.735e-04 [train_loss] tar_ll 1.4982 loss -1.4982 (6.108 secs)\n",
      "tnpa:tnpa_periodic step 47200 lr 2.720e-04 [train_loss] tar_ll 1.4865 loss -1.4865 (5.920 secs)\n",
      "tnpa:tnpa_periodic step 47400 lr 2.704e-04 [train_loss] tar_ll 1.5745 loss -1.5745 (5.843 secs)\n",
      "tnpa:tnpa_periodic step 47600 lr 2.688e-04 [train_loss] tar_ll 1.5534 loss -1.5534 (6.058 secs)\n",
      "tnpa:tnpa_periodic step 47800 lr 2.673e-04 [train_loss] tar_ll 1.5053 loss -1.5053 (5.341 secs)\n",
      "tnpa:tnpa_periodic step 48000 lr 2.657e-04 [train_loss] tar_ll 1.5578 loss -1.5578 (5.622 secs)\n",
      "tnpa:tnpa_periodic step 48200 lr 2.641e-04 [train_loss] tar_ll 1.5434 loss -1.5434 (5.331 secs)\n",
      "tnpa:tnpa_periodic step 48400 lr 2.626e-04 [train_loss] tar_ll 1.5215 loss -1.5215 (5.124 secs)\n",
      "tnpa:tnpa_periodic step 48600 lr 2.610e-04 [train_loss] tar_ll 1.5129 loss -1.5129 (5.182 secs)\n",
      "tnpa:tnpa_periodic step 48800 lr 2.594e-04 [train_loss] tar_ll 1.5518 loss -1.5518 (5.171 secs)\n",
      "tnpa:tnpa_periodic step 49000 lr 2.579e-04 [train_loss] tar_ll 1.5616 loss -1.5616 (5.499 secs)\n",
      "tnpa:tnpa_periodic step 49200 lr 2.563e-04 [train_loss] tar_ll 1.5453 loss -1.5453 (6.113 secs)\n",
      "tnpa:tnpa_periodic step 49400 lr 2.547e-04 [train_loss] tar_ll 1.5340 loss -1.5340 (5.951 secs)\n",
      "tnpa:tnpa_periodic step 49600 lr 2.531e-04 [train_loss] tar_ll 1.5477 loss -1.5477 (5.686 secs)\n",
      "tnpa:tnpa_periodic step 49800 lr 2.516e-04 [train_loss] tar_ll 1.5431 loss -1.5431 (5.801 secs)\n",
      "tnpa:tnpa_periodic step 50000 lr 2.500e-04 [train_loss] tar_ll 1.5493 loss -1.5493 (5.667 secs)\n",
      "100%|##########| 3000/3000 [00:18<00:00, 159.29it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.4563 loss 2.4563 (18.838 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 50200 lr 2.484e-04 [train_loss] tar_ll 1.5149 loss -1.5149 (6.226 secs)\n",
      "tnpa:tnpa_periodic step 50400 lr 2.469e-04 [train_loss] tar_ll 1.5461 loss -1.5461 (6.178 secs)\n",
      "tnpa:tnpa_periodic step 50600 lr 2.453e-04 [train_loss] tar_ll 1.4859 loss -1.4859 (5.719 secs)\n",
      "tnpa:tnpa_periodic step 50800 lr 2.437e-04 [train_loss] tar_ll 1.4773 loss -1.4773 (6.116 secs)\n",
      "tnpa:tnpa_periodic step 51000 lr 2.421e-04 [train_loss] tar_ll 1.5186 loss -1.5186 (5.782 secs)\n",
      "tnpa:tnpa_periodic step 51200 lr 2.406e-04 [train_loss] tar_ll 1.5951 loss -1.5951 (5.732 secs)\n",
      "tnpa:tnpa_periodic step 51400 lr 2.390e-04 [train_loss] tar_ll 1.5717 loss -1.5717 (6.110 secs)\n",
      "tnpa:tnpa_periodic step 51600 lr 2.374e-04 [train_loss] tar_ll 1.5860 loss -1.5860 (6.074 secs)\n",
      "tnpa:tnpa_periodic step 51800 lr 2.359e-04 [train_loss] tar_ll 1.5867 loss -1.5867 (6.481 secs)\n",
      "tnpa:tnpa_periodic step 52000 lr 2.343e-04 [train_loss] tar_ll 1.5268 loss -1.5268 (6.185 secs)\n",
      "tnpa:tnpa_periodic step 52200 lr 2.327e-04 [train_loss] tar_ll 1.5369 loss -1.5369 (6.236 secs)\n",
      "tnpa:tnpa_periodic step 52400 lr 2.312e-04 [train_loss] tar_ll 1.5149 loss -1.5149 (6.174 secs)\n",
      "tnpa:tnpa_periodic step 52600 lr 2.296e-04 [train_loss] tar_ll 1.5552 loss -1.5552 (6.189 secs)\n",
      "tnpa:tnpa_periodic step 52800 lr 2.280e-04 [train_loss] tar_ll 1.5079 loss -1.5079 (6.077 secs)\n",
      "tnpa:tnpa_periodic step 53000 lr 2.265e-04 [train_loss] tar_ll 1.6147 loss -1.6147 (6.315 secs)\n",
      "tnpa:tnpa_periodic step 53200 lr 2.249e-04 [train_loss] tar_ll 1.5221 loss -1.5221 (5.966 secs)\n",
      "tnpa:tnpa_periodic step 53400 lr 2.233e-04 [train_loss] tar_ll 1.6035 loss -1.6035 (6.273 secs)\n",
      "tnpa:tnpa_periodic step 53600 lr 2.218e-04 [train_loss] tar_ll 1.5586 loss -1.5586 (6.177 secs)\n",
      "tnpa:tnpa_periodic step 53800 lr 2.202e-04 [train_loss] tar_ll 1.5371 loss -1.5371 (6.168 secs)\n",
      "tnpa:tnpa_periodic step 54000 lr 2.187e-04 [train_loss] tar_ll 1.5626 loss -1.5626 (6.022 secs)\n",
      "tnpa:tnpa_periodic step 54200 lr 2.171e-04 [train_loss] tar_ll 1.5236 loss -1.5236 (6.041 secs)\n",
      "tnpa:tnpa_periodic step 54400 lr 2.156e-04 [train_loss] tar_ll 1.5723 loss -1.5723 (8.137 secs)\n",
      "tnpa:tnpa_periodic step 54600 lr 2.140e-04 [train_loss] tar_ll 1.5166 loss -1.5166 (6.021 secs)\n",
      "tnpa:tnpa_periodic step 54800 lr 2.124e-04 [train_loss] tar_ll 1.4889 loss -1.4889 (6.164 secs)\n",
      "tnpa:tnpa_periodic step 55000 lr 2.109e-04 [train_loss] tar_ll 1.5876 loss -1.5876 (6.235 secs)\n",
      "100%|##########| 3000/3000 [00:19<00:00, 153.33it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.4313 loss 2.4313 (19.569 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 55200 lr 2.093e-04 [train_loss] tar_ll 1.5616 loss -1.5616 (6.029 secs)\n",
      "tnpa:tnpa_periodic step 55400 lr 2.078e-04 [train_loss] tar_ll 1.5520 loss -1.5520 (6.287 secs)\n",
      "tnpa:tnpa_periodic step 55600 lr 2.062e-04 [train_loss] tar_ll 1.5599 loss -1.5599 (5.974 secs)\n",
      "tnpa:tnpa_periodic step 55800 lr 2.047e-04 [train_loss] tar_ll 1.6084 loss -1.6084 (6.203 secs)\n",
      "tnpa:tnpa_periodic step 56000 lr 2.032e-04 [train_loss] tar_ll 1.5676 loss -1.5676 (5.818 secs)\n",
      "tnpa:tnpa_periodic step 56200 lr 2.016e-04 [train_loss] tar_ll 1.5496 loss -1.5496 (5.954 secs)\n",
      "tnpa:tnpa_periodic step 56400 lr 2.001e-04 [train_loss] tar_ll 1.5907 loss -1.5907 (6.063 secs)\n",
      "tnpa:tnpa_periodic step 56600 lr 1.985e-04 [train_loss] tar_ll 1.6152 loss -1.6152 (6.228 secs)\n",
      "tnpa:tnpa_periodic step 56800 lr 1.970e-04 [train_loss] tar_ll 1.5777 loss -1.5777 (5.858 secs)\n",
      "tnpa:tnpa_periodic step 57000 lr 1.955e-04 [train_loss] tar_ll 1.4900 loss -1.4900 (6.101 secs)\n",
      "tnpa:tnpa_periodic step 57200 lr 1.939e-04 [train_loss] tar_ll 1.5612 loss -1.5612 (6.189 secs)\n",
      "tnpa:tnpa_periodic step 57400 lr 1.924e-04 [train_loss] tar_ll 1.5436 loss -1.5436 (5.288 secs)\n",
      "tnpa:tnpa_periodic step 57600 lr 1.909e-04 [train_loss] tar_ll 1.4898 loss -1.4898 (5.178 secs)\n",
      "tnpa:tnpa_periodic step 57800 lr 1.894e-04 [train_loss] tar_ll 1.5551 loss -1.5551 (5.122 secs)\n",
      "tnpa:tnpa_periodic step 58000 lr 1.878e-04 [train_loss] tar_ll 1.5170 loss -1.5170 (5.582 secs)\n",
      "tnpa:tnpa_periodic step 58200 lr 1.863e-04 [train_loss] tar_ll 1.5088 loss -1.5088 (6.322 secs)\n",
      "tnpa:tnpa_periodic step 58400 lr 1.848e-04 [train_loss] tar_ll 1.5750 loss -1.5750 (5.686 secs)\n",
      "tnpa:tnpa_periodic step 58600 lr 1.833e-04 [train_loss] tar_ll 1.5513 loss -1.5513 (5.770 secs)\n",
      "tnpa:tnpa_periodic step 58800 lr 1.818e-04 [train_loss] tar_ll 1.6087 loss -1.6087 (5.818 secs)\n",
      "tnpa:tnpa_periodic step 59000 lr 1.803e-04 [train_loss] tar_ll 1.5855 loss -1.5855 (6.175 secs)\n",
      "tnpa:tnpa_periodic step 59200 lr 1.787e-04 [train_loss] tar_ll 1.5674 loss -1.5674 (6.405 secs)\n",
      "tnpa:tnpa_periodic step 59400 lr 1.772e-04 [train_loss] tar_ll 1.6178 loss -1.6178 (6.305 secs)\n",
      "tnpa:tnpa_periodic step 59600 lr 1.757e-04 [train_loss] tar_ll 1.5266 loss -1.5266 (6.544 secs)\n",
      "tnpa:tnpa_periodic step 59800 lr 1.742e-04 [train_loss] tar_ll 1.5439 loss -1.5439 (6.232 secs)\n",
      "tnpa:tnpa_periodic step 60000 lr 1.727e-04 [train_loss] tar_ll 1.5855 loss -1.5855 (6.359 secs)\n",
      "100%|##########| 3000/3000 [00:19<00:00, 154.32it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.6568 loss 2.6568 (19.444 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 60200 lr 1.713e-04 [train_loss] tar_ll 1.5873 loss -1.5873 (6.188 secs)\n",
      "tnpa:tnpa_periodic step 60400 lr 1.698e-04 [train_loss] tar_ll 1.5672 loss -1.5672 (6.025 secs)\n",
      "tnpa:tnpa_periodic step 60600 lr 1.683e-04 [train_loss] tar_ll 1.6160 loss -1.6160 (6.373 secs)\n",
      "tnpa:tnpa_periodic step 60800 lr 1.668e-04 [train_loss] tar_ll 1.5661 loss -1.5661 (6.149 secs)\n",
      "tnpa:tnpa_periodic step 61000 lr 1.653e-04 [train_loss] tar_ll 1.5443 loss -1.5443 (6.090 secs)\n",
      "tnpa:tnpa_periodic step 61200 lr 1.638e-04 [train_loss] tar_ll 1.5618 loss -1.5618 (6.067 secs)\n",
      "tnpa:tnpa_periodic step 61400 lr 1.624e-04 [train_loss] tar_ll 1.5287 loss -1.5287 (6.138 secs)\n",
      "tnpa:tnpa_periodic step 61600 lr 1.609e-04 [train_loss] tar_ll 1.4775 loss -1.4775 (6.351 secs)\n",
      "tnpa:tnpa_periodic step 61800 lr 1.594e-04 [train_loss] tar_ll 1.6225 loss -1.6225 (6.145 secs)\n",
      "tnpa:tnpa_periodic step 62000 lr 1.580e-04 [train_loss] tar_ll 1.5671 loss -1.5671 (5.993 secs)\n",
      "tnpa:tnpa_periodic step 62200 lr 1.565e-04 [train_loss] tar_ll 1.5781 loss -1.5781 (6.255 secs)\n",
      "tnpa:tnpa_periodic step 62400 lr 1.551e-04 [train_loss] tar_ll 1.5785 loss -1.5785 (5.908 secs)\n",
      "tnpa:tnpa_periodic step 62600 lr 1.536e-04 [train_loss] tar_ll 1.6111 loss -1.6111 (6.345 secs)\n",
      "tnpa:tnpa_periodic step 62800 lr 1.522e-04 [train_loss] tar_ll 1.5751 loss -1.5751 (6.053 secs)\n",
      "tnpa:tnpa_periodic step 63000 lr 1.507e-04 [train_loss] tar_ll 1.6127 loss -1.6127 (6.003 secs)\n",
      "tnpa:tnpa_periodic step 63200 lr 1.493e-04 [train_loss] tar_ll 1.5751 loss -1.5751 (6.188 secs)\n",
      "tnpa:tnpa_periodic step 63400 lr 1.478e-04 [train_loss] tar_ll 1.6078 loss -1.6078 (5.777 secs)\n",
      "tnpa:tnpa_periodic step 63600 lr 1.464e-04 [train_loss] tar_ll 1.5919 loss -1.5919 (5.974 secs)\n",
      "tnpa:tnpa_periodic step 63800 lr 1.450e-04 [train_loss] tar_ll 1.5904 loss -1.5904 (5.766 secs)\n",
      "tnpa:tnpa_periodic step 64000 lr 1.436e-04 [train_loss] tar_ll 1.5766 loss -1.5766 (5.874 secs)\n",
      "tnpa:tnpa_periodic step 64200 lr 1.421e-04 [train_loss] tar_ll 1.5752 loss -1.5752 (5.954 secs)\n",
      "tnpa:tnpa_periodic step 64400 lr 1.407e-04 [train_loss] tar_ll 1.6379 loss -1.6379 (5.864 secs)\n",
      "tnpa:tnpa_periodic step 64600 lr 1.393e-04 [train_loss] tar_ll 1.6243 loss -1.6243 (5.790 secs)\n",
      "tnpa:tnpa_periodic step 64800 lr 1.379e-04 [train_loss] tar_ll 1.5408 loss -1.5408 (5.970 secs)\n",
      "tnpa:tnpa_periodic step 65000 lr 1.365e-04 [train_loss] tar_ll 1.6399 loss -1.6399 (5.770 secs)\n",
      "100%|##########| 3000/3000 [00:18<00:00, 159.16it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.1382 loss 2.1382 (18.854 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 65200 lr 1.351e-04 [train_loss] tar_ll 1.6042 loss -1.6042 (6.459 secs)\n",
      "tnpa:tnpa_periodic step 65400 lr 1.337e-04 [train_loss] tar_ll 1.5641 loss -1.5641 (4.996 secs)\n",
      "tnpa:tnpa_periodic step 65600 lr 1.323e-04 [train_loss] tar_ll 1.5462 loss -1.5462 (5.133 secs)\n",
      "tnpa:tnpa_periodic step 65800 lr 1.309e-04 [train_loss] tar_ll 1.6618 loss -1.6618 (5.233 secs)\n",
      "tnpa:tnpa_periodic step 66000 lr 1.296e-04 [train_loss] tar_ll 1.5683 loss -1.5683 (6.013 secs)\n",
      "tnpa:tnpa_periodic step 66200 lr 1.282e-04 [train_loss] tar_ll 1.6047 loss -1.6047 (5.883 secs)\n",
      "tnpa:tnpa_periodic step 66400 lr 1.268e-04 [train_loss] tar_ll 1.6031 loss -1.6031 (5.863 secs)\n",
      "tnpa:tnpa_periodic step 66600 lr 1.255e-04 [train_loss] tar_ll 1.5911 loss -1.5911 (6.126 secs)\n",
      "tnpa:tnpa_periodic step 66800 lr 1.241e-04 [train_loss] tar_ll 1.5999 loss -1.5999 (6.232 secs)\n",
      "tnpa:tnpa_periodic step 67000 lr 1.227e-04 [train_loss] tar_ll 1.5599 loss -1.5599 (6.212 secs)\n",
      "tnpa:tnpa_periodic step 67200 lr 1.214e-04 [train_loss] tar_ll 1.5937 loss -1.5937 (6.130 secs)\n",
      "tnpa:tnpa_periodic step 67400 lr 1.200e-04 [train_loss] tar_ll 1.5931 loss -1.5931 (6.208 secs)\n",
      "tnpa:tnpa_periodic step 67600 lr 1.187e-04 [train_loss] tar_ll 1.5510 loss -1.5510 (6.197 secs)\n",
      "tnpa:tnpa_periodic step 67800 lr 1.174e-04 [train_loss] tar_ll 1.5780 loss -1.5780 (6.391 secs)\n",
      "tnpa:tnpa_periodic step 68000 lr 1.160e-04 [train_loss] tar_ll 1.6123 loss -1.6123 (6.003 secs)\n",
      "tnpa:tnpa_periodic step 68200 lr 1.147e-04 [train_loss] tar_ll 1.6252 loss -1.6252 (6.230 secs)\n",
      "tnpa:tnpa_periodic step 68400 lr 1.134e-04 [train_loss] tar_ll 1.5850 loss -1.5850 (6.416 secs)\n",
      "tnpa:tnpa_periodic step 68600 lr 1.121e-04 [train_loss] tar_ll 1.5816 loss -1.5816 (6.132 secs)\n",
      "tnpa:tnpa_periodic step 68800 lr 1.108e-04 [train_loss] tar_ll 1.5699 loss -1.5699 (6.324 secs)\n",
      "tnpa:tnpa_periodic step 69000 lr 1.095e-04 [train_loss] tar_ll 1.6116 loss -1.6116 (6.071 secs)\n",
      "tnpa:tnpa_periodic step 69200 lr 1.082e-04 [train_loss] tar_ll 1.6129 loss -1.6129 (5.933 secs)\n",
      "tnpa:tnpa_periodic step 69400 lr 1.069e-04 [train_loss] tar_ll 1.6278 loss -1.6278 (6.387 secs)\n",
      "tnpa:tnpa_periodic step 69600 lr 1.056e-04 [train_loss] tar_ll 1.6250 loss -1.6250 (6.157 secs)\n",
      "tnpa:tnpa_periodic step 69800 lr 1.043e-04 [train_loss] tar_ll 1.6325 loss -1.6325 (6.019 secs)\n",
      "tnpa:tnpa_periodic step 70000 lr 1.031e-04 [train_loss] tar_ll 1.5943 loss -1.5943 (6.421 secs)\n",
      "100%|##########| 3000/3000 [00:19<00:00, 155.89it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.4979 loss 2.4979 (19.247 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 70200 lr 1.018e-04 [train_loss] tar_ll 1.6239 loss -1.6239 (6.061 secs)\n",
      "tnpa:tnpa_periodic step 70400 lr 1.005e-04 [train_loss] tar_ll 1.5835 loss -1.5835 (5.796 secs)\n",
      "tnpa:tnpa_periodic step 70600 lr 9.927e-05 [train_loss] tar_ll 1.6294 loss -1.6294 (5.840 secs)\n",
      "tnpa:tnpa_periodic step 70800 lr 9.802e-05 [train_loss] tar_ll 1.6050 loss -1.6050 (6.209 secs)\n",
      "tnpa:tnpa_periodic step 71000 lr 9.677e-05 [train_loss] tar_ll 1.6489 loss -1.6489 (6.091 secs)\n",
      "tnpa:tnpa_periodic step 71200 lr 9.554e-05 [train_loss] tar_ll 1.5831 loss -1.5831 (5.907 secs)\n",
      "tnpa:tnpa_periodic step 71400 lr 9.430e-05 [train_loss] tar_ll 1.6640 loss -1.6640 (5.998 secs)\n",
      "tnpa:tnpa_periodic step 71600 lr 9.308e-05 [train_loss] tar_ll 1.6026 loss -1.6026 (5.634 secs)\n",
      "tnpa:tnpa_periodic step 71800 lr 9.186e-05 [train_loss] tar_ll 1.6039 loss -1.6039 (5.694 secs)\n",
      "tnpa:tnpa_periodic step 72000 lr 9.064e-05 [train_loss] tar_ll 1.6658 loss -1.6658 (6.428 secs)\n",
      "tnpa:tnpa_periodic step 72200 lr 8.944e-05 [train_loss] tar_ll 1.5869 loss -1.5869 (6.111 secs)\n",
      "tnpa:tnpa_periodic step 72400 lr 8.824e-05 [train_loss] tar_ll 1.5714 loss -1.5714 (5.907 secs)\n",
      "tnpa:tnpa_periodic step 72600 lr 8.704e-05 [train_loss] tar_ll 1.5938 loss -1.5938 (5.715 secs)\n",
      "tnpa:tnpa_periodic step 72800 lr 8.585e-05 [train_loss] tar_ll 1.6163 loss -1.6163 (6.233 secs)\n",
      "tnpa:tnpa_periodic step 73000 lr 8.467e-05 [train_loss] tar_ll 1.6658 loss -1.6658 (6.419 secs)\n",
      "tnpa:tnpa_periodic step 73200 lr 8.350e-05 [train_loss] tar_ll 1.5976 loss -1.5976 (6.128 secs)\n",
      "tnpa:tnpa_periodic step 73400 lr 8.233e-05 [train_loss] tar_ll 1.6308 loss -1.6308 (6.475 secs)\n",
      "tnpa:tnpa_periodic step 73600 lr 8.117e-05 [train_loss] tar_ll 1.6143 loss -1.6143 (6.065 secs)\n",
      "tnpa:tnpa_periodic step 73800 lr 8.001e-05 [train_loss] tar_ll 1.5521 loss -1.5521 (6.163 secs)\n",
      "tnpa:tnpa_periodic step 74000 lr 7.886e-05 [train_loss] tar_ll 1.6218 loss -1.6218 (6.433 secs)\n",
      "tnpa:tnpa_periodic step 74200 lr 7.772e-05 [train_loss] tar_ll 1.6059 loss -1.6059 (5.896 secs)\n",
      "tnpa:tnpa_periodic step 74400 lr 7.659e-05 [train_loss] tar_ll 1.5906 loss -1.5906 (6.158 secs)\n",
      "tnpa:tnpa_periodic step 74600 lr 7.546e-05 [train_loss] tar_ll 1.6010 loss -1.6010 (6.073 secs)\n",
      "tnpa:tnpa_periodic step 74800 lr 7.434e-05 [train_loss] tar_ll 1.6209 loss -1.6209 (6.130 secs)\n",
      "tnpa:tnpa_periodic step 75000 lr 7.322e-05 [train_loss] tar_ll 1.5733 loss -1.5733 (6.496 secs)\n",
      "100%|##########| 3000/3000 [00:16<00:00, 177.20it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.3733 loss 2.3733 (16.934 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 75200 lr 7.212e-05 [train_loss] tar_ll 1.6025 loss -1.6025 (6.060 secs)\n",
      "tnpa:tnpa_periodic step 75400 lr 7.102e-05 [train_loss] tar_ll 1.6174 loss -1.6174 (6.419 secs)\n",
      "tnpa:tnpa_periodic step 75600 lr 6.992e-05 [train_loss] tar_ll 1.6430 loss -1.6430 (6.057 secs)\n",
      "tnpa:tnpa_periodic step 75800 lr 6.884e-05 [train_loss] tar_ll 1.6041 loss -1.6041 (5.810 secs)\n",
      "tnpa:tnpa_periodic step 76000 lr 6.776e-05 [train_loss] tar_ll 1.6080 loss -1.6080 (6.130 secs)\n",
      "tnpa:tnpa_periodic step 76200 lr 6.669e-05 [train_loss] tar_ll 1.6801 loss -1.6801 (6.172 secs)\n",
      "tnpa:tnpa_periodic step 76400 lr 6.562e-05 [train_loss] tar_ll 1.6376 loss -1.6376 (5.930 secs)\n",
      "tnpa:tnpa_periodic step 76600 lr 6.456e-05 [train_loss] tar_ll 1.6118 loss -1.6118 (6.088 secs)\n",
      "tnpa:tnpa_periodic step 76800 lr 6.351e-05 [train_loss] tar_ll 1.5821 loss -1.5821 (6.176 secs)\n",
      "tnpa:tnpa_periodic step 77000 lr 6.247e-05 [train_loss] tar_ll 1.6223 loss -1.6223 (6.107 secs)\n",
      "tnpa:tnpa_periodic step 77200 lr 6.144e-05 [train_loss] tar_ll 1.5986 loss -1.5986 (5.756 secs)\n",
      "tnpa:tnpa_periodic step 77400 lr 6.041e-05 [train_loss] tar_ll 1.6338 loss -1.6338 (5.873 secs)\n",
      "tnpa:tnpa_periodic step 77600 lr 5.939e-05 [train_loss] tar_ll 1.6160 loss -1.6160 (6.082 secs)\n",
      "tnpa:tnpa_periodic step 77800 lr 5.838e-05 [train_loss] tar_ll 1.6322 loss -1.6322 (5.736 secs)\n",
      "tnpa:tnpa_periodic step 78000 lr 5.737e-05 [train_loss] tar_ll 1.6270 loss -1.6270 (5.963 secs)\n",
      "tnpa:tnpa_periodic step 78200 lr 5.637e-05 [train_loss] tar_ll 1.6452 loss -1.6452 (6.219 secs)\n",
      "tnpa:tnpa_periodic step 78400 lr 5.538e-05 [train_loss] tar_ll 1.5950 loss -1.5950 (5.877 secs)\n",
      "tnpa:tnpa_periodic step 78600 lr 5.440e-05 [train_loss] tar_ll 1.6496 loss -1.6496 (5.984 secs)\n",
      "tnpa:tnpa_periodic step 78800 lr 5.343e-05 [train_loss] tar_ll 1.6280 loss -1.6280 (6.139 secs)\n",
      "tnpa:tnpa_periodic step 79000 lr 5.246e-05 [train_loss] tar_ll 1.6204 loss -1.6204 (5.802 secs)\n",
      "tnpa:tnpa_periodic step 79200 lr 5.150e-05 [train_loss] tar_ll 1.6163 loss -1.6163 (6.187 secs)\n",
      "tnpa:tnpa_periodic step 79400 lr 5.055e-05 [train_loss] tar_ll 1.6088 loss -1.6088 (6.104 secs)\n",
      "tnpa:tnpa_periodic step 79600 lr 4.961e-05 [train_loss] tar_ll 1.6145 loss -1.6145 (5.943 secs)\n",
      "tnpa:tnpa_periodic step 79800 lr 4.867e-05 [train_loss] tar_ll 1.6474 loss -1.6474 (6.392 secs)\n",
      "tnpa:tnpa_periodic step 80000 lr 4.775e-05 [train_loss] tar_ll 1.6594 loss -1.6594 (6.116 secs)\n",
      "100%|##########| 3000/3000 [00:20<00:00, 149.91it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.7764 loss 2.7764 (20.016 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 80200 lr 4.683e-05 [train_loss] tar_ll 1.6282 loss -1.6282 (6.024 secs)\n",
      "tnpa:tnpa_periodic step 80400 lr 4.592e-05 [train_loss] tar_ll 1.6183 loss -1.6183 (6.147 secs)\n",
      "tnpa:tnpa_periodic step 80600 lr 4.501e-05 [train_loss] tar_ll 1.6202 loss -1.6202 (6.386 secs)\n",
      "tnpa:tnpa_periodic step 80800 lr 4.412e-05 [train_loss] tar_ll 1.6082 loss -1.6082 (5.957 secs)\n",
      "tnpa:tnpa_periodic step 81000 lr 4.323e-05 [train_loss] tar_ll 1.6367 loss -1.6367 (6.268 secs)\n",
      "tnpa:tnpa_periodic step 81200 lr 4.235e-05 [train_loss] tar_ll 1.6851 loss -1.6851 (6.041 secs)\n",
      "tnpa:tnpa_periodic step 81400 lr 4.148e-05 [train_loss] tar_ll 1.6229 loss -1.6229 (6.070 secs)\n",
      "tnpa:tnpa_periodic step 81600 lr 4.062e-05 [train_loss] tar_ll 1.6042 loss -1.6042 (6.162 secs)\n",
      "tnpa:tnpa_periodic step 81800 lr 3.976e-05 [train_loss] tar_ll 1.5883 loss -1.5883 (5.894 secs)\n",
      "tnpa:tnpa_periodic step 82000 lr 3.892e-05 [train_loss] tar_ll 1.5952 loss -1.5952 (6.086 secs)\n",
      "tnpa:tnpa_periodic step 82200 lr 3.808e-05 [train_loss] tar_ll 1.6156 loss -1.6156 (6.320 secs)\n",
      "tnpa:tnpa_periodic step 82400 lr 3.725e-05 [train_loss] tar_ll 1.6866 loss -1.6866 (6.086 secs)\n",
      "tnpa:tnpa_periodic step 82600 lr 3.643e-05 [train_loss] tar_ll 1.6016 loss -1.6016 (6.164 secs)\n",
      "tnpa:tnpa_periodic step 82800 lr 3.562e-05 [train_loss] tar_ll 1.6081 loss -1.6081 (6.057 secs)\n",
      "tnpa:tnpa_periodic step 83000 lr 3.481e-05 [train_loss] tar_ll 1.5727 loss -1.5727 (6.024 secs)\n",
      "tnpa:tnpa_periodic step 83200 lr 3.402e-05 [train_loss] tar_ll 1.6055 loss -1.6055 (6.141 secs)\n",
      "tnpa:tnpa_periodic step 83400 lr 3.323e-05 [train_loss] tar_ll 1.6257 loss -1.6257 (5.988 secs)\n",
      "tnpa:tnpa_periodic step 83600 lr 3.245e-05 [train_loss] tar_ll 1.6059 loss -1.6059 (5.939 secs)\n",
      "tnpa:tnpa_periodic step 83800 lr 3.168e-05 [train_loss] tar_ll 1.6024 loss -1.6024 (6.041 secs)\n",
      "tnpa:tnpa_periodic step 84000 lr 3.092e-05 [train_loss] tar_ll 1.6767 loss -1.6767 (6.118 secs)\n",
      "tnpa:tnpa_periodic step 84200 lr 3.017e-05 [train_loss] tar_ll 1.6492 loss -1.6492 (6.300 secs)\n",
      "tnpa:tnpa_periodic step 84400 lr 2.943e-05 [train_loss] tar_ll 1.6184 loss -1.6184 (6.251 secs)\n",
      "tnpa:tnpa_periodic step 84600 lr 2.869e-05 [train_loss] tar_ll 1.5956 loss -1.5956 (5.407 secs)\n",
      "tnpa:tnpa_periodic step 84800 lr 2.797e-05 [train_loss] tar_ll 1.5751 loss -1.5751 (5.292 secs)\n",
      "tnpa:tnpa_periodic step 85000 lr 2.725e-05 [train_loss] tar_ll 1.5575 loss -1.5575 (5.208 secs)\n",
      "100%|##########| 3000/3000 [00:18<00:00, 158.82it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.4011 loss 2.4011 (18.892 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 85200 lr 2.654e-05 [train_loss] tar_ll 1.6504 loss -1.6504 (6.394 secs)\n",
      "tnpa:tnpa_periodic step 85400 lr 2.584e-05 [train_loss] tar_ll 1.6483 loss -1.6483 (6.102 secs)\n",
      "tnpa:tnpa_periodic step 85600 lr 2.515e-05 [train_loss] tar_ll 1.6257 loss -1.6257 (6.092 secs)\n",
      "tnpa:tnpa_periodic step 85800 lr 2.447e-05 [train_loss] tar_ll 1.6439 loss -1.6439 (6.099 secs)\n",
      "tnpa:tnpa_periodic step 86000 lr 2.379e-05 [train_loss] tar_ll 1.6601 loss -1.6601 (5.697 secs)\n",
      "tnpa:tnpa_periodic step 86200 lr 2.313e-05 [train_loss] tar_ll 1.6566 loss -1.6566 (6.194 secs)\n",
      "tnpa:tnpa_periodic step 86400 lr 2.247e-05 [train_loss] tar_ll 1.6287 loss -1.6287 (5.743 secs)\n",
      "tnpa:tnpa_periodic step 86600 lr 2.183e-05 [train_loss] tar_ll 1.6563 loss -1.6563 (5.820 secs)\n",
      "tnpa:tnpa_periodic step 86800 lr 2.119e-05 [train_loss] tar_ll 1.6553 loss -1.6553 (6.321 secs)\n",
      "tnpa:tnpa_periodic step 87000 lr 2.056e-05 [train_loss] tar_ll 1.6880 loss -1.6880 (6.179 secs)\n",
      "tnpa:tnpa_periodic step 87200 lr 1.994e-05 [train_loss] tar_ll 1.6830 loss -1.6830 (6.098 secs)\n",
      "tnpa:tnpa_periodic step 87400 lr 1.933e-05 [train_loss] tar_ll 1.6775 loss -1.6775 (6.380 secs)\n",
      "tnpa:tnpa_periodic step 87600 lr 1.873e-05 [train_loss] tar_ll 1.6780 loss -1.6780 (6.136 secs)\n",
      "tnpa:tnpa_periodic step 87800 lr 1.814e-05 [train_loss] tar_ll 1.6701 loss -1.6701 (6.361 secs)\n",
      "tnpa:tnpa_periodic step 88000 lr 1.756e-05 [train_loss] tar_ll 1.6863 loss -1.6863 (5.855 secs)\n",
      "tnpa:tnpa_periodic step 88200 lr 1.698e-05 [train_loss] tar_ll 1.7195 loss -1.7195 (6.186 secs)\n",
      "tnpa:tnpa_periodic step 88400 lr 1.642e-05 [train_loss] tar_ll 1.6140 loss -1.6140 (6.222 secs)\n",
      "tnpa:tnpa_periodic step 88600 lr 1.586e-05 [train_loss] tar_ll 1.6178 loss -1.6178 (6.112 secs)\n",
      "tnpa:tnpa_periodic step 88800 lr 1.532e-05 [train_loss] tar_ll 1.7010 loss -1.7010 (6.075 secs)\n",
      "tnpa:tnpa_periodic step 89000 lr 1.478e-05 [train_loss] tar_ll 1.6342 loss -1.6342 (5.995 secs)\n",
      "tnpa:tnpa_periodic step 89200 lr 1.425e-05 [train_loss] tar_ll 1.6781 loss -1.6781 (5.852 secs)\n",
      "tnpa:tnpa_periodic step 89400 lr 1.373e-05 [train_loss] tar_ll 1.6515 loss -1.6515 (6.307 secs)\n",
      "tnpa:tnpa_periodic step 89600 lr 1.323e-05 [train_loss] tar_ll 1.5791 loss -1.5791 (6.108 secs)\n",
      "tnpa:tnpa_periodic step 89800 lr 1.273e-05 [train_loss] tar_ll 1.6193 loss -1.6193 (6.068 secs)\n",
      "tnpa:tnpa_periodic step 90000 lr 1.224e-05 [train_loss] tar_ll 1.6522 loss -1.6522 (6.265 secs)\n",
      "100%|##########| 3000/3000 [00:18<00:00, 158.33it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.7318 loss 2.7318 (18.949 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 90200 lr 1.176e-05 [train_loss] tar_ll 1.6428 loss -1.6428 (6.524 secs)\n",
      "tnpa:tnpa_periodic step 90400 lr 1.128e-05 [train_loss] tar_ll 1.6307 loss -1.6307 (5.164 secs)\n",
      "tnpa:tnpa_periodic step 90600 lr 1.082e-05 [train_loss] tar_ll 1.6940 loss -1.6940 (5.161 secs)\n",
      "tnpa:tnpa_periodic step 90800 lr 1.037e-05 [train_loss] tar_ll 1.6428 loss -1.6428 (5.349 secs)\n",
      "tnpa:tnpa_periodic step 91000 lr 9.927e-06 [train_loss] tar_ll 1.7046 loss -1.7046 (5.386 secs)\n",
      "tnpa:tnpa_periodic step 91200 lr 9.493e-06 [train_loss] tar_ll 1.6521 loss -1.6521 (5.409 secs)\n",
      "tnpa:tnpa_periodic step 91400 lr 9.069e-06 [train_loss] tar_ll 1.5778 loss -1.5778 (5.153 secs)\n",
      "tnpa:tnpa_periodic step 91600 lr 8.655e-06 [train_loss] tar_ll 1.6466 loss -1.6466 (5.747 secs)\n",
      "tnpa:tnpa_periodic step 91800 lr 8.250e-06 [train_loss] tar_ll 1.6783 loss -1.6783 (5.880 secs)\n",
      "tnpa:tnpa_periodic step 92000 lr 7.854e-06 [train_loss] tar_ll 1.6703 loss -1.6703 (5.660 secs)\n",
      "tnpa:tnpa_periodic step 92200 lr 7.468e-06 [train_loss] tar_ll 1.6608 loss -1.6608 (5.654 secs)\n",
      "tnpa:tnpa_periodic step 92400 lr 7.092e-06 [train_loss] tar_ll 1.5623 loss -1.5623 (5.903 secs)\n",
      "tnpa:tnpa_periodic step 92600 lr 6.725e-06 [train_loss] tar_ll 1.6171 loss -1.6171 (5.800 secs)\n",
      "tnpa:tnpa_periodic step 92800 lr 6.368e-06 [train_loss] tar_ll 1.6409 loss -1.6409 (5.908 secs)\n",
      "tnpa:tnpa_periodic step 93000 lr 6.021e-06 [train_loss] tar_ll 1.6279 loss -1.6279 (5.941 secs)\n",
      "tnpa:tnpa_periodic step 93200 lr 5.683e-06 [train_loss] tar_ll 1.6313 loss -1.6313 (6.113 secs)\n",
      "tnpa:tnpa_periodic step 93400 lr 5.355e-06 [train_loss] tar_ll 1.6185 loss -1.6185 (5.902 secs)\n",
      "tnpa:tnpa_periodic step 93600 lr 5.036e-06 [train_loss] tar_ll 1.6186 loss -1.6186 (5.868 secs)\n",
      "tnpa:tnpa_periodic step 93800 lr 4.727e-06 [train_loss] tar_ll 1.6808 loss -1.6808 (6.065 secs)\n",
      "tnpa:tnpa_periodic step 94000 lr 4.428e-06 [train_loss] tar_ll 1.6167 loss -1.6167 (6.381 secs)\n",
      "tnpa:tnpa_periodic step 94200 lr 4.139e-06 [train_loss] tar_ll 1.6905 loss -1.6905 (6.284 secs)\n",
      "tnpa:tnpa_periodic step 94400 lr 3.859e-06 [train_loss] tar_ll 1.6753 loss -1.6753 (6.155 secs)\n",
      "tnpa:tnpa_periodic step 94600 lr 3.589e-06 [train_loss] tar_ll 1.6507 loss -1.6507 (6.003 secs)\n",
      "tnpa:tnpa_periodic step 94800 lr 3.329e-06 [train_loss] tar_ll 1.6108 loss -1.6108 (6.482 secs)\n",
      "tnpa:tnpa_periodic step 95000 lr 3.078e-06 [train_loss] tar_ll 1.5987 loss -1.5987 (6.093 secs)\n",
      "100%|##########| 3000/3000 [00:19<00:00, 152.04it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.6875 loss 2.6875 (19.736 secs)\n",
      "\n",
      "tnpa:tnpa_periodic step 95200 lr 2.837e-06 [train_loss] tar_ll 1.6463 loss -1.6463 (5.628 secs)\n",
      "tnpa:tnpa_periodic step 95400 lr 2.606e-06 [train_loss] tar_ll 1.6147 loss -1.6147 (5.397 secs)\n",
      "tnpa:tnpa_periodic step 95600 lr 2.385e-06 [train_loss] tar_ll 1.6576 loss -1.6576 (5.636 secs)\n",
      "tnpa:tnpa_periodic step 95800 lr 2.173e-06 [train_loss] tar_ll 1.6326 loss -1.6326 (5.640 secs)\n",
      "tnpa:tnpa_periodic step 96000 lr 1.971e-06 [train_loss] tar_ll 1.7211 loss -1.7211 (5.451 secs)\n",
      "tnpa:tnpa_periodic step 96200 lr 1.779e-06 [train_loss] tar_ll 1.6819 loss -1.6819 (5.783 secs)\n",
      "tnpa:tnpa_periodic step 96400 lr 1.597e-06 [train_loss] tar_ll 1.6977 loss -1.6977 (5.978 secs)\n",
      "tnpa:tnpa_periodic step 96600 lr 1.425e-06 [train_loss] tar_ll 1.6800 loss -1.6800 (5.855 secs)\n",
      "tnpa:tnpa_periodic step 96800 lr 1.262e-06 [train_loss] tar_ll 1.6839 loss -1.6839 (6.158 secs)\n",
      "tnpa:tnpa_periodic step 97000 lr 1.110e-06 [train_loss] tar_ll 1.6287 loss -1.6287 (5.838 secs)\n",
      "tnpa:tnpa_periodic step 97200 lr 9.666e-07 [train_loss] tar_ll 1.6731 loss -1.6731 (5.996 secs)\n",
      "tnpa:tnpa_periodic step 97400 lr 8.335e-07 [train_loss] tar_ll 1.6948 loss -1.6948 (6.170 secs)\n",
      "tnpa:tnpa_periodic step 97600 lr 7.103e-07 [train_loss] tar_ll 1.6321 loss -1.6321 (6.117 secs)\n",
      "tnpa:tnpa_periodic step 97800 lr 5.969e-07 [train_loss] tar_ll 1.6270 loss -1.6270 (6.155 secs)\n",
      "tnpa:tnpa_periodic step 98000 lr 4.933e-07 [train_loss] tar_ll 1.6856 loss -1.6856 (5.903 secs)\n",
      "tnpa:tnpa_periodic step 98200 lr 3.996e-07 [train_loss] tar_ll 1.6286 loss -1.6286 (6.131 secs)\n",
      "tnpa:tnpa_periodic step 98400 lr 3.158e-07 [train_loss] tar_ll 1.6660 loss -1.6660 (6.106 secs)\n",
      "tnpa:tnpa_periodic step 98600 lr 2.418e-07 [train_loss] tar_ll 1.6338 loss -1.6338 (6.174 secs)\n",
      "tnpa:tnpa_periodic step 98800 lr 1.776e-07 [train_loss] tar_ll 1.7024 loss -1.7024 (6.314 secs)\n",
      "tnpa:tnpa_periodic step 99000 lr 1.234e-07 [train_loss] tar_ll 1.6352 loss -1.6352 (6.073 secs)\n",
      "tnpa:tnpa_periodic step 99200 lr 7.895e-08 [train_loss] tar_ll 1.6425 loss -1.6425 (5.890 secs)\n",
      "tnpa:tnpa_periodic step 99400 lr 4.441e-08 [train_loss] tar_ll 1.6757 loss -1.6757 (6.052 secs)\n",
      "tnpa:tnpa_periodic step 99600 lr 1.974e-08 [train_loss] tar_ll 1.6572 loss -1.6572 (5.820 secs)\n",
      "tnpa:tnpa_periodic step 99800 lr 4.935e-09 [train_loss] tar_ll 1.6592 loss -1.6592 (5.760 secs)\n",
      "tnpa:tnpa_periodic step 100000 lr 0.000e+00 [train_loss] tar_ll 1.6638 loss -1.6638 (6.307 secs)\n",
      "100%|##########| 3000/3000 [00:18<00:00, 161.67it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.6158 loss 2.6158 (18.560 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:18<00:00, 165.89it/s]\n",
      "tnpa:tnpa_periodic periodic tar_ll -2.6158 loss 2.6158 (18.087 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3264891.0 miliseconds\n",
      "Execution time: 3264.891 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 65.59423828125 MB\n",
      "Memory Usage Change: 49.34423828125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='tnpa', name='tnpa_periodic',val_seed=100, val_l=None,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136fe06a-c40c-46e6-a279-4edfb0f05ea3",
   "metadata": {},
   "source": [
    "## ISANP (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aeafe7d-6224-427f-a6f5-e980835e612e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: isanp-isanp-num_latents-8_periodic\n",
      "Total number of parameters: 785602\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:isanp-num_latents-8_periodic step 200 lr 5.000e-04 [train_loss] tar_ll -0.6835 loss 0.6835 (14.114 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 400 lr 5.000e-04 [train_loss] tar_ll -0.6857 loss 0.6857 (14.054 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 600 lr 5.000e-04 [train_loss] tar_ll -0.5928 loss 0.5928 (13.863 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 800 lr 4.999e-04 [train_loss] tar_ll -0.5803 loss 0.5803 (14.139 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 1000 lr 4.999e-04 [train_loss] tar_ll -0.5912 loss 0.5912 (13.816 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 1200 lr 4.998e-04 [train_loss] tar_ll -0.6273 loss 0.6273 (13.925 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 1400 lr 4.998e-04 [train_loss] tar_ll -0.5242 loss 0.5242 (13.861 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 1600 lr 4.997e-04 [train_loss] tar_ll -0.4342 loss 0.4342 (13.688 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 1800 lr 4.996e-04 [train_loss] tar_ll -0.3285 loss 0.3285 (13.938 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 2000 lr 4.995e-04 [train_loss] tar_ll -0.2885 loss 0.2885 (13.704 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 2200 lr 4.994e-04 [train_loss] tar_ll -0.1512 loss 0.1512 (14.128 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 2400 lr 4.993e-04 [train_loss] tar_ll -0.0198 loss 0.0198 (13.911 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 2600 lr 4.992e-04 [train_loss] tar_ll 0.0351 loss -0.0351 (13.840 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 2800 lr 4.990e-04 [train_loss] tar_ll 0.1155 loss -0.1155 (13.797 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 3000 lr 4.989e-04 [train_loss] tar_ll 0.1626 loss -0.1626 (14.042 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 3200 lr 4.987e-04 [train_loss] tar_ll 0.1787 loss -0.1787 (14.325 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 3400 lr 4.986e-04 [train_loss] tar_ll 0.2517 loss -0.2517 (14.209 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2704 loss -0.2704 (13.862 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 3800 lr 4.982e-04 [train_loss] tar_ll 0.3270 loss -0.3270 (13.939 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 4000 lr 4.980e-04 [train_loss] tar_ll 0.3141 loss -0.3141 (14.154 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3048 loss -0.3048 (13.997 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 4400 lr 4.976e-04 [train_loss] tar_ll 0.1575 loss -0.1575 (13.740 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 4600 lr 4.974e-04 [train_loss] tar_ll 0.3748 loss -0.3748 (13.863 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4123 loss -0.4123 (13.773 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 5000 lr 4.969e-04 [train_loss] tar_ll 0.4208 loss -0.4208 (13.678 secs)\n",
      "100%|##########| 3000/3000 [01:05<00:00, 45.57it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -1.0435 loss 1.0435 (65.833 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 5200 lr 4.967e-04 [train_loss] tar_ll 0.3925 loss -0.3925 (14.025 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 5400 lr 4.964e-04 [train_loss] tar_ll 0.4794 loss -0.4794 (14.080 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 5600 lr 4.961e-04 [train_loss] tar_ll 0.2238 loss -0.2238 (14.245 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 5800 lr 4.959e-04 [train_loss] tar_ll 0.4436 loss -0.4436 (14.095 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4133 loss -0.4133 (13.919 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 6200 lr 4.953e-04 [train_loss] tar_ll 0.4226 loss -0.4226 (14.393 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 6400 lr 4.950e-04 [train_loss] tar_ll 0.5153 loss -0.5153 (13.672 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5658 loss -0.5658 (14.119 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 6800 lr 4.943e-04 [train_loss] tar_ll 0.4769 loss -0.4769 (13.980 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 7000 lr 4.940e-04 [train_loss] tar_ll 0.4973 loss -0.4973 (13.655 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 7200 lr 4.936e-04 [train_loss] tar_ll 0.4941 loss -0.4941 (14.089 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5257 loss -0.5257 (14.101 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 7600 lr 4.929e-04 [train_loss] tar_ll 0.4987 loss -0.4987 (13.785 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 7800 lr 4.925e-04 [train_loss] tar_ll 0.6279 loss -0.6279 (13.767 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 8000 lr 4.921e-04 [train_loss] tar_ll 0.5828 loss -0.5828 (13.897 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 8200 lr 4.918e-04 [train_loss] tar_ll 0.5650 loss -0.5650 (13.840 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 8400 lr 4.913e-04 [train_loss] tar_ll 0.6019 loss -0.6019 (13.715 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 8600 lr 4.909e-04 [train_loss] tar_ll 0.5172 loss -0.5172 (13.984 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 8800 lr 4.905e-04 [train_loss] tar_ll 0.5928 loss -0.5928 (14.061 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 9000 lr 4.901e-04 [train_loss] tar_ll 0.5031 loss -0.5031 (14.342 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 9200 lr 4.896e-04 [train_loss] tar_ll 0.5959 loss -0.5959 (14.156 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 9400 lr 4.892e-04 [train_loss] tar_ll 0.6156 loss -0.6156 (14.154 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 9600 lr 4.887e-04 [train_loss] tar_ll 0.5817 loss -0.5817 (14.097 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 9800 lr 4.882e-04 [train_loss] tar_ll 0.5511 loss -0.5511 (14.218 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 10000 lr 4.878e-04 [train_loss] tar_ll 0.5425 loss -0.5425 (14.025 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 44.97it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -1.1496 loss 1.1496 (66.719 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 10200 lr 4.873e-04 [train_loss] tar_ll 0.6436 loss -0.6436 (13.677 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 10400 lr 4.868e-04 [train_loss] tar_ll 0.6443 loss -0.6443 (13.646 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 10600 lr 4.863e-04 [train_loss] tar_ll 0.5898 loss -0.5898 (13.525 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 10800 lr 4.857e-04 [train_loss] tar_ll 0.5936 loss -0.5936 (13.574 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 11000 lr 4.852e-04 [train_loss] tar_ll 0.6729 loss -0.6729 (14.006 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 11200 lr 4.847e-04 [train_loss] tar_ll 0.6407 loss -0.6407 (14.403 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 11400 lr 4.841e-04 [train_loss] tar_ll 0.6406 loss -0.6406 (14.106 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 11600 lr 4.836e-04 [train_loss] tar_ll 0.4381 loss -0.4381 (14.051 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 11800 lr 4.830e-04 [train_loss] tar_ll 0.6517 loss -0.6517 (14.010 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 12000 lr 4.824e-04 [train_loss] tar_ll 0.5909 loss -0.5909 (14.005 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 12200 lr 4.819e-04 [train_loss] tar_ll 0.6145 loss -0.6145 (14.242 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 12400 lr 4.813e-04 [train_loss] tar_ll 0.5916 loss -0.5916 (13.973 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 12600 lr 4.807e-04 [train_loss] tar_ll 0.6746 loss -0.6746 (13.996 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 12800 lr 4.801e-04 [train_loss] tar_ll 0.5992 loss -0.5992 (13.758 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 13000 lr 4.794e-04 [train_loss] tar_ll 0.6633 loss -0.6633 (13.598 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 13200 lr 4.788e-04 [train_loss] tar_ll 0.6629 loss -0.6629 (14.023 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 13400 lr 4.782e-04 [train_loss] tar_ll 0.6170 loss -0.6170 (13.993 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 13600 lr 4.775e-04 [train_loss] tar_ll 0.5993 loss -0.5993 (13.893 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 13800 lr 4.769e-04 [train_loss] tar_ll 0.7430 loss -0.7430 (13.994 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 14000 lr 4.762e-04 [train_loss] tar_ll 0.6508 loss -0.6508 (13.839 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 14200 lr 4.755e-04 [train_loss] tar_ll 0.7289 loss -0.7289 (14.042 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 14400 lr 4.749e-04 [train_loss] tar_ll 0.6564 loss -0.6564 (13.792 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 14600 lr 4.742e-04 [train_loss] tar_ll 0.6268 loss -0.6268 (14.084 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 14800 lr 4.735e-04 [train_loss] tar_ll 0.6764 loss -0.6764 (14.047 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 15000 lr 4.728e-04 [train_loss] tar_ll 0.7106 loss -0.7106 (14.233 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.63it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -1.1412 loss 1.1412 (67.221 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 15200 lr 4.720e-04 [train_loss] tar_ll 0.6836 loss -0.6836 (13.718 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 15400 lr 4.713e-04 [train_loss] tar_ll 0.6883 loss -0.6883 (13.958 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 15600 lr 4.706e-04 [train_loss] tar_ll 0.7118 loss -0.7118 (13.877 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 15800 lr 4.698e-04 [train_loss] tar_ll 0.6927 loss -0.6927 (13.878 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 16000 lr 4.691e-04 [train_loss] tar_ll 0.7464 loss -0.7464 (13.636 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 16200 lr 4.683e-04 [train_loss] tar_ll 0.6720 loss -0.6720 (14.131 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 16400 lr 4.675e-04 [train_loss] tar_ll 0.7494 loss -0.7494 (13.819 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 16600 lr 4.668e-04 [train_loss] tar_ll 0.7106 loss -0.7106 (13.719 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 16800 lr 4.660e-04 [train_loss] tar_ll 0.7350 loss -0.7350 (13.747 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 17000 lr 4.652e-04 [train_loss] tar_ll 0.8063 loss -0.8063 (13.953 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 17200 lr 4.644e-04 [train_loss] tar_ll 0.7814 loss -0.7814 (14.153 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 17400 lr 4.636e-04 [train_loss] tar_ll 0.7386 loss -0.7386 (13.989 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 17600 lr 4.627e-04 [train_loss] tar_ll 0.7694 loss -0.7694 (13.875 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 17800 lr 4.619e-04 [train_loss] tar_ll 0.8348 loss -0.8348 (13.928 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 18000 lr 4.611e-04 [train_loss] tar_ll 0.6482 loss -0.6482 (14.026 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 18200 lr 4.602e-04 [train_loss] tar_ll 0.6357 loss -0.6357 (14.268 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 18400 lr 4.594e-04 [train_loss] tar_ll 0.7618 loss -0.7618 (14.215 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 18600 lr 4.585e-04 [train_loss] tar_ll 0.7691 loss -0.7691 (14.028 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 18800 lr 4.576e-04 [train_loss] tar_ll 0.6976 loss -0.6976 (13.722 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 19000 lr 4.568e-04 [train_loss] tar_ll 0.6703 loss -0.6703 (14.003 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 19200 lr 4.559e-04 [train_loss] tar_ll 0.7535 loss -0.7535 (14.038 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 19400 lr 4.550e-04 [train_loss] tar_ll 0.7369 loss -0.7369 (13.779 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 19600 lr 4.541e-04 [train_loss] tar_ll 0.7295 loss -0.7295 (13.837 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 19800 lr 4.532e-04 [train_loss] tar_ll 0.8554 loss -0.8554 (13.784 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 20000 lr 4.523e-04 [train_loss] tar_ll 0.8634 loss -0.8634 (13.658 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 44.89it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -1.4604 loss 1.4604 (66.832 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 20200 lr 4.513e-04 [train_loss] tar_ll 0.7168 loss -0.7168 (14.097 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 20400 lr 4.504e-04 [train_loss] tar_ll 0.7099 loss -0.7099 (14.051 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 20600 lr 4.494e-04 [train_loss] tar_ll 0.7922 loss -0.7922 (14.149 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 20800 lr 4.485e-04 [train_loss] tar_ll 0.7326 loss -0.7326 (14.160 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 21000 lr 4.475e-04 [train_loss] tar_ll 0.8560 loss -0.8560 (13.812 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 21200 lr 4.466e-04 [train_loss] tar_ll 0.8200 loss -0.8200 (13.853 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 21400 lr 4.456e-04 [train_loss] tar_ll 0.7999 loss -0.7999 (13.789 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 21600 lr 4.446e-04 [train_loss] tar_ll 0.8671 loss -0.8671 (14.309 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 21800 lr 4.436e-04 [train_loss] tar_ll 0.8173 loss -0.8173 (14.128 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 22000 lr 4.426e-04 [train_loss] tar_ll 0.7912 loss -0.7912 (13.939 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 22200 lr 4.416e-04 [train_loss] tar_ll 0.8890 loss -0.8890 (14.329 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 22400 lr 4.406e-04 [train_loss] tar_ll 0.8219 loss -0.8219 (13.741 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 22600 lr 4.396e-04 [train_loss] tar_ll 0.7575 loss -0.7575 (13.609 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 22800 lr 4.386e-04 [train_loss] tar_ll 0.8423 loss -0.8423 (14.048 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 23000 lr 4.375e-04 [train_loss] tar_ll 0.8247 loss -0.8247 (14.082 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 23200 lr 4.365e-04 [train_loss] tar_ll 0.8180 loss -0.8180 (14.229 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 23400 lr 4.354e-04 [train_loss] tar_ll 0.8369 loss -0.8369 (14.107 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 23600 lr 4.344e-04 [train_loss] tar_ll 0.8266 loss -0.8266 (14.266 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 23800 lr 4.333e-04 [train_loss] tar_ll 0.8136 loss -0.8136 (13.880 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 24000 lr 4.322e-04 [train_loss] tar_ll 0.8003 loss -0.8003 (14.142 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 24200 lr 4.312e-04 [train_loss] tar_ll 0.7792 loss -0.7792 (14.130 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 24400 lr 4.301e-04 [train_loss] tar_ll 0.7623 loss -0.7623 (14.456 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 24600 lr 4.290e-04 [train_loss] tar_ll 0.7990 loss -0.7990 (13.905 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 24800 lr 4.279e-04 [train_loss] tar_ll 0.9076 loss -0.9076 (13.787 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 25000 lr 4.268e-04 [train_loss] tar_ll 0.8240 loss -0.8240 (13.716 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.22it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -2.0692 loss 2.0692 (66.340 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 25200 lr 4.257e-04 [train_loss] tar_ll 0.8315 loss -0.8315 (13.931 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 25400 lr 4.245e-04 [train_loss] tar_ll 0.7340 loss -0.7340 (13.938 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 25600 lr 4.234e-04 [train_loss] tar_ll 0.7782 loss -0.7782 (14.294 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 25800 lr 4.223e-04 [train_loss] tar_ll 0.8690 loss -0.8690 (14.030 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 26000 lr 4.211e-04 [train_loss] tar_ll 0.8545 loss -0.8545 (14.302 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 26200 lr 4.200e-04 [train_loss] tar_ll 0.8028 loss -0.8028 (14.226 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 26400 lr 4.188e-04 [train_loss] tar_ll 0.6945 loss -0.6945 (14.146 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 26600 lr 4.177e-04 [train_loss] tar_ll 0.8742 loss -0.8742 (14.033 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 26800 lr 4.165e-04 [train_loss] tar_ll 0.8075 loss -0.8075 (13.950 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 27000 lr 4.153e-04 [train_loss] tar_ll 0.8790 loss -0.8790 (13.816 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 27200 lr 4.141e-04 [train_loss] tar_ll 0.7476 loss -0.7476 (13.718 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 27400 lr 4.130e-04 [train_loss] tar_ll 0.8957 loss -0.8957 (13.728 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 27600 lr 4.118e-04 [train_loss] tar_ll 0.9042 loss -0.9042 (13.734 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 27800 lr 4.106e-04 [train_loss] tar_ll 0.7958 loss -0.7958 (13.738 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 28000 lr 4.094e-04 [train_loss] tar_ll 0.8213 loss -0.8213 (13.645 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 28200 lr 4.081e-04 [train_loss] tar_ll 0.8906 loss -0.8906 (13.539 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 28400 lr 4.069e-04 [train_loss] tar_ll 0.9127 loss -0.9127 (13.767 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 28600 lr 4.057e-04 [train_loss] tar_ll 0.8935 loss -0.8935 (14.182 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 28800 lr 4.045e-04 [train_loss] tar_ll 0.8169 loss -0.8169 (14.017 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 29000 lr 4.032e-04 [train_loss] tar_ll 0.9088 loss -0.9088 (13.892 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 29200 lr 4.020e-04 [train_loss] tar_ll 0.8554 loss -0.8554 (14.194 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 29400 lr 4.007e-04 [train_loss] tar_ll 0.8038 loss -0.8038 (14.003 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 29600 lr 3.995e-04 [train_loss] tar_ll 0.8181 loss -0.8181 (14.119 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 29800 lr 3.982e-04 [train_loss] tar_ll 0.8799 loss -0.8799 (14.247 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 30000 lr 3.969e-04 [train_loss] tar_ll 0.8843 loss -0.8843 (14.033 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.15it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -1.5419 loss 1.5419 (66.445 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 30200 lr 3.957e-04 [train_loss] tar_ll 0.8610 loss -0.8610 (13.938 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 30400 lr 3.944e-04 [train_loss] tar_ll 0.8510 loss -0.8510 (13.812 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 30600 lr 3.931e-04 [train_loss] tar_ll 0.9037 loss -0.9037 (13.893 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 30800 lr 3.918e-04 [train_loss] tar_ll 0.8718 loss -0.8718 (13.710 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 31000 lr 3.905e-04 [train_loss] tar_ll 0.8499 loss -0.8499 (13.912 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 31200 lr 3.892e-04 [train_loss] tar_ll 0.8709 loss -0.8709 (14.186 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 31400 lr 3.879e-04 [train_loss] tar_ll 0.9101 loss -0.9101 (14.095 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 31600 lr 3.866e-04 [train_loss] tar_ll 0.8580 loss -0.8580 (14.146 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 31800 lr 3.853e-04 [train_loss] tar_ll 0.8898 loss -0.8898 (14.131 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 32000 lr 3.840e-04 [train_loss] tar_ll 1.0026 loss -1.0026 (13.961 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 32200 lr 3.826e-04 [train_loss] tar_ll 0.7931 loss -0.7931 (14.153 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 32400 lr 3.813e-04 [train_loss] tar_ll 0.9481 loss -0.9481 (14.385 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 32600 lr 3.800e-04 [train_loss] tar_ll 0.8835 loss -0.8835 (14.027 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 32800 lr 3.786e-04 [train_loss] tar_ll 0.8734 loss -0.8734 (13.990 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 33000 lr 3.773e-04 [train_loss] tar_ll 0.7415 loss -0.7415 (13.793 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 33200 lr 3.759e-04 [train_loss] tar_ll 0.9440 loss -0.9440 (13.899 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 33400 lr 3.745e-04 [train_loss] tar_ll 0.8543 loss -0.8543 (13.658 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 33600 lr 3.732e-04 [train_loss] tar_ll 0.8723 loss -0.8723 (13.735 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 33800 lr 3.718e-04 [train_loss] tar_ll 0.9134 loss -0.9134 (13.675 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 34000 lr 3.704e-04 [train_loss] tar_ll 0.9114 loss -0.9114 (13.755 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 34200 lr 3.691e-04 [train_loss] tar_ll 0.9286 loss -0.9286 (13.733 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 34400 lr 3.677e-04 [train_loss] tar_ll 0.8496 loss -0.8496 (13.599 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 34600 lr 3.663e-04 [train_loss] tar_ll 0.9625 loss -0.9625 (14.069 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 34800 lr 3.649e-04 [train_loss] tar_ll 0.9217 loss -0.9217 (13.915 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 35000 lr 3.635e-04 [train_loss] tar_ll 0.8986 loss -0.8986 (14.173 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.74it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -1.6841 loss 1.6841 (67.060 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 35200 lr 3.621e-04 [train_loss] tar_ll 0.9589 loss -0.9589 (14.120 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 35400 lr 3.607e-04 [train_loss] tar_ll 0.8938 loss -0.8938 (13.893 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 35600 lr 3.593e-04 [train_loss] tar_ll 0.8931 loss -0.8931 (13.675 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 35800 lr 3.579e-04 [train_loss] tar_ll 0.8786 loss -0.8786 (14.004 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 36000 lr 3.564e-04 [train_loss] tar_ll 0.8894 loss -0.8894 (13.900 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 36200 lr 3.550e-04 [train_loss] tar_ll 0.9879 loss -0.9879 (13.937 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 36400 lr 3.536e-04 [train_loss] tar_ll 0.9395 loss -0.9395 (13.693 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 36600 lr 3.522e-04 [train_loss] tar_ll 0.9245 loss -0.9245 (13.911 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 36800 lr 3.507e-04 [train_loss] tar_ll 0.9319 loss -0.9319 (13.811 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 37000 lr 3.493e-04 [train_loss] tar_ll 0.9567 loss -0.9567 (13.820 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 37200 lr 3.478e-04 [train_loss] tar_ll 0.9915 loss -0.9915 (14.163 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 37400 lr 3.464e-04 [train_loss] tar_ll 1.0241 loss -1.0241 (13.996 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 37600 lr 3.449e-04 [train_loss] tar_ll 0.9609 loss -0.9609 (14.124 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 37800 lr 3.435e-04 [train_loss] tar_ll 0.9665 loss -0.9665 (14.081 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 38000 lr 3.420e-04 [train_loss] tar_ll 0.9782 loss -0.9782 (14.219 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 38200 lr 3.406e-04 [train_loss] tar_ll 0.7485 loss -0.7485 (14.065 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 38400 lr 3.391e-04 [train_loss] tar_ll 0.8842 loss -0.8842 (13.985 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 38600 lr 3.376e-04 [train_loss] tar_ll 0.9256 loss -0.9256 (14.376 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 38800 lr 3.362e-04 [train_loss] tar_ll 0.8851 loss -0.8851 (14.190 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 39000 lr 3.347e-04 [train_loss] tar_ll 0.8707 loss -0.8707 (13.671 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 39200 lr 3.332e-04 [train_loss] tar_ll 0.9177 loss -0.9177 (13.762 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 39400 lr 3.317e-04 [train_loss] tar_ll 0.9798 loss -0.9798 (13.581 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 39600 lr 3.302e-04 [train_loss] tar_ll 0.9512 loss -0.9512 (13.898 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 39800 lr 3.287e-04 [train_loss] tar_ll 0.9037 loss -0.9037 (14.038 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 40000 lr 3.273e-04 [train_loss] tar_ll 1.0189 loss -1.0189 (13.840 secs)\n",
      "100%|##########| 3000/3000 [01:05<00:00, 45.46it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -1.7195 loss 1.7195 (65.996 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 40200 lr 3.258e-04 [train_loss] tar_ll 0.9224 loss -0.9224 (14.528 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 40400 lr 3.243e-04 [train_loss] tar_ll 1.0570 loss -1.0570 (14.003 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 40600 lr 3.228e-04 [train_loss] tar_ll 0.9895 loss -0.9895 (14.424 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 40800 lr 3.213e-04 [train_loss] tar_ll 0.8794 loss -0.8794 (14.039 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 41000 lr 3.197e-04 [train_loss] tar_ll 0.8707 loss -0.8707 (13.752 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 41200 lr 3.182e-04 [train_loss] tar_ll 0.9910 loss -0.9910 (14.215 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 41400 lr 3.167e-04 [train_loss] tar_ll 1.0162 loss -1.0162 (13.682 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 41600 lr 3.152e-04 [train_loss] tar_ll 0.9915 loss -0.9915 (14.309 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 41800 lr 3.137e-04 [train_loss] tar_ll 1.0448 loss -1.0448 (14.371 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 42000 lr 3.122e-04 [train_loss] tar_ll 0.8897 loss -0.8897 (14.206 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 42200 lr 3.106e-04 [train_loss] tar_ll 0.9319 loss -0.9319 (13.722 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 42400 lr 3.091e-04 [train_loss] tar_ll 0.9448 loss -0.9448 (13.819 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 42600 lr 3.076e-04 [train_loss] tar_ll 0.9954 loss -0.9954 (13.578 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 42800 lr 3.061e-04 [train_loss] tar_ll 1.0203 loss -1.0203 (13.591 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 43000 lr 3.045e-04 [train_loss] tar_ll 0.8916 loss -0.8916 (13.762 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 43200 lr 3.030e-04 [train_loss] tar_ll 1.0517 loss -1.0517 (14.052 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 43400 lr 3.015e-04 [train_loss] tar_ll 0.9955 loss -0.9955 (14.004 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 43600 lr 2.999e-04 [train_loss] tar_ll 1.0414 loss -1.0414 (13.791 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 43800 lr 2.984e-04 [train_loss] tar_ll 1.0520 loss -1.0520 (14.027 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 44000 lr 2.968e-04 [train_loss] tar_ll 1.0347 loss -1.0347 (14.048 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 44200 lr 2.953e-04 [train_loss] tar_ll 1.0173 loss -1.0173 (14.250 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 44400 lr 2.938e-04 [train_loss] tar_ll 1.0557 loss -1.0557 (13.888 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 44600 lr 2.922e-04 [train_loss] tar_ll 0.9626 loss -0.9626 (13.912 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 44800 lr 2.907e-04 [train_loss] tar_ll 1.0683 loss -1.0683 (13.920 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 45000 lr 2.891e-04 [train_loss] tar_ll 0.9825 loss -0.9825 (13.750 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 44.80it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -1.7116 loss 1.7116 (66.974 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 45200 lr 2.876e-04 [train_loss] tar_ll 1.0368 loss -1.0368 (13.756 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 45400 lr 2.860e-04 [train_loss] tar_ll 1.0618 loss -1.0618 (13.700 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 45600 lr 2.844e-04 [train_loss] tar_ll 1.0516 loss -1.0516 (13.908 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 45800 lr 2.829e-04 [train_loss] tar_ll 1.0004 loss -1.0004 (14.420 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 46000 lr 2.813e-04 [train_loss] tar_ll 1.0560 loss -1.0560 (14.220 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 46200 lr 2.798e-04 [train_loss] tar_ll 0.9980 loss -0.9980 (14.338 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 46400 lr 2.782e-04 [train_loss] tar_ll 1.0671 loss -1.0671 (14.109 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 46600 lr 2.767e-04 [train_loss] tar_ll 1.0679 loss -1.0679 (14.137 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 46800 lr 2.751e-04 [train_loss] tar_ll 0.9951 loss -0.9951 (14.419 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 47000 lr 2.735e-04 [train_loss] tar_ll 0.9699 loss -0.9699 (13.981 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 47200 lr 2.720e-04 [train_loss] tar_ll 1.0860 loss -1.0860 (13.857 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 47400 lr 2.704e-04 [train_loss] tar_ll 1.0045 loss -1.0045 (13.472 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 47600 lr 2.688e-04 [train_loss] tar_ll 0.9382 loss -0.9382 (14.070 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 47800 lr 2.673e-04 [train_loss] tar_ll 0.9914 loss -0.9914 (13.924 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 48000 lr 2.657e-04 [train_loss] tar_ll 1.0669 loss -1.0669 (13.709 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 48200 lr 2.641e-04 [train_loss] tar_ll 0.9870 loss -0.9870 (13.832 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 48400 lr 2.626e-04 [train_loss] tar_ll 1.0247 loss -1.0247 (13.903 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 48600 lr 2.610e-04 [train_loss] tar_ll 1.0646 loss -1.0646 (13.392 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 48800 lr 2.594e-04 [train_loss] tar_ll 1.0604 loss -1.0604 (13.573 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 49000 lr 2.579e-04 [train_loss] tar_ll 1.0368 loss -1.0368 (13.877 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 49200 lr 2.563e-04 [train_loss] tar_ll 1.0308 loss -1.0308 (14.192 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 49400 lr 2.547e-04 [train_loss] tar_ll 1.0117 loss -1.0117 (13.878 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 49600 lr 2.531e-04 [train_loss] tar_ll 1.0269 loss -1.0269 (13.820 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 49800 lr 2.516e-04 [train_loss] tar_ll 1.0112 loss -1.0112 (13.734 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 50000 lr 2.500e-04 [train_loss] tar_ll 1.1386 loss -1.1386 (14.241 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 44.97it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -2.4234 loss 2.4234 (66.720 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 50200 lr 2.484e-04 [train_loss] tar_ll 1.1017 loss -1.1017 (14.200 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 50400 lr 2.469e-04 [train_loss] tar_ll 1.0281 loss -1.0281 (13.560 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 50600 lr 2.453e-04 [train_loss] tar_ll 0.9411 loss -0.9411 (13.862 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 50800 lr 2.437e-04 [train_loss] tar_ll 1.0350 loss -1.0350 (13.803 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 51000 lr 2.421e-04 [train_loss] tar_ll 1.0314 loss -1.0314 (13.523 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 51200 lr 2.406e-04 [train_loss] tar_ll 0.9767 loss -0.9767 (13.914 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 51400 lr 2.390e-04 [train_loss] tar_ll 1.0855 loss -1.0855 (13.876 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 51600 lr 2.374e-04 [train_loss] tar_ll 1.1220 loss -1.1220 (14.757 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 51800 lr 2.359e-04 [train_loss] tar_ll 1.0564 loss -1.0564 (15.032 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 52000 lr 2.343e-04 [train_loss] tar_ll 1.1154 loss -1.1154 (15.354 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 52200 lr 2.327e-04 [train_loss] tar_ll 1.0304 loss -1.0304 (15.950 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 52400 lr 2.312e-04 [train_loss] tar_ll 1.0874 loss -1.0874 (15.639 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 52600 lr 2.296e-04 [train_loss] tar_ll 1.0663 loss -1.0663 (15.592 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 52800 lr 2.280e-04 [train_loss] tar_ll 1.0550 loss -1.0550 (15.615 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 53000 lr 2.265e-04 [train_loss] tar_ll 1.0383 loss -1.0383 (15.129 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 53200 lr 2.249e-04 [train_loss] tar_ll 1.0600 loss -1.0600 (15.089 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 53400 lr 2.233e-04 [train_loss] tar_ll 0.9540 loss -0.9540 (14.850 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 53600 lr 2.218e-04 [train_loss] tar_ll 1.1226 loss -1.1226 (14.960 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 53800 lr 2.202e-04 [train_loss] tar_ll 1.0387 loss -1.0387 (15.052 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 54000 lr 2.187e-04 [train_loss] tar_ll 1.1177 loss -1.1177 (14.928 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 54200 lr 2.171e-04 [train_loss] tar_ll 1.0991 loss -1.0991 (15.239 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 54400 lr 2.156e-04 [train_loss] tar_ll 0.9730 loss -0.9730 (15.512 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 54600 lr 2.140e-04 [train_loss] tar_ll 1.0581 loss -1.0581 (15.608 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 54800 lr 2.124e-04 [train_loss] tar_ll 1.1614 loss -1.1614 (15.670 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 55000 lr 2.109e-04 [train_loss] tar_ll 1.0882 loss -1.0882 (15.458 secs)\n",
      "100%|##########| 3000/3000 [01:14<00:00, 40.36it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -2.0908 loss 2.0908 (74.339 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 55200 lr 2.093e-04 [train_loss] tar_ll 1.0989 loss -1.0989 (15.354 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 55400 lr 2.078e-04 [train_loss] tar_ll 1.1083 loss -1.1083 (15.118 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 55600 lr 2.062e-04 [train_loss] tar_ll 1.1105 loss -1.1105 (15.318 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 55800 lr 2.047e-04 [train_loss] tar_ll 1.0754 loss -1.0754 (15.329 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 56000 lr 2.032e-04 [train_loss] tar_ll 1.1471 loss -1.1471 (14.782 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 56200 lr 2.016e-04 [train_loss] tar_ll 1.0277 loss -1.0277 (14.579 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 56400 lr 2.001e-04 [train_loss] tar_ll 1.0789 loss -1.0789 (14.463 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 56600 lr 1.985e-04 [train_loss] tar_ll 1.1995 loss -1.1995 (14.754 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 56800 lr 1.970e-04 [train_loss] tar_ll 1.1044 loss -1.1044 (15.005 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 57000 lr 1.955e-04 [train_loss] tar_ll 1.1106 loss -1.1106 (15.121 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 57200 lr 1.939e-04 [train_loss] tar_ll 1.1116 loss -1.1116 (15.179 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 57400 lr 1.924e-04 [train_loss] tar_ll 1.1240 loss -1.1240 (15.431 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 57600 lr 1.909e-04 [train_loss] tar_ll 1.1489 loss -1.1489 (15.245 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 57800 lr 1.894e-04 [train_loss] tar_ll 1.1393 loss -1.1393 (15.251 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 58000 lr 1.878e-04 [train_loss] tar_ll 1.0980 loss -1.0980 (15.187 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 58200 lr 1.863e-04 [train_loss] tar_ll 1.1106 loss -1.1106 (15.390 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 58400 lr 1.848e-04 [train_loss] tar_ll 1.1822 loss -1.1822 (15.009 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 58600 lr 1.833e-04 [train_loss] tar_ll 1.1261 loss -1.1261 (14.845 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 58800 lr 1.818e-04 [train_loss] tar_ll 1.0962 loss -1.0962 (15.381 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 59000 lr 1.803e-04 [train_loss] tar_ll 1.0240 loss -1.0240 (15.089 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 59200 lr 1.787e-04 [train_loss] tar_ll 1.1842 loss -1.1842 (15.444 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 59400 lr 1.772e-04 [train_loss] tar_ll 1.0745 loss -1.0745 (15.135 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 59600 lr 1.757e-04 [train_loss] tar_ll 0.8944 loss -0.8944 (15.045 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 59800 lr 1.742e-04 [train_loss] tar_ll 1.1377 loss -1.1377 (15.342 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 60000 lr 1.727e-04 [train_loss] tar_ll 1.1090 loss -1.1090 (15.419 secs)\n",
      "100%|##########| 3000/3000 [01:12<00:00, 41.10it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -2.4561 loss 2.4561 (73.006 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 60200 lr 1.713e-04 [train_loss] tar_ll 1.0370 loss -1.0370 (15.565 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 60400 lr 1.698e-04 [train_loss] tar_ll 1.1696 loss -1.1696 (14.997 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 60600 lr 1.683e-04 [train_loss] tar_ll 1.1183 loss -1.1183 (14.738 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 60800 lr 1.668e-04 [train_loss] tar_ll 1.1425 loss -1.1425 (14.906 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 61000 lr 1.653e-04 [train_loss] tar_ll 1.0868 loss -1.0868 (14.961 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 61200 lr 1.638e-04 [train_loss] tar_ll 1.1276 loss -1.1276 (15.148 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 61400 lr 1.624e-04 [train_loss] tar_ll 1.0899 loss -1.0899 (14.948 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 61600 lr 1.609e-04 [train_loss] tar_ll 1.1205 loss -1.1205 (14.724 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 61800 lr 1.594e-04 [train_loss] tar_ll 1.1626 loss -1.1626 (14.586 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 62000 lr 1.580e-04 [train_loss] tar_ll 1.1853 loss -1.1853 (15.164 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 62200 lr 1.565e-04 [train_loss] tar_ll 1.1425 loss -1.1425 (15.706 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 62400 lr 1.551e-04 [train_loss] tar_ll 1.1209 loss -1.1209 (15.711 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 62600 lr 1.536e-04 [train_loss] tar_ll 1.1680 loss -1.1680 (15.736 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 62800 lr 1.522e-04 [train_loss] tar_ll 1.1756 loss -1.1756 (15.740 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 63000 lr 1.507e-04 [train_loss] tar_ll 1.1456 loss -1.1456 (15.790 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 63200 lr 1.493e-04 [train_loss] tar_ll 1.1366 loss -1.1366 (15.775 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 63400 lr 1.478e-04 [train_loss] tar_ll 1.1733 loss -1.1733 (15.633 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 63600 lr 1.464e-04 [train_loss] tar_ll 1.1646 loss -1.1646 (15.355 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 63800 lr 1.450e-04 [train_loss] tar_ll 1.1730 loss -1.1730 (15.227 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 64000 lr 1.436e-04 [train_loss] tar_ll 1.1650 loss -1.1650 (15.294 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 64200 lr 1.421e-04 [train_loss] tar_ll 1.1270 loss -1.1270 (15.322 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 64400 lr 1.407e-04 [train_loss] tar_ll 1.1098 loss -1.1098 (14.970 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 64600 lr 1.393e-04 [train_loss] tar_ll 1.1113 loss -1.1113 (14.902 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 64800 lr 1.379e-04 [train_loss] tar_ll 1.1720 loss -1.1720 (15.189 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 65000 lr 1.365e-04 [train_loss] tar_ll 1.1820 loss -1.1820 (14.986 secs)\n",
      "100%|##########| 3000/3000 [01:12<00:00, 41.24it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -1.8770 loss 1.8770 (72.746 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 65200 lr 1.351e-04 [train_loss] tar_ll 1.1456 loss -1.1456 (15.451 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 65400 lr 1.337e-04 [train_loss] tar_ll 1.1168 loss -1.1168 (15.348 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 65600 lr 1.323e-04 [train_loss] tar_ll 1.0892 loss -1.0892 (15.217 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 65800 lr 1.309e-04 [train_loss] tar_ll 1.1643 loss -1.1643 (15.190 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 66000 lr 1.296e-04 [train_loss] tar_ll 1.1417 loss -1.1417 (15.142 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 66200 lr 1.282e-04 [train_loss] tar_ll 1.1887 loss -1.1887 (15.312 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 66400 lr 1.268e-04 [train_loss] tar_ll 1.1312 loss -1.1312 (15.056 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 66600 lr 1.255e-04 [train_loss] tar_ll 1.1831 loss -1.1831 (15.142 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 66800 lr 1.241e-04 [train_loss] tar_ll 1.1609 loss -1.1609 (15.168 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 67000 lr 1.227e-04 [train_loss] tar_ll 1.1355 loss -1.1355 (14.965 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 67200 lr 1.214e-04 [train_loss] tar_ll 1.2583 loss -1.2583 (15.166 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 67400 lr 1.200e-04 [train_loss] tar_ll 1.2347 loss -1.2347 (15.635 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 67600 lr 1.187e-04 [train_loss] tar_ll 1.1591 loss -1.1591 (15.590 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 67800 lr 1.174e-04 [train_loss] tar_ll 1.2338 loss -1.2338 (15.840 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 68000 lr 1.160e-04 [train_loss] tar_ll 1.1415 loss -1.1415 (15.663 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 68200 lr 1.147e-04 [train_loss] tar_ll 1.2557 loss -1.2557 (15.932 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 68400 lr 1.134e-04 [train_loss] tar_ll 1.1365 loss -1.1365 (15.603 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 68600 lr 1.121e-04 [train_loss] tar_ll 1.1891 loss -1.1891 (15.488 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 68800 lr 1.108e-04 [train_loss] tar_ll 1.1783 loss -1.1783 (15.304 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 69000 lr 1.095e-04 [train_loss] tar_ll 1.2129 loss -1.2129 (14.857 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 69200 lr 1.082e-04 [train_loss] tar_ll 1.1526 loss -1.1526 (15.266 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 69400 lr 1.069e-04 [train_loss] tar_ll 1.1943 loss -1.1943 (15.436 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 69600 lr 1.056e-04 [train_loss] tar_ll 1.2322 loss -1.2322 (15.033 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 69800 lr 1.043e-04 [train_loss] tar_ll 1.1606 loss -1.1606 (14.966 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 70000 lr 1.031e-04 [train_loss] tar_ll 1.1548 loss -1.1548 (15.094 secs)\n",
      "100%|##########| 3000/3000 [01:12<00:00, 41.52it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -2.3007 loss 2.3007 (72.267 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 70200 lr 1.018e-04 [train_loss] tar_ll 1.2853 loss -1.2853 (15.441 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 70400 lr 1.005e-04 [train_loss] tar_ll 1.1954 loss -1.1954 (15.564 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 70600 lr 9.927e-05 [train_loss] tar_ll 1.2009 loss -1.2009 (15.706 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 70800 lr 9.802e-05 [train_loss] tar_ll 1.1722 loss -1.1722 (15.414 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 71000 lr 9.677e-05 [train_loss] tar_ll 1.2396 loss -1.2396 (15.210 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 71200 lr 9.554e-05 [train_loss] tar_ll 1.2465 loss -1.2465 (15.159 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 71400 lr 9.430e-05 [train_loss] tar_ll 1.1719 loss -1.1719 (15.183 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 71600 lr 9.308e-05 [train_loss] tar_ll 1.1693 loss -1.1693 (15.179 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 71800 lr 9.186e-05 [train_loss] tar_ll 1.1815 loss -1.1815 (15.014 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 72000 lr 9.064e-05 [train_loss] tar_ll 1.0931 loss -1.0931 (15.234 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 72200 lr 8.944e-05 [train_loss] tar_ll 1.2557 loss -1.2557 (14.790 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 72400 lr 8.824e-05 [train_loss] tar_ll 1.2392 loss -1.2392 (14.578 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 72600 lr 8.704e-05 [train_loss] tar_ll 1.2485 loss -1.2485 (15.357 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 72800 lr 8.585e-05 [train_loss] tar_ll 1.3013 loss -1.3013 (15.753 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 73000 lr 8.467e-05 [train_loss] tar_ll 1.2420 loss -1.2420 (15.564 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 73200 lr 8.350e-05 [train_loss] tar_ll 1.2347 loss -1.2347 (15.666 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 73400 lr 8.233e-05 [train_loss] tar_ll 1.1858 loss -1.1858 (15.554 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 73600 lr 8.117e-05 [train_loss] tar_ll 1.2125 loss -1.2125 (15.590 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 73800 lr 8.001e-05 [train_loss] tar_ll 1.2989 loss -1.2989 (15.875 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 74000 lr 7.886e-05 [train_loss] tar_ll 1.2450 loss -1.2450 (15.747 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 74200 lr 7.772e-05 [train_loss] tar_ll 1.1671 loss -1.1671 (15.548 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 74400 lr 7.659e-05 [train_loss] tar_ll 1.2964 loss -1.2964 (15.450 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 74600 lr 7.546e-05 [train_loss] tar_ll 1.1402 loss -1.1402 (15.229 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 74800 lr 7.434e-05 [train_loss] tar_ll 1.2058 loss -1.2058 (15.104 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 75000 lr 7.322e-05 [train_loss] tar_ll 1.3082 loss -1.3082 (15.171 secs)\n",
      "100%|##########| 3000/3000 [01:12<00:00, 41.48it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -2.4465 loss 2.4465 (72.315 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 75200 lr 7.212e-05 [train_loss] tar_ll 1.2317 loss -1.2317 (15.063 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 75400 lr 7.102e-05 [train_loss] tar_ll 1.2188 loss -1.2188 (15.120 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 75600 lr 6.992e-05 [train_loss] tar_ll 1.2425 loss -1.2425 (15.378 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 75800 lr 6.884e-05 [train_loss] tar_ll 1.2259 loss -1.2259 (15.323 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 76000 lr 6.776e-05 [train_loss] tar_ll 1.2616 loss -1.2616 (15.441 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 76200 lr 6.669e-05 [train_loss] tar_ll 1.2771 loss -1.2771 (15.474 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 76400 lr 6.562e-05 [train_loss] tar_ll 1.1806 loss -1.1806 (15.043 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 76600 lr 6.456e-05 [train_loss] tar_ll 1.2872 loss -1.2872 (15.157 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 76800 lr 6.351e-05 [train_loss] tar_ll 1.1657 loss -1.1657 (15.030 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 77000 lr 6.247e-05 [train_loss] tar_ll 1.2243 loss -1.2243 (14.855 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 77200 lr 6.144e-05 [train_loss] tar_ll 1.2509 loss -1.2509 (14.889 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 77400 lr 6.041e-05 [train_loss] tar_ll 1.2441 loss -1.2441 (14.999 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 77600 lr 5.939e-05 [train_loss] tar_ll 1.2233 loss -1.2233 (14.957 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2305 loss -1.2305 (14.455 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 78000 lr 5.737e-05 [train_loss] tar_ll 1.3057 loss -1.3057 (15.630 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 78200 lr 5.637e-05 [train_loss] tar_ll 1.1963 loss -1.1963 (15.730 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 78400 lr 5.538e-05 [train_loss] tar_ll 1.1698 loss -1.1698 (15.415 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 78600 lr 5.440e-05 [train_loss] tar_ll 1.2168 loss -1.2168 (15.720 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 78800 lr 5.343e-05 [train_loss] tar_ll 1.2003 loss -1.2003 (16.103 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 79000 lr 5.246e-05 [train_loss] tar_ll 1.2359 loss -1.2359 (15.731 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 79200 lr 5.150e-05 [train_loss] tar_ll 1.2313 loss -1.2313 (15.777 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 79400 lr 5.055e-05 [train_loss] tar_ll 1.2973 loss -1.2973 (15.720 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 79600 lr 4.961e-05 [train_loss] tar_ll 1.2828 loss -1.2828 (15.148 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 79800 lr 4.867e-05 [train_loss] tar_ll 1.2493 loss -1.2493 (15.126 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 80000 lr 4.775e-05 [train_loss] tar_ll 1.0546 loss -1.0546 (15.036 secs)\n",
      "100%|##########| 3000/3000 [01:12<00:00, 41.60it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -2.1644 loss 2.1644 (72.113 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 80200 lr 4.683e-05 [train_loss] tar_ll 1.2573 loss -1.2573 (15.330 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 80400 lr 4.592e-05 [train_loss] tar_ll 1.2804 loss -1.2804 (15.359 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1936 loss -1.1936 (15.378 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2405 loss -1.2405 (15.386 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 81000 lr 4.323e-05 [train_loss] tar_ll 1.2451 loss -1.2451 (15.289 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 81200 lr 4.235e-05 [train_loss] tar_ll 1.2301 loss -1.2301 (15.666 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 81400 lr 4.148e-05 [train_loss] tar_ll 1.1913 loss -1.1913 (15.336 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 81600 lr 4.062e-05 [train_loss] tar_ll 1.3412 loss -1.3412 (15.221 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2469 loss -1.2469 (15.180 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 82000 lr 3.892e-05 [train_loss] tar_ll 1.2117 loss -1.2117 (15.190 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 82200 lr 3.808e-05 [train_loss] tar_ll 1.2406 loss -1.2406 (15.602 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 82400 lr 3.725e-05 [train_loss] tar_ll 1.3179 loss -1.3179 (15.108 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 82600 lr 3.643e-05 [train_loss] tar_ll 1.2661 loss -1.2661 (15.356 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 82800 lr 3.562e-05 [train_loss] tar_ll 1.1519 loss -1.1519 (15.040 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 83000 lr 3.481e-05 [train_loss] tar_ll 1.3040 loss -1.3040 (14.830 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 83200 lr 3.402e-05 [train_loss] tar_ll 1.1957 loss -1.1957 (15.560 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 83400 lr 3.323e-05 [train_loss] tar_ll 1.2108 loss -1.2108 (15.578 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 83600 lr 3.245e-05 [train_loss] tar_ll 1.3016 loss -1.3016 (15.707 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 83800 lr 3.168e-05 [train_loss] tar_ll 1.3811 loss -1.3811 (15.670 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 84000 lr 3.092e-05 [train_loss] tar_ll 1.2630 loss -1.2630 (15.262 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 84200 lr 3.017e-05 [train_loss] tar_ll 1.3049 loss -1.3049 (15.632 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 84400 lr 2.943e-05 [train_loss] tar_ll 1.2583 loss -1.2583 (15.420 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 84600 lr 2.869e-05 [train_loss] tar_ll 1.3327 loss -1.3327 (15.445 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 84800 lr 2.797e-05 [train_loss] tar_ll 1.2533 loss -1.2533 (15.305 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 85000 lr 2.725e-05 [train_loss] tar_ll 1.3080 loss -1.3080 (15.287 secs)\n",
      "100%|##########| 3000/3000 [01:12<00:00, 41.62it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -2.6609 loss 2.6609 (72.079 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 85200 lr 2.654e-05 [train_loss] tar_ll 1.1950 loss -1.1950 (14.868 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 85400 lr 2.584e-05 [train_loss] tar_ll 1.3016 loss -1.3016 (15.300 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 85600 lr 2.515e-05 [train_loss] tar_ll 1.2946 loss -1.2946 (15.548 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 85800 lr 2.447e-05 [train_loss] tar_ll 1.3024 loss -1.3024 (15.250 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 86000 lr 2.379e-05 [train_loss] tar_ll 1.2566 loss -1.2566 (15.260 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 86200 lr 2.313e-05 [train_loss] tar_ll 1.3391 loss -1.3391 (15.353 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 86400 lr 2.247e-05 [train_loss] tar_ll 1.2728 loss -1.2728 (15.311 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 86600 lr 2.183e-05 [train_loss] tar_ll 1.2381 loss -1.2381 (15.474 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 86800 lr 2.119e-05 [train_loss] tar_ll 1.2577 loss -1.2577 (15.480 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 87000 lr 2.056e-05 [train_loss] tar_ll 1.2902 loss -1.2902 (15.148 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 87200 lr 1.994e-05 [train_loss] tar_ll 1.3036 loss -1.3036 (14.887 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 87400 lr 1.933e-05 [train_loss] tar_ll 1.2753 loss -1.2753 (15.009 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 87600 lr 1.873e-05 [train_loss] tar_ll 1.2449 loss -1.2449 (15.028 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 87800 lr 1.814e-05 [train_loss] tar_ll 1.1845 loss -1.1845 (14.830 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 88000 lr 1.756e-05 [train_loss] tar_ll 1.3155 loss -1.3155 (15.190 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 88200 lr 1.698e-05 [train_loss] tar_ll 1.2495 loss -1.2495 (14.821 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 88400 lr 1.642e-05 [train_loss] tar_ll 1.2344 loss -1.2344 (14.664 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 88600 lr 1.586e-05 [train_loss] tar_ll 1.3433 loss -1.3433 (15.154 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 88800 lr 1.532e-05 [train_loss] tar_ll 1.2912 loss -1.2912 (15.202 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 89000 lr 1.478e-05 [train_loss] tar_ll 1.2726 loss -1.2726 (15.348 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 89200 lr 1.425e-05 [train_loss] tar_ll 1.2459 loss -1.2459 (15.536 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 89400 lr 1.373e-05 [train_loss] tar_ll 1.2371 loss -1.2371 (15.289 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 89600 lr 1.323e-05 [train_loss] tar_ll 1.3541 loss -1.3541 (15.340 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 89800 lr 1.273e-05 [train_loss] tar_ll 1.3135 loss -1.3135 (15.627 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 90000 lr 1.224e-05 [train_loss] tar_ll 1.2459 loss -1.2459 (15.529 secs)\n",
      "100%|##########| 3000/3000 [01:12<00:00, 41.17it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -2.7361 loss 2.7361 (72.864 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 90200 lr 1.176e-05 [train_loss] tar_ll 1.2850 loss -1.2850 (15.276 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 90400 lr 1.128e-05 [train_loss] tar_ll 1.2981 loss -1.2981 (15.330 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 90600 lr 1.082e-05 [train_loss] tar_ll 1.3218 loss -1.3218 (15.231 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 90800 lr 1.037e-05 [train_loss] tar_ll 1.2427 loss -1.2427 (15.354 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 91000 lr 9.927e-06 [train_loss] tar_ll 1.3798 loss -1.3798 (15.406 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2737 loss -1.2737 (15.409 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2014 loss -1.2014 (15.220 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 91600 lr 8.655e-06 [train_loss] tar_ll 1.3150 loss -1.3150 (15.047 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2886 loss -1.2886 (14.135 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 92000 lr 7.854e-06 [train_loss] tar_ll 1.3080 loss -1.3080 (14.740 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 92200 lr 7.468e-06 [train_loss] tar_ll 1.3085 loss -1.3085 (15.486 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 92400 lr 7.092e-06 [train_loss] tar_ll 1.2652 loss -1.2652 (15.053 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 92600 lr 6.725e-06 [train_loss] tar_ll 1.2117 loss -1.2117 (14.245 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 92800 lr 6.368e-06 [train_loss] tar_ll 1.3439 loss -1.3439 (13.888 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 93000 lr 6.021e-06 [train_loss] tar_ll 1.3077 loss -1.3077 (15.091 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 93200 lr 5.683e-06 [train_loss] tar_ll 1.3037 loss -1.3037 (15.087 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 93400 lr 5.355e-06 [train_loss] tar_ll 1.2984 loss -1.2984 (15.042 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 93600 lr 5.036e-06 [train_loss] tar_ll 1.2373 loss -1.2373 (14.636 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 93800 lr 4.727e-06 [train_loss] tar_ll 1.1926 loss -1.1926 (14.707 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 94000 lr 4.428e-06 [train_loss] tar_ll 1.2936 loss -1.2936 (15.657 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 94200 lr 4.139e-06 [train_loss] tar_ll 1.2176 loss -1.2176 (15.869 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 94400 lr 3.859e-06 [train_loss] tar_ll 1.2729 loss -1.2729 (15.878 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 94600 lr 3.589e-06 [train_loss] tar_ll 1.3451 loss -1.3451 (15.715 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 94800 lr 3.329e-06 [train_loss] tar_ll 1.2888 loss -1.2888 (15.996 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 95000 lr 3.078e-06 [train_loss] tar_ll 1.2766 loss -1.2766 (15.960 secs)\n",
      "100%|##########| 3000/3000 [01:13<00:00, 40.65it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -2.8078 loss 2.8078 (73.800 secs)\n",
      "\n",
      "isanp:isanp-num_latents-8_periodic step 95200 lr 2.837e-06 [train_loss] tar_ll 1.3504 loss -1.3504 (15.100 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 95400 lr 2.606e-06 [train_loss] tar_ll 1.2852 loss -1.2852 (15.042 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 95600 lr 2.385e-06 [train_loss] tar_ll 1.2857 loss -1.2857 (15.058 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 95800 lr 2.173e-06 [train_loss] tar_ll 1.1883 loss -1.1883 (15.038 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 96000 lr 1.971e-06 [train_loss] tar_ll 1.3749 loss -1.3749 (15.221 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 96200 lr 1.779e-06 [train_loss] tar_ll 1.3084 loss -1.3084 (15.508 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 96400 lr 1.597e-06 [train_loss] tar_ll 1.2606 loss -1.2606 (15.229 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 96600 lr 1.425e-06 [train_loss] tar_ll 1.1978 loss -1.1978 (15.276 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 96800 lr 1.262e-06 [train_loss] tar_ll 1.3161 loss -1.3161 (15.448 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 97000 lr 1.110e-06 [train_loss] tar_ll 1.3316 loss -1.3316 (15.251 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 97200 lr 9.666e-07 [train_loss] tar_ll 1.2316 loss -1.2316 (15.366 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 97400 lr 8.335e-07 [train_loss] tar_ll 1.3010 loss -1.3010 (15.323 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 97600 lr 7.103e-07 [train_loss] tar_ll 1.2543 loss -1.2543 (15.444 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 97800 lr 5.969e-07 [train_loss] tar_ll 1.2723 loss -1.2723 (15.057 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 98000 lr 4.933e-07 [train_loss] tar_ll 1.3363 loss -1.3363 (15.646 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 98200 lr 3.996e-07 [train_loss] tar_ll 1.3308 loss -1.3308 (15.930 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 98400 lr 3.158e-07 [train_loss] tar_ll 1.3771 loss -1.3771 (15.290 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 98600 lr 2.418e-07 [train_loss] tar_ll 1.2717 loss -1.2717 (15.233 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2906 loss -1.2906 (14.815 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 99000 lr 1.234e-07 [train_loss] tar_ll 1.3466 loss -1.3466 (14.520 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 99200 lr 7.895e-08 [train_loss] tar_ll 1.3160 loss -1.3160 (15.778 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2637 loss -1.2637 (15.847 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 99600 lr 1.974e-08 [train_loss] tar_ll 1.3189 loss -1.3189 (15.887 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 99800 lr 4.935e-09 [train_loss] tar_ll 1.3207 loss -1.3207 (15.929 secs)\n",
      "isanp:isanp-num_latents-8_periodic step 100000 lr 0.000e+00 [train_loss] tar_ll 1.2571 loss -1.2571 (15.728 secs)\n",
      "100%|##########| 3000/3000 [01:14<00:00, 40.35it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -2.8531 loss 2.8531 (74.347 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [01:12<00:00, 41.16it/s]\n",
      "isanp:isanp-num_latents-8_periodic periodic tar_ll -2.8531 loss 2.8531 (72.909 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 8799162.0 miliseconds\n",
      "Execution time: 8799.162 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 55.09033203125 MB\n",
      "Memory Usage Change: 38.84033203125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='isanp-num_latents-8_periodic',val_seed=100, val_l=8,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443339bb-913e-4d2c-9ce1-04d3de2dea7c",
   "metadata": {},
   "source": [
    "## ISANP (128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "049635d3-af38-4f9b-9080-a4cee02d9f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: isanp-lbanp-num_latents-128_periodic\n",
      "Total number of parameters: 793282\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp:lbanp-num_latents-128_periodic step 200 lr 5.000e-04 [train_loss] tar_ll -0.7012 loss 0.7012 (15.374 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 400 lr 5.000e-04 [train_loss] tar_ll -0.6345 loss 0.6345 (15.554 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 600 lr 5.000e-04 [train_loss] tar_ll -0.5108 loss 0.5108 (15.420 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 800 lr 4.999e-04 [train_loss] tar_ll -0.4223 loss 0.4223 (15.475 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 1000 lr 4.999e-04 [train_loss] tar_ll -0.3418 loss 0.3418 (15.410 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 1200 lr 4.998e-04 [train_loss] tar_ll -0.2821 loss 0.2821 (15.390 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 1400 lr 4.998e-04 [train_loss] tar_ll -0.1880 loss 0.1880 (15.500 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 1600 lr 4.997e-04 [train_loss] tar_ll -0.0906 loss 0.0906 (15.345 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 1800 lr 4.996e-04 [train_loss] tar_ll 0.0105 loss -0.0105 (15.049 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 2000 lr 4.995e-04 [train_loss] tar_ll -0.0308 loss 0.0308 (15.230 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 2200 lr 4.994e-04 [train_loss] tar_ll 0.0989 loss -0.0989 (15.536 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 2400 lr 4.993e-04 [train_loss] tar_ll 0.1723 loss -0.1723 (15.193 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 2600 lr 4.992e-04 [train_loss] tar_ll 0.1832 loss -0.1832 (15.439 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 2800 lr 4.990e-04 [train_loss] tar_ll 0.2199 loss -0.2199 (15.120 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 3000 lr 4.989e-04 [train_loss] tar_ll 0.3250 loss -0.3250 (14.978 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 3200 lr 4.987e-04 [train_loss] tar_ll 0.2521 loss -0.2521 (14.740 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 3400 lr 4.986e-04 [train_loss] tar_ll 0.2671 loss -0.2671 (15.190 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 3600 lr 4.984e-04 [train_loss] tar_ll 0.1760 loss -0.1760 (15.851 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 3800 lr 4.982e-04 [train_loss] tar_ll -0.0003 loss 0.0003 (15.370 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 4000 lr 4.980e-04 [train_loss] tar_ll 0.2274 loss -0.2274 (15.416 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3127 loss -0.3127 (15.617 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 4400 lr 4.976e-04 [train_loss] tar_ll 0.3273 loss -0.3273 (15.753 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4079 loss -0.4079 (15.615 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4900 loss -0.4900 (15.766 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 5000 lr 4.969e-04 [train_loss] tar_ll 0.3929 loss -0.3929 (15.523 secs)\n",
      "100%|##########| 3000/3000 [01:13<00:00, 41.03it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -1.6346 loss 1.6346 (73.130 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 5200 lr 4.967e-04 [train_loss] tar_ll 0.4358 loss -0.4358 (15.444 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 5400 lr 4.964e-04 [train_loss] tar_ll 0.4098 loss -0.4098 (15.342 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4693 loss -0.4693 (15.136 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 5800 lr 4.959e-04 [train_loss] tar_ll 0.5018 loss -0.5018 (15.336 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 6000 lr 4.956e-04 [train_loss] tar_ll 0.5081 loss -0.5081 (15.393 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 6200 lr 4.953e-04 [train_loss] tar_ll 0.4833 loss -0.4833 (15.627 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 6400 lr 4.950e-04 [train_loss] tar_ll 0.3916 loss -0.3916 (15.251 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 6600 lr 4.946e-04 [train_loss] tar_ll 0.3734 loss -0.3734 (15.184 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 6800 lr 4.943e-04 [train_loss] tar_ll 0.3997 loss -0.3997 (15.021 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 7000 lr 4.940e-04 [train_loss] tar_ll 0.4827 loss -0.4827 (15.218 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 7200 lr 4.936e-04 [train_loss] tar_ll 0.4022 loss -0.4022 (15.260 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 7400 lr 4.933e-04 [train_loss] tar_ll 0.3621 loss -0.3621 (15.530 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 7600 lr 4.929e-04 [train_loss] tar_ll 0.4890 loss -0.4890 (14.911 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 7800 lr 4.925e-04 [train_loss] tar_ll 0.5363 loss -0.5363 (15.233 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 8000 lr 4.921e-04 [train_loss] tar_ll 0.5385 loss -0.5385 (15.111 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 8200 lr 4.918e-04 [train_loss] tar_ll 0.4882 loss -0.4882 (15.311 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 8400 lr 4.913e-04 [train_loss] tar_ll 0.3120 loss -0.3120 (15.020 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 8600 lr 4.909e-04 [train_loss] tar_ll 0.2319 loss -0.2319 (14.936 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 8800 lr 4.905e-04 [train_loss] tar_ll 0.3496 loss -0.3496 (15.636 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 9000 lr 4.901e-04 [train_loss] tar_ll 0.5757 loss -0.5757 (15.524 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 9200 lr 4.896e-04 [train_loss] tar_ll 0.6158 loss -0.6158 (16.160 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 9400 lr 4.892e-04 [train_loss] tar_ll 0.4490 loss -0.4490 (16.065 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 9600 lr 4.887e-04 [train_loss] tar_ll 0.5328 loss -0.5328 (15.600 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 9800 lr 4.882e-04 [train_loss] tar_ll 0.5783 loss -0.5783 (15.778 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 10000 lr 4.878e-04 [train_loss] tar_ll 0.5907 loss -0.5907 (15.778 secs)\n",
      "100%|##########| 3000/3000 [01:13<00:00, 40.67it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -1.3228 loss 1.3228 (73.764 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 10200 lr 4.873e-04 [train_loss] tar_ll 0.6073 loss -0.6073 (15.135 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 10400 lr 4.868e-04 [train_loss] tar_ll 0.5366 loss -0.5366 (15.281 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 10600 lr 4.863e-04 [train_loss] tar_ll 0.5737 loss -0.5737 (15.227 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 10800 lr 4.857e-04 [train_loss] tar_ll 0.6164 loss -0.6164 (15.293 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 11000 lr 4.852e-04 [train_loss] tar_ll 0.5996 loss -0.5996 (15.479 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 11200 lr 4.847e-04 [train_loss] tar_ll 0.6521 loss -0.6521 (15.650 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 11400 lr 4.841e-04 [train_loss] tar_ll 0.6451 loss -0.6451 (15.869 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 11600 lr 4.836e-04 [train_loss] tar_ll 0.6424 loss -0.6424 (15.285 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 11800 lr 4.830e-04 [train_loss] tar_ll 0.4895 loss -0.4895 (15.311 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 12000 lr 4.824e-04 [train_loss] tar_ll 0.4816 loss -0.4816 (15.209 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 12200 lr 4.819e-04 [train_loss] tar_ll 0.6113 loss -0.6113 (15.323 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 12400 lr 4.813e-04 [train_loss] tar_ll 0.5865 loss -0.5865 (15.202 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 12600 lr 4.807e-04 [train_loss] tar_ll 0.6718 loss -0.6718 (15.257 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 12800 lr 4.801e-04 [train_loss] tar_ll 0.5828 loss -0.5828 (15.277 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 13000 lr 4.794e-04 [train_loss] tar_ll 0.5168 loss -0.5168 (15.234 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 13200 lr 4.788e-04 [train_loss] tar_ll 0.6881 loss -0.6881 (15.047 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 13400 lr 4.782e-04 [train_loss] tar_ll 0.6230 loss -0.6230 (14.894 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 13600 lr 4.775e-04 [train_loss] tar_ll 0.6801 loss -0.6801 (14.652 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 13800 lr 4.769e-04 [train_loss] tar_ll 0.6389 loss -0.6389 (15.004 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 14000 lr 4.762e-04 [train_loss] tar_ll 0.6652 loss -0.6652 (15.571 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 14200 lr 4.755e-04 [train_loss] tar_ll 0.6091 loss -0.6091 (16.068 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 14400 lr 4.749e-04 [train_loss] tar_ll 0.5634 loss -0.5634 (15.596 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 14600 lr 4.742e-04 [train_loss] tar_ll 0.6109 loss -0.6109 (15.554 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 14800 lr 4.735e-04 [train_loss] tar_ll 0.6295 loss -0.6295 (15.583 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 15000 lr 4.728e-04 [train_loss] tar_ll 0.6706 loss -0.6706 (15.483 secs)\n",
      "100%|##########| 3000/3000 [01:09<00:00, 43.21it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -1.3800 loss 1.3800 (69.436 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 15200 lr 4.720e-04 [train_loss] tar_ll 0.5766 loss -0.5766 (14.170 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 15400 lr 4.713e-04 [train_loss] tar_ll 0.5609 loss -0.5609 (14.313 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 15600 lr 4.706e-04 [train_loss] tar_ll 0.4733 loss -0.4733 (14.241 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 15800 lr 4.698e-04 [train_loss] tar_ll 0.6653 loss -0.6653 (14.211 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 16000 lr 4.691e-04 [train_loss] tar_ll 0.5791 loss -0.5791 (14.426 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 16200 lr 4.683e-04 [train_loss] tar_ll 0.6966 loss -0.6966 (14.572 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 16400 lr 4.675e-04 [train_loss] tar_ll 0.6644 loss -0.6644 (14.322 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 16600 lr 4.668e-04 [train_loss] tar_ll 0.5940 loss -0.5940 (14.261 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 16800 lr 4.660e-04 [train_loss] tar_ll 0.6804 loss -0.6804 (14.442 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 17000 lr 4.652e-04 [train_loss] tar_ll 0.6301 loss -0.6301 (14.346 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 17200 lr 4.644e-04 [train_loss] tar_ll 0.6439 loss -0.6439 (14.417 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 17400 lr 4.636e-04 [train_loss] tar_ll 0.6913 loss -0.6913 (14.374 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 17600 lr 4.627e-04 [train_loss] tar_ll 0.6239 loss -0.6239 (14.193 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 17800 lr 4.619e-04 [train_loss] tar_ll 0.6813 loss -0.6813 (14.372 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 18000 lr 4.611e-04 [train_loss] tar_ll 0.6206 loss -0.6206 (13.743 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 18200 lr 4.602e-04 [train_loss] tar_ll 0.5677 loss -0.5677 (14.063 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 18400 lr 4.594e-04 [train_loss] tar_ll 0.5212 loss -0.5212 (14.005 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 18600 lr 4.585e-04 [train_loss] tar_ll 0.6719 loss -0.6719 (13.854 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 18800 lr 4.576e-04 [train_loss] tar_ll 0.5104 loss -0.5104 (13.917 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 19000 lr 4.568e-04 [train_loss] tar_ll 0.6309 loss -0.6309 (13.906 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 19200 lr 4.559e-04 [train_loss] tar_ll 0.6703 loss -0.6703 (14.007 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 19400 lr 4.550e-04 [train_loss] tar_ll 0.5745 loss -0.5745 (14.162 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 19600 lr 4.541e-04 [train_loss] tar_ll 0.5060 loss -0.5060 (13.996 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 19800 lr 4.532e-04 [train_loss] tar_ll 0.6261 loss -0.6261 (14.363 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 20000 lr 4.523e-04 [train_loss] tar_ll 0.3506 loss -0.3506 (14.197 secs)\n",
      "100%|##########| 3000/3000 [01:08<00:00, 43.54it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -1.8117 loss 1.8117 (68.911 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 20200 lr 4.513e-04 [train_loss] tar_ll 0.6489 loss -0.6489 (14.634 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 20400 lr 4.504e-04 [train_loss] tar_ll 0.7372 loss -0.7372 (14.066 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 20600 lr 4.494e-04 [train_loss] tar_ll 0.7440 loss -0.7440 (14.230 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 20800 lr 4.485e-04 [train_loss] tar_ll 0.7478 loss -0.7478 (14.041 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 21000 lr 4.475e-04 [train_loss] tar_ll 0.7566 loss -0.7566 (14.290 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 21200 lr 4.466e-04 [train_loss] tar_ll 0.7986 loss -0.7986 (14.268 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 21400 lr 4.456e-04 [train_loss] tar_ll 0.7840 loss -0.7840 (14.201 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 21600 lr 4.446e-04 [train_loss] tar_ll 0.7658 loss -0.7658 (13.888 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 21800 lr 4.436e-04 [train_loss] tar_ll 0.7367 loss -0.7367 (14.101 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 22000 lr 4.426e-04 [train_loss] tar_ll 0.7825 loss -0.7825 (14.491 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 22200 lr 4.416e-04 [train_loss] tar_ll 0.7789 loss -0.7789 (14.390 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 22400 lr 4.406e-04 [train_loss] tar_ll 0.8529 loss -0.8529 (14.438 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 22600 lr 4.396e-04 [train_loss] tar_ll 0.8654 loss -0.8654 (14.374 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 22800 lr 4.386e-04 [train_loss] tar_ll 0.7321 loss -0.7321 (14.384 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 23000 lr 4.375e-04 [train_loss] tar_ll 0.7450 loss -0.7450 (14.370 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 23200 lr 4.365e-04 [train_loss] tar_ll 0.7218 loss -0.7218 (14.333 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 23400 lr 4.354e-04 [train_loss] tar_ll 0.6493 loss -0.6493 (14.220 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 23600 lr 4.344e-04 [train_loss] tar_ll 0.7017 loss -0.7017 (14.148 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 23800 lr 4.333e-04 [train_loss] tar_ll 0.7321 loss -0.7321 (14.006 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 24000 lr 4.322e-04 [train_loss] tar_ll 0.8486 loss -0.8486 (14.258 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 24200 lr 4.312e-04 [train_loss] tar_ll 0.8188 loss -0.8188 (14.296 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 24400 lr 4.301e-04 [train_loss] tar_ll 0.9117 loss -0.9117 (14.049 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 24600 lr 4.290e-04 [train_loss] tar_ll 0.7860 loss -0.7860 (14.233 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 24800 lr 4.279e-04 [train_loss] tar_ll 0.7212 loss -0.7212 (14.102 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 25000 lr 4.268e-04 [train_loss] tar_ll 0.9121 loss -0.9121 (13.725 secs)\n",
      "100%|##########| 3000/3000 [01:08<00:00, 43.85it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -1.4736 loss 1.4736 (68.415 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 25200 lr 4.257e-04 [train_loss] tar_ll 0.8207 loss -0.8207 (14.348 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 25400 lr 4.245e-04 [train_loss] tar_ll 0.7637 loss -0.7637 (14.291 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 25600 lr 4.234e-04 [train_loss] tar_ll 0.6926 loss -0.6926 (14.099 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 25800 lr 4.223e-04 [train_loss] tar_ll 0.7514 loss -0.7514 (14.307 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 26000 lr 4.211e-04 [train_loss] tar_ll 0.7522 loss -0.7522 (14.231 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 26200 lr 4.200e-04 [train_loss] tar_ll 0.8086 loss -0.8086 (14.267 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 26400 lr 4.188e-04 [train_loss] tar_ll 0.8766 loss -0.8766 (13.867 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 26600 lr 4.177e-04 [train_loss] tar_ll 0.7415 loss -0.7415 (14.434 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 26800 lr 4.165e-04 [train_loss] tar_ll 0.7365 loss -0.7365 (14.327 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 27000 lr 4.153e-04 [train_loss] tar_ll 0.8238 loss -0.8238 (15.112 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 27200 lr 4.141e-04 [train_loss] tar_ll 0.8505 loss -0.8505 (14.386 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 27400 lr 4.130e-04 [train_loss] tar_ll 0.8486 loss -0.8486 (14.121 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 27600 lr 4.118e-04 [train_loss] tar_ll 0.7492 loss -0.7492 (15.382 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 27800 lr 4.106e-04 [train_loss] tar_ll 0.7875 loss -0.7875 (15.521 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 28000 lr 4.094e-04 [train_loss] tar_ll 0.7407 loss -0.7407 (15.704 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 28200 lr 4.081e-04 [train_loss] tar_ll 0.7232 loss -0.7232 (15.865 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 28400 lr 4.069e-04 [train_loss] tar_ll 0.7378 loss -0.7378 (16.837 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 28600 lr 4.057e-04 [train_loss] tar_ll 0.7984 loss -0.7984 (16.742 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 28800 lr 4.045e-04 [train_loss] tar_ll 0.7364 loss -0.7364 (16.901 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 29000 lr 4.032e-04 [train_loss] tar_ll 0.7772 loss -0.7772 (16.414 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 29200 lr 4.020e-04 [train_loss] tar_ll 0.7604 loss -0.7604 (16.117 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 29400 lr 4.007e-04 [train_loss] tar_ll 0.5708 loss -0.5708 (15.786 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 29600 lr 3.995e-04 [train_loss] tar_ll 0.6790 loss -0.6790 (15.935 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 29800 lr 3.982e-04 [train_loss] tar_ll 0.7205 loss -0.7205 (16.471 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 30000 lr 3.969e-04 [train_loss] tar_ll 0.7473 loss -0.7473 (15.678 secs)\n",
      "100%|##########| 3000/3000 [01:15<00:00, 39.97it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -2.1043 loss 2.1043 (75.053 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 30200 lr 3.957e-04 [train_loss] tar_ll 0.8619 loss -0.8619 (15.532 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 30400 lr 3.944e-04 [train_loss] tar_ll 0.8358 loss -0.8358 (14.432 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 30600 lr 3.931e-04 [train_loss] tar_ll 0.8388 loss -0.8388 (14.964 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 30800 lr 3.918e-04 [train_loss] tar_ll 0.8121 loss -0.8121 (15.056 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 31000 lr 3.905e-04 [train_loss] tar_ll 0.8201 loss -0.8201 (14.728 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 31200 lr 3.892e-04 [train_loss] tar_ll 0.8469 loss -0.8469 (14.660 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 31400 lr 3.879e-04 [train_loss] tar_ll 0.9166 loss -0.9166 (14.335 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 31600 lr 3.866e-04 [train_loss] tar_ll 0.8331 loss -0.8331 (14.387 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 31800 lr 3.853e-04 [train_loss] tar_ll 0.8716 loss -0.8716 (14.262 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 32000 lr 3.840e-04 [train_loss] tar_ll 0.7762 loss -0.7762 (14.538 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 32200 lr 3.826e-04 [train_loss] tar_ll 0.8573 loss -0.8573 (14.555 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 32400 lr 3.813e-04 [train_loss] tar_ll 0.7292 loss -0.7292 (14.792 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 32600 lr 3.800e-04 [train_loss] tar_ll 0.8275 loss -0.8275 (15.827 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 32800 lr 3.786e-04 [train_loss] tar_ll 0.7977 loss -0.7977 (15.550 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 33000 lr 3.773e-04 [train_loss] tar_ll 0.1864 loss -0.1864 (15.927 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 33200 lr 3.759e-04 [train_loss] tar_ll 0.0471 loss -0.0471 (16.292 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 33400 lr 3.745e-04 [train_loss] tar_ll 0.2960 loss -0.2960 (16.046 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 33600 lr 3.732e-04 [train_loss] tar_ll 0.4079 loss -0.4079 (16.616 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 33800 lr 3.718e-04 [train_loss] tar_ll 0.4069 loss -0.4069 (16.392 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 34000 lr 3.704e-04 [train_loss] tar_ll 0.4462 loss -0.4462 (16.337 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 34200 lr 3.691e-04 [train_loss] tar_ll 0.6870 loss -0.6870 (16.408 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 34400 lr 3.677e-04 [train_loss] tar_ll 0.5724 loss -0.5724 (16.581 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 34600 lr 3.663e-04 [train_loss] tar_ll 0.5892 loss -0.5892 (16.321 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 34800 lr 3.649e-04 [train_loss] tar_ll 0.7323 loss -0.7323 (15.862 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 35000 lr 3.635e-04 [train_loss] tar_ll 0.7447 loss -0.7447 (15.846 secs)\n",
      "100%|##########| 3000/3000 [01:18<00:00, 38.33it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -1.9248 loss 1.9248 (78.268 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 35200 lr 3.621e-04 [train_loss] tar_ll 0.7128 loss -0.7128 (15.737 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 35400 lr 3.607e-04 [train_loss] tar_ll 0.8274 loss -0.8274 (14.928 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 35600 lr 3.593e-04 [train_loss] tar_ll 0.8779 loss -0.8779 (15.258 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 35800 lr 3.579e-04 [train_loss] tar_ll 0.8156 loss -0.8156 (15.548 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 36000 lr 3.564e-04 [train_loss] tar_ll 0.7925 loss -0.7925 (15.909 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 36200 lr 3.550e-04 [train_loss] tar_ll 0.8489 loss -0.8489 (16.732 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 36400 lr 3.536e-04 [train_loss] tar_ll 0.8900 loss -0.8900 (17.056 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 36600 lr 3.522e-04 [train_loss] tar_ll 0.8054 loss -0.8054 (16.485 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 36800 lr 3.507e-04 [train_loss] tar_ll 0.7558 loss -0.7558 (16.200 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 37000 lr 3.493e-04 [train_loss] tar_ll 0.7209 loss -0.7209 (16.069 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 37200 lr 3.478e-04 [train_loss] tar_ll 0.8988 loss -0.8988 (15.983 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 37400 lr 3.464e-04 [train_loss] tar_ll 0.9117 loss -0.9117 (15.796 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 37600 lr 3.449e-04 [train_loss] tar_ll 0.8876 loss -0.8876 (16.194 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 37800 lr 3.435e-04 [train_loss] tar_ll 0.9029 loss -0.9029 (15.693 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 38000 lr 3.420e-04 [train_loss] tar_ll 0.8328 loss -0.8328 (16.636 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 38200 lr 3.406e-04 [train_loss] tar_ll 0.9024 loss -0.9024 (16.681 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 38400 lr 3.391e-04 [train_loss] tar_ll 0.8907 loss -0.8907 (17.432 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 38600 lr 3.376e-04 [train_loss] tar_ll 0.8374 loss -0.8374 (16.588 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 38800 lr 3.362e-04 [train_loss] tar_ll 0.9288 loss -0.9288 (16.643 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 39000 lr 3.347e-04 [train_loss] tar_ll 0.9246 loss -0.9246 (16.168 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 39200 lr 3.332e-04 [train_loss] tar_ll 0.9443 loss -0.9443 (16.693 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 39400 lr 3.317e-04 [train_loss] tar_ll 0.9846 loss -0.9846 (16.140 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 39600 lr 3.302e-04 [train_loss] tar_ll 0.9961 loss -0.9961 (16.176 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 39800 lr 3.287e-04 [train_loss] tar_ll 0.9901 loss -0.9901 (15.804 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 40000 lr 3.273e-04 [train_loss] tar_ll 0.9266 loss -0.9266 (16.384 secs)\n",
      "100%|##########| 3000/3000 [01:19<00:00, 37.71it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -2.3806 loss 2.3806 (79.565 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 40200 lr 3.258e-04 [train_loss] tar_ll 0.8347 loss -0.8347 (16.819 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 40400 lr 3.243e-04 [train_loss] tar_ll 0.8748 loss -0.8748 (15.588 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 40600 lr 3.228e-04 [train_loss] tar_ll 0.7505 loss -0.7505 (15.311 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 40800 lr 3.213e-04 [train_loss] tar_ll 0.8781 loss -0.8781 (15.875 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 41000 lr 3.197e-04 [train_loss] tar_ll 0.8341 loss -0.8341 (16.070 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 41200 lr 3.182e-04 [train_loss] tar_ll 0.7956 loss -0.7956 (16.680 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 41400 lr 3.167e-04 [train_loss] tar_ll 0.9913 loss -0.9913 (16.309 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 41600 lr 3.152e-04 [train_loss] tar_ll 0.8554 loss -0.8554 (15.940 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 41800 lr 3.137e-04 [train_loss] tar_ll 0.9219 loss -0.9219 (15.901 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 42000 lr 3.122e-04 [train_loss] tar_ll 0.9350 loss -0.9350 (16.347 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 42200 lr 3.106e-04 [train_loss] tar_ll 0.9728 loss -0.9728 (16.136 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 42400 lr 3.091e-04 [train_loss] tar_ll 0.9924 loss -0.9924 (15.972 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 42600 lr 3.076e-04 [train_loss] tar_ll 0.9819 loss -0.9819 (15.652 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 42800 lr 3.061e-04 [train_loss] tar_ll 0.9399 loss -0.9399 (16.183 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 43000 lr 3.045e-04 [train_loss] tar_ll 0.9734 loss -0.9734 (17.519 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 43200 lr 3.030e-04 [train_loss] tar_ll 1.0548 loss -1.0548 (16.975 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 43400 lr 3.015e-04 [train_loss] tar_ll 0.9777 loss -0.9777 (16.499 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 43600 lr 2.999e-04 [train_loss] tar_ll 0.9665 loss -0.9665 (14.552 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 43800 lr 2.984e-04 [train_loss] tar_ll 0.9342 loss -0.9342 (15.699 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 44000 lr 2.968e-04 [train_loss] tar_ll 0.6755 loss -0.6755 (16.050 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 44200 lr 2.953e-04 [train_loss] tar_ll 0.7665 loss -0.7665 (17.158 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 44400 lr 2.938e-04 [train_loss] tar_ll 0.8961 loss -0.8961 (16.550 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 44600 lr 2.922e-04 [train_loss] tar_ll 0.9150 loss -0.9150 (15.798 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 44800 lr 2.907e-04 [train_loss] tar_ll 0.9019 loss -0.9019 (16.328 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 45000 lr 2.891e-04 [train_loss] tar_ll 1.0489 loss -1.0489 (16.051 secs)\n",
      "100%|##########| 3000/3000 [01:17<00:00, 38.88it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -2.3188 loss 2.3188 (77.167 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 45200 lr 2.876e-04 [train_loss] tar_ll 0.9832 loss -0.9832 (16.517 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 45400 lr 2.860e-04 [train_loss] tar_ll 1.0133 loss -1.0133 (16.166 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 45600 lr 2.844e-04 [train_loss] tar_ll 0.9465 loss -0.9465 (16.696 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 45800 lr 2.829e-04 [train_loss] tar_ll 0.9894 loss -0.9894 (16.731 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 46000 lr 2.813e-04 [train_loss] tar_ll 0.9613 loss -0.9613 (16.569 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 46200 lr 2.798e-04 [train_loss] tar_ll 0.8851 loss -0.8851 (16.399 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 46400 lr 2.782e-04 [train_loss] tar_ll 1.0192 loss -1.0192 (16.625 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 46600 lr 2.767e-04 [train_loss] tar_ll 0.9943 loss -0.9943 (16.230 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 46800 lr 2.751e-04 [train_loss] tar_ll 0.9339 loss -0.9339 (15.893 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 47000 lr 2.735e-04 [train_loss] tar_ll 0.9025 loss -0.9025 (15.791 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 47200 lr 2.720e-04 [train_loss] tar_ll 0.9451 loss -0.9451 (16.153 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 47400 lr 2.704e-04 [train_loss] tar_ll 0.8733 loss -0.8733 (15.658 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 47600 lr 2.688e-04 [train_loss] tar_ll 0.9614 loss -0.9614 (15.805 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 47800 lr 2.673e-04 [train_loss] tar_ll 0.9056 loss -0.9056 (16.403 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 48000 lr 2.657e-04 [train_loss] tar_ll 0.9815 loss -0.9815 (16.783 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 48200 lr 2.641e-04 [train_loss] tar_ll 0.8995 loss -0.8995 (16.830 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 48400 lr 2.626e-04 [train_loss] tar_ll 0.9848 loss -0.9848 (16.420 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 48600 lr 2.610e-04 [train_loss] tar_ll 0.9973 loss -0.9973 (16.213 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 48800 lr 2.594e-04 [train_loss] tar_ll 1.0579 loss -1.0579 (16.133 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 49000 lr 2.579e-04 [train_loss] tar_ll 0.9739 loss -0.9739 (16.331 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 49200 lr 2.563e-04 [train_loss] tar_ll 0.9499 loss -0.9499 (16.710 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 49400 lr 2.547e-04 [train_loss] tar_ll 0.9718 loss -0.9718 (16.229 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 49600 lr 2.531e-04 [train_loss] tar_ll 1.0070 loss -1.0070 (14.356 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 49800 lr 2.516e-04 [train_loss] tar_ll 0.9960 loss -0.9960 (15.043 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 50000 lr 2.500e-04 [train_loss] tar_ll 0.9955 loss -0.9955 (15.911 secs)\n",
      "100%|##########| 3000/3000 [01:18<00:00, 38.31it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -1.8828 loss 1.8828 (78.319 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 50200 lr 2.484e-04 [train_loss] tar_ll 0.9745 loss -0.9745 (16.709 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 50400 lr 2.469e-04 [train_loss] tar_ll 1.0096 loss -1.0096 (15.963 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 50600 lr 2.453e-04 [train_loss] tar_ll 1.0294 loss -1.0294 (14.484 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 50800 lr 2.437e-04 [train_loss] tar_ll 0.9832 loss -0.9832 (14.437 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 51000 lr 2.421e-04 [train_loss] tar_ll 0.9431 loss -0.9431 (15.420 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 51200 lr 2.406e-04 [train_loss] tar_ll 1.0391 loss -1.0391 (14.651 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 51400 lr 2.390e-04 [train_loss] tar_ll 1.1126 loss -1.1126 (13.259 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 51600 lr 2.374e-04 [train_loss] tar_ll 1.0520 loss -1.0520 (15.484 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 51800 lr 2.359e-04 [train_loss] tar_ll 1.0682 loss -1.0682 (14.050 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 52000 lr 2.343e-04 [train_loss] tar_ll 1.0501 loss -1.0501 (13.440 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 52200 lr 2.327e-04 [train_loss] tar_ll 1.0670 loss -1.0670 (14.022 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 52400 lr 2.312e-04 [train_loss] tar_ll 1.1115 loss -1.1115 (14.031 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 52600 lr 2.296e-04 [train_loss] tar_ll 0.9972 loss -0.9972 (13.174 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 52800 lr 2.280e-04 [train_loss] tar_ll 1.0266 loss -1.0266 (12.850 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 53000 lr 2.265e-04 [train_loss] tar_ll 1.0347 loss -1.0347 (13.380 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 53200 lr 2.249e-04 [train_loss] tar_ll 0.9556 loss -0.9556 (15.056 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 53400 lr 2.233e-04 [train_loss] tar_ll 0.9986 loss -0.9986 (15.375 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 53600 lr 2.218e-04 [train_loss] tar_ll 1.0374 loss -1.0374 (16.563 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 53800 lr 2.202e-04 [train_loss] tar_ll 1.1293 loss -1.1293 (15.516 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 54000 lr 2.187e-04 [train_loss] tar_ll 1.0134 loss -1.0134 (15.599 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 54200 lr 2.171e-04 [train_loss] tar_ll 1.1044 loss -1.1044 (16.117 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 54400 lr 2.156e-04 [train_loss] tar_ll 1.1524 loss -1.1524 (16.396 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 54600 lr 2.140e-04 [train_loss] tar_ll 1.1253 loss -1.1253 (15.204 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 54800 lr 2.124e-04 [train_loss] tar_ll 0.9739 loss -0.9739 (14.190 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 55000 lr 2.109e-04 [train_loss] tar_ll 0.9288 loss -0.9288 (14.624 secs)\n",
      "100%|##########| 3000/3000 [01:16<00:00, 39.13it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -2.3722 loss 2.3722 (76.677 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 55200 lr 2.093e-04 [train_loss] tar_ll 1.0146 loss -1.0146 (16.924 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 55400 lr 2.078e-04 [train_loss] tar_ll 1.0346 loss -1.0346 (14.242 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 55600 lr 2.062e-04 [train_loss] tar_ll 1.0498 loss -1.0498 (13.985 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 55800 lr 2.047e-04 [train_loss] tar_ll 0.8627 loss -0.8627 (14.077 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 56000 lr 2.032e-04 [train_loss] tar_ll 1.0646 loss -1.0646 (14.849 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 56200 lr 2.016e-04 [train_loss] tar_ll 1.0493 loss -1.0493 (13.778 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 56400 lr 2.001e-04 [train_loss] tar_ll 1.0787 loss -1.0787 (14.017 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 56600 lr 1.985e-04 [train_loss] tar_ll 1.0471 loss -1.0471 (14.410 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0143 loss -1.0143 (14.149 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 57000 lr 1.955e-04 [train_loss] tar_ll 1.1009 loss -1.1009 (13.508 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 57200 lr 1.939e-04 [train_loss] tar_ll 1.0464 loss -1.0464 (13.784 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 57400 lr 1.924e-04 [train_loss] tar_ll 0.9951 loss -0.9951 (14.869 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 57600 lr 1.909e-04 [train_loss] tar_ll 1.0292 loss -1.0292 (13.815 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 57800 lr 1.894e-04 [train_loss] tar_ll 1.1001 loss -1.1001 (13.727 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 58000 lr 1.878e-04 [train_loss] tar_ll 1.0689 loss -1.0689 (13.478 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 58200 lr 1.863e-04 [train_loss] tar_ll 1.1070 loss -1.1070 (14.258 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 58400 lr 1.848e-04 [train_loss] tar_ll 1.1688 loss -1.1688 (13.879 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 58600 lr 1.833e-04 [train_loss] tar_ll 1.0986 loss -1.0986 (13.773 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 58800 lr 1.818e-04 [train_loss] tar_ll 1.1667 loss -1.1667 (13.886 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 59000 lr 1.803e-04 [train_loss] tar_ll 1.0667 loss -1.0667 (14.061 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 59200 lr 1.787e-04 [train_loss] tar_ll 1.0650 loss -1.0650 (14.158 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 59400 lr 1.772e-04 [train_loss] tar_ll 1.0770 loss -1.0770 (13.974 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 59600 lr 1.757e-04 [train_loss] tar_ll 1.1416 loss -1.1416 (13.842 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 59800 lr 1.742e-04 [train_loss] tar_ll 0.9990 loss -0.9990 (13.896 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 60000 lr 1.727e-04 [train_loss] tar_ll 1.1473 loss -1.1473 (14.788 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.18it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -2.2139 loss 2.2139 (66.411 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 60200 lr 1.713e-04 [train_loss] tar_ll 1.1255 loss -1.1255 (13.499 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 60400 lr 1.698e-04 [train_loss] tar_ll 1.2053 loss -1.2053 (13.347 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 60600 lr 1.683e-04 [train_loss] tar_ll 1.0482 loss -1.0482 (13.854 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 60800 lr 1.668e-04 [train_loss] tar_ll 1.1878 loss -1.1878 (13.786 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 61000 lr 1.653e-04 [train_loss] tar_ll 1.1575 loss -1.1575 (13.894 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 61200 lr 1.638e-04 [train_loss] tar_ll 1.1537 loss -1.1537 (14.009 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 61400 lr 1.624e-04 [train_loss] tar_ll 1.1630 loss -1.1630 (14.190 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 61600 lr 1.609e-04 [train_loss] tar_ll 1.1345 loss -1.1345 (14.054 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 61800 lr 1.594e-04 [train_loss] tar_ll 1.1079 loss -1.1079 (14.409 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 62000 lr 1.580e-04 [train_loss] tar_ll 1.0867 loss -1.0867 (14.162 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 62200 lr 1.565e-04 [train_loss] tar_ll 1.0259 loss -1.0259 (14.280 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 62400 lr 1.551e-04 [train_loss] tar_ll 1.0698 loss -1.0698 (14.660 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 62600 lr 1.536e-04 [train_loss] tar_ll 1.1266 loss -1.1266 (13.788 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 62800 lr 1.522e-04 [train_loss] tar_ll 1.1882 loss -1.1882 (13.670 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 63000 lr 1.507e-04 [train_loss] tar_ll 1.0596 loss -1.0596 (13.791 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 63200 lr 1.493e-04 [train_loss] tar_ll 1.2066 loss -1.2066 (14.202 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 63400 lr 1.478e-04 [train_loss] tar_ll 1.1860 loss -1.1860 (13.691 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 63600 lr 1.464e-04 [train_loss] tar_ll 1.1263 loss -1.1263 (13.798 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 63800 lr 1.450e-04 [train_loss] tar_ll 1.0910 loss -1.0910 (13.584 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 64000 lr 1.436e-04 [train_loss] tar_ll 1.0725 loss -1.0725 (13.626 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 64200 lr 1.421e-04 [train_loss] tar_ll 1.1653 loss -1.1653 (14.796 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 64400 lr 1.407e-04 [train_loss] tar_ll 1.2546 loss -1.2546 (14.069 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 64600 lr 1.393e-04 [train_loss] tar_ll 1.1011 loss -1.1011 (13.895 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 64800 lr 1.379e-04 [train_loss] tar_ll 1.1736 loss -1.1736 (14.040 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 65000 lr 1.365e-04 [train_loss] tar_ll 1.1245 loss -1.1245 (14.299 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.27it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -3.4303 loss 3.4303 (67.777 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 65200 lr 1.351e-04 [train_loss] tar_ll 1.1971 loss -1.1971 (13.893 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 65400 lr 1.337e-04 [train_loss] tar_ll 1.1006 loss -1.1006 (14.185 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 65600 lr 1.323e-04 [train_loss] tar_ll 1.1747 loss -1.1747 (15.405 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 65800 lr 1.309e-04 [train_loss] tar_ll 1.1022 loss -1.1022 (14.701 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 66000 lr 1.296e-04 [train_loss] tar_ll 1.2153 loss -1.2153 (13.999 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 66200 lr 1.282e-04 [train_loss] tar_ll 1.1685 loss -1.1685 (13.854 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 66400 lr 1.268e-04 [train_loss] tar_ll 1.1775 loss -1.1775 (13.451 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 66600 lr 1.255e-04 [train_loss] tar_ll 1.1432 loss -1.1432 (13.243 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 66800 lr 1.241e-04 [train_loss] tar_ll 1.1678 loss -1.1678 (13.394 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 67000 lr 1.227e-04 [train_loss] tar_ll 1.1802 loss -1.1802 (13.908 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 67200 lr 1.214e-04 [train_loss] tar_ll 1.1367 loss -1.1367 (14.338 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 67400 lr 1.200e-04 [train_loss] tar_ll 1.1876 loss -1.1876 (13.670 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 67600 lr 1.187e-04 [train_loss] tar_ll 1.1615 loss -1.1615 (12.743 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 67800 lr 1.174e-04 [train_loss] tar_ll 1.1915 loss -1.1915 (12.675 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 68000 lr 1.160e-04 [train_loss] tar_ll 1.1747 loss -1.1747 (14.831 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 68200 lr 1.147e-04 [train_loss] tar_ll 1.1334 loss -1.1334 (14.253 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 68400 lr 1.134e-04 [train_loss] tar_ll 1.1157 loss -1.1157 (14.665 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 68600 lr 1.121e-04 [train_loss] tar_ll 1.2888 loss -1.2888 (13.588 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 68800 lr 1.108e-04 [train_loss] tar_ll 1.2014 loss -1.2014 (13.046 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 69000 lr 1.095e-04 [train_loss] tar_ll 1.1644 loss -1.1644 (12.975 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 69200 lr 1.082e-04 [train_loss] tar_ll 1.2491 loss -1.2491 (13.042 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 69400 lr 1.069e-04 [train_loss] tar_ll 1.0821 loss -1.0821 (12.928 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 69600 lr 1.056e-04 [train_loss] tar_ll 1.1336 loss -1.1336 (12.829 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 69800 lr 1.043e-04 [train_loss] tar_ll 1.1251 loss -1.1251 (13.318 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 70000 lr 1.031e-04 [train_loss] tar_ll 1.1423 loss -1.1423 (12.813 secs)\n",
      "100%|##########| 3000/3000 [01:11<00:00, 41.68it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -4.0198 loss 4.0198 (71.982 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 70200 lr 1.018e-04 [train_loss] tar_ll 1.2127 loss -1.2127 (14.918 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 70400 lr 1.005e-04 [train_loss] tar_ll 1.0942 loss -1.0942 (14.834 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 70600 lr 9.927e-05 [train_loss] tar_ll 1.1672 loss -1.1672 (13.547 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 70800 lr 9.802e-05 [train_loss] tar_ll 1.2880 loss -1.2880 (12.809 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 71000 lr 9.677e-05 [train_loss] tar_ll 1.1918 loss -1.1918 (13.173 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 71200 lr 9.554e-05 [train_loss] tar_ll 1.2063 loss -1.2063 (14.402 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 71400 lr 9.430e-05 [train_loss] tar_ll 1.2787 loss -1.2787 (15.486 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 71600 lr 9.308e-05 [train_loss] tar_ll 1.1766 loss -1.1766 (15.626 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 71800 lr 9.186e-05 [train_loss] tar_ll 1.1695 loss -1.1695 (15.367 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 72000 lr 9.064e-05 [train_loss] tar_ll 1.1746 loss -1.1746 (15.131 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 72200 lr 8.944e-05 [train_loss] tar_ll 1.3185 loss -1.3185 (15.720 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 72400 lr 8.824e-05 [train_loss] tar_ll 1.1501 loss -1.1501 (15.281 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 72600 lr 8.704e-05 [train_loss] tar_ll 1.2682 loss -1.2682 (14.914 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 72800 lr 8.585e-05 [train_loss] tar_ll 1.2730 loss -1.2730 (13.707 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 73000 lr 8.467e-05 [train_loss] tar_ll 1.1942 loss -1.1942 (13.794 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 73200 lr 8.350e-05 [train_loss] tar_ll 1.1739 loss -1.1739 (14.309 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 73400 lr 8.233e-05 [train_loss] tar_ll 1.3200 loss -1.3200 (13.898 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 73600 lr 8.117e-05 [train_loss] tar_ll 1.2757 loss -1.2757 (13.881 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 73800 lr 8.001e-05 [train_loss] tar_ll 1.2102 loss -1.2102 (13.840 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 74000 lr 7.886e-05 [train_loss] tar_ll 1.2247 loss -1.2247 (13.772 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 74200 lr 7.772e-05 [train_loss] tar_ll 1.2077 loss -1.2077 (13.924 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 74400 lr 7.659e-05 [train_loss] tar_ll 1.1949 loss -1.1949 (13.663 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 74600 lr 7.546e-05 [train_loss] tar_ll 1.2383 loss -1.2383 (13.188 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 74800 lr 7.434e-05 [train_loss] tar_ll 1.3248 loss -1.3248 (13.306 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1477 loss -1.1477 (12.966 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.50it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -3.2280 loss 3.2280 (67.414 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 75200 lr 7.212e-05 [train_loss] tar_ll 1.2515 loss -1.2515 (14.690 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 75400 lr 7.102e-05 [train_loss] tar_ll 1.2091 loss -1.2091 (14.883 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 75600 lr 6.992e-05 [train_loss] tar_ll 1.1850 loss -1.1850 (12.985 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 75800 lr 6.884e-05 [train_loss] tar_ll 1.2935 loss -1.2935 (12.303 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 76000 lr 6.776e-05 [train_loss] tar_ll 1.2357 loss -1.2357 (12.336 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 76200 lr 6.669e-05 [train_loss] tar_ll 1.2331 loss -1.2331 (12.686 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 76400 lr 6.562e-05 [train_loss] tar_ll 1.1633 loss -1.1633 (12.865 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 76600 lr 6.456e-05 [train_loss] tar_ll 1.2539 loss -1.2539 (16.277 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 76800 lr 6.351e-05 [train_loss] tar_ll 1.1423 loss -1.1423 (16.292 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 77000 lr 6.247e-05 [train_loss] tar_ll 1.2472 loss -1.2472 (14.193 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 77200 lr 6.144e-05 [train_loss] tar_ll 1.1635 loss -1.1635 (13.970 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 77400 lr 6.041e-05 [train_loss] tar_ll 1.2604 loss -1.2604 (14.832 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 77600 lr 5.939e-05 [train_loss] tar_ll 1.2173 loss -1.2173 (15.003 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2161 loss -1.2161 (14.086 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 78000 lr 5.737e-05 [train_loss] tar_ll 1.2886 loss -1.2886 (14.442 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 78200 lr 5.637e-05 [train_loss] tar_ll 1.3586 loss -1.3586 (16.167 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 78400 lr 5.538e-05 [train_loss] tar_ll 1.2724 loss -1.2724 (16.111 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 78600 lr 5.440e-05 [train_loss] tar_ll 1.2899 loss -1.2899 (14.031 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 78800 lr 5.343e-05 [train_loss] tar_ll 1.2639 loss -1.2639 (14.280 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 79000 lr 5.246e-05 [train_loss] tar_ll 1.3216 loss -1.3216 (14.322 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 79200 lr 5.150e-05 [train_loss] tar_ll 1.3007 loss -1.3007 (14.490 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 79400 lr 5.055e-05 [train_loss] tar_ll 1.3039 loss -1.3039 (14.100 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 79600 lr 4.961e-05 [train_loss] tar_ll 1.3093 loss -1.3093 (14.289 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 79800 lr 4.867e-05 [train_loss] tar_ll 1.2015 loss -1.2015 (14.247 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 80000 lr 4.775e-05 [train_loss] tar_ll 1.3153 loss -1.3153 (13.990 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 44.80it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -3.3307 loss 3.3307 (66.960 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 80200 lr 4.683e-05 [train_loss] tar_ll 1.3157 loss -1.3157 (12.525 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 80400 lr 4.592e-05 [train_loss] tar_ll 1.2111 loss -1.2111 (12.568 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 80600 lr 4.501e-05 [train_loss] tar_ll 1.2131 loss -1.2131 (12.571 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2164 loss -1.2164 (12.579 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 81000 lr 4.323e-05 [train_loss] tar_ll 1.2461 loss -1.2461 (14.589 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 81200 lr 4.235e-05 [train_loss] tar_ll 1.2605 loss -1.2605 (15.550 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 81400 lr 4.148e-05 [train_loss] tar_ll 1.2963 loss -1.2963 (15.318 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 81600 lr 4.062e-05 [train_loss] tar_ll 1.2280 loss -1.2280 (15.546 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2930 loss -1.2930 (15.885 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 82000 lr 3.892e-05 [train_loss] tar_ll 1.2643 loss -1.2643 (14.840 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 82200 lr 3.808e-05 [train_loss] tar_ll 1.2585 loss -1.2585 (15.352 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 82400 lr 3.725e-05 [train_loss] tar_ll 1.3254 loss -1.3254 (14.972 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 82600 lr 3.643e-05 [train_loss] tar_ll 1.1688 loss -1.1688 (15.114 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 82800 lr 3.562e-05 [train_loss] tar_ll 1.2878 loss -1.2878 (15.003 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 83000 lr 3.481e-05 [train_loss] tar_ll 1.2347 loss -1.2347 (15.221 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 83200 lr 3.402e-05 [train_loss] tar_ll 1.2349 loss -1.2349 (14.811 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 83400 lr 3.323e-05 [train_loss] tar_ll 1.2906 loss -1.2906 (14.700 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 83600 lr 3.245e-05 [train_loss] tar_ll 1.2332 loss -1.2332 (14.957 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 83800 lr 3.168e-05 [train_loss] tar_ll 1.2858 loss -1.2858 (15.190 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 84000 lr 3.092e-05 [train_loss] tar_ll 1.2328 loss -1.2328 (15.974 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2874 loss -1.2874 (15.368 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 84400 lr 2.943e-05 [train_loss] tar_ll 1.3544 loss -1.3544 (14.507 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 84600 lr 2.869e-05 [train_loss] tar_ll 1.3245 loss -1.3245 (14.733 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 84800 lr 2.797e-05 [train_loss] tar_ll 1.2943 loss -1.2943 (15.015 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 85000 lr 2.725e-05 [train_loss] tar_ll 1.2705 loss -1.2705 (15.043 secs)\n",
      "100%|##########| 3000/3000 [01:09<00:00, 42.94it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -3.6057 loss 3.6057 (69.874 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 85200 lr 2.654e-05 [train_loss] tar_ll 1.2337 loss -1.2337 (13.375 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 85400 lr 2.584e-05 [train_loss] tar_ll 1.3080 loss -1.3080 (13.985 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 85600 lr 2.515e-05 [train_loss] tar_ll 1.3534 loss -1.3534 (13.335 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 85800 lr 2.447e-05 [train_loss] tar_ll 1.3079 loss -1.3079 (14.035 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 86000 lr 2.379e-05 [train_loss] tar_ll 1.3553 loss -1.3553 (13.489 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 86200 lr 2.313e-05 [train_loss] tar_ll 1.3398 loss -1.3398 (13.234 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 86400 lr 2.247e-05 [train_loss] tar_ll 1.2701 loss -1.2701 (13.884 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 86600 lr 2.183e-05 [train_loss] tar_ll 1.2233 loss -1.2233 (15.060 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 86800 lr 2.119e-05 [train_loss] tar_ll 1.3276 loss -1.3276 (15.173 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 87000 lr 2.056e-05 [train_loss] tar_ll 1.3505 loss -1.3505 (15.027 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 87200 lr 1.994e-05 [train_loss] tar_ll 1.3374 loss -1.3374 (16.074 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 87400 lr 1.933e-05 [train_loss] tar_ll 1.3391 loss -1.3391 (15.191 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 87600 lr 1.873e-05 [train_loss] tar_ll 1.2957 loss -1.2957 (15.566 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 87800 lr 1.814e-05 [train_loss] tar_ll 1.3126 loss -1.3126 (15.148 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 88000 lr 1.756e-05 [train_loss] tar_ll 1.3119 loss -1.3119 (15.069 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 88200 lr 1.698e-05 [train_loss] tar_ll 1.3656 loss -1.3656 (14.733 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 88400 lr 1.642e-05 [train_loss] tar_ll 1.2461 loss -1.2461 (15.079 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 88600 lr 1.586e-05 [train_loss] tar_ll 1.3123 loss -1.3123 (15.846 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 88800 lr 1.532e-05 [train_loss] tar_ll 1.2272 loss -1.2272 (15.483 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 89000 lr 1.478e-05 [train_loss] tar_ll 1.3366 loss -1.3366 (14.765 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 89200 lr 1.425e-05 [train_loss] tar_ll 1.3093 loss -1.3093 (14.907 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 89400 lr 1.373e-05 [train_loss] tar_ll 1.3103 loss -1.3103 (16.471 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 89600 lr 1.323e-05 [train_loss] tar_ll 1.2980 loss -1.2980 (15.124 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 89800 lr 1.273e-05 [train_loss] tar_ll 1.3244 loss -1.3244 (14.831 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 90000 lr 1.224e-05 [train_loss] tar_ll 1.2103 loss -1.2103 (14.809 secs)\n",
      "100%|##########| 3000/3000 [01:10<00:00, 42.67it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -3.6917 loss 3.6917 (70.320 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 90200 lr 1.176e-05 [train_loss] tar_ll 1.4050 loss -1.4050 (14.515 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 90400 lr 1.128e-05 [train_loss] tar_ll 1.3825 loss -1.3825 (14.173 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 90600 lr 1.082e-05 [train_loss] tar_ll 1.2644 loss -1.2644 (14.189 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 90800 lr 1.037e-05 [train_loss] tar_ll 1.3081 loss -1.3081 (14.094 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 91000 lr 9.927e-06 [train_loss] tar_ll 1.3543 loss -1.3543 (13.557 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 91200 lr 9.493e-06 [train_loss] tar_ll 1.3157 loss -1.3157 (13.590 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2365 loss -1.2365 (13.882 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 91600 lr 8.655e-06 [train_loss] tar_ll 1.2124 loss -1.2124 (13.257 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 91800 lr 8.250e-06 [train_loss] tar_ll 1.3128 loss -1.3128 (13.230 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 92000 lr 7.854e-06 [train_loss] tar_ll 1.3355 loss -1.3355 (12.980 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 92200 lr 7.468e-06 [train_loss] tar_ll 1.3757 loss -1.3757 (15.878 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 92400 lr 7.092e-06 [train_loss] tar_ll 1.2560 loss -1.2560 (16.188 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 92600 lr 6.725e-06 [train_loss] tar_ll 1.3360 loss -1.3360 (17.057 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 92800 lr 6.368e-06 [train_loss] tar_ll 1.3465 loss -1.3465 (15.818 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 93000 lr 6.021e-06 [train_loss] tar_ll 1.1664 loss -1.1664 (15.266 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 93200 lr 5.683e-06 [train_loss] tar_ll 1.3424 loss -1.3424 (16.712 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 93400 lr 5.355e-06 [train_loss] tar_ll 1.3097 loss -1.3097 (16.933 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 93600 lr 5.036e-06 [train_loss] tar_ll 1.3131 loss -1.3131 (15.309 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 93800 lr 4.727e-06 [train_loss] tar_ll 1.2948 loss -1.2948 (14.386 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 94000 lr 4.428e-06 [train_loss] tar_ll 1.3308 loss -1.3308 (14.870 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 94200 lr 4.139e-06 [train_loss] tar_ll 1.3574 loss -1.3574 (14.984 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 94400 lr 3.859e-06 [train_loss] tar_ll 1.2564 loss -1.2564 (14.193 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 94600 lr 3.589e-06 [train_loss] tar_ll 1.3304 loss -1.3304 (14.541 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 94800 lr 3.329e-06 [train_loss] tar_ll 1.4066 loss -1.4066 (14.966 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 95000 lr 3.078e-06 [train_loss] tar_ll 1.2912 loss -1.2912 (14.745 secs)\n",
      "100%|##########| 3000/3000 [01:14<00:00, 40.32it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -3.7697 loss 3.7697 (74.403 secs)\n",
      "\n",
      "isanp:lbanp-num_latents-128_periodic step 95200 lr 2.837e-06 [train_loss] tar_ll 1.3514 loss -1.3514 (14.965 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 95400 lr 2.606e-06 [train_loss] tar_ll 1.3620 loss -1.3620 (14.792 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 95600 lr 2.385e-06 [train_loss] tar_ll 1.2902 loss -1.2902 (15.061 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 95800 lr 2.173e-06 [train_loss] tar_ll 1.3261 loss -1.3261 (14.346 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 96000 lr 1.971e-06 [train_loss] tar_ll 1.3361 loss -1.3361 (14.416 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2243 loss -1.2243 (14.417 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 96400 lr 1.597e-06 [train_loss] tar_ll 1.3406 loss -1.3406 (13.735 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 96600 lr 1.425e-06 [train_loss] tar_ll 1.3626 loss -1.3626 (14.079 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 96800 lr 1.262e-06 [train_loss] tar_ll 1.3582 loss -1.3582 (14.028 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 97000 lr 1.110e-06 [train_loss] tar_ll 1.3283 loss -1.3283 (13.856 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 97200 lr 9.666e-07 [train_loss] tar_ll 1.2662 loss -1.2662 (14.506 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 97400 lr 8.335e-07 [train_loss] tar_ll 1.2232 loss -1.2232 (14.378 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 97600 lr 7.103e-07 [train_loss] tar_ll 1.3131 loss -1.3131 (14.711 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 97800 lr 5.969e-07 [train_loss] tar_ll 1.3125 loss -1.3125 (14.222 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 98000 lr 4.933e-07 [train_loss] tar_ll 1.3340 loss -1.3340 (15.137 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 98200 lr 3.996e-07 [train_loss] tar_ll 1.3233 loss -1.3233 (15.507 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 98400 lr 3.158e-07 [train_loss] tar_ll 1.2694 loss -1.2694 (15.011 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 98600 lr 2.418e-07 [train_loss] tar_ll 1.3736 loss -1.3736 (15.037 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 98800 lr 1.776e-07 [train_loss] tar_ll 1.3247 loss -1.3247 (15.198 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 99000 lr 1.234e-07 [train_loss] tar_ll 1.2649 loss -1.2649 (15.079 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 99200 lr 7.895e-08 [train_loss] tar_ll 1.2806 loss -1.2806 (14.664 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 99400 lr 4.441e-08 [train_loss] tar_ll 1.3024 loss -1.3024 (14.668 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 99600 lr 1.974e-08 [train_loss] tar_ll 1.2947 loss -1.2947 (14.954 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 99800 lr 4.935e-09 [train_loss] tar_ll 1.2848 loss -1.2848 (14.497 secs)\n",
      "isanp:lbanp-num_latents-128_periodic step 100000 lr 0.000e+00 [train_loss] tar_ll 1.2619 loss -1.2619 (14.010 secs)\n",
      "100%|##########| 3000/3000 [01:11<00:00, 42.02it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -3.6989 loss 3.6989 (71.399 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [01:11<00:00, 42.25it/s]\n",
      "isanp:lbanp-num_latents-128_periodic periodic tar_ll -3.6989 loss 3.6989 (71.008 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 8992642.0 miliseconds\n",
      "Execution time: 8992.642 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 119.84375 MB\n",
      "Memory Usage Change: 103.59375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp', name='lbanp-num_latents-128_periodic',val_seed=100, val_l=128,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102d081-be95-4370-aba3-eb5dbc24f452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86778f60-3ced-4815-ac7d-5370067c86be",
   "metadata": {},
   "source": [
    "# ISANP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "253e1a72-8138-4b04-b3cf-a6b81315bba5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: isanp2-isanp2-num_latents-8_rbf_0\n",
      "Total number of parameters: 785602\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp2:isanp2-num_latents-8_rbf_0 step 200 lr 5.000e-04 [train_loss] tar_ll -0.7119 loss 0.7119 (27.546 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 400 lr 5.000e-04 [train_loss] tar_ll -0.5773 loss 0.5773 (18.035 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 600 lr 5.000e-04 [train_loss] tar_ll -0.2739 loss 0.2739 (14.723 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 800 lr 4.999e-04 [train_loss] tar_ll -0.1501 loss 0.1501 (14.642 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 1000 lr 4.999e-04 [train_loss] tar_ll -0.0936 loss 0.0936 (14.402 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 1200 lr 4.998e-04 [train_loss] tar_ll 0.0455 loss -0.0455 (14.742 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 1400 lr 4.998e-04 [train_loss] tar_ll 0.1923 loss -0.1923 (14.654 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 1600 lr 4.997e-04 [train_loss] tar_ll 0.1821 loss -0.1821 (14.534 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 1800 lr 4.996e-04 [train_loss] tar_ll 0.2009 loss -0.2009 (14.342 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 2000 lr 4.995e-04 [train_loss] tar_ll 0.2495 loss -0.2495 (14.484 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.3592 loss -0.3592 (15.228 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.3726 loss -0.3726 (14.372 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.3745 loss -0.3745 (14.019 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.3599 loss -0.3599 (14.192 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.4608 loss -0.4608 (14.374 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.2614 loss -0.2614 (14.292 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.3814 loss -0.3814 (14.175 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.3297 loss -0.3297 (14.032 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.4214 loss -0.4214 (13.847 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.4326 loss -0.4326 (13.877 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.4543 loss -0.4543 (13.989 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.4687 loss -0.4687 (14.460 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.4496 loss -0.4496 (14.000 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4820 loss -0.4820 (14.195 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.4930 loss -0.4930 (14.097 secs)\n",
      "100%|##########| 3000/3000 [01:10<00:00, 42.37it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 0.4110 loss -0.4110 (70.811 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.4625 loss -0.4625 (14.891 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.3787 loss -0.3787 (14.952 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.4308 loss -0.4308 (14.977 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.4013 loss -0.4013 (14.890 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4712 loss -0.4712 (14.666 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.4116 loss -0.4116 (14.576 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.5676 loss -0.5676 (14.684 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5884 loss -0.5884 (14.519 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.5026 loss -0.5026 (14.513 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.5835 loss -0.5835 (14.688 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.0929 loss -0.0929 (14.567 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.3470 loss -0.3470 (14.569 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.4606 loss -0.4606 (14.697 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.4493 loss -0.4493 (14.595 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.1208 loss -0.1208 (14.622 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 8200 lr 4.918e-04 [train_loss] tar_ll -0.0163 loss 0.0163 (14.914 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.2552 loss -0.2552 (14.448 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.3942 loss -0.3942 (14.424 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.4380 loss -0.4380 (14.489 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.5158 loss -0.5158 (14.346 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.4821 loss -0.4821 (14.555 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.4025 loss -0.4025 (14.590 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.4372 loss -0.4372 (14.254 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.4083 loss -0.4083 (14.379 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.4633 loss -0.4633 (14.665 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.67it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 0.5123 loss -0.5123 (67.164 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.5201 loss -0.5201 (15.059 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.4980 loss -0.4980 (14.850 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.5557 loss -0.5557 (14.919 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.3937 loss -0.3937 (14.756 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.4119 loss -0.4119 (15.374 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.4838 loss -0.4838 (15.075 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.3821 loss -0.3821 (15.173 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.2792 loss -0.2792 (14.498 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.2343 loss -0.2343 (14.535 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.4899 loss -0.4899 (14.796 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.4021 loss -0.4021 (14.585 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.6513 loss -0.6513 (14.337 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.6247 loss -0.6247 (14.646 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.5249 loss -0.5249 (14.339 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.5354 loss -0.5354 (14.262 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.6326 loss -0.6326 (14.902 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.5129 loss -0.5129 (14.728 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.5049 loss -0.5049 (14.671 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.5751 loss -0.5751 (15.289 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.6366 loss -0.6366 (14.662 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.6313 loss -0.6313 (14.885 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.5546 loss -0.5546 (14.926 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.2467 loss -0.2467 (15.992 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.1348 loss -0.1348 (15.592 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.0979 loss -0.0979 (14.652 secs)\n",
      "100%|##########| 3000/3000 [01:08<00:00, 44.01it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 0.0975 loss -0.0975 (68.174 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.2703 loss -0.2703 (14.119 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.4444 loss -0.4444 (13.982 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.1940 loss -0.1940 (14.528 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.2793 loss -0.2793 (14.677 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.5223 loss -0.5223 (14.201 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.4844 loss -0.4844 (15.381 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.4037 loss -0.4037 (15.168 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.5047 loss -0.5047 (16.853 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.5764 loss -0.5764 (17.975 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.5233 loss -0.5233 (17.041 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.3933 loss -0.3933 (16.552 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.3128 loss -0.3128 (15.950 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.4877 loss -0.4877 (15.591 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.6493 loss -0.6493 (15.295 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.2867 loss -0.2867 (14.871 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.1699 loss -0.1699 (15.135 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.1765 loss -0.1765 (15.364 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.3515 loss -0.3515 (15.062 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.2888 loss -0.2888 (16.004 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.3645 loss -0.3645 (15.992 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.4389 loss -0.4389 (16.337 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.5772 loss -0.5772 (15.522 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.5906 loss -0.5906 (14.825 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.4421 loss -0.4421 (15.497 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.5635 loss -0.5635 (14.956 secs)\n",
      "100%|##########| 3000/3000 [01:12<00:00, 41.56it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 0.6507 loss -0.6507 (72.187 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.5273 loss -0.5273 (14.412 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.4990 loss -0.4990 (13.999 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.5796 loss -0.5796 (14.738 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.6450 loss -0.6450 (15.559 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.3954 loss -0.3954 (16.602 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.4706 loss -0.4706 (16.428 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.6998 loss -0.6998 (16.200 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.5081 loss -0.5081 (16.319 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.3631 loss -0.3631 (16.007 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.5136 loss -0.5136 (15.971 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.4579 loss -0.4579 (16.515 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.5722 loss -0.5722 (15.227 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.6507 loss -0.6507 (15.319 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.5967 loss -0.5967 (15.128 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.6870 loss -0.6870 (15.112 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.6867 loss -0.6867 (14.915 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.6004 loss -0.6004 (14.304 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.6680 loss -0.6680 (14.508 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.2588 loss -0.2588 (14.172 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.6101 loss -0.6101 (13.922 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.7076 loss -0.7076 (14.356 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.6863 loss -0.6863 (13.924 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.7296 loss -0.7296 (13.867 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.6851 loss -0.6851 (14.104 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.7041 loss -0.7041 (14.288 secs)\n",
      "100%|##########| 3000/3000 [01:09<00:00, 42.90it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 0.6290 loss -0.6290 (69.935 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.6007 loss -0.6007 (13.999 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.7455 loss -0.7455 (14.023 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.7382 loss -0.7382 (10.491 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.6518 loss -0.6518 (6.771 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.6052 loss -0.6052 (6.867 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.5498 loss -0.5498 (6.765 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.6195 loss -0.6195 (6.697 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.6532 loss -0.6532 (6.526 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.6784 loss -0.6784 (6.664 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.5419 loss -0.5419 (6.555 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.5458 loss -0.5458 (6.538 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.6428 loss -0.6428 (6.905 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.7013 loss -0.7013 (6.669 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.4824 loss -0.4824 (6.778 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.6033 loss -0.6033 (6.664 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.6281 loss -0.6281 (6.938 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.3682 loss -0.3682 (6.799 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.6711 loss -0.6711 (6.633 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.6638 loss -0.6638 (6.958 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.7102 loss -0.7102 (6.862 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.6811 loss -0.6811 (6.979 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.7290 loss -0.7290 (6.364 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.6640 loss -0.6640 (6.622 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.7709 loss -0.7709 (6.635 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.6871 loss -0.6871 (6.641 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.74it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 0.8139 loss -0.8139 (33.065 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.7501 loss -0.7501 (6.888 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.8543 loss -0.8543 (6.443 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.8189 loss -0.8189 (6.507 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.7448 loss -0.7448 (6.422 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.6336 loss -0.6336 (6.619 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.6618 loss -0.6618 (6.550 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.8731 loss -0.8731 (6.587 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.8073 loss -0.8073 (6.861 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.7062 loss -0.7062 (6.544 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.7439 loss -0.7439 (6.890 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.8150 loss -0.8150 (6.674 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.6865 loss -0.6865 (6.625 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.7900 loss -0.7900 (6.806 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.7867 loss -0.7867 (6.814 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.8985 loss -0.8985 (7.209 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.8466 loss -0.8466 (7.787 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.8895 loss -0.8895 (7.119 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.7923 loss -0.7923 (7.008 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.9059 loss -0.9059 (6.672 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.8093 loss -0.8093 (7.027 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.8242 loss -0.8242 (6.790 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.8177 loss -0.8177 (6.891 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.8589 loss -0.8589 (6.809 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.8548 loss -0.8548 (6.988 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.8565 loss -0.8565 (6.582 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 93.48it/s] \n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 0.9307 loss -0.9307 (32.094 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.8612 loss -0.8612 (7.079 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.8549 loss -0.8549 (6.791 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.8155 loss -0.8155 (7.115 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.8554 loss -0.8554 (6.984 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.7117 loss -0.7117 (6.969 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.8856 loss -0.8856 (7.235 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.8415 loss -0.8415 (7.131 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.7868 loss -0.7868 (6.692 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.8931 loss -0.8931 (6.857 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.8428 loss -0.8428 (6.811 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.8852 loss -0.8852 (6.946 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.9341 loss -0.9341 (7.010 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.8628 loss -0.8628 (6.987 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.8727 loss -0.8727 (6.903 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.8763 loss -0.8763 (6.928 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.8987 loss -0.8987 (6.936 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.8174 loss -0.8174 (6.792 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.8263 loss -0.8263 (6.887 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.8699 loss -0.8699 (7.029 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.9050 loss -0.9050 (6.884 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.8201 loss -0.8201 (7.013 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.8662 loss -0.8662 (6.744 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.8976 loss -0.8976 (6.562 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.8943 loss -0.8943 (7.224 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.9348 loss -0.9348 (7.209 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 94.91it/s] \n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 0.9217 loss -0.9217 (31.611 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.8952 loss -0.8952 (6.695 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.9475 loss -0.9475 (6.726 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.8861 loss -0.8861 (7.008 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.9058 loss -0.9058 (7.217 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.9195 loss -0.9195 (7.081 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.9237 loss -0.9237 (7.421 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.9066 loss -0.9066 (7.239 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.8976 loss -0.8976 (7.091 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.9924 loss -0.9924 (6.956 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.9394 loss -0.9394 (6.876 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.8865 loss -0.8865 (7.173 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.8882 loss -0.8882 (6.828 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 42600 lr 3.076e-04 [train_loss] tar_ll 0.8247 loss -0.8247 (7.043 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.8828 loss -0.8828 (6.995 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.9007 loss -0.9007 (7.008 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.8851 loss -0.8851 (6.998 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.8506 loss -0.8506 (7.074 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.9466 loss -0.9466 (7.256 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.9279 loss -0.9279 (12.577 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.8908 loss -0.8908 (14.093 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.9490 loss -0.9490 (14.666 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.9396 loss -0.9396 (15.119 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.9715 loss -0.9715 (15.121 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.8451 loss -0.8451 (14.895 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.9201 loss -0.9201 (14.565 secs)\n",
      "100%|##########| 3000/3000 [00:41<00:00, 71.47it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 0.8127 loss -0.8127 (41.978 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.9266 loss -0.9266 (6.697 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.9262 loss -0.9262 (6.767 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.9182 loss -0.9182 (6.907 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.9922 loss -0.9922 (6.778 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.9206 loss -0.9206 (6.782 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.9017 loss -0.9017 (6.899 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.9554 loss -0.9554 (6.837 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.9218 loss -0.9218 (6.469 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.8973 loss -0.8973 (6.522 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.9334 loss -0.9334 (6.482 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.8759 loss -0.8759 (6.626 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.6299 loss -0.6299 (6.528 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 47600 lr 2.688e-04 [train_loss] tar_ll 0.8241 loss -0.8241 (6.479 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.8475 loss -0.8475 (6.671 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.8911 loss -0.8911 (6.605 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.9274 loss -0.9274 (6.878 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 48400 lr 2.626e-04 [train_loss] tar_ll 0.9195 loss -0.9195 (7.115 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.9845 loss -0.9845 (6.882 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 48800 lr 2.594e-04 [train_loss] tar_ll 1.0672 loss -1.0672 (6.970 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.8007 loss -0.8007 (6.656 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 49200 lr 2.563e-04 [train_loss] tar_ll 0.8400 loss -0.8400 (6.687 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.9307 loss -0.9307 (6.533 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 49600 lr 2.531e-04 [train_loss] tar_ll 0.9503 loss -0.9503 (6.625 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.9782 loss -0.9782 (6.568 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 50000 lr 2.500e-04 [train_loss] tar_ll 0.9573 loss -0.9573 (6.475 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 95.18it/s] \n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 0.9316 loss -0.9316 (31.521 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 50200 lr 2.484e-04 [train_loss] tar_ll 0.9850 loss -0.9850 (6.729 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 50400 lr 2.469e-04 [train_loss] tar_ll 1.0097 loss -1.0097 (6.782 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 50600 lr 2.453e-04 [train_loss] tar_ll 0.9634 loss -0.9634 (6.832 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 50800 lr 2.437e-04 [train_loss] tar_ll 0.9637 loss -0.9637 (6.576 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 51000 lr 2.421e-04 [train_loss] tar_ll 0.8748 loss -0.8748 (6.606 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 51200 lr 2.406e-04 [train_loss] tar_ll 0.9583 loss -0.9583 (6.771 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 51400 lr 2.390e-04 [train_loss] tar_ll 1.0425 loss -1.0425 (6.613 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 51600 lr 2.374e-04 [train_loss] tar_ll 0.9837 loss -0.9837 (6.681 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 51800 lr 2.359e-04 [train_loss] tar_ll 1.0110 loss -1.0110 (6.782 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 52000 lr 2.343e-04 [train_loss] tar_ll 0.9093 loss -0.9093 (6.588 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 52200 lr 2.327e-04 [train_loss] tar_ll 1.0353 loss -1.0353 (6.628 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 52400 lr 2.312e-04 [train_loss] tar_ll 0.9746 loss -0.9746 (6.510 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 52600 lr 2.296e-04 [train_loss] tar_ll 0.9904 loss -0.9904 (6.628 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 52800 lr 2.280e-04 [train_loss] tar_ll 0.8800 loss -0.8800 (6.528 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 53000 lr 2.265e-04 [train_loss] tar_ll 1.0676 loss -1.0676 (6.469 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 53200 lr 2.249e-04 [train_loss] tar_ll 0.9945 loss -0.9945 (6.533 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 53400 lr 2.233e-04 [train_loss] tar_ll 0.9987 loss -0.9987 (6.511 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 53600 lr 2.218e-04 [train_loss] tar_ll 0.9726 loss -0.9726 (6.582 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 53800 lr 2.202e-04 [train_loss] tar_ll 1.0174 loss -1.0174 (6.757 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 54000 lr 2.187e-04 [train_loss] tar_ll 1.0034 loss -1.0034 (6.769 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 54200 lr 2.171e-04 [train_loss] tar_ll 0.9551 loss -0.9551 (6.639 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 54400 lr 2.156e-04 [train_loss] tar_ll 1.0042 loss -1.0042 (6.790 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 54600 lr 2.140e-04 [train_loss] tar_ll 0.9643 loss -0.9643 (6.800 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 54800 lr 2.124e-04 [train_loss] tar_ll 0.8945 loss -0.8945 (6.709 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 55000 lr 2.109e-04 [train_loss] tar_ll 0.9528 loss -0.9528 (6.803 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 90.97it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 1.0388 loss -1.0388 (32.980 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 55200 lr 2.093e-04 [train_loss] tar_ll 1.0099 loss -1.0099 (6.810 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 55400 lr 2.078e-04 [train_loss] tar_ll 0.9883 loss -0.9883 (6.692 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 55600 lr 2.062e-04 [train_loss] tar_ll 0.9773 loss -0.9773 (6.306 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 55800 lr 2.047e-04 [train_loss] tar_ll 1.0108 loss -1.0108 (6.577 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 56000 lr 2.032e-04 [train_loss] tar_ll 1.0167 loss -1.0167 (6.360 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 56200 lr 2.016e-04 [train_loss] tar_ll 1.0476 loss -1.0476 (6.418 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 56400 lr 2.001e-04 [train_loss] tar_ll 1.0058 loss -1.0058 (6.714 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 56600 lr 1.985e-04 [train_loss] tar_ll 1.0472 loss -1.0472 (6.435 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0249 loss -1.0249 (6.734 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 57000 lr 1.955e-04 [train_loss] tar_ll 1.0227 loss -1.0227 (6.553 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 57200 lr 1.939e-04 [train_loss] tar_ll 0.9776 loss -0.9776 (6.550 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 57400 lr 1.924e-04 [train_loss] tar_ll 1.0805 loss -1.0805 (6.679 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 57600 lr 1.909e-04 [train_loss] tar_ll 1.0444 loss -1.0444 (6.576 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 57800 lr 1.894e-04 [train_loss] tar_ll 0.9207 loss -0.9207 (6.632 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 58000 lr 1.878e-04 [train_loss] tar_ll 1.0004 loss -1.0004 (6.669 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 58200 lr 1.863e-04 [train_loss] tar_ll 1.0286 loss -1.0286 (6.561 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 58400 lr 1.848e-04 [train_loss] tar_ll 1.0796 loss -1.0796 (6.597 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 58600 lr 1.833e-04 [train_loss] tar_ll 1.0488 loss -1.0488 (6.552 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 58800 lr 1.818e-04 [train_loss] tar_ll 1.0161 loss -1.0161 (6.589 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 59000 lr 1.803e-04 [train_loss] tar_ll 1.0088 loss -1.0088 (6.453 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 59200 lr 1.787e-04 [train_loss] tar_ll 1.0512 loss -1.0512 (6.611 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 59400 lr 1.772e-04 [train_loss] tar_ll 0.9864 loss -0.9864 (6.503 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 59600 lr 1.757e-04 [train_loss] tar_ll 1.0121 loss -1.0121 (6.436 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 59800 lr 1.742e-04 [train_loss] tar_ll 1.1437 loss -1.1437 (6.504 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 60000 lr 1.727e-04 [train_loss] tar_ll 1.1033 loss -1.1033 (6.567 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 93.92it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 1.0651 loss -1.0651 (31.943 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 60200 lr 1.713e-04 [train_loss] tar_ll 1.0046 loss -1.0046 (6.793 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 60400 lr 1.698e-04 [train_loss] tar_ll 0.8892 loss -0.8892 (6.539 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 60600 lr 1.683e-04 [train_loss] tar_ll 1.0136 loss -1.0136 (6.631 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.0395 loss -1.0395 (6.562 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 61000 lr 1.653e-04 [train_loss] tar_ll 0.9378 loss -0.9378 (6.509 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 61200 lr 1.638e-04 [train_loss] tar_ll 1.0628 loss -1.0628 (6.701 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 61400 lr 1.624e-04 [train_loss] tar_ll 1.0363 loss -1.0363 (6.515 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 61600 lr 1.609e-04 [train_loss] tar_ll 0.9849 loss -0.9849 (6.582 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.0799 loss -1.0799 (6.548 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 62000 lr 1.580e-04 [train_loss] tar_ll 1.0374 loss -1.0374 (6.439 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 62200 lr 1.565e-04 [train_loss] tar_ll 1.0388 loss -1.0388 (6.707 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 62400 lr 1.551e-04 [train_loss] tar_ll 1.0129 loss -1.0129 (6.415 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.1257 loss -1.1257 (6.569 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.0849 loss -1.0849 (6.716 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.0173 loss -1.0173 (6.644 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.0759 loss -1.0759 (6.741 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.1051 loss -1.1051 (6.616 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.0456 loss -1.0456 (6.801 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.0743 loss -1.0743 (6.755 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.0628 loss -1.0628 (6.753 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 64200 lr 1.421e-04 [train_loss] tar_ll 1.1167 loss -1.1167 (6.806 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 64400 lr 1.407e-04 [train_loss] tar_ll 1.1161 loss -1.1161 (6.665 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.1095 loss -1.1095 (6.779 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.0782 loss -1.0782 (6.775 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 65000 lr 1.365e-04 [train_loss] tar_ll 1.0752 loss -1.0752 (6.728 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 95.54it/s] \n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 1.1281 loss -1.1281 (31.403 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.1207 loss -1.1207 (6.463 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.0824 loss -1.0824 (6.377 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.1475 loss -1.1475 (6.521 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.1986 loss -1.1986 (6.493 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.0756 loss -1.0756 (6.571 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 66200 lr 1.282e-04 [train_loss] tar_ll 1.1437 loss -1.1437 (6.529 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.1342 loss -1.1342 (6.488 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.1088 loss -1.1088 (6.599 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.0882 loss -1.0882 (6.520 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.1676 loss -1.1676 (6.880 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 67200 lr 1.214e-04 [train_loss] tar_ll 1.1061 loss -1.1061 (7.001 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.1000 loss -1.1000 (6.855 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.1052 loss -1.1052 (6.982 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.0805 loss -1.0805 (6.871 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.0957 loss -1.0957 (6.672 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.0761 loss -1.0761 (6.481 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 68400 lr 1.134e-04 [train_loss] tar_ll 0.9916 loss -0.9916 (6.466 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 68600 lr 1.121e-04 [train_loss] tar_ll 1.0752 loss -1.0752 (6.412 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.1583 loss -1.1583 (6.502 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.1014 loss -1.1014 (6.599 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.1810 loss -1.1810 (6.561 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.1024 loss -1.1024 (6.564 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.0955 loss -1.0955 (6.545 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.1184 loss -1.1184 (6.591 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.1105 loss -1.1105 (6.651 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.50it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 1.1301 loss -1.1301 (33.900 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.1187 loss -1.1187 (6.588 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.0978 loss -1.0978 (6.604 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.1336 loss -1.1336 (6.538 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.1168 loss -1.1168 (6.569 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.0509 loss -1.0509 (6.474 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 71200 lr 9.554e-05 [train_loss] tar_ll 0.9889 loss -0.9889 (6.664 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.0621 loss -1.0621 (6.536 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.0884 loss -1.0884 (6.670 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.0883 loss -1.0883 (6.782 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 72000 lr 9.064e-05 [train_loss] tar_ll 1.1255 loss -1.1255 (6.581 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.0728 loss -1.0728 (6.765 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.1307 loss -1.1307 (6.714 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.1471 loss -1.1471 (6.660 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.2320 loss -1.2320 (6.841 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.0567 loss -1.0567 (6.736 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.0884 loss -1.0884 (6.865 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.1153 loss -1.1153 (6.727 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.1129 loss -1.1129 (6.792 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.1560 loss -1.1560 (6.841 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.2633 loss -1.2633 (6.826 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.1467 loss -1.1467 (6.564 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.1616 loss -1.1616 (6.369 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.2125 loss -1.2125 (6.626 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.1884 loss -1.1884 (6.458 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.0322 loss -1.0322 (6.394 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 93.06it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 1.1463 loss -1.1463 (32.241 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1484 loss -1.1484 (6.531 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.1780 loss -1.1780 (6.646 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.1986 loss -1.1986 (6.578 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.1816 loss -1.1816 (6.647 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.2308 loss -1.2308 (6.597 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.1746 loss -1.1746 (6.658 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.1715 loss -1.1715 (6.607 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.1478 loss -1.1478 (6.531 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.0967 loss -1.0967 (6.438 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.1283 loss -1.1283 (6.538 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.2650 loss -1.2650 (6.519 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.1398 loss -1.1398 (6.492 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.2868 loss -1.2868 (6.589 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.2179 loss -1.2179 (6.609 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.1991 loss -1.1991 (6.575 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.1612 loss -1.1612 (6.574 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.1665 loss -1.1665 (6.653 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.1652 loss -1.1652 (6.679 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.1394 loss -1.1394 (6.655 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.1454 loss -1.1454 (6.699 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.1479 loss -1.1479 (6.598 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.1936 loss -1.1936 (6.771 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.2134 loss -1.2134 (6.505 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.2081 loss -1.2081 (6.549 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.0591 loss -1.0591 (6.657 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 94.18it/s] \n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 1.1915 loss -1.1915 (31.854 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.2060 loss -1.2060 (6.568 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.2267 loss -1.2267 (6.646 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1919 loss -1.1919 (6.647 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.2441 loss -1.2441 (6.666 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.1904 loss -1.1904 (6.672 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.1424 loss -1.1424 (6.819 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.2020 loss -1.2020 (6.790 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.1530 loss -1.1530 (6.623 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2488 loss -1.2488 (6.684 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.2455 loss -1.2455 (6.681 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.1882 loss -1.1882 (6.862 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.1953 loss -1.1953 (6.816 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.1940 loss -1.1940 (6.850 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.2385 loss -1.2385 (6.737 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.1697 loss -1.1697 (6.845 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.1390 loss -1.1390 (6.675 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.2097 loss -1.2097 (6.455 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.1141 loss -1.1141 (6.485 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.2135 loss -1.2135 (6.512 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.1557 loss -1.1557 (6.488 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.2117 loss -1.2117 (6.656 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.1613 loss -1.1613 (6.444 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.2350 loss -1.2350 (6.473 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.2280 loss -1.2280 (6.579 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.1896 loss -1.1896 (6.448 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 93.43it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 1.2012 loss -1.2012 (32.112 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.1775 loss -1.1775 (6.750 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.1584 loss -1.1584 (6.933 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.2383 loss -1.2383 (6.862 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.2284 loss -1.2284 (6.478 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.2381 loss -1.2381 (6.480 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.1793 loss -1.1793 (6.672 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.2060 loss -1.2060 (6.553 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.2243 loss -1.2243 (6.844 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.1937 loss -1.1937 (6.630 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.1615 loss -1.1615 (6.557 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.1935 loss -1.1935 (6.658 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.2569 loss -1.2569 (6.581 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.1782 loss -1.1782 (6.639 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.2440 loss -1.2440 (6.736 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.2620 loss -1.2620 (6.671 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.1585 loss -1.1585 (6.580 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.1618 loss -1.1618 (6.557 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.2191 loss -1.2191 (6.589 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.2023 loss -1.2023 (6.473 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.2147 loss -1.2147 (6.648 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.2244 loss -1.2244 (6.473 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.1623 loss -1.1623 (6.463 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.2078 loss -1.2078 (6.614 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.1430 loss -1.1430 (6.423 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.1228 loss -1.1228 (6.530 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 90.90it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 1.2133 loss -1.2133 (33.008 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.2404 loss -1.2404 (6.760 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.1795 loss -1.1795 (6.794 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.1195 loss -1.1195 (6.562 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.2079 loss -1.2079 (6.703 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.1828 loss -1.1828 (6.706 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2235 loss -1.2235 (6.496 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2941 loss -1.2941 (6.598 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.2284 loss -1.2284 (6.501 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2447 loss -1.2447 (6.444 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2010 loss -1.2010 (6.637 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.1603 loss -1.1603 (6.432 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.1910 loss -1.1910 (6.610 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.2926 loss -1.2926 (6.484 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.1482 loss -1.1482 (6.522 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.2265 loss -1.2265 (6.530 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.2304 loss -1.2304 (6.613 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.1982 loss -1.1982 (6.516 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.1958 loss -1.1958 (6.768 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.1673 loss -1.1673 (6.635 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.2697 loss -1.2697 (6.602 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.1821 loss -1.1821 (6.601 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.1588 loss -1.1588 (6.675 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.1098 loss -1.1098 (6.594 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.1626 loss -1.1626 (6.619 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.2306 loss -1.2306 (6.578 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.94it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 1.2232 loss -1.2232 (32.632 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.2276 loss -1.2276 (6.558 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.2321 loss -1.2321 (6.618 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.3156 loss -1.3156 (6.684 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.1688 loss -1.1688 (6.806 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.2470 loss -1.2470 (6.611 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.1807 loss -1.1807 (6.823 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.2395 loss -1.2395 (6.683 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.1042 loss -1.1042 (6.747 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.2059 loss -1.2059 (6.648 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.2224 loss -1.2224 (6.614 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.1950 loss -1.1950 (6.903 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.1607 loss -1.1607 (7.070 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.2837 loss -1.2837 (6.958 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.2554 loss -1.2554 (6.620 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.2219 loss -1.2219 (6.514 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.2424 loss -1.2424 (6.620 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.2347 loss -1.2347 (6.640 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.3116 loss -1.3116 (6.833 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.1573 loss -1.1573 (6.508 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.1732 loss -1.1732 (6.688 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.2367 loss -1.2367 (6.868 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2423 loss -1.2423 (7.024 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.2978 loss -1.2978 (7.083 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.2462 loss -1.2462 (6.928 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.3777 loss -1.3777 (7.075 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.23it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 1.2254 loss -1.2254 (33.624 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.42it/s]\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 1.2254 loss -1.2254 (32.817 secs)\n",
      "isanp2:isanp2-num_latents-8_rbf_0 rbf tar_ll 1.2254 loss -1.2254 (32.817 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 5356772.5 miliseconds\n",
      "Execution time: 5356.7725 seconds\n",
      "Initial Memory Usage: 0.0 MB\n",
      "Final Memory Usage: 55.94873046875 MB\n",
      "Memory Usage Change: 55.94873046875 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp2', name='isanp2-num_latents-8_rbf_0',val_seed=0, val_l=8,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416804b-39d6-40dc-8439-27e8173aa710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: isanp2-isanp2-num_latents-128_rbf_0\n",
      "Total number of parameters: 793282\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp2:isanp2-num_latents-128_rbf_0 step 200 lr 5.000e-04 [train_loss] tar_ll -0.7005 loss 0.7005 (15.949 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 400 lr 5.000e-04 [train_loss] tar_ll -0.4317 loss 0.4317 (6.736 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 600 lr 5.000e-04 [train_loss] tar_ll -0.1501 loss 0.1501 (6.497 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 800 lr 4.999e-04 [train_loss] tar_ll -0.0461 loss 0.0461 (6.414 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 1000 lr 4.999e-04 [train_loss] tar_ll 0.0134 loss -0.0134 (6.383 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 1200 lr 4.998e-04 [train_loss] tar_ll 0.1047 loss -0.1047 (6.303 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 1400 lr 4.998e-04 [train_loss] tar_ll 0.2467 loss -0.2467 (6.137 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 1600 lr 4.997e-04 [train_loss] tar_ll 0.2128 loss -0.2128 (6.174 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 1800 lr 4.996e-04 [train_loss] tar_ll 0.2567 loss -0.2567 (6.148 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 2000 lr 4.995e-04 [train_loss] tar_ll 0.2665 loss -0.2665 (6.104 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.4304 loss -0.4304 (6.393 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.3747 loss -0.3747 (6.438 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.3899 loss -0.3899 (6.607 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.4046 loss -0.4046 (6.330 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.4799 loss -0.4799 (6.053 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.2625 loss -0.2625 (6.263 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.3875 loss -0.3875 (5.987 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.5268 loss -0.5268 (6.205 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.4200 loss -0.4200 (6.073 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.3722 loss -0.3722 (6.020 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.3727 loss -0.3727 (6.309 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.4338 loss -0.4338 (6.101 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.5911 loss -0.5911 (6.328 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.5360 loss -0.5360 (6.358 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.5112 loss -0.5112 (6.154 secs)\n",
      "100%|##########| 3000/3000 [00:30<00:00, 98.54it/s] \n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 0.5365 loss -0.5365 (30.448 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.4815 loss -0.4815 (6.330 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.5407 loss -0.5407 (6.260 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.3121 loss -0.3121 (6.162 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.4771 loss -0.4771 (6.783 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.6002 loss -0.6002 (6.429 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.5433 loss -0.5433 (6.256 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.5089 loss -0.5089 (6.074 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5492 loss -0.5492 (6.128 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.5956 loss -0.5956 (6.342 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.6904 loss -0.6904 (6.804 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.4897 loss -0.4897 (6.870 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5003 loss -0.5003 (6.739 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.5237 loss -0.5237 (6.808 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.5844 loss -0.5844 (6.256 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.5842 loss -0.5842 (6.324 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.5865 loss -0.5865 (6.561 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.5013 loss -0.5013 (6.592 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.4827 loss -0.4827 (6.350 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.5658 loss -0.5658 (6.597 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.5559 loss -0.5559 (6.336 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.4101 loss -0.4101 (6.484 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.3770 loss -0.3770 (6.377 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.5166 loss -0.5166 (6.354 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.5652 loss -0.5652 (6.300 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.5811 loss -0.5811 (6.232 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 95.96it/s] \n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 0.2650 loss -0.2650 (31.265 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.5608 loss -0.5608 (6.549 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.5516 loss -0.5516 (6.379 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.5924 loss -0.5924 (6.390 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.5206 loss -0.5206 (6.348 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.5132 loss -0.5132 (6.424 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.4856 loss -0.4856 (6.402 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.3358 loss -0.3358 (6.136 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.5190 loss -0.5190 (6.166 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.5586 loss -0.5586 (6.221 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.3333 loss -0.3333 (6.118 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.0226 loss -0.0226 (6.296 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.1070 loss -0.1070 (6.056 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.0303 loss -0.0303 (6.091 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.1692 loss -0.1692 (6.252 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.3279 loss -0.3279 (6.602 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.2061 loss -0.2061 (7.364 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.3627 loss -0.3627 (6.890 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.3806 loss -0.3806 (14.855 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.4874 loss -0.4874 (10.142 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.5146 loss -0.5146 (16.141 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.3924 loss -0.3924 (16.811 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.5719 loss -0.5719 (15.073 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.4602 loss -0.4602 (14.459 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.4048 loss -0.4048 (6.927 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.4049 loss -0.4049 (7.150 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 91.50it/s] \n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 0.6002 loss -0.6002 (32.787 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.5452 loss -0.5452 (6.785 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.5082 loss -0.5082 (6.462 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.5116 loss -0.5116 (6.384 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.4971 loss -0.4971 (6.218 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.6031 loss -0.6031 (6.316 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.1364 loss -0.1364 (6.303 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.3280 loss -0.3280 (6.337 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.2745 loss -0.2745 (6.457 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.0831 loss -0.0831 (6.401 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.3226 loss -0.3226 (6.351 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.4327 loss -0.4327 (6.328 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.3397 loss -0.3397 (6.458 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.3089 loss -0.3089 (6.379 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.4096 loss -0.4096 (6.309 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.5190 loss -0.5190 (6.793 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.4819 loss -0.4819 (6.495 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.3593 loss -0.3593 (6.351 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.4489 loss -0.4489 (6.472 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.5490 loss -0.5490 (6.834 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.3007 loss -0.3007 (6.609 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.3801 loss -0.3801 (6.390 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.4176 loss -0.4176 (6.371 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.3644 loss -0.3644 (6.465 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.6017 loss -0.6017 (6.465 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.4738 loss -0.4738 (6.628 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 96.75it/s] \n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 0.6437 loss -0.6437 (31.009 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.5401 loss -0.5401 (6.279 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.6362 loss -0.6362 (6.771 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.5813 loss -0.5813 (6.710 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.6608 loss -0.6608 (7.362 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.6557 loss -0.6557 (11.179 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.7078 loss -0.7078 (15.633 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.6189 loss -0.6189 (15.012 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.5724 loss -0.5724 (16.098 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.6261 loss -0.6261 (18.130 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.6221 loss -0.6221 (17.021 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.7014 loss -0.7014 (16.911 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.6984 loss -0.6984 (16.477 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.7234 loss -0.7234 (16.574 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.6207 loss -0.6207 (16.140 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.5382 loss -0.5382 (15.324 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.6049 loss -0.6049 (14.741 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.6804 loss -0.6804 (14.943 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.6907 loss -0.6907 (14.935 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.4523 loss -0.4523 (14.500 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.3264 loss -0.3264 (15.142 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.4899 loss -0.4899 (14.988 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.4502 loss -0.4502 (14.472 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.4322 loss -0.4322 (14.334 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.3528 loss -0.3528 (14.523 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.4605 loss -0.4605 (13.732 secs)\n",
      "100%|##########| 3000/3000 [01:10<00:00, 42.65it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 0.5968 loss -0.5968 (70.340 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.6350 loss -0.6350 (14.111 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.5835 loss -0.5835 (14.993 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.0306 loss -0.0306 (16.079 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.3808 loss -0.3808 (15.970 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.4699 loss -0.4699 (15.740 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.4517 loss -0.4517 (15.843 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.5413 loss -0.5413 (15.187 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.6050 loss -0.6050 (15.625 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.6398 loss -0.6398 (15.127 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 27000 lr 4.153e-04 [train_loss] tar_ll -0.0882 loss 0.0882 (15.217 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.1394 loss -0.1394 (15.203 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.2703 loss -0.2703 (15.382 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.4321 loss -0.4321 (15.217 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.4797 loss -0.4797 (15.236 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.3669 loss -0.3669 (15.324 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.4924 loss -0.4924 (15.163 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.5876 loss -0.5876 (14.839 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.6784 loss -0.6784 (15.647 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.5179 loss -0.5179 (16.301 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.6976 loss -0.6976 (15.236 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.6293 loss -0.6293 (16.142 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.7244 loss -0.7244 (14.587 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.7073 loss -0.7073 (14.853 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.2369 loss -0.2369 (15.922 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.4210 loss -0.4210 (14.609 secs)\n",
      "100%|##########| 3000/3000 [00:47<00:00, 63.60it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll -0.0313 loss 0.0313 (47.172 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.2899 loss -0.2899 (6.580 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.4351 loss -0.4351 (6.869 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.3203 loss -0.3203 (6.694 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.5317 loss -0.5317 (6.824 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.5564 loss -0.5564 (6.751 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.5611 loss -0.5611 (6.833 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.5170 loss -0.5170 (6.829 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.6635 loss -0.6635 (6.560 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.6586 loss -0.6586 (6.842 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.7116 loss -0.7116 (6.731 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.6817 loss -0.6817 (6.583 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.6779 loss -0.6779 (6.589 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.5770 loss -0.5770 (6.620 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.4501 loss -0.4501 (6.757 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.6481 loss -0.6481 (6.737 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.5637 loss -0.5637 (6.652 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.6548 loss -0.6548 (6.410 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.6881 loss -0.6881 (6.708 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.7042 loss -0.7042 (6.808 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.6384 loss -0.6384 (6.645 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.7077 loss -0.7077 (6.995 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.7984 loss -0.7984 (6.896 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.7378 loss -0.7378 (6.767 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.4437 loss -0.4437 (6.678 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.3502 loss -0.3502 (6.693 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 88.76it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 0.5507 loss -0.5507 (33.801 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.5291 loss -0.5291 (6.979 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.3845 loss -0.3845 (7.145 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.5786 loss -0.5786 (6.677 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.3412 loss -0.3412 (6.465 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.4548 loss -0.4548 (6.671 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.6038 loss -0.6038 (6.696 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.6663 loss -0.6663 (6.656 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.7550 loss -0.7550 (6.527 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.5939 loss -0.5939 (6.379 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.4170 loss -0.4170 (6.894 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.7126 loss -0.7126 (6.561 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.6598 loss -0.6598 (6.925 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.5751 loss -0.5751 (6.929 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.7549 loss -0.7549 (6.888 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.4254 loss -0.4254 (6.744 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.7137 loss -0.7137 (6.752 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.6658 loss -0.6658 (6.781 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.7414 loss -0.7414 (6.823 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.7628 loss -0.7628 (6.933 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.8526 loss -0.8526 (7.011 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.4071 loss -0.4071 (6.817 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.6312 loss -0.6312 (6.749 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.7137 loss -0.7137 (6.717 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.5247 loss -0.5247 (6.762 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.7193 loss -0.7193 (6.504 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 92.29it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll -0.0934 loss 0.0934 (32.509 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.2716 loss -0.2716 (6.621 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.4771 loss -0.4771 (6.489 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.5509 loss -0.5509 (6.733 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.6697 loss -0.6697 (6.674 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.6197 loss -0.6197 (6.874 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.6664 loss -0.6664 (6.844 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.7257 loss -0.7257 (6.486 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.6313 loss -0.6313 (6.698 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.7276 loss -0.7276 (6.652 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.6685 loss -0.6685 (6.572 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.7009 loss -0.7009 (6.936 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.7499 loss -0.7499 (6.567 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 42600 lr 3.076e-04 [train_loss] tar_ll 0.6968 loss -0.6968 (6.555 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.4236 loss -0.4236 (6.496 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.7939 loss -0.7939 (6.372 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.7518 loss -0.7518 (6.618 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.7324 loss -0.7324 (6.344 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.8533 loss -0.8533 (6.470 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.8518 loss -0.8518 (6.387 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.7478 loss -0.7478 (6.336 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.8647 loss -0.8647 (6.738 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.8326 loss -0.8326 (6.341 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.3273 loss -0.3273 (6.776 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.4928 loss -0.4928 (6.503 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.5770 loss -0.5770 (6.884 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 93.81it/s] \n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 0.7157 loss -0.7157 (31.979 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.6963 loss -0.6963 (6.607 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.7508 loss -0.7508 (6.866 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.6122 loss -0.6122 (6.866 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.3292 loss -0.3292 (6.665 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.5081 loss -0.5081 (6.792 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.6073 loss -0.6073 (6.854 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.6678 loss -0.6678 (6.893 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.5792 loss -0.5792 (6.995 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.5993 loss -0.5993 (6.732 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.4486 loss -0.4486 (7.134 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.5804 loss -0.5804 (6.910 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.4654 loss -0.4654 (6.740 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 47600 lr 2.688e-04 [train_loss] tar_ll 0.6005 loss -0.6005 (6.731 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.6564 loss -0.6564 (6.966 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.4750 loss -0.4750 (6.677 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.6226 loss -0.6226 (6.505 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 48400 lr 2.626e-04 [train_loss] tar_ll 0.7232 loss -0.7232 (6.621 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.8676 loss -0.8676 (6.475 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 48800 lr 2.594e-04 [train_loss] tar_ll 0.7674 loss -0.7674 (6.372 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.7272 loss -0.7272 (6.725 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 49200 lr 2.563e-04 [train_loss] tar_ll 0.8506 loss -0.8506 (6.618 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.7889 loss -0.7889 (6.600 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 49600 lr 2.531e-04 [train_loss] tar_ll 0.8524 loss -0.8524 (6.562 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.8834 loss -0.8834 (6.595 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 50000 lr 2.500e-04 [train_loss] tar_ll 0.8742 loss -0.8742 (6.543 secs)\n",
      "100%|##########| 3000/3000 [00:30<00:00, 97.03it/s] \n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 0.5734 loss -0.5734 (30.921 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 50200 lr 2.484e-04 [train_loss] tar_ll 0.5398 loss -0.5398 (6.336 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 50400 lr 2.469e-04 [train_loss] tar_ll 0.5430 loss -0.5430 (6.434 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 50600 lr 2.453e-04 [train_loss] tar_ll 0.6000 loss -0.6000 (6.368 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 50800 lr 2.437e-04 [train_loss] tar_ll 0.7972 loss -0.7972 (6.419 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 51000 lr 2.421e-04 [train_loss] tar_ll 0.6792 loss -0.6792 (6.350 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 51200 lr 2.406e-04 [train_loss] tar_ll 0.7963 loss -0.7963 (6.331 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 51400 lr 2.390e-04 [train_loss] tar_ll 0.8821 loss -0.8821 (6.324 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 51600 lr 2.374e-04 [train_loss] tar_ll 0.8481 loss -0.8481 (6.345 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 51800 lr 2.359e-04 [train_loss] tar_ll 0.8176 loss -0.8176 (6.377 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 52000 lr 2.343e-04 [train_loss] tar_ll 0.8463 loss -0.8463 (6.316 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 52200 lr 2.327e-04 [train_loss] tar_ll 0.7946 loss -0.7946 (6.307 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 52400 lr 2.312e-04 [train_loss] tar_ll 0.8767 loss -0.8767 (6.607 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 52600 lr 2.296e-04 [train_loss] tar_ll 0.7411 loss -0.7411 (6.513 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 52800 lr 2.280e-04 [train_loss] tar_ll 0.6737 loss -0.6737 (6.866 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 53000 lr 2.265e-04 [train_loss] tar_ll 0.9638 loss -0.9638 (6.665 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 53200 lr 2.249e-04 [train_loss] tar_ll 0.8779 loss -0.8779 (6.687 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 53400 lr 2.233e-04 [train_loss] tar_ll 0.9379 loss -0.9379 (6.790 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 53600 lr 2.218e-04 [train_loss] tar_ll 0.8403 loss -0.8403 (6.651 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 53800 lr 2.202e-04 [train_loss] tar_ll 0.8716 loss -0.8716 (6.832 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 54000 lr 2.187e-04 [train_loss] tar_ll 0.7333 loss -0.7333 (6.801 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 54200 lr 2.171e-04 [train_loss] tar_ll 0.5257 loss -0.5257 (6.709 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 54400 lr 2.156e-04 [train_loss] tar_ll 0.7699 loss -0.7699 (6.596 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 54600 lr 2.140e-04 [train_loss] tar_ll 0.7470 loss -0.7470 (6.629 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 54800 lr 2.124e-04 [train_loss] tar_ll 0.8368 loss -0.8368 (6.696 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 55000 lr 2.109e-04 [train_loss] tar_ll 0.8625 loss -0.8625 (6.549 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 92.80it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 0.8730 loss -0.8730 (32.330 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 55200 lr 2.093e-04 [train_loss] tar_ll 0.6870 loss -0.6870 (7.137 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 55400 lr 2.078e-04 [train_loss] tar_ll 0.6650 loss -0.6650 (7.215 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 55600 lr 2.062e-04 [train_loss] tar_ll 0.8134 loss -0.8134 (6.617 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 55800 lr 2.047e-04 [train_loss] tar_ll 0.8132 loss -0.8132 (6.574 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 56000 lr 2.032e-04 [train_loss] tar_ll 0.8208 loss -0.8208 (6.795 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 56200 lr 2.016e-04 [train_loss] tar_ll 0.8035 loss -0.8035 (6.816 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 56400 lr 2.001e-04 [train_loss] tar_ll 0.8741 loss -0.8741 (6.915 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 56600 lr 1.985e-04 [train_loss] tar_ll 0.8536 loss -0.8536 (6.963 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 56800 lr 1.970e-04 [train_loss] tar_ll 0.9362 loss -0.9362 (6.571 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 57000 lr 1.955e-04 [train_loss] tar_ll 0.9451 loss -0.9451 (6.578 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 57200 lr 1.939e-04 [train_loss] tar_ll 0.8193 loss -0.8193 (6.466 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 57400 lr 1.924e-04 [train_loss] tar_ll 0.7423 loss -0.7423 (6.726 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 57600 lr 1.909e-04 [train_loss] tar_ll 0.7781 loss -0.7781 (6.723 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 57800 lr 1.894e-04 [train_loss] tar_ll 0.9010 loss -0.9010 (6.831 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 58000 lr 1.878e-04 [train_loss] tar_ll 0.8859 loss -0.8859 (6.544 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 58200 lr 1.863e-04 [train_loss] tar_ll 1.0124 loss -1.0124 (6.563 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 58400 lr 1.848e-04 [train_loss] tar_ll 1.0203 loss -1.0203 (6.462 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 58600 lr 1.833e-04 [train_loss] tar_ll 0.9599 loss -0.9599 (6.560 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 58800 lr 1.818e-04 [train_loss] tar_ll 0.8750 loss -0.8750 (6.518 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 59000 lr 1.803e-04 [train_loss] tar_ll 0.9269 loss -0.9269 (6.598 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 59200 lr 1.787e-04 [train_loss] tar_ll 0.8604 loss -0.8604 (6.726 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 59400 lr 1.772e-04 [train_loss] tar_ll 0.9960 loss -0.9960 (7.086 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 59600 lr 1.757e-04 [train_loss] tar_ll 0.9127 loss -0.9127 (6.704 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 59800 lr 1.742e-04 [train_loss] tar_ll 0.9247 loss -0.9247 (6.720 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 60000 lr 1.727e-04 [train_loss] tar_ll 0.6230 loss -0.6230 (6.933 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 92.33it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 0.7958 loss -0.7958 (32.496 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 60200 lr 1.713e-04 [train_loss] tar_ll 0.9063 loss -0.9063 (6.525 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 60400 lr 1.698e-04 [train_loss] tar_ll 0.7340 loss -0.7340 (6.629 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 60600 lr 1.683e-04 [train_loss] tar_ll 0.8708 loss -0.8708 (6.593 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 60800 lr 1.668e-04 [train_loss] tar_ll 0.8270 loss -0.8270 (6.527 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 61000 lr 1.653e-04 [train_loss] tar_ll 0.8053 loss -0.8053 (6.660 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 61200 lr 1.638e-04 [train_loss] tar_ll 0.8768 loss -0.8768 (6.850 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 61400 lr 1.624e-04 [train_loss] tar_ll 0.8982 loss -0.8982 (6.809 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 61600 lr 1.609e-04 [train_loss] tar_ll 0.8853 loss -0.8853 (6.635 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 61800 lr 1.594e-04 [train_loss] tar_ll 1.0438 loss -1.0438 (6.704 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 62000 lr 1.580e-04 [train_loss] tar_ll 0.9713 loss -0.9713 (6.610 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 62200 lr 1.565e-04 [train_loss] tar_ll 0.9337 loss -0.9337 (6.533 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 62400 lr 1.551e-04 [train_loss] tar_ll 0.9932 loss -0.9932 (6.484 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 62600 lr 1.536e-04 [train_loss] tar_ll 0.9508 loss -0.9508 (6.368 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 62800 lr 1.522e-04 [train_loss] tar_ll 1.0145 loss -1.0145 (6.506 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 63000 lr 1.507e-04 [train_loss] tar_ll 0.9926 loss -0.9926 (6.497 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 63200 lr 1.493e-04 [train_loss] tar_ll 0.8834 loss -0.8834 (6.368 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 63400 lr 1.478e-04 [train_loss] tar_ll 0.8698 loss -0.8698 (6.543 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 63600 lr 1.464e-04 [train_loss] tar_ll 0.8363 loss -0.8363 (6.581 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 63800 lr 1.450e-04 [train_loss] tar_ll 0.8799 loss -0.8799 (6.735 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 64000 lr 1.436e-04 [train_loss] tar_ll 0.8230 loss -0.8230 (6.479 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 64200 lr 1.421e-04 [train_loss] tar_ll 0.8938 loss -0.8938 (6.517 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 64400 lr 1.407e-04 [train_loss] tar_ll 0.9596 loss -0.9596 (6.419 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.0033 loss -1.0033 (6.617 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.0109 loss -1.0109 (6.545 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 65000 lr 1.365e-04 [train_loss] tar_ll 0.9189 loss -0.9189 (7.323 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.17it/s] \n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 1.0130 loss -1.0130 (33.647 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 65200 lr 1.351e-04 [train_loss] tar_ll 1.0308 loss -1.0308 (6.797 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 65400 lr 1.337e-04 [train_loss] tar_ll 0.9585 loss -0.9585 (7.028 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 65600 lr 1.323e-04 [train_loss] tar_ll 0.9738 loss -0.9738 (6.818 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 65800 lr 1.309e-04 [train_loss] tar_ll 0.9615 loss -0.9615 (6.592 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 66000 lr 1.296e-04 [train_loss] tar_ll 0.9997 loss -0.9997 (7.612 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 66200 lr 1.282e-04 [train_loss] tar_ll 0.8873 loss -0.8873 (13.208 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 66400 lr 1.268e-04 [train_loss] tar_ll 0.9942 loss -0.9942 (7.299 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.0166 loss -1.0166 (6.787 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 66800 lr 1.241e-04 [train_loss] tar_ll 0.9427 loss -0.9427 (7.081 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 67000 lr 1.227e-04 [train_loss] tar_ll 0.9505 loss -0.9505 (6.901 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 67200 lr 1.214e-04 [train_loss] tar_ll 0.9031 loss -0.9031 (6.487 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.0339 loss -1.0339 (6.673 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 67600 lr 1.187e-04 [train_loss] tar_ll 0.9979 loss -0.9979 (6.408 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.0156 loss -1.0156 (6.509 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.0426 loss -1.0426 (6.437 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.0902 loss -1.0902 (6.422 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 68400 lr 1.134e-04 [train_loss] tar_ll 0.9870 loss -0.9870 (6.790 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 68600 lr 1.121e-04 [train_loss] tar_ll 0.9504 loss -0.9504 (6.568 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.0299 loss -1.0299 (6.372 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 69000 lr 1.095e-04 [train_loss] tar_ll 0.9753 loss -0.9753 (6.491 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.0274 loss -1.0274 (6.444 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 69400 lr 1.069e-04 [train_loss] tar_ll 0.9862 loss -0.9862 (6.402 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.0263 loss -1.0263 (6.372 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.0649 loss -1.0649 (6.430 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.0216 loss -1.0216 (6.864 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 96.71it/s] \n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 1.0551 loss -1.0551 (31.022 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 70200 lr 1.018e-04 [train_loss] tar_ll 0.9810 loss -0.9810 (6.483 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.1075 loss -1.1075 (6.403 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.0784 loss -1.0784 (6.292 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.0400 loss -1.0400 (6.481 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.1091 loss -1.1091 (6.275 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.0203 loss -1.0203 (6.393 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.0979 loss -1.0979 (6.390 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.0170 loss -1.0170 (6.325 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 71800 lr 9.186e-05 [train_loss] tar_ll 0.9790 loss -0.9790 (6.573 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 72000 lr 9.064e-05 [train_loss] tar_ll 0.8965 loss -0.8965 (6.307 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.1157 loss -1.1157 (6.227 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.0424 loss -1.0424 (6.796 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.0453 loss -1.0453 (6.269 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.0268 loss -1.0268 (6.547 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.0123 loss -1.0123 (6.473 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.0885 loss -1.0885 (6.658 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.0423 loss -1.0423 (6.683 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.0550 loss -1.0550 (6.541 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.0155 loss -1.0155 (7.215 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.0687 loss -1.0687 (6.939 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.0264 loss -1.0264 (6.696 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.1335 loss -1.1335 (6.454 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.0305 loss -1.0305 (6.527 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.0796 loss -1.0796 (6.910 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1020 loss -1.1020 (7.035 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 94.72it/s] \n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 1.0957 loss -1.0957 (31.673 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1039 loss -1.1039 (6.701 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.1155 loss -1.1155 (6.546 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.0714 loss -1.0714 (6.798 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.0268 loss -1.0268 (7.206 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.1230 loss -1.1230 (7.065 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 76200 lr 6.669e-05 [train_loss] tar_ll 0.9763 loss -0.9763 (14.853 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.1142 loss -1.1142 (14.527 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.1153 loss -1.1153 (14.153 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.0324 loss -1.0324 (13.976 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.0287 loss -1.0287 (14.452 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.1360 loss -1.1360 (14.351 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.0720 loss -1.0720 (14.845 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.1177 loss -1.1177 (15.684 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.1262 loss -1.1262 (14.867 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.1510 loss -1.1510 (14.595 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.0647 loss -1.0647 (15.079 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.0823 loss -1.0823 (14.778 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.1097 loss -1.1097 (14.937 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.1455 loss -1.1455 (15.087 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.0583 loss -1.0583 (15.420 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.1234 loss -1.1234 (15.663 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.1197 loss -1.1197 (15.241 secs)\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp2', name='isanp2-num_latents-128_rbf_0',val_seed=0, val_l=128,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf48bb6d-04b2-4b86-a2dd-d0165186ff46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp2:isanp2-num_latents-128_rbf_0 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.1277 loss -1.1277 (15.669 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.0590 loss -1.0590 (14.770 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1225 loss -1.1225 (14.818 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.1174 loss -1.1174 (14.865 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.1142 loss -1.1142 (14.433 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.0512 loss -1.0512 (15.804 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.1275 loss -1.1275 (14.803 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.1313 loss -1.1313 (14.613 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.0757 loss -1.0757 (14.680 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.0660 loss -1.0660 (14.674 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.1644 loss -1.1644 (14.638 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.1065 loss -1.1065 (14.327 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.1224 loss -1.1224 (14.739 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.1328 loss -1.1328 (14.923 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.1881 loss -1.1881 (14.697 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.0342 loss -1.0342 (14.960 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.0959 loss -1.0959 (14.729 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.1925 loss -1.1925 (14.707 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.0997 loss -1.0997 (14.753 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.0932 loss -1.0932 (14.570 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.1095 loss -1.1095 (14.678 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.1197 loss -1.1197 (14.727 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.1151 loss -1.1151 (14.694 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.1249 loss -1.1249 (14.735 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.1332 loss -1.1332 (14.583 secs)\n",
      "100%|##########| 3000/3000 [01:09<00:00, 43.45it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 1.1488 loss -1.1488 (69.043 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.1897 loss -1.1897 (14.975 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.0800 loss -1.0800 (15.001 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.1391 loss -1.1391 (14.806 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.1452 loss -1.1452 (14.673 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.1736 loss -1.1736 (14.778 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.0452 loss -1.0452 (14.871 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.1799 loss -1.1799 (14.663 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.1124 loss -1.1124 (14.535 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.1714 loss -1.1714 (14.446 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.1711 loss -1.1711 (14.703 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.1683 loss -1.1683 (14.682 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.1686 loss -1.1686 (14.579 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.1330 loss -1.1330 (14.568 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.1315 loss -1.1315 (14.586 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.1074 loss -1.1074 (14.978 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.1083 loss -1.1083 (14.908 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.0612 loss -1.0612 (14.712 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.1353 loss -1.1353 (14.791 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.0993 loss -1.0993 (14.843 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.1313 loss -1.1313 (14.707 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.1371 loss -1.1371 (14.791 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.0793 loss -1.0793 (14.566 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.1060 loss -1.1060 (14.962 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.0904 loss -1.0904 (14.429 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.1037 loss -1.1037 (14.520 secs)\n",
      "100%|##########| 3000/3000 [01:09<00:00, 42.94it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 1.1594 loss -1.1594 (69.866 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.1360 loss -1.1360 (14.731 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.0887 loss -1.0887 (14.587 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.1543 loss -1.1543 (15.091 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.1718 loss -1.1718 (14.806 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.1898 loss -1.1898 (14.919 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.1555 loss -1.1555 (14.891 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.1958 loss -1.1958 (14.738 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.1291 loss -1.1291 (14.867 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.1494 loss -1.1494 (14.690 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.0695 loss -1.0695 (14.708 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.1047 loss -1.1047 (14.780 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.1753 loss -1.1753 (14.718 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.1719 loss -1.1719 (14.975 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.1406 loss -1.1406 (14.842 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.1425 loss -1.1425 (14.539 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.1548 loss -1.1548 (14.712 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.1032 loss -1.1032 (14.582 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.1012 loss -1.1012 (14.538 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.1486 loss -1.1486 (14.843 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.1208 loss -1.1208 (14.847 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.1415 loss -1.1415 (14.932 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.1896 loss -1.1896 (14.942 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.1158 loss -1.1158 (15.029 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.1780 loss -1.1780 (14.806 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.2533 loss -1.2533 (14.678 secs)\n",
      "100%|##########| 3000/3000 [01:10<00:00, 42.60it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 1.1681 loss -1.1681 (70.430 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.1474 loss -1.1474 (14.554 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.2199 loss -1.2199 (14.797 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.1781 loss -1.1781 (14.592 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.1414 loss -1.1414 (14.180 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.1621 loss -1.1621 (14.925 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2114 loss -1.2114 (14.916 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.0273 loss -1.0273 (14.874 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.1788 loss -1.1788 (14.967 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.0691 loss -1.0691 (14.841 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.1163 loss -1.1163 (14.798 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.1241 loss -1.1241 (14.747 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.0823 loss -1.0823 (14.817 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.1022 loss -1.1022 (14.846 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.1049 loss -1.1049 (14.622 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.1819 loss -1.1819 (14.641 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.1046 loss -1.1046 (14.862 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.0981 loss -1.0981 (14.614 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.1921 loss -1.1921 (14.638 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.2599 loss -1.2599 (14.584 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.1490 loss -1.1490 (14.522 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.0799 loss -1.0799 (14.750 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.2022 loss -1.2022 (14.988 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.0695 loss -1.0695 (14.886 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.0440 loss -1.0440 (15.026 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.1256 loss -1.1256 (14.872 secs)\n",
      "100%|##########| 3000/3000 [01:11<00:00, 42.10it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 1.1699 loss -1.1699 (71.269 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [01:09<00:00, 42.90it/s]\n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 1.1699 loss -1.1699 (69.933 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 rbf tar_ll 1.1699 loss -1.1699 (69.933 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1835463.625 miliseconds\n",
      "Execution time: 1835.463625 seconds\n",
      "Initial Memory Usage: 0.0 MB\n",
      "Final Memory Usage: 128.0048828125 MB\n",
      "Memory Usage Change: 128.0048828125 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp2', name='isanp2-num_latents-128_rbf_0',val_seed=0, val_l=128,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6697fc51-4497-416b-80d5-4869ae5f0f38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: isanp2-isanp2-num_latents-128_rbf_0\n",
      "Total number of parameters: 793282\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp2:isanp2-num_latents-128_rbf_0 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6653 loss 0.6653 (15.340 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 400 lr 5.000e-04 [train_loss] tar_ll -0.4760 loss 0.4760 (15.236 secs)\n",
      "isanp2:isanp2-num_latents-128_rbf_0 step 600 lr 5.000e-04 [train_loss] tar_ll -0.1722 loss 0.1722 (15.142 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 61467.62890625 miliseconds\n",
      "Execution time: 61.46762890625 seconds\n",
      "Initial Memory Usage: 16.25 MB\n",
      "Final Memory Usage: 118.8115234375 MB\n",
      "Memory Usage Change: 102.5615234375 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp2', name='isanp2-num_latents-128_rbf_0',val_seed=0, val_l=128,val_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1b0053-df57-4501-b7f8-dd94faffb23b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: isanp2-isanp2-num_latents-8_matern_0\n",
      "Total number of parameters: 785602\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp2:isanp2-num_latents-8_matern_0 step 200 lr 5.000e-04 [train_loss] tar_ll -0.7249 loss 0.7249 (15.850 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 400 lr 5.000e-04 [train_loss] tar_ll -0.5240 loss 0.5240 (14.460 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 600 lr 5.000e-04 [train_loss] tar_ll -0.2367 loss 0.2367 (14.194 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 800 lr 4.999e-04 [train_loss] tar_ll -0.1821 loss 0.1821 (14.139 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 1000 lr 4.999e-04 [train_loss] tar_ll -0.0833 loss 0.0833 (14.601 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 1200 lr 4.998e-04 [train_loss] tar_ll 0.0752 loss -0.0752 (14.359 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 1400 lr 4.998e-04 [train_loss] tar_ll 0.1478 loss -0.1478 (14.259 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 1600 lr 4.997e-04 [train_loss] tar_ll 0.2316 loss -0.2316 (14.425 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 1800 lr 4.996e-04 [train_loss] tar_ll 0.1962 loss -0.1962 (14.446 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 2000 lr 4.995e-04 [train_loss] tar_ll 0.2258 loss -0.2258 (14.646 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.3681 loss -0.3681 (14.886 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.3579 loss -0.3579 (14.650 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.3469 loss -0.3469 (14.668 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.3792 loss -0.3792 (14.553 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.4672 loss -0.4672 (14.238 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.3132 loss -0.3132 (14.328 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.2589 loss -0.2589 (14.354 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.2920 loss -0.2920 (14.192 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.3832 loss -0.3832 (14.172 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.3530 loss -0.3530 (14.201 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.4111 loss -0.4111 (14.884 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.4495 loss -0.4495 (14.426 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.5239 loss -0.5239 (14.339 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4099 loss -0.4099 (13.876 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.4197 loss -0.4197 (14.359 secs)\n",
      "100%|##########| 3000/3000 [01:08<00:00, 43.80it/s]\n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll -0.3388 loss 0.3388 (68.509 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.2784 loss -0.2784 (14.576 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.1032 loss -0.1032 (14.670 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.1166 loss -0.1166 (14.439 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.3472 loss -0.3472 (14.475 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.4833 loss -0.4833 (14.501 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.2625 loss -0.2625 (14.450 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.2157 loss -0.2157 (14.338 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.1316 loss -0.1316 (14.239 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.2413 loss -0.2413 (14.469 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.2919 loss -0.2919 (14.715 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.4648 loss -0.4648 (14.665 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.4415 loss -0.4415 (14.449 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.4464 loss -0.4464 (14.455 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.5188 loss -0.5188 (14.578 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.5516 loss -0.5516 (14.526 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.5885 loss -0.5885 (14.600 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.5374 loss -0.5374 (14.622 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.5730 loss -0.5730 (14.149 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.5747 loss -0.5747 (14.362 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.5558 loss -0.5558 (14.292 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.6307 loss -0.6307 (14.583 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.6174 loss -0.6174 (14.539 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.6137 loss -0.6137 (14.493 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.6139 loss -0.6139 (14.420 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.6062 loss -0.6062 (14.328 secs)\n",
      "100%|##########| 3000/3000 [01:08<00:00, 43.91it/s]\n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.2886 loss -0.2886 (68.328 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.6124 loss -0.6124 (14.949 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.6125 loss -0.6125 (14.934 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.6634 loss -0.6634 (15.152 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.6070 loss -0.6070 (14.903 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.4362 loss -0.4362 (14.645 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 11200 lr 4.847e-04 [train_loss] tar_ll 0.6494 loss -0.6494 (14.623 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.5701 loss -0.5701 (14.593 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.6378 loss -0.6378 (14.396 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.5924 loss -0.5924 (14.359 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.5903 loss -0.5903 (14.578 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.6771 loss -0.6771 (14.463 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.6551 loss -0.6551 (14.352 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.6253 loss -0.6253 (14.452 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.5650 loss -0.5650 (14.164 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.7065 loss -0.7065 (14.358 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.5999 loss -0.5999 (14.919 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.6497 loss -0.6497 (14.686 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.6137 loss -0.6137 (14.619 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.6557 loss -0.6557 (14.719 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.1981 loss -0.1981 (14.349 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 14200 lr 4.755e-04 [train_loss] tar_ll 0.0395 loss -0.0395 (14.678 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.3030 loss -0.3030 (14.299 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.4648 loss -0.4648 (14.486 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.5768 loss -0.5768 (14.479 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.5558 loss -0.5558 (14.337 secs)\n",
      "100%|##########| 3000/3000 [01:08<00:00, 43.52it/s]\n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.5003 loss -0.5003 (68.929 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.4727 loss -0.4727 (14.202 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.4844 loss -0.4844 (14.797 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.7274 loss -0.7274 (14.870 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.6630 loss -0.6630 (14.588 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.6975 loss -0.6975 (14.773 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.6268 loss -0.6268 (14.662 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.4038 loss -0.4038 (14.676 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.6907 loss -0.6907 (14.853 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.5896 loss -0.5896 (13.988 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.3982 loss -0.3982 (14.373 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.7144 loss -0.7144 (14.344 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.6791 loss -0.6791 (14.801 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.6692 loss -0.6692 (14.612 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.6616 loss -0.6616 (14.507 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.4360 loss -0.4360 (14.334 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.5975 loss -0.5975 (14.470 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.4851 loss -0.4851 (14.374 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.6939 loss -0.6939 (14.668 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.5629 loss -0.5629 (14.616 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.5766 loss -0.5766 (14.680 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.5840 loss -0.5840 (14.631 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.2718 loss -0.2718 (14.742 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.5484 loss -0.5484 (14.581 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.6228 loss -0.6228 (14.554 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.6628 loss -0.6628 (14.379 secs)\n",
      "100%|##########| 3000/3000 [01:08<00:00, 43.99it/s]\n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll -0.1776 loss 0.1776 (68.195 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.5895 loss -0.5895 (14.287 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.6083 loss -0.6083 (14.441 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.6286 loss -0.6286 (14.200 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.2354 loss -0.2354 (14.044 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.2529 loss -0.2529 (14.477 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.3213 loss -0.3213 (14.702 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.2841 loss -0.2841 (14.678 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.4272 loss -0.4272 (14.797 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.4420 loss -0.4420 (14.794 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.4273 loss -0.4273 (14.803 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.5708 loss -0.5708 (14.612 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.4773 loss -0.4773 (14.577 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.4966 loss -0.4966 (14.533 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.5334 loss -0.5334 (14.389 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.4867 loss -0.4867 (14.539 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.3229 loss -0.3229 (14.541 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.3551 loss -0.3551 (14.621 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.4602 loss -0.4602 (14.602 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.3670 loss -0.3670 (14.455 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.5654 loss -0.5654 (14.300 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.5716 loss -0.5716 (14.398 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.5334 loss -0.5334 (14.504 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.7369 loss -0.7369 (14.467 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.6375 loss -0.6375 (14.735 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.7714 loss -0.7714 (14.454 secs)\n",
      "100%|##########| 3000/3000 [01:09<00:00, 43.01it/s]\n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.4644 loss -0.4644 (69.749 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.5823 loss -0.5823 (14.455 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.4209 loss -0.4209 (14.386 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.5047 loss -0.5047 (14.457 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.4288 loss -0.4288 (14.350 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.2360 loss -0.2360 (14.475 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.4741 loss -0.4741 (14.529 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.5022 loss -0.5022 (14.481 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.5610 loss -0.5610 (14.558 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.4942 loss -0.4942 (14.600 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.6166 loss -0.6166 (14.940 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.2878 loss -0.2878 (14.931 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.4298 loss -0.4298 (14.854 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.4740 loss -0.4740 (14.791 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.5744 loss -0.5744 (14.653 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.3957 loss -0.3957 (14.776 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.2354 loss -0.2354 (14.542 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.1637 loss -0.1637 (14.375 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.3799 loss -0.3799 (14.439 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.1922 loss -0.1922 (14.514 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.3561 loss -0.3561 (14.657 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.5369 loss -0.5369 (14.481 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.5224 loss -0.5224 (14.621 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.5178 loss -0.5178 (14.270 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.5470 loss -0.5470 (14.136 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.5606 loss -0.5606 (14.912 secs)\n",
      "100%|##########| 3000/3000 [01:09<00:00, 43.10it/s]\n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.4373 loss -0.4373 (69.613 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.6288 loss -0.6288 (14.343 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.6326 loss -0.6326 (14.160 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.7201 loss -0.7201 (14.552 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.2405 loss -0.2405 (14.445 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.5488 loss -0.5488 (14.408 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.3993 loss -0.3993 (14.311 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.5818 loss -0.5818 (14.513 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.6242 loss -0.6242 (14.475 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.5890 loss -0.5890 (14.666 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.6883 loss -0.6883 (14.297 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.5342 loss -0.5342 (14.590 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.5977 loss -0.5977 (14.630 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.6258 loss -0.6258 (14.683 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.8199 loss -0.8199 (14.610 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.3916 loss -0.3916 (14.547 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.4563 loss -0.4563 (14.543 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.6760 loss -0.6760 (14.887 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.6774 loss -0.6774 (14.298 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.6327 loss -0.6327 (14.332 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.4332 loss -0.4332 (14.209 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.6687 loss -0.6687 (14.636 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.6860 loss -0.6860 (14.720 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.7647 loss -0.7647 (14.428 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.6535 loss -0.6535 (14.433 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.7579 loss -0.7579 (14.585 secs)\n",
      "100%|##########| 3000/3000 [01:10<00:00, 42.71it/s]\n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.5588 loss -0.5588 (70.250 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.4775 loss -0.4775 (14.724 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.6221 loss -0.6221 (14.810 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.5862 loss -0.5862 (14.692 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.6127 loss -0.6127 (14.425 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.1701 loss -0.1701 (14.521 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.5101 loss -0.5101 (14.539 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.5711 loss -0.5711 (14.223 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.3783 loss -0.3783 (14.654 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.4971 loss -0.4971 (14.311 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.6518 loss -0.6518 (14.595 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.6012 loss -0.6012 (14.528 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.5566 loss -0.5566 (15.513 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.4724 loss -0.4724 (14.223 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.4503 loss -0.4503 (14.465 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.5806 loss -0.5806 (14.572 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.4116 loss -0.4116 (15.648 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.5476 loss -0.5476 (15.230 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.6541 loss -0.6541 (14.225 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.7834 loss -0.7834 (14.384 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.6692 loss -0.6692 (14.350 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.4823 loss -0.4823 (14.490 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.2980 loss -0.2980 (14.467 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.6677 loss -0.6677 (14.352 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.2537 loss -0.2537 (14.393 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.6002 loss -0.6002 (14.313 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.26it/s]\n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.3841 loss -0.3841 (66.280 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.4540 loss -0.4540 (14.272 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.4559 loss -0.4559 (14.104 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.6201 loss -0.6201 (14.144 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.7804 loss -0.7804 (14.287 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.6655 loss -0.6655 (14.480 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.6957 loss -0.6957 (14.348 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.6663 loss -0.6663 (14.010 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.6122 loss -0.6122 (14.128 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.6156 loss -0.6156 (14.107 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.5498 loss -0.5498 (14.207 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.8244 loss -0.8244 (14.429 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.8016 loss -0.8016 (14.015 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 42600 lr 3.076e-04 [train_loss] tar_ll 0.6595 loss -0.6595 (14.259 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.6503 loss -0.6503 (13.987 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.7810 loss -0.7810 (14.090 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.5669 loss -0.5669 (14.198 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.7084 loss -0.7084 (13.828 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.5183 loss -0.5183 (14.301 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.8560 loss -0.8560 (14.245 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.6567 loss -0.6567 (14.347 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.5806 loss -0.5806 (14.445 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.6968 loss -0.6968 (14.250 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.8008 loss -0.8008 (14.478 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.7323 loss -0.7323 (14.458 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.8196 loss -0.8196 (14.759 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.20it/s]\n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.5715 loss -0.5715 (67.876 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.6258 loss -0.6258 (14.179 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.5943 loss -0.5943 (14.273 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.8026 loss -0.8026 (14.318 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.7374 loss -0.7374 (14.353 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.5679 loss -0.5679 (14.490 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.4004 loss -0.4004 (14.463 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.7044 loss -0.7044 (14.587 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.8433 loss -0.8433 (14.473 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.8486 loss -0.8486 (14.480 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.7056 loss -0.7056 (14.351 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.5990 loss -0.5990 (14.649 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.6993 loss -0.6993 (14.423 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 47600 lr 2.688e-04 [train_loss] tar_ll 0.2670 loss -0.2670 (14.626 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.0785 loss -0.0785 (14.393 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.3874 loss -0.3874 (14.640 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.4840 loss -0.4840 (14.302 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 48400 lr 2.626e-04 [train_loss] tar_ll 0.5988 loss -0.5988 (14.311 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.7292 loss -0.7292 (14.283 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 48800 lr 2.594e-04 [train_loss] tar_ll 0.7749 loss -0.7749 (14.063 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.6480 loss -0.6480 (14.153 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 49200 lr 2.563e-04 [train_loss] tar_ll 0.7093 loss -0.7093 (14.068 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.7500 loss -0.7500 (14.415 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 49600 lr 2.531e-04 [train_loss] tar_ll 0.7022 loss -0.7022 (14.489 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.4397 loss -0.4397 (14.438 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 50000 lr 2.500e-04 [train_loss] tar_ll 0.5535 loss -0.5535 (14.592 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.37it/s]\n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.4608 loss -0.4608 (67.617 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 50200 lr 2.484e-04 [train_loss] tar_ll 0.6767 loss -0.6767 (14.497 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 50400 lr 2.469e-04 [train_loss] tar_ll 0.7290 loss -0.7290 (14.449 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 50600 lr 2.453e-04 [train_loss] tar_ll 0.7613 loss -0.7613 (14.386 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 50800 lr 2.437e-04 [train_loss] tar_ll 0.7767 loss -0.7767 (14.543 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 51000 lr 2.421e-04 [train_loss] tar_ll 0.8381 loss -0.8381 (14.328 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 51200 lr 2.406e-04 [train_loss] tar_ll 0.6513 loss -0.6513 (14.418 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 51400 lr 2.390e-04 [train_loss] tar_ll 0.9658 loss -0.9658 (13.613 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 51600 lr 2.374e-04 [train_loss] tar_ll 0.8308 loss -0.8308 (14.469 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 51800 lr 2.359e-04 [train_loss] tar_ll 0.8722 loss -0.8722 (14.781 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 52000 lr 2.343e-04 [train_loss] tar_ll 0.8233 loss -0.8233 (14.496 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 52200 lr 2.327e-04 [train_loss] tar_ll 0.7326 loss -0.7326 (14.867 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 52400 lr 2.312e-04 [train_loss] tar_ll 0.7459 loss -0.7459 (14.759 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 52600 lr 2.296e-04 [train_loss] tar_ll 0.7099 loss -0.7099 (14.446 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 52800 lr 2.280e-04 [train_loss] tar_ll 0.6013 loss -0.6013 (14.726 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 53000 lr 2.265e-04 [train_loss] tar_ll 0.6063 loss -0.6063 (14.062 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 53200 lr 2.249e-04 [train_loss] tar_ll 0.7230 loss -0.7230 (14.561 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 53400 lr 2.233e-04 [train_loss] tar_ll 0.7447 loss -0.7447 (14.499 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 53600 lr 2.218e-04 [train_loss] tar_ll 0.7481 loss -0.7481 (14.272 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 53800 lr 2.202e-04 [train_loss] tar_ll 0.7862 loss -0.7862 (14.495 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 54000 lr 2.187e-04 [train_loss] tar_ll 0.8942 loss -0.8942 (14.467 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 54200 lr 2.171e-04 [train_loss] tar_ll 0.8872 loss -0.8872 (14.472 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 54400 lr 2.156e-04 [train_loss] tar_ll 0.8275 loss -0.8275 (14.458 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 54600 lr 2.140e-04 [train_loss] tar_ll 0.7426 loss -0.7426 (13.739 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 54800 lr 2.124e-04 [train_loss] tar_ll 0.8122 loss -0.8122 (13.893 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 55000 lr 2.109e-04 [train_loss] tar_ll 0.8405 loss -0.8405 (14.335 secs)\n",
      "100%|##########| 3000/3000 [01:08<00:00, 43.66it/s]\n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.5074 loss -0.5074 (68.718 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 55200 lr 2.093e-04 [train_loss] tar_ll 0.8234 loss -0.8234 (14.594 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 55400 lr 2.078e-04 [train_loss] tar_ll 0.8273 loss -0.8273 (14.501 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 55600 lr 2.062e-04 [train_loss] tar_ll 0.7787 loss -0.7787 (14.378 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 55800 lr 2.047e-04 [train_loss] tar_ll 0.8975 loss -0.8975 (14.084 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 56000 lr 2.032e-04 [train_loss] tar_ll 0.8553 loss -0.8553 (14.062 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 56200 lr 2.016e-04 [train_loss] tar_ll 0.6110 loss -0.6110 (14.466 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 56400 lr 2.001e-04 [train_loss] tar_ll 0.8522 loss -0.8522 (14.219 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 56600 lr 1.985e-04 [train_loss] tar_ll 0.7855 loss -0.7855 (14.109 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 56800 lr 1.970e-04 [train_loss] tar_ll 0.6927 loss -0.6927 (14.254 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 57000 lr 1.955e-04 [train_loss] tar_ll 0.8381 loss -0.8381 (14.192 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 57200 lr 1.939e-04 [train_loss] tar_ll 0.9118 loss -0.9118 (14.510 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 57400 lr 1.924e-04 [train_loss] tar_ll 0.8602 loss -0.8602 (14.715 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 57600 lr 1.909e-04 [train_loss] tar_ll 0.7209 loss -0.7209 (14.463 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 57800 lr 1.894e-04 [train_loss] tar_ll 0.8834 loss -0.8834 (14.310 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 58000 lr 1.878e-04 [train_loss] tar_ll 0.9068 loss -0.9068 (14.488 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 58200 lr 1.863e-04 [train_loss] tar_ll 0.9173 loss -0.9173 (14.594 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 58400 lr 1.848e-04 [train_loss] tar_ll 0.6713 loss -0.6713 (14.610 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 58600 lr 1.833e-04 [train_loss] tar_ll 0.6637 loss -0.6637 (14.389 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 58800 lr 1.818e-04 [train_loss] tar_ll 0.7275 loss -0.7275 (14.341 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 59000 lr 1.803e-04 [train_loss] tar_ll 0.7664 loss -0.7664 (14.556 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 59200 lr 1.787e-04 [train_loss] tar_ll 0.7832 loss -0.7832 (14.262 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 59400 lr 1.772e-04 [train_loss] tar_ll 0.7337 loss -0.7337 (14.216 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 59600 lr 1.757e-04 [train_loss] tar_ll 0.9087 loss -0.9087 (14.314 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 59800 lr 1.742e-04 [train_loss] tar_ll 0.8971 loss -0.8971 (14.218 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 60000 lr 1.727e-04 [train_loss] tar_ll 0.9168 loss -0.9168 (14.207 secs)\n",
      "100%|##########| 3000/3000 [01:05<00:00, 45.49it/s]\n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.6877 loss -0.6877 (65.946 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 60200 lr 1.713e-04 [train_loss] tar_ll 0.9839 loss -0.9839 (14.443 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 60400 lr 1.698e-04 [train_loss] tar_ll 0.9347 loss -0.9347 (14.383 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 60600 lr 1.683e-04 [train_loss] tar_ll 0.9430 loss -0.9430 (14.252 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.0052 loss -1.0052 (14.565 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 61000 lr 1.653e-04 [train_loss] tar_ll 0.8115 loss -0.8115 (14.359 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 61200 lr 1.638e-04 [train_loss] tar_ll 0.8473 loss -0.8473 (14.156 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 61400 lr 1.624e-04 [train_loss] tar_ll 0.9235 loss -0.9235 (14.140 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 61600 lr 1.609e-04 [train_loss] tar_ll 0.9481 loss -0.9481 (13.988 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 61800 lr 1.594e-04 [train_loss] tar_ll 0.9059 loss -0.9059 (14.240 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 62000 lr 1.580e-04 [train_loss] tar_ll 0.8226 loss -0.8226 (14.253 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 62200 lr 1.565e-04 [train_loss] tar_ll 0.8731 loss -0.8731 (14.396 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 62400 lr 1.551e-04 [train_loss] tar_ll 0.9506 loss -0.9506 (14.346 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 62600 lr 1.536e-04 [train_loss] tar_ll 0.9253 loss -0.9253 (14.414 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 62800 lr 1.522e-04 [train_loss] tar_ll 0.8011 loss -0.8011 (13.851 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.0124 loss -1.0124 (14.059 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 63200 lr 1.493e-04 [train_loss] tar_ll 0.9296 loss -0.9296 (15.159 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 63400 lr 1.478e-04 [train_loss] tar_ll 0.3147 loss -0.3147 (14.390 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 63600 lr 1.464e-04 [train_loss] tar_ll 0.6942 loss -0.6942 (14.847 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 63800 lr 1.450e-04 [train_loss] tar_ll 0.8321 loss -0.8321 (14.457 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 64000 lr 1.436e-04 [train_loss] tar_ll 0.8795 loss -0.8795 (14.454 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 64200 lr 1.421e-04 [train_loss] tar_ll 0.8541 loss -0.8541 (15.205 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 64400 lr 1.407e-04 [train_loss] tar_ll 0.8292 loss -0.8292 (15.515 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 64600 lr 1.393e-04 [train_loss] tar_ll 0.8311 loss -0.8311 (8.583 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 64800 lr 1.379e-04 [train_loss] tar_ll 0.8754 loss -0.8754 (6.983 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 65000 lr 1.365e-04 [train_loss] tar_ll 0.9501 loss -0.9501 (6.964 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 93.36it/s] \n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.5657 loss -0.5657 (32.136 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 65200 lr 1.351e-04 [train_loss] tar_ll 0.8757 loss -0.8757 (7.229 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 65400 lr 1.337e-04 [train_loss] tar_ll 0.8808 loss -0.8808 (6.863 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 65600 lr 1.323e-04 [train_loss] tar_ll 0.9663 loss -0.9663 (6.594 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 65800 lr 1.309e-04 [train_loss] tar_ll 0.5623 loss -0.5623 (7.007 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 66000 lr 1.296e-04 [train_loss] tar_ll 0.7526 loss -0.7526 (6.853 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 66200 lr 1.282e-04 [train_loss] tar_ll 0.8801 loss -0.8801 (7.253 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 66400 lr 1.268e-04 [train_loss] tar_ll 0.8650 loss -0.8650 (7.315 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.0096 loss -1.0096 (7.009 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 66800 lr 1.241e-04 [train_loss] tar_ll 0.9247 loss -0.9247 (6.765 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 67000 lr 1.227e-04 [train_loss] tar_ll 0.8097 loss -0.8097 (6.710 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 67200 lr 1.214e-04 [train_loss] tar_ll 0.9994 loss -0.9994 (7.198 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.0260 loss -1.0260 (7.126 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 67600 lr 1.187e-04 [train_loss] tar_ll 0.9782 loss -0.9782 (7.120 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.0215 loss -1.0215 (6.934 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 68000 lr 1.160e-04 [train_loss] tar_ll 0.9621 loss -0.9621 (7.003 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 68200 lr 1.147e-04 [train_loss] tar_ll 0.8595 loss -0.8595 (6.789 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 68400 lr 1.134e-04 [train_loss] tar_ll 1.0079 loss -1.0079 (6.352 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 68600 lr 1.121e-04 [train_loss] tar_ll 0.8695 loss -0.8695 (6.455 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 68800 lr 1.108e-04 [train_loss] tar_ll 0.9321 loss -0.9321 (6.374 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 69000 lr 1.095e-04 [train_loss] tar_ll 0.9319 loss -0.9319 (6.565 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 69200 lr 1.082e-04 [train_loss] tar_ll 0.7596 loss -0.7596 (6.513 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 69400 lr 1.069e-04 [train_loss] tar_ll 0.8336 loss -0.8336 (6.540 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 69600 lr 1.056e-04 [train_loss] tar_ll 0.9113 loss -0.9113 (6.466 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 69800 lr 1.043e-04 [train_loss] tar_ll 0.9972 loss -0.9972 (6.445 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 70000 lr 1.031e-04 [train_loss] tar_ll 0.9602 loss -0.9602 (6.610 secs)\n",
      "100%|##########| 3000/3000 [00:30<00:00, 96.84it/s] \n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.6947 loss -0.6947 (30.981 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 70200 lr 1.018e-04 [train_loss] tar_ll 0.8488 loss -0.8488 (6.756 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.0343 loss -1.0343 (6.597 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 70600 lr 9.927e-05 [train_loss] tar_ll 0.9581 loss -0.9581 (6.583 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 70800 lr 9.802e-05 [train_loss] tar_ll 0.9496 loss -0.9496 (6.598 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 71000 lr 9.677e-05 [train_loss] tar_ll 0.9220 loss -0.9220 (6.557 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 71200 lr 9.554e-05 [train_loss] tar_ll 0.8323 loss -0.8323 (6.537 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 71400 lr 9.430e-05 [train_loss] tar_ll 0.8417 loss -0.8417 (6.666 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 71600 lr 9.308e-05 [train_loss] tar_ll 0.8761 loss -0.8761 (6.590 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 71800 lr 9.186e-05 [train_loss] tar_ll 0.9121 loss -0.9121 (6.694 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 72000 lr 9.064e-05 [train_loss] tar_ll 0.9440 loss -0.9440 (6.700 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.0260 loss -1.0260 (6.681 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 72400 lr 8.824e-05 [train_loss] tar_ll 0.9604 loss -0.9604 (6.731 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 72600 lr 8.704e-05 [train_loss] tar_ll 0.9423 loss -0.9423 (6.579 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 72800 lr 8.585e-05 [train_loss] tar_ll 0.9621 loss -0.9621 (6.703 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 73000 lr 8.467e-05 [train_loss] tar_ll 0.9417 loss -0.9417 (6.715 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 73200 lr 8.350e-05 [train_loss] tar_ll 0.9949 loss -0.9949 (6.720 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 73400 lr 8.233e-05 [train_loss] tar_ll 0.9479 loss -0.9479 (6.712 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.0409 loss -1.0409 (6.626 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.0776 loss -1.0776 (6.903 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 74000 lr 7.886e-05 [train_loss] tar_ll 0.9710 loss -0.9710 (6.648 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 74200 lr 7.772e-05 [train_loss] tar_ll 0.9638 loss -0.9638 (6.579 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 74400 lr 7.659e-05 [train_loss] tar_ll 0.9509 loss -0.9509 (6.681 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 74600 lr 7.546e-05 [train_loss] tar_ll 0.9675 loss -0.9675 (6.597 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.0669 loss -1.0669 (6.574 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 75000 lr 7.322e-05 [train_loss] tar_ll 0.9603 loss -0.9603 (6.516 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 96.49it/s] \n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.6927 loss -0.6927 (31.095 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.0057 loss -1.0057 (6.745 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 75400 lr 7.102e-05 [train_loss] tar_ll 0.9672 loss -0.9672 (6.800 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 75600 lr 6.992e-05 [train_loss] tar_ll 0.9846 loss -0.9846 (6.736 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.0316 loss -1.0316 (6.769 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.0254 loss -1.0254 (6.664 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.0454 loss -1.0454 (6.824 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.0538 loss -1.0538 (6.846 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.0176 loss -1.0176 (6.954 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 76800 lr 6.351e-05 [train_loss] tar_ll 0.9572 loss -0.9572 (6.934 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.0551 loss -1.0551 (6.853 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 77200 lr 6.144e-05 [train_loss] tar_ll 0.9714 loss -0.9714 (6.685 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.0536 loss -1.0536 (6.393 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 77600 lr 5.939e-05 [train_loss] tar_ll 0.9264 loss -0.9264 (6.523 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.0932 loss -1.0932 (6.373 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 78000 lr 5.737e-05 [train_loss] tar_ll 0.9635 loss -0.9635 (6.428 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.0453 loss -1.0453 (6.662 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 78400 lr 5.538e-05 [train_loss] tar_ll 0.9780 loss -0.9780 (6.490 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.0312 loss -1.0312 (6.558 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.0378 loss -1.0378 (6.511 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.0845 loss -1.0845 (6.500 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.0258 loss -1.0258 (7.002 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.0464 loss -1.0464 (7.007 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.0859 loss -1.0859 (6.992 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.0740 loss -1.0740 (6.898 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.0632 loss -1.0632 (6.798 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 94.13it/s] \n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.7440 loss -0.7440 (31.872 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.1359 loss -1.1359 (6.507 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.0214 loss -1.0214 (6.477 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.0177 loss -1.0177 (6.566 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 80800 lr 4.412e-05 [train_loss] tar_ll 0.9856 loss -0.9856 (6.473 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.0617 loss -1.0617 (6.598 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.0483 loss -1.0483 (6.545 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.1293 loss -1.1293 (6.560 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.0766 loss -1.0766 (6.693 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.1105 loss -1.1105 (6.606 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.0821 loss -1.0821 (6.709 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.0534 loss -1.0534 (6.785 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.0904 loss -1.0904 (6.690 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.0193 loss -1.0193 (6.643 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.0459 loss -1.0459 (6.806 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.0956 loss -1.0956 (6.709 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.0067 loss -1.0067 (6.912 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.0989 loss -1.0989 (6.690 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.0728 loss -1.0728 (6.589 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.0561 loss -1.0561 (6.689 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.0131 loss -1.0131 (6.701 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.1003 loss -1.1003 (6.688 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.0753 loss -1.0753 (6.821 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.1646 loss -1.1646 (6.730 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.0968 loss -1.0968 (6.822 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.0705 loss -1.0705 (6.745 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 94.21it/s]\n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.7440 loss -0.7440 (31.846 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.0740 loss -1.0740 (7.016 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.1134 loss -1.1134 (7.014 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.1353 loss -1.1353 (7.006 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.1147 loss -1.1147 (6.994 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.1062 loss -1.1062 (6.656 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.1414 loss -1.1414 (6.670 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 86400 lr 2.247e-05 [train_loss] tar_ll 0.9953 loss -0.9953 (6.554 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.1332 loss -1.1332 (6.470 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.1148 loss -1.1148 (6.520 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.0979 loss -1.0979 (6.652 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.0608 loss -1.0608 (6.648 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.1369 loss -1.1369 (6.647 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.0124 loss -1.0124 (6.647 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.0857 loss -1.0857 (6.692 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.0097 loss -1.0097 (6.781 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.0988 loss -1.0988 (7.105 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.0708 loss -1.0708 (6.877 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.0795 loss -1.0795 (7.029 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.1540 loss -1.1540 (7.082 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.1482 loss -1.1482 (6.770 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.1070 loss -1.1070 (6.588 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.1498 loss -1.1498 (6.521 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.1695 loss -1.1695 (6.635 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.0418 loss -1.0418 (6.485 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.0564 loss -1.0564 (6.630 secs)\n",
      "100%|##########| 3000/3000 [00:30<00:00, 97.85it/s] \n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.7425 loss -0.7425 (30.660 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.1405 loss -1.1405 (6.583 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 90400 lr 1.128e-05 [train_loss] tar_ll 0.9519 loss -0.9519 (6.607 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 90600 lr 1.082e-05 [train_loss] tar_ll 0.9619 loss -0.9619 (6.744 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.1176 loss -1.1176 (6.702 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.0896 loss -1.0896 (6.732 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.1952 loss -1.1952 (6.654 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.2077 loss -1.2077 (6.545 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.0393 loss -1.0393 (6.794 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.1777 loss -1.1777 (6.525 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.1437 loss -1.1437 (6.638 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.0567 loss -1.0567 (6.677 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.1523 loss -1.1523 (6.622 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.0787 loss -1.0787 (6.589 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.0534 loss -1.0534 (6.590 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.1536 loss -1.1536 (6.663 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.1075 loss -1.1075 (6.798 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.0280 loss -1.0280 (6.965 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.1091 loss -1.1091 (6.802 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.0387 loss -1.0387 (6.719 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.1232 loss -1.1232 (6.817 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.1273 loss -1.1273 (6.785 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.1591 loss -1.1591 (6.808 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.0707 loss -1.0707 (6.865 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.0576 loss -1.0576 (6.800 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.0729 loss -1.0729 (7.076 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 94.93it/s] \n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.7709 loss -0.7709 (31.605 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.1211 loss -1.1211 (6.572 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.0691 loss -1.0691 (6.550 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.0641 loss -1.0641 (6.384 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.1902 loss -1.1902 (6.601 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.0815 loss -1.0815 (6.495 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.1649 loss -1.1649 (6.707 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.0536 loss -1.0536 (6.499 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.1014 loss -1.1014 (6.621 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.1805 loss -1.1805 (6.707 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.0869 loss -1.0869 (6.469 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.0703 loss -1.0703 (6.764 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.1315 loss -1.1315 (6.623 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.1119 loss -1.1119 (6.521 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.1051 loss -1.1051 (6.770 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.0947 loss -1.0947 (6.551 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.1029 loss -1.1029 (6.998 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.0216 loss -1.0216 (6.714 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.1115 loss -1.1115 (6.483 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.1954 loss -1.1954 (6.515 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.0915 loss -1.0915 (6.471 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.1154 loss -1.1154 (6.620 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.1031 loss -1.1031 (6.730 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.1444 loss -1.1444 (6.866 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.1120 loss -1.1120 (6.623 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.0563 loss -1.0563 (6.484 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 93.68it/s] \n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.7646 loss -0.7646 (32.026 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:34<00:00, 85.80it/s]\n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.7646 loss -0.7646 (34.968 secs)\n",
      "isanp2:isanp2-num_latents-8_matern_0 matern tar_ll 0.7646 loss -0.7646 (34.968 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 6985351.5 miliseconds\n",
      "Execution time: 6985.3515 seconds\n",
      "Initial Memory Usage: 0.0 MB\n",
      "Final Memory Usage: 55.94873046875 MB\n",
      "Memory Usage Change: 55.94873046875 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp2', name='isanp2-num_latents-8_matern_0',val_seed=0, val_l=8,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd4ade7-1f6f-4127-bf29-786490d80f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: isanp2-isanp2-num_latents-128_matern_0\n",
      "Total number of parameters: 793282\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp2:isanp2-num_latents-128_matern_0 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6798 loss 0.6798 (15.248 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 400 lr 5.000e-04 [train_loss] tar_ll -0.4515 loss 0.4515 (14.230 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 600 lr 5.000e-04 [train_loss] tar_ll -0.1402 loss 0.1402 (8.299 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 800 lr 4.999e-04 [train_loss] tar_ll -0.1146 loss 0.1146 (7.315 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 1000 lr 4.999e-04 [train_loss] tar_ll 0.0105 loss -0.0105 (6.729 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 1200 lr 4.998e-04 [train_loss] tar_ll 0.0910 loss -0.0910 (6.848 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 1400 lr 4.998e-04 [train_loss] tar_ll 0.2059 loss -0.2059 (8.354 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 1600 lr 4.997e-04 [train_loss] tar_ll 0.2245 loss -0.2245 (7.880 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 1800 lr 4.996e-04 [train_loss] tar_ll 0.2064 loss -0.2064 (7.113 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 2000 lr 4.995e-04 [train_loss] tar_ll 0.2678 loss -0.2678 (8.278 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 2200 lr 4.994e-04 [train_loss] tar_ll 0.3812 loss -0.3812 (7.974 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 2400 lr 4.993e-04 [train_loss] tar_ll 0.3739 loss -0.3739 (7.698 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 2600 lr 4.992e-04 [train_loss] tar_ll 0.3474 loss -0.3474 (7.118 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 2800 lr 4.990e-04 [train_loss] tar_ll 0.3786 loss -0.3786 (7.883 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 3000 lr 4.989e-04 [train_loss] tar_ll 0.4147 loss -0.4147 (7.565 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 3200 lr 4.987e-04 [train_loss] tar_ll 0.2603 loss -0.2603 (7.100 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 3400 lr 4.986e-04 [train_loss] tar_ll 0.3932 loss -0.3932 (7.367 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 3600 lr 4.984e-04 [train_loss] tar_ll 0.3716 loss -0.3716 (7.368 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 3800 lr 4.982e-04 [train_loss] tar_ll 0.4122 loss -0.4122 (7.264 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 4000 lr 4.980e-04 [train_loss] tar_ll 0.4198 loss -0.4198 (6.948 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 4200 lr 4.978e-04 [train_loss] tar_ll 0.4249 loss -0.4249 (7.321 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 4400 lr 4.976e-04 [train_loss] tar_ll 0.3676 loss -0.3676 (7.476 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 4600 lr 4.974e-04 [train_loss] tar_ll 0.5441 loss -0.5441 (6.822 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 4800 lr 4.972e-04 [train_loss] tar_ll 0.4415 loss -0.4415 (7.184 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 5000 lr 4.969e-04 [train_loss] tar_ll 0.3625 loss -0.3625 (7.227 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.01it/s] \n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll -0.1848 loss 0.1848 (33.708 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 5200 lr 4.967e-04 [train_loss] tar_ll 0.2621 loss -0.2621 (7.745 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 5400 lr 4.964e-04 [train_loss] tar_ll 0.4297 loss -0.4297 (7.015 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 5600 lr 4.961e-04 [train_loss] tar_ll 0.0957 loss -0.0957 (6.710 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 5800 lr 4.959e-04 [train_loss] tar_ll 0.4484 loss -0.4484 (7.080 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 6000 lr 4.956e-04 [train_loss] tar_ll 0.5291 loss -0.5291 (7.350 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 6200 lr 4.953e-04 [train_loss] tar_ll 0.5080 loss -0.5080 (6.665 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 6400 lr 4.950e-04 [train_loss] tar_ll 0.4087 loss -0.4087 (6.880 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 6600 lr 4.946e-04 [train_loss] tar_ll 0.5408 loss -0.5408 (7.302 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 6800 lr 4.943e-04 [train_loss] tar_ll 0.6179 loss -0.6179 (6.794 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 7000 lr 4.940e-04 [train_loss] tar_ll 0.5199 loss -0.5199 (7.101 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 7200 lr 4.936e-04 [train_loss] tar_ll 0.6251 loss -0.6251 (7.205 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 7400 lr 4.933e-04 [train_loss] tar_ll 0.5937 loss -0.5937 (6.969 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 7600 lr 4.929e-04 [train_loss] tar_ll 0.6403 loss -0.6403 (6.682 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 7800 lr 4.925e-04 [train_loss] tar_ll 0.6884 loss -0.6884 (7.240 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 8000 lr 4.921e-04 [train_loss] tar_ll 0.6397 loss -0.6397 (7.057 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 8200 lr 4.918e-04 [train_loss] tar_ll 0.6492 loss -0.6492 (6.663 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 8400 lr 4.913e-04 [train_loss] tar_ll 0.6808 loss -0.6808 (7.186 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 8600 lr 4.909e-04 [train_loss] tar_ll 0.5859 loss -0.5859 (7.262 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 8800 lr 4.905e-04 [train_loss] tar_ll 0.4932 loss -0.4932 (6.791 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 9000 lr 4.901e-04 [train_loss] tar_ll 0.4508 loss -0.4508 (6.938 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 9200 lr 4.896e-04 [train_loss] tar_ll 0.5843 loss -0.5843 (7.189 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 9400 lr 4.892e-04 [train_loss] tar_ll 0.3159 loss -0.3159 (6.577 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 9600 lr 4.887e-04 [train_loss] tar_ll 0.6445 loss -0.6445 (6.913 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 9800 lr 4.882e-04 [train_loss] tar_ll 0.2953 loss -0.2953 (7.141 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 10000 lr 4.878e-04 [train_loss] tar_ll 0.5374 loss -0.5374 (7.040 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.72it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.1425 loss -0.1425 (33.438 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 10200 lr 4.873e-04 [train_loss] tar_ll 0.6442 loss -0.6442 (7.299 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 10400 lr 4.868e-04 [train_loss] tar_ll 0.4882 loss -0.4882 (6.747 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 10600 lr 4.863e-04 [train_loss] tar_ll 0.2127 loss -0.2127 (7.213 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 10800 lr 4.857e-04 [train_loss] tar_ll 0.4364 loss -0.4364 (7.084 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 11000 lr 4.852e-04 [train_loss] tar_ll 0.4690 loss -0.4690 (7.291 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 11200 lr 4.847e-04 [train_loss] tar_ll -0.0689 loss 0.0689 (7.006 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 11400 lr 4.841e-04 [train_loss] tar_ll 0.1715 loss -0.1715 (6.265 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 11600 lr 4.836e-04 [train_loss] tar_ll 0.2855 loss -0.2855 (6.406 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 11800 lr 4.830e-04 [train_loss] tar_ll 0.2457 loss -0.2457 (6.674 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 12000 lr 4.824e-04 [train_loss] tar_ll 0.3507 loss -0.3507 (6.480 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 12200 lr 4.819e-04 [train_loss] tar_ll 0.4352 loss -0.4352 (6.250 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 12400 lr 4.813e-04 [train_loss] tar_ll 0.4523 loss -0.4523 (6.344 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 12600 lr 4.807e-04 [train_loss] tar_ll 0.5154 loss -0.5154 (6.193 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 12800 lr 4.801e-04 [train_loss] tar_ll 0.4461 loss -0.4461 (6.259 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 13000 lr 4.794e-04 [train_loss] tar_ll 0.5550 loss -0.5550 (7.562 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 13200 lr 4.788e-04 [train_loss] tar_ll 0.3664 loss -0.3664 (11.226 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 13400 lr 4.782e-04 [train_loss] tar_ll 0.0867 loss -0.0867 (13.404 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 13600 lr 4.775e-04 [train_loss] tar_ll 0.2785 loss -0.2785 (6.737 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 13800 lr 4.769e-04 [train_loss] tar_ll 0.3968 loss -0.3968 (6.714 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 14000 lr 4.762e-04 [train_loss] tar_ll 0.3780 loss -0.3780 (6.782 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 14200 lr 4.755e-04 [train_loss] tar_ll -0.0647 loss 0.0647 (6.399 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 14400 lr 4.749e-04 [train_loss] tar_ll 0.1999 loss -0.1999 (6.578 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 14600 lr 4.742e-04 [train_loss] tar_ll 0.3428 loss -0.3428 (6.645 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 14800 lr 4.735e-04 [train_loss] tar_ll 0.5050 loss -0.5050 (6.498 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 15000 lr 4.728e-04 [train_loss] tar_ll 0.3691 loss -0.3691 (6.486 secs)\n",
      "100%|##########| 3000/3000 [00:31<00:00, 96.25it/s] \n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.2742 loss -0.2742 (31.172 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 15200 lr 4.720e-04 [train_loss] tar_ll 0.4171 loss -0.4171 (6.493 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 15400 lr 4.713e-04 [train_loss] tar_ll 0.4218 loss -0.4218 (6.219 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 15600 lr 4.706e-04 [train_loss] tar_ll 0.3790 loss -0.3790 (6.218 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 15800 lr 4.698e-04 [train_loss] tar_ll 0.4712 loss -0.4712 (6.021 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 16000 lr 4.691e-04 [train_loss] tar_ll 0.5399 loss -0.5399 (6.251 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 16200 lr 4.683e-04 [train_loss] tar_ll 0.4939 loss -0.4939 (6.073 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 16400 lr 4.675e-04 [train_loss] tar_ll 0.5419 loss -0.5419 (6.358 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 16600 lr 4.668e-04 [train_loss] tar_ll 0.5402 loss -0.5402 (7.103 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 16800 lr 4.660e-04 [train_loss] tar_ll 0.5969 loss -0.5969 (7.043 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 17000 lr 4.652e-04 [train_loss] tar_ll 0.7721 loss -0.7721 (6.459 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 17200 lr 4.644e-04 [train_loss] tar_ll 0.5523 loss -0.5523 (6.577 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 17400 lr 4.636e-04 [train_loss] tar_ll 0.2942 loss -0.2942 (6.324 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 17600 lr 4.627e-04 [train_loss] tar_ll 0.3013 loss -0.3013 (6.292 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 17800 lr 4.619e-04 [train_loss] tar_ll 0.4253 loss -0.4253 (6.322 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 18000 lr 4.611e-04 [train_loss] tar_ll 0.5219 loss -0.5219 (6.392 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 18200 lr 4.602e-04 [train_loss] tar_ll 0.6464 loss -0.6464 (6.365 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 18400 lr 4.594e-04 [train_loss] tar_ll 0.5886 loss -0.5886 (6.579 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 18600 lr 4.585e-04 [train_loss] tar_ll 0.6085 loss -0.6085 (6.367 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 18800 lr 4.576e-04 [train_loss] tar_ll 0.6160 loss -0.6160 (6.494 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 19000 lr 4.568e-04 [train_loss] tar_ll 0.6236 loss -0.6236 (6.648 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 19200 lr 4.559e-04 [train_loss] tar_ll 0.4996 loss -0.4996 (6.545 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 19400 lr 4.550e-04 [train_loss] tar_ll 0.6412 loss -0.6412 (6.726 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 19600 lr 4.541e-04 [train_loss] tar_ll 0.6942 loss -0.6942 (7.682 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 19800 lr 4.532e-04 [train_loss] tar_ll 0.4480 loss -0.4480 (8.227 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 20000 lr 4.523e-04 [train_loss] tar_ll 0.4912 loss -0.4912 (8.925 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.28it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.4355 loss -0.4355 (34.770 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 20200 lr 4.513e-04 [train_loss] tar_ll 0.6331 loss -0.6331 (7.312 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 20400 lr 4.504e-04 [train_loss] tar_ll 0.5929 loss -0.5929 (7.380 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 20600 lr 4.494e-04 [train_loss] tar_ll 0.6039 loss -0.6039 (7.489 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 20800 lr 4.485e-04 [train_loss] tar_ll 0.6008 loss -0.6008 (7.375 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 21000 lr 4.475e-04 [train_loss] tar_ll 0.4496 loss -0.4496 (7.407 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 21200 lr 4.466e-04 [train_loss] tar_ll 0.7445 loss -0.7445 (7.515 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 21400 lr 4.456e-04 [train_loss] tar_ll 0.7887 loss -0.7887 (7.450 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 21600 lr 4.446e-04 [train_loss] tar_ll 0.7060 loss -0.7060 (7.289 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 21800 lr 4.436e-04 [train_loss] tar_ll 0.6244 loss -0.6244 (7.375 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 22000 lr 4.426e-04 [train_loss] tar_ll 0.5740 loss -0.5740 (7.484 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 22200 lr 4.416e-04 [train_loss] tar_ll 0.4576 loss -0.4576 (7.410 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 22400 lr 4.406e-04 [train_loss] tar_ll 0.6981 loss -0.6981 (7.508 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 22600 lr 4.396e-04 [train_loss] tar_ll 0.5721 loss -0.5721 (7.363 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 22800 lr 4.386e-04 [train_loss] tar_ll 0.6441 loss -0.6441 (7.292 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 23000 lr 4.375e-04 [train_loss] tar_ll 0.7816 loss -0.7816 (7.072 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 23200 lr 4.365e-04 [train_loss] tar_ll 0.6313 loss -0.6313 (7.224 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 23400 lr 4.354e-04 [train_loss] tar_ll 0.4984 loss -0.4984 (7.197 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 23600 lr 4.344e-04 [train_loss] tar_ll 0.6371 loss -0.6371 (7.362 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 23800 lr 4.333e-04 [train_loss] tar_ll 0.5406 loss -0.5406 (7.190 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 24000 lr 4.322e-04 [train_loss] tar_ll 0.5180 loss -0.5180 (7.159 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 24200 lr 4.312e-04 [train_loss] tar_ll 0.7301 loss -0.7301 (7.412 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 24400 lr 4.301e-04 [train_loss] tar_ll 0.6489 loss -0.6489 (7.162 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 24600 lr 4.290e-04 [train_loss] tar_ll 0.6248 loss -0.6248 (7.543 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 24800 lr 4.279e-04 [train_loss] tar_ll 0.6079 loss -0.6079 (7.293 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 25000 lr 4.268e-04 [train_loss] tar_ll 0.4608 loss -0.4608 (7.504 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 85.59it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.3293 loss -0.3293 (35.053 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 25200 lr 4.257e-04 [train_loss] tar_ll 0.5992 loss -0.5992 (8.020 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 25400 lr 4.245e-04 [train_loss] tar_ll 0.5571 loss -0.5571 (7.277 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 25600 lr 4.234e-04 [train_loss] tar_ll 0.7294 loss -0.7294 (7.325 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 25800 lr 4.223e-04 [train_loss] tar_ll 0.7571 loss -0.7571 (7.393 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 26000 lr 4.211e-04 [train_loss] tar_ll 0.8585 loss -0.8585 (7.387 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 26200 lr 4.200e-04 [train_loss] tar_ll 0.7616 loss -0.7616 (7.647 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 26400 lr 4.188e-04 [train_loss] tar_ll 0.5889 loss -0.5889 (7.372 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 26600 lr 4.177e-04 [train_loss] tar_ll 0.6526 loss -0.6526 (7.474 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 26800 lr 4.165e-04 [train_loss] tar_ll 0.5565 loss -0.5565 (7.385 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 27000 lr 4.153e-04 [train_loss] tar_ll 0.6478 loss -0.6478 (7.719 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 27200 lr 4.141e-04 [train_loss] tar_ll 0.7642 loss -0.7642 (7.463 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 27400 lr 4.130e-04 [train_loss] tar_ll 0.6345 loss -0.6345 (7.487 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 27600 lr 4.118e-04 [train_loss] tar_ll 0.6797 loss -0.6797 (7.316 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 27800 lr 4.106e-04 [train_loss] tar_ll 0.7002 loss -0.7002 (7.570 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 28000 lr 4.094e-04 [train_loss] tar_ll 0.7692 loss -0.7692 (7.377 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 28200 lr 4.081e-04 [train_loss] tar_ll 0.5462 loss -0.5462 (7.362 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 28400 lr 4.069e-04 [train_loss] tar_ll 0.4401 loss -0.4401 (7.620 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 28600 lr 4.057e-04 [train_loss] tar_ll 0.6607 loss -0.6607 (7.192 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 28800 lr 4.045e-04 [train_loss] tar_ll 0.5301 loss -0.5301 (7.460 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 29000 lr 4.032e-04 [train_loss] tar_ll 0.6711 loss -0.6711 (7.419 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 29200 lr 4.020e-04 [train_loss] tar_ll 0.7491 loss -0.7491 (7.615 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 29400 lr 4.007e-04 [train_loss] tar_ll 0.8190 loss -0.8190 (7.477 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 29600 lr 3.995e-04 [train_loss] tar_ll 0.8486 loss -0.8486 (7.581 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 29800 lr 3.982e-04 [train_loss] tar_ll 0.7551 loss -0.7551 (7.543 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 30000 lr 3.969e-04 [train_loss] tar_ll 0.7730 loss -0.7730 (7.603 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 83.54it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.5506 loss -0.5506 (35.913 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 30200 lr 3.957e-04 [train_loss] tar_ll 0.7139 loss -0.7139 (7.510 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 30400 lr 3.944e-04 [train_loss] tar_ll 0.8164 loss -0.8164 (7.705 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 30600 lr 3.931e-04 [train_loss] tar_ll 0.7040 loss -0.7040 (6.648 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 30800 lr 3.918e-04 [train_loss] tar_ll 0.7584 loss -0.7584 (7.007 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 31000 lr 3.905e-04 [train_loss] tar_ll 0.4931 loss -0.4931 (7.136 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 31200 lr 3.892e-04 [train_loss] tar_ll 0.6992 loss -0.6992 (7.296 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 31400 lr 3.879e-04 [train_loss] tar_ll 0.6991 loss -0.6991 (7.179 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 31600 lr 3.866e-04 [train_loss] tar_ll 0.4502 loss -0.4502 (7.483 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 31800 lr 3.853e-04 [train_loss] tar_ll 0.7504 loss -0.7504 (7.415 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 32000 lr 3.840e-04 [train_loss] tar_ll 0.7930 loss -0.7930 (7.562 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 32200 lr 3.826e-04 [train_loss] tar_ll 0.6672 loss -0.6672 (7.160 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 32400 lr 3.813e-04 [train_loss] tar_ll 0.6394 loss -0.6394 (7.990 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 32600 lr 3.800e-04 [train_loss] tar_ll 0.7212 loss -0.7212 (7.881 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 32800 lr 3.786e-04 [train_loss] tar_ll 0.5418 loss -0.5418 (6.888 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 33000 lr 3.773e-04 [train_loss] tar_ll 0.6550 loss -0.6550 (6.493 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 33200 lr 3.759e-04 [train_loss] tar_ll 0.6954 loss -0.6954 (6.811 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 33400 lr 3.745e-04 [train_loss] tar_ll 0.7625 loss -0.7625 (7.309 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 33600 lr 3.732e-04 [train_loss] tar_ll 0.5975 loss -0.5975 (7.283 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 33800 lr 3.718e-04 [train_loss] tar_ll 0.6882 loss -0.6882 (7.310 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 34000 lr 3.704e-04 [train_loss] tar_ll 0.8509 loss -0.8509 (7.399 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 34200 lr 3.691e-04 [train_loss] tar_ll 0.8852 loss -0.8852 (7.369 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 34400 lr 3.677e-04 [train_loss] tar_ll 0.8486 loss -0.8486 (7.199 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 34600 lr 3.663e-04 [train_loss] tar_ll 0.7684 loss -0.7684 (7.232 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 34800 lr 3.649e-04 [train_loss] tar_ll 0.8964 loss -0.8964 (7.373 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 35000 lr 3.635e-04 [train_loss] tar_ll 0.9367 loss -0.9367 (7.264 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 84.96it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.6233 loss -0.6233 (35.312 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 35200 lr 3.621e-04 [train_loss] tar_ll 0.7884 loss -0.7884 (7.382 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 35400 lr 3.607e-04 [train_loss] tar_ll 0.9292 loss -0.9292 (7.574 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 35600 lr 3.593e-04 [train_loss] tar_ll 0.8055 loss -0.8055 (7.343 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 35800 lr 3.579e-04 [train_loss] tar_ll 0.8100 loss -0.8100 (7.336 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 36000 lr 3.564e-04 [train_loss] tar_ll 0.8695 loss -0.8695 (7.516 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 36200 lr 3.550e-04 [train_loss] tar_ll 0.8385 loss -0.8385 (7.337 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 36400 lr 3.536e-04 [train_loss] tar_ll 0.6442 loss -0.6442 (7.696 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 36600 lr 3.522e-04 [train_loss] tar_ll 0.8329 loss -0.8329 (7.406 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 36800 lr 3.507e-04 [train_loss] tar_ll 0.8437 loss -0.8437 (7.505 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 37000 lr 3.493e-04 [train_loss] tar_ll 0.7795 loss -0.7795 (7.624 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 37200 lr 3.478e-04 [train_loss] tar_ll 0.7538 loss -0.7538 (7.694 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 37400 lr 3.464e-04 [train_loss] tar_ll 0.8266 loss -0.8266 (7.634 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 37600 lr 3.449e-04 [train_loss] tar_ll 0.8604 loss -0.8604 (7.886 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 37800 lr 3.435e-04 [train_loss] tar_ll 0.4903 loss -0.4903 (6.768 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 38000 lr 3.420e-04 [train_loss] tar_ll 0.8290 loss -0.8290 (6.766 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 38200 lr 3.406e-04 [train_loss] tar_ll 0.8041 loss -0.8041 (6.604 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 38400 lr 3.391e-04 [train_loss] tar_ll 0.7822 loss -0.7822 (6.553 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 38600 lr 3.376e-04 [train_loss] tar_ll 0.8633 loss -0.8633 (6.585 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 38800 lr 3.362e-04 [train_loss] tar_ll 0.8855 loss -0.8855 (6.430 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 39000 lr 3.347e-04 [train_loss] tar_ll 0.8651 loss -0.8651 (6.580 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 39200 lr 3.332e-04 [train_loss] tar_ll 0.8379 loss -0.8379 (6.427 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 39400 lr 3.317e-04 [train_loss] tar_ll 0.9265 loss -0.9265 (6.432 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 39600 lr 3.302e-04 [train_loss] tar_ll 0.9184 loss -0.9184 (6.486 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 39800 lr 3.287e-04 [train_loss] tar_ll 0.7822 loss -0.7822 (6.485 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 40000 lr 3.273e-04 [train_loss] tar_ll 0.8220 loss -0.8220 (6.521 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 93.64it/s] \n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.5634 loss -0.5634 (32.039 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 40200 lr 3.258e-04 [train_loss] tar_ll 0.8624 loss -0.8624 (6.854 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 40400 lr 3.243e-04 [train_loss] tar_ll 0.8577 loss -0.8577 (6.605 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 40600 lr 3.228e-04 [train_loss] tar_ll 0.7708 loss -0.7708 (7.213 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 40800 lr 3.213e-04 [train_loss] tar_ll 0.8336 loss -0.8336 (7.606 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 41000 lr 3.197e-04 [train_loss] tar_ll 0.8555 loss -0.8555 (7.889 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 41200 lr 3.182e-04 [train_loss] tar_ll 0.8147 loss -0.8147 (7.793 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 41400 lr 3.167e-04 [train_loss] tar_ll 0.8450 loss -0.8450 (7.444 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 41600 lr 3.152e-04 [train_loss] tar_ll 0.8983 loss -0.8983 (7.368 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 41800 lr 3.137e-04 [train_loss] tar_ll 0.8608 loss -0.8608 (7.468 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 42000 lr 3.122e-04 [train_loss] tar_ll 0.7933 loss -0.7933 (7.381 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 42200 lr 3.106e-04 [train_loss] tar_ll 0.9099 loss -0.9099 (7.817 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 42400 lr 3.091e-04 [train_loss] tar_ll 0.7204 loss -0.7204 (7.443 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 42600 lr 3.076e-04 [train_loss] tar_ll 0.8994 loss -0.8994 (7.536 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 42800 lr 3.061e-04 [train_loss] tar_ll 0.7758 loss -0.7758 (7.497 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 43000 lr 3.045e-04 [train_loss] tar_ll 0.8174 loss -0.8174 (7.535 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 43200 lr 3.030e-04 [train_loss] tar_ll 0.8817 loss -0.8817 (7.487 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 43400 lr 3.015e-04 [train_loss] tar_ll 0.7934 loss -0.7934 (7.567 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 43600 lr 2.999e-04 [train_loss] tar_ll 0.6977 loss -0.6977 (7.472 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 43800 lr 2.984e-04 [train_loss] tar_ll 0.8389 loss -0.8389 (7.498 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 44000 lr 2.968e-04 [train_loss] tar_ll 0.8504 loss -0.8504 (7.555 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 44200 lr 2.953e-04 [train_loss] tar_ll 0.4908 loss -0.4908 (7.714 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 44400 lr 2.938e-04 [train_loss] tar_ll 0.7738 loss -0.7738 (7.412 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 44600 lr 2.922e-04 [train_loss] tar_ll 0.7691 loss -0.7691 (7.410 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 44800 lr 2.907e-04 [train_loss] tar_ll 0.7709 loss -0.7709 (7.613 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 45000 lr 2.891e-04 [train_loss] tar_ll 0.8314 loss -0.8314 (7.484 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 83.00it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.4883 loss -0.4883 (36.146 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 45200 lr 2.876e-04 [train_loss] tar_ll 0.7835 loss -0.7835 (7.535 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 45400 lr 2.860e-04 [train_loss] tar_ll 0.7654 loss -0.7654 (7.559 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 45600 lr 2.844e-04 [train_loss] tar_ll 0.8093 loss -0.8093 (7.509 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 45800 lr 2.829e-04 [train_loss] tar_ll 0.6512 loss -0.6512 (7.640 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 46000 lr 2.813e-04 [train_loss] tar_ll 0.6690 loss -0.6690 (7.650 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 46200 lr 2.798e-04 [train_loss] tar_ll 0.8745 loss -0.8745 (7.567 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 46400 lr 2.782e-04 [train_loss] tar_ll 0.8818 loss -0.8818 (7.438 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 46600 lr 2.767e-04 [train_loss] tar_ll 0.8517 loss -0.8517 (7.451 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 46800 lr 2.751e-04 [train_loss] tar_ll 0.7304 loss -0.7304 (7.572 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 47000 lr 2.735e-04 [train_loss] tar_ll 0.8779 loss -0.8779 (7.547 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 47200 lr 2.720e-04 [train_loss] tar_ll 0.9486 loss -0.9486 (7.677 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 47400 lr 2.704e-04 [train_loss] tar_ll 0.8835 loss -0.8835 (7.228 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 47600 lr 2.688e-04 [train_loss] tar_ll 0.8894 loss -0.8894 (6.611 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 47800 lr 2.673e-04 [train_loss] tar_ll 0.7982 loss -0.7982 (6.621 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 48000 lr 2.657e-04 [train_loss] tar_ll 0.8453 loss -0.8453 (6.672 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 48200 lr 2.641e-04 [train_loss] tar_ll 0.8892 loss -0.8892 (6.519 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 48400 lr 2.626e-04 [train_loss] tar_ll 0.7407 loss -0.7407 (6.527 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 48600 lr 2.610e-04 [train_loss] tar_ll 0.7879 loss -0.7879 (6.549 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 48800 lr 2.594e-04 [train_loss] tar_ll 0.9110 loss -0.9110 (6.472 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 49000 lr 2.579e-04 [train_loss] tar_ll 0.7398 loss -0.7398 (6.583 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 49200 lr 2.563e-04 [train_loss] tar_ll 0.9072 loss -0.9072 (7.018 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 49400 lr 2.547e-04 [train_loss] tar_ll 0.9695 loss -0.9695 (7.885 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 49600 lr 2.531e-04 [train_loss] tar_ll 1.0610 loss -1.0610 (6.853 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 49800 lr 2.516e-04 [train_loss] tar_ll 0.7811 loss -0.7811 (6.750 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 50000 lr 2.500e-04 [train_loss] tar_ll 0.8056 loss -0.8056 (6.758 secs)\n",
      "100%|##########| 3000/3000 [00:32<00:00, 93.02it/s] \n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.5953 loss -0.5953 (32.250 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 50200 lr 2.484e-04 [train_loss] tar_ll 0.8957 loss -0.8957 (6.747 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 50400 lr 2.469e-04 [train_loss] tar_ll 0.9226 loss -0.9226 (7.286 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 50600 lr 2.453e-04 [train_loss] tar_ll 0.9938 loss -0.9938 (7.186 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 50800 lr 2.437e-04 [train_loss] tar_ll 0.8980 loss -0.8980 (7.427 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 51000 lr 2.421e-04 [train_loss] tar_ll 0.8522 loss -0.8522 (7.546 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 51200 lr 2.406e-04 [train_loss] tar_ll 0.8864 loss -0.8864 (7.635 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 51400 lr 2.390e-04 [train_loss] tar_ll 1.0257 loss -1.0257 (7.591 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 51600 lr 2.374e-04 [train_loss] tar_ll 0.9423 loss -0.9423 (7.483 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 51800 lr 2.359e-04 [train_loss] tar_ll 0.8237 loss -0.8237 (7.413 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 52000 lr 2.343e-04 [train_loss] tar_ll 0.7463 loss -0.7463 (7.326 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 52200 lr 2.327e-04 [train_loss] tar_ll 0.8838 loss -0.8838 (7.358 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 52400 lr 2.312e-04 [train_loss] tar_ll 0.9276 loss -0.9276 (7.607 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 52600 lr 2.296e-04 [train_loss] tar_ll 0.8431 loss -0.8431 (7.484 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 52800 lr 2.280e-04 [train_loss] tar_ll 0.8749 loss -0.8749 (7.562 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 53000 lr 2.265e-04 [train_loss] tar_ll 0.9143 loss -0.9143 (7.648 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 53200 lr 2.249e-04 [train_loss] tar_ll 1.0271 loss -1.0271 (7.496 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 53400 lr 2.233e-04 [train_loss] tar_ll 0.9629 loss -0.9629 (7.467 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 53600 lr 2.218e-04 [train_loss] tar_ll 0.9963 loss -0.9963 (7.263 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 53800 lr 2.202e-04 [train_loss] tar_ll 1.0108 loss -1.0108 (7.611 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 54000 lr 2.187e-04 [train_loss] tar_ll 0.9411 loss -0.9411 (7.604 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 54200 lr 2.171e-04 [train_loss] tar_ll 0.9275 loss -0.9275 (7.761 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 54400 lr 2.156e-04 [train_loss] tar_ll 0.9557 loss -0.9557 (7.620 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 54600 lr 2.140e-04 [train_loss] tar_ll 0.9080 loss -0.9080 (7.679 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 54800 lr 2.124e-04 [train_loss] tar_ll 0.7647 loss -0.7647 (7.540 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 55000 lr 2.109e-04 [train_loss] tar_ll 1.0188 loss -1.0188 (7.785 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.29it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.7067 loss -0.7067 (36.459 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 55200 lr 2.093e-04 [train_loss] tar_ll 0.9783 loss -0.9783 (7.634 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 55400 lr 2.078e-04 [train_loss] tar_ll 0.8693 loss -0.8693 (7.688 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 55600 lr 2.062e-04 [train_loss] tar_ll 1.0041 loss -1.0041 (7.487 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 55800 lr 2.047e-04 [train_loss] tar_ll 0.9263 loss -0.9263 (7.405 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 56000 lr 2.032e-04 [train_loss] tar_ll 0.9898 loss -0.9898 (7.344 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 56200 lr 2.016e-04 [train_loss] tar_ll 1.0212 loss -1.0212 (7.646 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 56400 lr 2.001e-04 [train_loss] tar_ll 1.0112 loss -1.0112 (7.377 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 56600 lr 1.985e-04 [train_loss] tar_ll 0.9906 loss -0.9906 (7.478 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 56800 lr 1.970e-04 [train_loss] tar_ll 1.0690 loss -1.0690 (7.384 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 57000 lr 1.955e-04 [train_loss] tar_ll 0.9808 loss -0.9808 (7.855 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 57200 lr 1.939e-04 [train_loss] tar_ll 0.9353 loss -0.9353 (6.937 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 57400 lr 1.924e-04 [train_loss] tar_ll 0.9225 loss -0.9225 (6.596 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 57600 lr 1.909e-04 [train_loss] tar_ll 0.9173 loss -0.9173 (6.797 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 57800 lr 1.894e-04 [train_loss] tar_ll 1.0224 loss -1.0224 (6.560 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 58000 lr 1.878e-04 [train_loss] tar_ll 0.6748 loss -0.6748 (6.561 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 58200 lr 1.863e-04 [train_loss] tar_ll 1.0630 loss -1.0630 (7.248 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 58400 lr 1.848e-04 [train_loss] tar_ll 0.9908 loss -0.9908 (8.098 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 58600 lr 1.833e-04 [train_loss] tar_ll 1.0628 loss -1.0628 (7.707 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 58800 lr 1.818e-04 [train_loss] tar_ll 0.5856 loss -0.5856 (7.694 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 59000 lr 1.803e-04 [train_loss] tar_ll 0.7472 loss -0.7472 (7.559 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 59200 lr 1.787e-04 [train_loss] tar_ll 0.7914 loss -0.7914 (8.117 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 59400 lr 1.772e-04 [train_loss] tar_ll 0.8399 loss -0.8399 (8.249 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 59600 lr 1.757e-04 [train_loss] tar_ll 0.9665 loss -0.9665 (8.198 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 59800 lr 1.742e-04 [train_loss] tar_ll 1.0155 loss -1.0155 (9.173 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 60000 lr 1.727e-04 [train_loss] tar_ll 0.9384 loss -0.9384 (8.949 secs)\n",
      "100%|##########| 3000/3000 [00:39<00:00, 76.73it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.6616 loss -0.6616 (39.101 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 60200 lr 1.713e-04 [train_loss] tar_ll 0.9616 loss -0.9616 (7.969 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 60400 lr 1.698e-04 [train_loss] tar_ll 1.0041 loss -1.0041 (7.633 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 60600 lr 1.683e-04 [train_loss] tar_ll 0.9397 loss -0.9397 (7.549 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 60800 lr 1.668e-04 [train_loss] tar_ll 1.0013 loss -1.0013 (7.818 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 61000 lr 1.653e-04 [train_loss] tar_ll 1.0083 loss -1.0083 (7.496 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 61200 lr 1.638e-04 [train_loss] tar_ll 0.8409 loss -0.8409 (7.461 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 61400 lr 1.624e-04 [train_loss] tar_ll 0.9335 loss -0.9335 (7.537 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 61600 lr 1.609e-04 [train_loss] tar_ll 1.0148 loss -1.0148 (7.375 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 61800 lr 1.594e-04 [train_loss] tar_ll 0.9905 loss -0.9905 (7.536 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 62000 lr 1.580e-04 [train_loss] tar_ll 0.9662 loss -0.9662 (7.451 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 62200 lr 1.565e-04 [train_loss] tar_ll 1.0440 loss -1.0440 (7.643 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 62400 lr 1.551e-04 [train_loss] tar_ll 0.9798 loss -0.9798 (7.559 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 62600 lr 1.536e-04 [train_loss] tar_ll 1.0136 loss -1.0136 (7.405 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 62800 lr 1.522e-04 [train_loss] tar_ll 0.9879 loss -0.9879 (7.546 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 63000 lr 1.507e-04 [train_loss] tar_ll 1.0244 loss -1.0244 (7.397 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 63200 lr 1.493e-04 [train_loss] tar_ll 1.0737 loss -1.0737 (7.710 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 63400 lr 1.478e-04 [train_loss] tar_ll 1.0579 loss -1.0579 (7.436 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 63600 lr 1.464e-04 [train_loss] tar_ll 1.1023 loss -1.1023 (7.702 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 63800 lr 1.450e-04 [train_loss] tar_ll 1.0160 loss -1.0160 (7.392 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 64000 lr 1.436e-04 [train_loss] tar_ll 1.0604 loss -1.0604 (7.795 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 64200 lr 1.421e-04 [train_loss] tar_ll 0.9996 loss -0.9996 (7.440 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 64400 lr 1.407e-04 [train_loss] tar_ll 0.9962 loss -0.9962 (7.733 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 64600 lr 1.393e-04 [train_loss] tar_ll 1.0353 loss -1.0353 (7.480 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 64800 lr 1.379e-04 [train_loss] tar_ll 1.1139 loss -1.1139 (7.504 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 65000 lr 1.365e-04 [train_loss] tar_ll 0.9598 loss -0.9598 (7.634 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.55it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.6479 loss -0.6479 (36.789 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 65200 lr 1.351e-04 [train_loss] tar_ll 0.9531 loss -0.9531 (7.967 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 65400 lr 1.337e-04 [train_loss] tar_ll 1.0542 loss -1.0542 (6.912 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 65600 lr 1.323e-04 [train_loss] tar_ll 1.0050 loss -1.0050 (7.375 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 65800 lr 1.309e-04 [train_loss] tar_ll 1.0599 loss -1.0599 (7.762 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 66000 lr 1.296e-04 [train_loss] tar_ll 1.0874 loss -1.0874 (7.769 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 66200 lr 1.282e-04 [train_loss] tar_ll 0.9789 loss -0.9789 (7.649 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 66400 lr 1.268e-04 [train_loss] tar_ll 1.0655 loss -1.0655 (7.834 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 66600 lr 1.255e-04 [train_loss] tar_ll 1.0219 loss -1.0219 (7.592 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 66800 lr 1.241e-04 [train_loss] tar_ll 1.1345 loss -1.1345 (7.926 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 67000 lr 1.227e-04 [train_loss] tar_ll 1.0751 loss -1.0751 (7.867 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 67200 lr 1.214e-04 [train_loss] tar_ll 0.9660 loss -0.9660 (7.910 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 67400 lr 1.200e-04 [train_loss] tar_ll 1.0614 loss -1.0614 (7.665 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 67600 lr 1.187e-04 [train_loss] tar_ll 1.0598 loss -1.0598 (7.833 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 67800 lr 1.174e-04 [train_loss] tar_ll 1.0612 loss -1.0612 (7.773 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 68000 lr 1.160e-04 [train_loss] tar_ll 1.0537 loss -1.0537 (7.882 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 68200 lr 1.147e-04 [train_loss] tar_ll 1.1011 loss -1.1011 (7.910 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 68400 lr 1.134e-04 [train_loss] tar_ll 0.9978 loss -0.9978 (7.853 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 68600 lr 1.121e-04 [train_loss] tar_ll 0.9811 loss -0.9811 (7.702 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 68800 lr 1.108e-04 [train_loss] tar_ll 1.0016 loss -1.0016 (7.864 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 69000 lr 1.095e-04 [train_loss] tar_ll 1.1451 loss -1.1451 (7.688 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 69200 lr 1.082e-04 [train_loss] tar_ll 1.0760 loss -1.0760 (7.872 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 69400 lr 1.069e-04 [train_loss] tar_ll 1.0613 loss -1.0613 (7.849 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 69600 lr 1.056e-04 [train_loss] tar_ll 1.1068 loss -1.1068 (7.738 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 69800 lr 1.043e-04 [train_loss] tar_ll 1.1267 loss -1.1267 (7.615 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 70000 lr 1.031e-04 [train_loss] tar_ll 1.0731 loss -1.0731 (7.958 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 81.67it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.7972 loss -0.7972 (36.735 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 70200 lr 1.018e-04 [train_loss] tar_ll 1.1147 loss -1.1147 (7.758 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 70400 lr 1.005e-04 [train_loss] tar_ll 1.0140 loss -1.0140 (7.859 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 70600 lr 9.927e-05 [train_loss] tar_ll 1.1053 loss -1.1053 (7.692 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 70800 lr 9.802e-05 [train_loss] tar_ll 1.0744 loss -1.0744 (7.818 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 71000 lr 9.677e-05 [train_loss] tar_ll 1.0571 loss -1.0571 (7.718 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 71200 lr 9.554e-05 [train_loss] tar_ll 1.1700 loss -1.1700 (7.753 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 71400 lr 9.430e-05 [train_loss] tar_ll 1.0809 loss -1.0809 (6.600 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 71600 lr 9.308e-05 [train_loss] tar_ll 1.0976 loss -1.0976 (7.105 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 71800 lr 9.186e-05 [train_loss] tar_ll 1.0853 loss -1.0853 (7.472 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 72000 lr 9.064e-05 [train_loss] tar_ll 0.9895 loss -0.9895 (7.418 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 72200 lr 8.944e-05 [train_loss] tar_ll 1.0587 loss -1.0587 (7.356 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 72400 lr 8.824e-05 [train_loss] tar_ll 1.1315 loss -1.1315 (7.426 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 72600 lr 8.704e-05 [train_loss] tar_ll 1.1603 loss -1.1603 (7.435 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 72800 lr 8.585e-05 [train_loss] tar_ll 1.0287 loss -1.0287 (7.390 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 73000 lr 8.467e-05 [train_loss] tar_ll 1.0667 loss -1.0667 (7.482 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 73200 lr 8.350e-05 [train_loss] tar_ll 1.1330 loss -1.1330 (7.399 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 73400 lr 8.233e-05 [train_loss] tar_ll 1.2084 loss -1.2084 (7.477 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 73600 lr 8.117e-05 [train_loss] tar_ll 1.0546 loss -1.0546 (7.438 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 73800 lr 8.001e-05 [train_loss] tar_ll 1.1646 loss -1.1646 (7.575 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 74000 lr 7.886e-05 [train_loss] tar_ll 1.0974 loss -1.0974 (7.436 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 74200 lr 7.772e-05 [train_loss] tar_ll 1.1308 loss -1.1308 (7.517 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 74400 lr 7.659e-05 [train_loss] tar_ll 1.0871 loss -1.0871 (7.300 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 74600 lr 7.546e-05 [train_loss] tar_ll 1.1285 loss -1.1285 (7.513 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 74800 lr 7.434e-05 [train_loss] tar_ll 1.0549 loss -1.0549 (7.484 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 75000 lr 7.322e-05 [train_loss] tar_ll 1.1778 loss -1.1778 (7.388 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 86.02it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.7942 loss -0.7942 (34.877 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 75200 lr 7.212e-05 [train_loss] tar_ll 1.1275 loss -1.1275 (7.507 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 75400 lr 7.102e-05 [train_loss] tar_ll 1.0540 loss -1.0540 (7.634 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 75600 lr 6.992e-05 [train_loss] tar_ll 1.1450 loss -1.1450 (7.559 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 75800 lr 6.884e-05 [train_loss] tar_ll 1.1337 loss -1.1337 (7.782 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 76000 lr 6.776e-05 [train_loss] tar_ll 1.1818 loss -1.1818 (7.532 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 76200 lr 6.669e-05 [train_loss] tar_ll 1.1481 loss -1.1481 (7.547 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 76400 lr 6.562e-05 [train_loss] tar_ll 1.1384 loss -1.1384 (7.538 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 76600 lr 6.456e-05 [train_loss] tar_ll 1.1551 loss -1.1551 (7.708 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 76800 lr 6.351e-05 [train_loss] tar_ll 1.1559 loss -1.1559 (7.409 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 77000 lr 6.247e-05 [train_loss] tar_ll 1.1011 loss -1.1011 (7.578 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 77200 lr 6.144e-05 [train_loss] tar_ll 1.0219 loss -1.0219 (7.521 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 77400 lr 6.041e-05 [train_loss] tar_ll 1.1029 loss -1.1029 (7.511 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 77600 lr 5.939e-05 [train_loss] tar_ll 1.0817 loss -1.0817 (7.693 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 77800 lr 5.838e-05 [train_loss] tar_ll 1.1079 loss -1.1079 (7.851 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 78000 lr 5.737e-05 [train_loss] tar_ll 1.1368 loss -1.1368 (7.793 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 78200 lr 5.637e-05 [train_loss] tar_ll 1.1656 loss -1.1656 (7.626 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 78400 lr 5.538e-05 [train_loss] tar_ll 1.1713 loss -1.1713 (7.746 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 78600 lr 5.440e-05 [train_loss] tar_ll 1.1870 loss -1.1870 (7.677 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 78800 lr 5.343e-05 [train_loss] tar_ll 1.1362 loss -1.1362 (7.583 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 79000 lr 5.246e-05 [train_loss] tar_ll 1.1029 loss -1.1029 (6.839 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 79200 lr 5.150e-05 [train_loss] tar_ll 1.0821 loss -1.0821 (7.001 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 79400 lr 5.055e-05 [train_loss] tar_ll 1.1012 loss -1.1012 (7.837 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 79600 lr 4.961e-05 [train_loss] tar_ll 1.1441 loss -1.1441 (7.625 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 79800 lr 4.867e-05 [train_loss] tar_ll 1.1419 loss -1.1419 (7.732 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 80000 lr 4.775e-05 [train_loss] tar_ll 1.1401 loss -1.1401 (7.441 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 83.90it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.8012 loss -0.8012 (35.759 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 80200 lr 4.683e-05 [train_loss] tar_ll 1.0802 loss -1.0802 (7.590 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 80400 lr 4.592e-05 [train_loss] tar_ll 1.1241 loss -1.1241 (7.464 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 80600 lr 4.501e-05 [train_loss] tar_ll 1.1387 loss -1.1387 (7.377 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 80800 lr 4.412e-05 [train_loss] tar_ll 1.1285 loss -1.1285 (7.513 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 81000 lr 4.323e-05 [train_loss] tar_ll 1.0607 loss -1.0607 (7.443 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 81200 lr 4.235e-05 [train_loss] tar_ll 1.1715 loss -1.1715 (7.627 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 81400 lr 4.148e-05 [train_loss] tar_ll 1.1461 loss -1.1461 (7.658 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 81600 lr 4.062e-05 [train_loss] tar_ll 1.0807 loss -1.0807 (7.671 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 81800 lr 3.976e-05 [train_loss] tar_ll 1.2124 loss -1.2124 (7.766 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 82000 lr 3.892e-05 [train_loss] tar_ll 1.0963 loss -1.0963 (7.573 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 82200 lr 3.808e-05 [train_loss] tar_ll 1.1474 loss -1.1474 (7.423 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 82400 lr 3.725e-05 [train_loss] tar_ll 1.1202 loss -1.1202 (7.440 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 82600 lr 3.643e-05 [train_loss] tar_ll 1.1989 loss -1.1989 (7.409 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 82800 lr 3.562e-05 [train_loss] tar_ll 1.0796 loss -1.0796 (7.391 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 83000 lr 3.481e-05 [train_loss] tar_ll 1.0757 loss -1.0757 (7.578 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 83200 lr 3.402e-05 [train_loss] tar_ll 1.2206 loss -1.2206 (7.516 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 83400 lr 3.323e-05 [train_loss] tar_ll 1.1800 loss -1.1800 (7.508 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 83600 lr 3.245e-05 [train_loss] tar_ll 1.1232 loss -1.1232 (7.450 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 83800 lr 3.168e-05 [train_loss] tar_ll 1.1580 loss -1.1580 (7.561 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 84000 lr 3.092e-05 [train_loss] tar_ll 1.1620 loss -1.1620 (7.452 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 84200 lr 3.017e-05 [train_loss] tar_ll 1.1440 loss -1.1440 (7.511 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 84400 lr 2.943e-05 [train_loss] tar_ll 1.0948 loss -1.0948 (7.634 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 84600 lr 2.869e-05 [train_loss] tar_ll 1.0544 loss -1.0544 (7.721 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 84800 lr 2.797e-05 [train_loss] tar_ll 1.1811 loss -1.1811 (7.447 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 85000 lr 2.725e-05 [train_loss] tar_ll 1.0888 loss -1.0888 (7.561 secs)\n",
      "100%|##########| 3000/3000 [00:34<00:00, 87.05it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.8190 loss -0.8190 (34.465 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 85200 lr 2.654e-05 [train_loss] tar_ll 1.1927 loss -1.1927 (7.771 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 85400 lr 2.584e-05 [train_loss] tar_ll 1.1741 loss -1.1741 (7.811 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 85600 lr 2.515e-05 [train_loss] tar_ll 1.1507 loss -1.1507 (7.747 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 85800 lr 2.447e-05 [train_loss] tar_ll 1.1063 loss -1.1063 (7.689 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 86000 lr 2.379e-05 [train_loss] tar_ll 1.1701 loss -1.1701 (7.673 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 86200 lr 2.313e-05 [train_loss] tar_ll 1.0777 loss -1.0777 (7.819 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 86400 lr 2.247e-05 [train_loss] tar_ll 1.0587 loss -1.0587 (7.608 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 86600 lr 2.183e-05 [train_loss] tar_ll 1.1347 loss -1.1347 (7.840 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 86800 lr 2.119e-05 [train_loss] tar_ll 1.1386 loss -1.1386 (7.448 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 87000 lr 2.056e-05 [train_loss] tar_ll 1.0334 loss -1.0334 (7.337 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 87200 lr 1.994e-05 [train_loss] tar_ll 1.1984 loss -1.1984 (8.288 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 87400 lr 1.933e-05 [train_loss] tar_ll 1.1179 loss -1.1179 (7.758 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 87600 lr 1.873e-05 [train_loss] tar_ll 1.0901 loss -1.0901 (7.568 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 87800 lr 1.814e-05 [train_loss] tar_ll 1.2046 loss -1.2046 (7.648 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 88000 lr 1.756e-05 [train_loss] tar_ll 1.2140 loss -1.2140 (7.636 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 88200 lr 1.698e-05 [train_loss] tar_ll 1.1427 loss -1.1427 (7.676 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 88400 lr 1.642e-05 [train_loss] tar_ll 1.2414 loss -1.2414 (7.613 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 88600 lr 1.586e-05 [train_loss] tar_ll 1.1962 loss -1.1962 (7.603 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 88800 lr 1.532e-05 [train_loss] tar_ll 1.2024 loss -1.2024 (7.590 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 89000 lr 1.478e-05 [train_loss] tar_ll 1.1605 loss -1.1605 (7.754 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 89200 lr 1.425e-05 [train_loss] tar_ll 1.1794 loss -1.1794 (7.771 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 89400 lr 1.373e-05 [train_loss] tar_ll 1.1782 loss -1.1782 (7.681 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 89600 lr 1.323e-05 [train_loss] tar_ll 1.2047 loss -1.2047 (7.773 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 89800 lr 1.273e-05 [train_loss] tar_ll 1.1985 loss -1.1985 (7.623 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 90000 lr 1.224e-05 [train_loss] tar_ll 1.0821 loss -1.0821 (7.826 secs)\n",
      "100%|##########| 3000/3000 [00:36<00:00, 82.65it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.8242 loss -0.8242 (36.301 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 90200 lr 1.176e-05 [train_loss] tar_ll 1.1236 loss -1.1236 (7.785 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 90400 lr 1.128e-05 [train_loss] tar_ll 1.1706 loss -1.1706 (7.786 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 90600 lr 1.082e-05 [train_loss] tar_ll 1.1638 loss -1.1638 (7.685 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 90800 lr 1.037e-05 [train_loss] tar_ll 1.1732 loss -1.1732 (7.770 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 91000 lr 9.927e-06 [train_loss] tar_ll 1.1530 loss -1.1530 (7.776 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 91200 lr 9.493e-06 [train_loss] tar_ll 1.2084 loss -1.2084 (7.852 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 91400 lr 9.069e-06 [train_loss] tar_ll 1.1191 loss -1.1191 (7.775 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 91600 lr 8.655e-06 [train_loss] tar_ll 1.1556 loss -1.1556 (7.764 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 91800 lr 8.250e-06 [train_loss] tar_ll 1.2015 loss -1.2015 (7.846 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 92000 lr 7.854e-06 [train_loss] tar_ll 1.2005 loss -1.2005 (7.655 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 92200 lr 7.468e-06 [train_loss] tar_ll 1.0849 loss -1.0849 (7.702 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 92400 lr 7.092e-06 [train_loss] tar_ll 1.1532 loss -1.1532 (7.672 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 92600 lr 6.725e-06 [train_loss] tar_ll 1.1981 loss -1.1981 (7.817 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 92800 lr 6.368e-06 [train_loss] tar_ll 1.2242 loss -1.2242 (7.452 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 93000 lr 6.021e-06 [train_loss] tar_ll 1.0812 loss -1.0812 (6.832 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 93200 lr 5.683e-06 [train_loss] tar_ll 1.2003 loss -1.2003 (6.591 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 93400 lr 5.355e-06 [train_loss] tar_ll 1.1622 loss -1.1622 (6.312 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 93600 lr 5.036e-06 [train_loss] tar_ll 1.1496 loss -1.1496 (6.630 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 93800 lr 4.727e-06 [train_loss] tar_ll 1.1364 loss -1.1364 (6.426 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 94000 lr 4.428e-06 [train_loss] tar_ll 1.2124 loss -1.2124 (6.289 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 94200 lr 4.139e-06 [train_loss] tar_ll 1.1251 loss -1.1251 (6.842 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 94400 lr 3.859e-06 [train_loss] tar_ll 1.2200 loss -1.2200 (6.814 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 94600 lr 3.589e-06 [train_loss] tar_ll 1.1516 loss -1.1516 (6.553 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 94800 lr 3.329e-06 [train_loss] tar_ll 1.2193 loss -1.2193 (6.340 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 95000 lr 3.078e-06 [train_loss] tar_ll 1.1475 loss -1.1475 (6.314 secs)\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.79it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.8330 loss -0.8330 (33.415 secs)\n",
      "\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 95200 lr 2.837e-06 [train_loss] tar_ll 1.1873 loss -1.1873 (6.765 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 95400 lr 2.606e-06 [train_loss] tar_ll 1.1731 loss -1.1731 (6.485 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 95600 lr 2.385e-06 [train_loss] tar_ll 1.1903 loss -1.1903 (6.393 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 95800 lr 2.173e-06 [train_loss] tar_ll 1.2003 loss -1.2003 (6.215 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 96000 lr 1.971e-06 [train_loss] tar_ll 1.1898 loss -1.1898 (6.306 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 96200 lr 1.779e-06 [train_loss] tar_ll 1.2110 loss -1.2110 (6.245 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 96400 lr 1.597e-06 [train_loss] tar_ll 1.2415 loss -1.2415 (6.305 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 96600 lr 1.425e-06 [train_loss] tar_ll 1.1715 loss -1.1715 (6.401 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 96800 lr 1.262e-06 [train_loss] tar_ll 1.1529 loss -1.1529 (6.368 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 97000 lr 1.110e-06 [train_loss] tar_ll 1.1533 loss -1.1533 (6.373 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 97200 lr 9.666e-07 [train_loss] tar_ll 1.1854 loss -1.1854 (6.355 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 97400 lr 8.335e-07 [train_loss] tar_ll 1.2039 loss -1.2039 (6.359 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 97600 lr 7.103e-07 [train_loss] tar_ll 1.2169 loss -1.2169 (6.376 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 97800 lr 5.969e-07 [train_loss] tar_ll 1.1819 loss -1.1819 (6.357 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 98000 lr 4.933e-07 [train_loss] tar_ll 1.1525 loss -1.1525 (6.489 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 98200 lr 3.996e-07 [train_loss] tar_ll 1.1259 loss -1.1259 (7.087 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 98400 lr 3.158e-07 [train_loss] tar_ll 1.2627 loss -1.2627 (6.993 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 98600 lr 2.418e-07 [train_loss] tar_ll 1.1938 loss -1.1938 (6.932 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 98800 lr 1.776e-07 [train_loss] tar_ll 1.0871 loss -1.0871 (6.827 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 99000 lr 1.234e-07 [train_loss] tar_ll 1.1438 loss -1.1438 (6.566 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 99200 lr 7.895e-08 [train_loss] tar_ll 1.1415 loss -1.1415 (6.585 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 99400 lr 4.441e-08 [train_loss] tar_ll 1.1245 loss -1.1245 (6.457 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 99600 lr 1.974e-08 [train_loss] tar_ll 1.1639 loss -1.1639 (6.309 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 99800 lr 4.935e-09 [train_loss] tar_ll 1.1268 loss -1.1268 (6.460 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 step 100000 lr 0.000e+00 [train_loss] tar_ll 1.1441 loss -1.1441 (6.531 secs)\n",
      "100%|##########| 3000/3000 [00:35<00:00, 84.62it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.8342 loss -0.8342 (35.457 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [00:33<00:00, 89.12it/s]\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.8342 loss -0.8342 (33.664 secs)\n",
      "isanp2:isanp2-num_latents-128_matern_0 matern tar_ll 0.8342 loss -0.8342 (33.664 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4423433.5 miliseconds\n",
      "Execution time: 4423.4335 seconds\n",
      "Initial Memory Usage: 0.0 MB\n",
      "Final Memory Usage: 118.8115234375 MB\n",
      "Memory Usage Change: 118.8115234375 MB\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp2', name='isanp2-num_latents-128_matern_0',val_seed=0, val_l=128,val_kernel='matern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dcc1a9-da82-411c-a22f-32e5fdc80054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Experiment: isanp2-isanp2-num_latents-8_periodic_0\n",
      "Total number of parameters: 785602\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "isanp2:isanp2-num_latents-8_periodic_0 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6755 loss 0.6755 (7.136 secs)\n"
     ]
    }
   ],
   "source": [
    "train_mod(modelo='isanp2', name='isanp2-num_latents-8_periodic_0',val_seed=0, val_l=8,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9776ad1-15d0-4ece-abd0-929ec90715a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mod(modelo='isanp2', name='isanp2-num_latents-128_periodic_0',val_seed=0, val_l=128,val_kernel='periodic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead42771-ff81-4d48-920d-1417112d7041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1aa1d6-c2d0-4bbf-a0a0-67a3357b3b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f6cb4-cd97-4a9f-a7c2-013b0da84c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d8fe3-1c4f-478e-ac04-435ecbf1f904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3990f84-7afc-40ba-b2b1-a743e311728e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4073a5-6a38-4f47-bb4c-4308138ef7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c3f4b-c67a-4f39-a91a-4b0bca986b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ebb943-8e95-4b97-b6ab-973b39638177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6ca2f5-c4ab-4ef3-a1c8-67931129f8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mod_track={\n",
    "    'MaxPoints':[50,150,500],\n",
    "    'NP_Memory':[769.05859375,770.7421875,781.95703125],\n",
    "    'NP_Time':[1014.7923424243927,1159.3817811012268, 1820.7999279499054],\n",
    "    'NP_ctx_ll':[0.6772,0.6943,0.5942],\n",
    "    'NP_tar_ll':[0.2588,0.2036,-0.0852],\n",
    "    'NP_Params':[232194],\n",
    "\n",
    "    'ANP_Memory':[784.1796875,790.89453125,806.34765625],\n",
    "    'ANP_Time':[1474.354235649109,1778.3196830749512,4319.987019777298],\n",
    "    'ANP_ctx_ll':[1.3759,1.3762,1.3707],\n",
    "    'ANP_tar_ll':[0.8074,0.7825,0.7193],\n",
    "    'ANP_Params':[348418],\n",
    "\n",
    "    'CNP_Memory':[811.3671875,815.29296875,820.171875],\n",
    "    'CNP_Time':[654.3417699337006,763.373316526413,1293.1838011741638],\n",
    "    'CNP_ctx_ll':[0.7193,0.7080,0.5416],\n",
    "    'CNP_tar_ll':[0.2602,0.2779,0.1377],\n",
    "    'CNP_Params':[215682],\n",
    "\n",
    "    'TNPD_Memory':[825.234375,829.6796875,834.67578125],\n",
    "    'TNPD_Time':[1452.5826616287231,1642.5698385238647,3483.8328511714935],\n",
    "    'TNPD_loss':[-1.3770,-1.3641,-1.0365],\n",
    "    'TNPD_tar_ll':[1.3770,1.3641,1.0365],\n",
    "    'TNPD_Params':[222082],\n",
    "    \n",
    "    'CANP_Memory':[732.3359375,738.859375,754.83984375],\n",
    "    'CANP_Time':[2234.8930954933167,2960.1566257476807,3605.0051345825195],\n",
    "    'CANP_ctx_ll':[1.3757,1.3770,1.3380],\n",
    "    'CANP_tar_ll':[0.7855,0.7790,0.6786],\n",
    "    'CANP_Params':[331906],\n",
    "    \n",
    "    'BNP_Memory':[767.59765625,773.84375,790.48046875],\n",
    "    'BNP_Time':[3085.67346906662,3363.1239960193634,4997.319174528122],\n",
    "    'BNP_ctx_ll':[0.7933,0.7807,0.6223],\n",
    "    'BNP_tar_ll':[0.3820,0.4000,0.2815],\n",
    "    'BNP_Params':[248450],\n",
    "    \n",
    "    #'BANP_Memory':[813.54296875,819.14453125,], MISSING THE LAST 500 CONTEXT POINTS\n",
    "    #'BANP_Time':[4787.938134908676,7226.91014289856,],\n",
    "    #'BANP_ctx_ll':[1.3756,1.3759,],\n",
    "    #'BANP_tar_ll':[0.8255,0.8127,],\n",
    "    #'BANP_Params':[364674],\n",
    "    \n",
    "    'TNPA_Memory':[421.16015625,421.71875,421.78125], #neeed to check\n",
    "    'TNPA_Time':[3137.888471841812,2720.928036928177,9286.913450241089],\n",
    "    'TNPA_ctx_ll':[1.6515,1.6254,1.5095],\n",
    "    'TNPA_tar_ll':[-1.6515,-1.6254,-1.5095],\n",
    "    'TNPA_Params':[222082],\n",
    "    \n",
    "    \n",
    "    #'LBANP8_Memory':[745.69140625,,],\n",
    "    #'LBANP8_Time':[4355.146617889404,,],\n",
    "    #'LBANP8_loss':[-1.2030,,],\n",
    "    #'LBANP8_tar_ll':[1.2030,,],\n",
    "    #'LBANP8_Params':[784834]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04797479-af1e-4e03-b8ea-182d07ac49d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['NP', 'ANP', 'CNP', 'TNPD', 'CANP', 'BNP', 'TNPA']#BANP, TNPA 'LBANP8']\n",
    "colors = ['blue', 'orange', 'green', 'red', 'black', 'purple' ,'grey', 'yellow']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for model, color in zip(models, colors):\n",
    "    plt.plot(mod_track['MaxPoints'], mod_track[f'{model}_Memory'], marker='o', color=color, label=model)\n",
    "\n",
    "plt.title('Memory Usage vs. MaxPoints')\n",
    "plt.xlabel('MaxPoints')\n",
    "plt.ylabel('Memory Usage (MB)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21d49061-3444-491c-bbd6-fc2e75315909",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ['NP', 'ANP', 'CNP', 'TNPD', 'CANP', 'BNP', 'TNPA']#BANP, TNPA 'LBANP8']\n",
    "colors = ['blue', 'orange', 'green', 'red', 'black', 'purple' ,'grey', 'yellow']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for model, color in zip(models, colors):\n",
    "    plt.plot(mod_track['MaxPoints'], mod_track[f'{model}_Time'], marker='o', color=color, label=model)\n",
    "\n",
    "plt.title('Processing Time vs. MaxPoints')\n",
    "plt.xlabel('MaxPoints')\n",
    "plt.ylabel('Processing Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3e3ab6-9956-4ebe-a902-0f8006aa556c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ['NP', 'ANP', 'CNP', 'TNPD', 'CANP', 'BNP', 'TNPA']#BANP, TNPA 'LBANP8']\n",
    "colors = ['blue', 'orange', 'green', 'red', 'black', 'purple' ,'grey', 'yellow']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for model, color in zip(models, colors):\n",
    "    plt.plot(mod_track['MaxPoints'], mod_track[f'{model}_tar_ll'], marker='o', color=color, label=model)\n",
    "\n",
    "plt.title('tar_ll vs. MaxPoints')\n",
    "plt.xlabel('MaxPoints')\n",
    "plt.ylabel('tar_ll')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4e39a-b5e1-4226-be7b-697604669dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd738b-4828-4fea-9efc-86ba5aee8f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b848a34-70f5-43c1-974e-b0ade5cdaa10",
   "metadata": {},
   "source": [
    "## Testing for different values of L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f2bf70-7f72-4b76-9d3c-71a29625b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 68.3984375 MB\n",
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: lbanp-lbanp-num_latents-8\n",
      "Total number of parameters: 784834\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "rbf-seed0.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6614 loss 0.6614 (14.634 secs)\n",
      "lbanp:lbanp-num_latents-8 step 400 lr 4.999e-04 [train_loss] tar_ll -0.5650 loss 0.5650 (14.052 secs)\n",
      "lbanp:lbanp-num_latents-8 step 600 lr 4.998e-04 [train_loss] tar_ll -0.4314 loss 0.4314 (13.744 secs)\n",
      "lbanp:lbanp-num_latents-8 step 800 lr 4.997e-04 [train_loss] tar_ll -0.2684 loss 0.2684 (14.572 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1000 lr 4.995e-04 [train_loss] tar_ll -0.0248 loss 0.0248 (14.086 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1200 lr 4.993e-04 [train_loss] tar_ll -0.0134 loss 0.0134 (14.410 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1400 lr 4.990e-04 [train_loss] tar_ll 0.1698 loss -0.1698 (14.357 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1600 lr 4.987e-04 [train_loss] tar_ll 0.2980 loss -0.2980 (14.476 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1800 lr 4.984e-04 [train_loss] tar_ll 0.4493 loss -0.4493 (14.485 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2000 lr 4.980e-04 [train_loss] tar_ll 0.3759 loss -0.3759 (14.189 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2200 lr 4.976e-04 [train_loss] tar_ll 0.4020 loss -0.4020 (15.133 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2400 lr 4.972e-04 [train_loss] tar_ll 0.5513 loss -0.5513 (15.262 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2600 lr 4.967e-04 [train_loss] tar_ll 0.4659 loss -0.4659 (14.657 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2800 lr 4.961e-04 [train_loss] tar_ll 0.6150 loss -0.6150 (14.658 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3000 lr 4.956e-04 [train_loss] tar_ll 0.7180 loss -0.7180 (14.507 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3200 lr 4.950e-04 [train_loss] tar_ll 0.4445 loss -0.4445 (14.680 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3400 lr 4.943e-04 [train_loss] tar_ll 0.5744 loss -0.5744 (15.910 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3600 lr 4.936e-04 [train_loss] tar_ll 0.7177 loss -0.7177 (15.842 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3800 lr 4.929e-04 [train_loss] tar_ll 0.7077 loss -0.7077 (15.213 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4000 lr 4.921e-04 [train_loss] tar_ll 0.6908 loss -0.6908 (15.506 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4200 lr 4.913e-04 [train_loss] tar_ll 0.8190 loss -0.8190 (15.155 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4400 lr 4.905e-04 [train_loss] tar_ll 0.8299 loss -0.8299 (15.195 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4600 lr 4.896e-04 [train_loss] tar_ll 0.8392 loss -0.8392 (15.125 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4800 lr 4.887e-04 [train_loss] tar_ll 0.7782 loss -0.7782 (15.350 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5000 lr 4.878e-04 [train_loss] tar_ll 0.8485 loss -0.8485 (15.396 secs)\n",
      "100%|##########| 3000/3000 [01:08<00:00, 43.52it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.4491 loss -0.4491 (68.943 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 5200 lr 4.868e-04 [train_loss] tar_ll 0.8247 loss -0.8247 (14.806 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5400 lr 4.857e-04 [train_loss] tar_ll 0.8825 loss -0.8825 (14.801 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5600 lr 4.847e-04 [train_loss] tar_ll 0.8104 loss -0.8104 (14.799 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5800 lr 4.836e-04 [train_loss] tar_ll 0.8924 loss -0.8924 (14.889 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6000 lr 4.824e-04 [train_loss] tar_ll 0.6882 loss -0.6882 (14.639 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6200 lr 4.813e-04 [train_loss] tar_ll 0.8847 loss -0.8847 (15.063 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6400 lr 4.801e-04 [train_loss] tar_ll 0.8134 loss -0.8134 (15.340 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6600 lr 4.788e-04 [train_loss] tar_ll 0.7847 loss -0.7847 (15.323 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6800 lr 4.775e-04 [train_loss] tar_ll 0.9733 loss -0.9733 (15.208 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7000 lr 4.762e-04 [train_loss] tar_ll 0.9447 loss -0.9447 (15.006 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7200 lr 4.749e-04 [train_loss] tar_ll 0.6946 loss -0.6946 (15.530 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7400 lr 4.735e-04 [train_loss] tar_ll 0.8353 loss -0.8353 (15.060 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7600 lr 4.720e-04 [train_loss] tar_ll 0.8863 loss -0.8863 (15.143 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7800 lr 4.706e-04 [train_loss] tar_ll 0.9008 loss -0.9008 (14.946 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8000 lr 4.691e-04 [train_loss] tar_ll 1.0154 loss -1.0154 (14.623 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8200 lr 4.675e-04 [train_loss] tar_ll 0.9928 loss -0.9928 (15.117 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8400 lr 4.660e-04 [train_loss] tar_ll 1.0295 loss -1.0295 (15.093 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8600 lr 4.644e-04 [train_loss] tar_ll 0.9715 loss -0.9715 (14.947 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8800 lr 4.627e-04 [train_loss] tar_ll 1.0248 loss -1.0248 (14.711 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9000 lr 4.611e-04 [train_loss] tar_ll 1.0287 loss -1.0287 (14.571 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9200 lr 4.594e-04 [train_loss] tar_ll 1.0629 loss -1.0629 (14.594 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9400 lr 4.576e-04 [train_loss] tar_ll 0.9447 loss -0.9447 (14.343 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9600 lr 4.559e-04 [train_loss] tar_ll 1.0366 loss -1.0366 (14.996 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9800 lr 4.541e-04 [train_loss] tar_ll 1.0322 loss -1.0322 (14.916 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10000 lr 4.523e-04 [train_loss] tar_ll 1.0088 loss -1.0088 (14.846 secs)\n",
      "100%|##########| 3000/3000 [01:08<00:00, 43.91it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.6382 loss -0.6382 (68.329 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 10200 lr 4.504e-04 [train_loss] tar_ll 1.1258 loss -1.1258 (14.799 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10400 lr 4.485e-04 [train_loss] tar_ll 1.1115 loss -1.1115 (14.636 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10600 lr 4.466e-04 [train_loss] tar_ll 1.0862 loss -1.0862 (14.563 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10800 lr 4.446e-04 [train_loss] tar_ll 1.0365 loss -1.0365 (14.798 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11000 lr 4.426e-04 [train_loss] tar_ll 1.2004 loss -1.2004 (14.719 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11200 lr 4.406e-04 [train_loss] tar_ll 1.1393 loss -1.1393 (14.496 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11400 lr 4.386e-04 [train_loss] tar_ll 1.1653 loss -1.1653 (14.872 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11600 lr 4.365e-04 [train_loss] tar_ll 1.1906 loss -1.1906 (14.522 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11800 lr 4.344e-04 [train_loss] tar_ll 1.1745 loss -1.1745 (14.948 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12000 lr 4.322e-04 [train_loss] tar_ll 1.1763 loss -1.1763 (15.239 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12200 lr 4.301e-04 [train_loss] tar_ll 1.1483 loss -1.1483 (15.456 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12400 lr 4.279e-04 [train_loss] tar_ll 1.1288 loss -1.1288 (15.177 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12600 lr 4.257e-04 [train_loss] tar_ll 1.1433 loss -1.1433 (14.934 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12800 lr 4.234e-04 [train_loss] tar_ll 1.1435 loss -1.1435 (14.979 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13000 lr 4.211e-04 [train_loss] tar_ll 1.2282 loss -1.2282 (15.041 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13200 lr 4.188e-04 [train_loss] tar_ll 1.2293 loss -1.2293 (15.203 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13400 lr 4.165e-04 [train_loss] tar_ll 1.0376 loss -1.0376 (14.813 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13600 lr 4.141e-04 [train_loss] tar_ll 1.0798 loss -1.0798 (14.866 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13800 lr 4.118e-04 [train_loss] tar_ll 1.2143 loss -1.2143 (14.458 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14000 lr 4.094e-04 [train_loss] tar_ll 1.2135 loss -1.2135 (14.403 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14200 lr 4.069e-04 [train_loss] tar_ll 1.2094 loss -1.2094 (14.917 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14400 lr 4.045e-04 [train_loss] tar_ll 1.2580 loss -1.2580 (14.639 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14600 lr 4.020e-04 [train_loss] tar_ll 1.2477 loss -1.2477 (14.656 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14800 lr 3.995e-04 [train_loss] tar_ll 1.1872 loss -1.1872 (14.217 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15000 lr 3.969e-04 [train_loss] tar_ll 1.2050 loss -1.2050 (14.997 secs)\n",
      "100%|##########| 3000/3000 [01:09<00:00, 43.38it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.7794 loss -0.7794 (69.164 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 15200 lr 3.944e-04 [train_loss] tar_ll 1.1790 loss -1.1790 (15.069 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15400 lr 3.918e-04 [train_loss] tar_ll 1.2310 loss -1.2310 (15.020 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15600 lr 3.892e-04 [train_loss] tar_ll 1.2337 loss -1.2337 (14.831 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15800 lr 3.866e-04 [train_loss] tar_ll 1.2916 loss -1.2916 (14.838 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16000 lr 3.840e-04 [train_loss] tar_ll 1.1379 loss -1.1379 (15.009 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16200 lr 3.813e-04 [train_loss] tar_ll 1.2668 loss -1.2668 (14.996 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16400 lr 3.786e-04 [train_loss] tar_ll 1.2456 loss -1.2456 (14.891 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16600 lr 3.759e-04 [train_loss] tar_ll 1.2891 loss -1.2891 (14.716 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16800 lr 3.732e-04 [train_loss] tar_ll 1.2301 loss -1.2301 (14.501 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17000 lr 3.704e-04 [train_loss] tar_ll 1.2750 loss -1.2750 (14.960 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17200 lr 3.677e-04 [train_loss] tar_ll 1.2774 loss -1.2774 (15.152 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17400 lr 3.649e-04 [train_loss] tar_ll 1.2715 loss -1.2715 (15.046 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17600 lr 3.621e-04 [train_loss] tar_ll 1.3722 loss -1.3722 (14.890 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17800 lr 3.593e-04 [train_loss] tar_ll 1.3396 loss -1.3396 (14.807 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18000 lr 3.564e-04 [train_loss] tar_ll 1.2750 loss -1.2750 (15.087 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18200 lr 3.536e-04 [train_loss] tar_ll 1.2340 loss -1.2340 (14.911 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18400 lr 3.507e-04 [train_loss] tar_ll 1.3300 loss -1.3300 (14.943 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18600 lr 3.478e-04 [train_loss] tar_ll 1.3458 loss -1.3458 (14.787 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18800 lr 3.449e-04 [train_loss] tar_ll 1.2568 loss -1.2568 (14.719 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19000 lr 3.420e-04 [train_loss] tar_ll 1.3125 loss -1.3125 (14.699 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19200 lr 3.391e-04 [train_loss] tar_ll 1.3055 loss -1.3055 (14.845 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19400 lr 3.362e-04 [train_loss] tar_ll 1.3872 loss -1.3872 (14.744 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19600 lr 3.332e-04 [train_loss] tar_ll 1.2947 loss -1.2947 (14.817 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19800 lr 3.302e-04 [train_loss] tar_ll 1.3577 loss -1.3577 (14.884 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20000 lr 3.273e-04 [train_loss] tar_ll 1.3140 loss -1.3140 (14.465 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.05it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.8215 loss -0.8215 (66.597 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 20200 lr 3.243e-04 [train_loss] tar_ll 1.2883 loss -1.2883 (14.896 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20400 lr 3.213e-04 [train_loss] tar_ll 1.3723 loss -1.3723 (14.932 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20600 lr 3.182e-04 [train_loss] tar_ll 1.3676 loss -1.3676 (14.756 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20800 lr 3.152e-04 [train_loss] tar_ll 1.3568 loss -1.3568 (15.020 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21000 lr 3.122e-04 [train_loss] tar_ll 1.4049 loss -1.4049 (15.039 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21200 lr 3.091e-04 [train_loss] tar_ll 1.3279 loss -1.3279 (14.996 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21400 lr 3.061e-04 [train_loss] tar_ll 1.4286 loss -1.4286 (14.487 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21600 lr 3.030e-04 [train_loss] tar_ll 1.4191 loss -1.4191 (14.495 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21800 lr 2.999e-04 [train_loss] tar_ll 1.3050 loss -1.3050 (14.480 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22000 lr 2.968e-04 [train_loss] tar_ll 1.4341 loss -1.4341 (14.712 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22200 lr 2.938e-04 [train_loss] tar_ll 1.4004 loss -1.4004 (14.692 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22400 lr 2.907e-04 [train_loss] tar_ll 1.3849 loss -1.3849 (14.572 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22600 lr 2.876e-04 [train_loss] tar_ll 1.4182 loss -1.4182 (14.723 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22800 lr 2.844e-04 [train_loss] tar_ll 1.3342 loss -1.3342 (14.573 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23000 lr 2.813e-04 [train_loss] tar_ll 1.4009 loss -1.4009 (15.217 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23200 lr 2.782e-04 [train_loss] tar_ll 1.4596 loss -1.4596 (15.151 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23400 lr 2.751e-04 [train_loss] tar_ll 1.4752 loss -1.4752 (14.868 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23600 lr 2.720e-04 [train_loss] tar_ll 1.4238 loss -1.4238 (14.987 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23800 lr 2.688e-04 [train_loss] tar_ll 1.4237 loss -1.4237 (14.867 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24000 lr 2.657e-04 [train_loss] tar_ll 1.3908 loss -1.3908 (15.017 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24200 lr 2.626e-04 [train_loss] tar_ll 1.4676 loss -1.4676 (15.020 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24400 lr 2.594e-04 [train_loss] tar_ll 1.4719 loss -1.4719 (14.804 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24600 lr 2.563e-04 [train_loss] tar_ll 1.4404 loss -1.4404 (15.093 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24800 lr 2.531e-04 [train_loss] tar_ll 1.4290 loss -1.4290 (14.731 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25000 lr 2.500e-04 [train_loss] tar_ll 1.5023 loss -1.5023 (14.863 secs)\n",
      "100%|##########| 3000/3000 [01:08<00:00, 43.88it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.9403 loss -0.9403 (68.371 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 25200 lr 2.469e-04 [train_loss] tar_ll 1.4683 loss -1.4683 (15.137 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25400 lr 2.437e-04 [train_loss] tar_ll 1.4073 loss -1.4073 (14.851 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25600 lr 2.406e-04 [train_loss] tar_ll 1.4381 loss -1.4381 (14.811 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25800 lr 2.374e-04 [train_loss] tar_ll 1.5171 loss -1.5171 (15.000 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26000 lr 2.343e-04 [train_loss] tar_ll 1.4451 loss -1.4451 (15.084 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26200 lr 2.312e-04 [train_loss] tar_ll 1.4208 loss -1.4208 (14.820 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26400 lr 2.280e-04 [train_loss] tar_ll 1.5604 loss -1.5604 (14.949 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26600 lr 2.249e-04 [train_loss] tar_ll 1.4840 loss -1.4840 (14.680 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26800 lr 2.218e-04 [train_loss] tar_ll 1.4516 loss -1.4516 (14.750 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27000 lr 2.187e-04 [train_loss] tar_ll 1.4977 loss -1.4977 (14.670 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27200 lr 2.156e-04 [train_loss] tar_ll 1.4527 loss -1.4527 (14.841 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27400 lr 2.124e-04 [train_loss] tar_ll 1.5385 loss -1.5385 (14.962 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27600 lr 2.093e-04 [train_loss] tar_ll 1.4034 loss -1.4034 (14.775 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27800 lr 2.062e-04 [train_loss] tar_ll 1.5201 loss -1.5201 (14.760 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28000 lr 2.032e-04 [train_loss] tar_ll 1.5276 loss -1.5276 (14.584 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28200 lr 2.001e-04 [train_loss] tar_ll 1.4943 loss -1.4943 (14.609 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28400 lr 1.970e-04 [train_loss] tar_ll 1.4927 loss -1.4927 (14.589 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28600 lr 1.939e-04 [train_loss] tar_ll 1.5786 loss -1.5786 (14.821 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28800 lr 1.909e-04 [train_loss] tar_ll 1.5446 loss -1.5446 (15.300 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29000 lr 1.878e-04 [train_loss] tar_ll 1.5318 loss -1.5318 (14.892 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29200 lr 1.848e-04 [train_loss] tar_ll 1.5803 loss -1.5803 (15.046 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29400 lr 1.818e-04 [train_loss] tar_ll 1.4964 loss -1.4964 (14.850 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29600 lr 1.787e-04 [train_loss] tar_ll 1.5124 loss -1.5124 (15.135 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29800 lr 1.757e-04 [train_loss] tar_ll 1.5086 loss -1.5086 (14.936 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30000 lr 1.727e-04 [train_loss] tar_ll 1.5385 loss -1.5385 (14.725 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.27it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.9563 loss -0.9563 (67.769 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 30200 lr 1.698e-04 [train_loss] tar_ll 1.5232 loss -1.5232 (14.756 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30400 lr 1.668e-04 [train_loss] tar_ll 1.5510 loss -1.5510 (14.366 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30600 lr 1.638e-04 [train_loss] tar_ll 1.5931 loss -1.5931 (14.407 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30800 lr 1.609e-04 [train_loss] tar_ll 1.5990 loss -1.5990 (15.107 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31000 lr 1.580e-04 [train_loss] tar_ll 1.6092 loss -1.6092 (14.789 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31200 lr 1.551e-04 [train_loss] tar_ll 1.5457 loss -1.5457 (15.067 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31400 lr 1.522e-04 [train_loss] tar_ll 1.5600 loss -1.5600 (14.830 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31600 lr 1.493e-04 [train_loss] tar_ll 1.6055 loss -1.6055 (14.704 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31800 lr 1.464e-04 [train_loss] tar_ll 1.5912 loss -1.5912 (14.672 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32000 lr 1.436e-04 [train_loss] tar_ll 1.5857 loss -1.5857 (14.524 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32200 lr 1.407e-04 [train_loss] tar_ll 1.5858 loss -1.5858 (14.858 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32400 lr 1.379e-04 [train_loss] tar_ll 1.5666 loss -1.5666 (14.477 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32600 lr 1.351e-04 [train_loss] tar_ll 1.5777 loss -1.5777 (14.702 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32800 lr 1.323e-04 [train_loss] tar_ll 1.5616 loss -1.5616 (14.466 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33000 lr 1.296e-04 [train_loss] tar_ll 1.5498 loss -1.5498 (14.541 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33200 lr 1.268e-04 [train_loss] tar_ll 1.6050 loss -1.6050 (14.376 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33400 lr 1.241e-04 [train_loss] tar_ll 1.6162 loss -1.6162 (14.387 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33600 lr 1.214e-04 [train_loss] tar_ll 1.6460 loss -1.6460 (14.494 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33800 lr 1.187e-04 [train_loss] tar_ll 1.6380 loss -1.6380 (14.777 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34000 lr 1.160e-04 [train_loss] tar_ll 1.4480 loss -1.4480 (14.681 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34200 lr 1.134e-04 [train_loss] tar_ll 1.6667 loss -1.6667 (14.888 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34400 lr 1.108e-04 [train_loss] tar_ll 1.5552 loss -1.5552 (14.637 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34600 lr 1.082e-04 [train_loss] tar_ll 1.6793 loss -1.6793 (14.647 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34800 lr 1.056e-04 [train_loss] tar_ll 1.6563 loss -1.6563 (14.767 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35000 lr 1.031e-04 [train_loss] tar_ll 1.6596 loss -1.6596 (14.573 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.03it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.0530 loss -1.0530 (66.621 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 35200 lr 1.005e-04 [train_loss] tar_ll 1.6315 loss -1.6315 (14.667 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35400 lr 9.802e-05 [train_loss] tar_ll 1.6595 loss -1.6595 (14.620 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35600 lr 9.554e-05 [train_loss] tar_ll 1.6247 loss -1.6247 (14.603 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35800 lr 9.308e-05 [train_loss] tar_ll 1.6550 loss -1.6550 (14.610 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36000 lr 9.064e-05 [train_loss] tar_ll 1.6358 loss -1.6358 (14.368 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36200 lr 8.824e-05 [train_loss] tar_ll 1.6223 loss -1.6223 (14.585 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36400 lr 8.585e-05 [train_loss] tar_ll 1.6862 loss -1.6862 (14.676 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36600 lr 8.350e-05 [train_loss] tar_ll 1.7235 loss -1.7235 (14.809 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36800 lr 8.117e-05 [train_loss] tar_ll 1.6547 loss -1.6547 (14.956 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37000 lr 7.886e-05 [train_loss] tar_ll 1.6237 loss -1.6237 (14.868 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37200 lr 7.659e-05 [train_loss] tar_ll 1.7185 loss -1.7185 (14.515 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37400 lr 7.434e-05 [train_loss] tar_ll 1.6475 loss -1.6475 (14.760 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37600 lr 7.212e-05 [train_loss] tar_ll 1.6216 loss -1.6216 (14.834 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37800 lr 6.992e-05 [train_loss] tar_ll 1.7165 loss -1.7165 (14.875 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38000 lr 6.776e-05 [train_loss] tar_ll 1.7148 loss -1.7148 (14.307 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38200 lr 6.562e-05 [train_loss] tar_ll 1.6844 loss -1.6844 (14.598 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38400 lr 6.351e-05 [train_loss] tar_ll 1.7067 loss -1.7067 (14.578 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38600 lr 6.144e-05 [train_loss] tar_ll 1.6757 loss -1.6757 (14.418 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38800 lr 5.939e-05 [train_loss] tar_ll 1.7659 loss -1.7659 (14.161 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39000 lr 5.737e-05 [train_loss] tar_ll 1.6560 loss -1.6560 (14.674 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39200 lr 5.538e-05 [train_loss] tar_ll 1.7128 loss -1.7128 (14.668 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39400 lr 5.343e-05 [train_loss] tar_ll 1.6737 loss -1.6737 (14.657 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39600 lr 5.150e-05 [train_loss] tar_ll 1.6927 loss -1.6927 (14.711 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39800 lr 4.961e-05 [train_loss] tar_ll 1.7199 loss -1.7199 (14.751 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40000 lr 4.775e-05 [train_loss] tar_ll 1.6287 loss -1.6287 (14.849 secs)\n",
      "100%|##########| 3000/3000 [01:08<00:00, 43.66it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.1165 loss -1.1165 (68.707 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 40200 lr 4.592e-05 [train_loss] tar_ll 1.6690 loss -1.6690 (14.668 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40400 lr 4.412e-05 [train_loss] tar_ll 1.7453 loss -1.7453 (14.362 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40600 lr 4.235e-05 [train_loss] tar_ll 1.6973 loss -1.6973 (14.540 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40800 lr 4.062e-05 [train_loss] tar_ll 1.6774 loss -1.6774 (14.524 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41000 lr 3.892e-05 [train_loss] tar_ll 1.6884 loss -1.6884 (14.348 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41200 lr 3.725e-05 [train_loss] tar_ll 1.6833 loss -1.6833 (14.542 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41400 lr 3.562e-05 [train_loss] tar_ll 1.6659 loss -1.6659 (14.143 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41600 lr 3.402e-05 [train_loss] tar_ll 1.7979 loss -1.7979 (14.269 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41800 lr 3.245e-05 [train_loss] tar_ll 1.7156 loss -1.7156 (14.879 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42000 lr 3.092e-05 [train_loss] tar_ll 1.7325 loss -1.7325 (14.843 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42200 lr 2.943e-05 [train_loss] tar_ll 1.6944 loss -1.6944 (14.817 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42400 lr 2.797e-05 [train_loss] tar_ll 1.7145 loss -1.7145 (14.627 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42600 lr 2.654e-05 [train_loss] tar_ll 1.7054 loss -1.7054 (14.317 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42800 lr 2.515e-05 [train_loss] tar_ll 1.6946 loss -1.6946 (14.562 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43000 lr 2.379e-05 [train_loss] tar_ll 1.7273 loss -1.7273 (14.547 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43200 lr 2.247e-05 [train_loss] tar_ll 1.7449 loss -1.7449 (14.586 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43400 lr 2.119e-05 [train_loss] tar_ll 1.7075 loss -1.7075 (14.530 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43600 lr 1.994e-05 [train_loss] tar_ll 1.7332 loss -1.7332 (14.445 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43800 lr 1.873e-05 [train_loss] tar_ll 1.6895 loss -1.6895 (14.342 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44000 lr 1.756e-05 [train_loss] tar_ll 1.7647 loss -1.7647 (14.536 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44200 lr 1.642e-05 [train_loss] tar_ll 1.6876 loss -1.6876 (14.237 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44400 lr 1.532e-05 [train_loss] tar_ll 1.6198 loss -1.6198 (14.311 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44600 lr 1.425e-05 [train_loss] tar_ll 1.7716 loss -1.7716 (14.334 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44800 lr 1.323e-05 [train_loss] tar_ll 1.6086 loss -1.6086 (14.286 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45000 lr 1.224e-05 [train_loss] tar_ll 1.8178 loss -1.8178 (14.195 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.44it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.1431 loss -1.1431 (67.514 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 45200 lr 1.128e-05 [train_loss] tar_ll 1.7835 loss -1.7835 (14.644 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45400 lr 1.037e-05 [train_loss] tar_ll 1.7547 loss -1.7547 (14.700 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45600 lr 9.493e-06 [train_loss] tar_ll 1.7747 loss -1.7747 (14.772 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45800 lr 8.655e-06 [train_loss] tar_ll 1.7924 loss -1.7924 (14.834 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46000 lr 7.854e-06 [train_loss] tar_ll 1.8165 loss -1.8165 (14.341 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46200 lr 7.092e-06 [train_loss] tar_ll 1.7508 loss -1.7508 (14.488 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46400 lr 6.368e-06 [train_loss] tar_ll 1.7378 loss -1.7378 (14.279 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46600 lr 5.683e-06 [train_loss] tar_ll 1.7038 loss -1.7038 (14.390 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46800 lr 5.036e-06 [train_loss] tar_ll 1.8260 loss -1.8260 (14.518 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47000 lr 4.428e-06 [train_loss] tar_ll 1.7615 loss -1.7615 (14.342 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47200 lr 3.859e-06 [train_loss] tar_ll 1.7772 loss -1.7772 (14.360 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47400 lr 3.329e-06 [train_loss] tar_ll 1.7344 loss -1.7344 (14.168 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47600 lr 2.837e-06 [train_loss] tar_ll 1.7407 loss -1.7407 (14.420 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47800 lr 2.385e-06 [train_loss] tar_ll 1.7454 loss -1.7454 (14.572 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48000 lr 1.971e-06 [train_loss] tar_ll 1.7188 loss -1.7188 (14.523 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48200 lr 1.597e-06 [train_loss] tar_ll 1.7562 loss -1.7562 (14.683 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48400 lr 1.262e-06 [train_loss] tar_ll 1.7345 loss -1.7345 (14.655 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48600 lr 9.666e-07 [train_loss] tar_ll 1.8301 loss -1.8301 (15.223 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48800 lr 7.103e-07 [train_loss] tar_ll 1.7814 loss -1.7814 (14.631 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49000 lr 4.933e-07 [train_loss] tar_ll 1.7436 loss -1.7436 (14.485 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49200 lr 3.158e-07 [train_loss] tar_ll 1.8087 loss -1.8087 (14.681 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49400 lr 1.776e-07 [train_loss] tar_ll 1.7566 loss -1.7566 (14.665 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49600 lr 7.895e-08 [train_loss] tar_ll 1.7442 loss -1.7442 (14.495 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49800 lr 1.974e-08 [train_loss] tar_ll 1.7414 loss -1.7414 (14.486 secs)\n",
      "lbanp:lbanp-num_latents-8 step 50000 lr 0.000e+00 [train_loss] tar_ll 1.7603 loss -1.7603 (14.567 secs)\n",
      "100%|##########| 3000/3000 [01:08<00:00, 44.07it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.1506 loss -1.1506 (68.068 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [01:08<00:00, 43.95it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.1506 loss -1.1506 (68.264 secs)\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.1506 loss -1.1506 (68.264 secs)\n",
      "Experiment: lbanp-lbanp-num_latents-8\n",
      "Total number of parameters: 788418\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4457.026439905167 seconds\n",
      "Memory usage: 744.46484375 MB\n",
      "Memory usage: 676.06640625 MB\n",
      "Memory usage: 744.46484375 MB\n",
      "Overriding argument num_latents: 64\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "rbf-seed0.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6771 loss 0.6771 (14.885 secs)\n",
      "lbanp:lbanp-num_latents-8 step 400 lr 4.999e-04 [train_loss] tar_ll -0.5786 loss 0.5786 (14.948 secs)\n",
      "lbanp:lbanp-num_latents-8 step 600 lr 4.998e-04 [train_loss] tar_ll -0.5062 loss 0.5062 (14.820 secs)\n",
      "lbanp:lbanp-num_latents-8 step 800 lr 4.997e-04 [train_loss] tar_ll -0.3057 loss 0.3057 (14.717 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1000 lr 4.995e-04 [train_loss] tar_ll -0.0813 loss 0.0813 (15.078 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1200 lr 4.993e-04 [train_loss] tar_ll -0.0839 loss 0.0839 (14.900 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1400 lr 4.990e-04 [train_loss] tar_ll 0.1923 loss -0.1923 (14.672 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1600 lr 4.987e-04 [train_loss] tar_ll 0.2882 loss -0.2882 (14.663 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1800 lr 4.984e-04 [train_loss] tar_ll 0.5115 loss -0.5115 (14.716 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2000 lr 4.980e-04 [train_loss] tar_ll 0.3573 loss -0.3573 (14.159 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2200 lr 4.976e-04 [train_loss] tar_ll 0.5251 loss -0.5251 (14.701 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2400 lr 4.972e-04 [train_loss] tar_ll 0.6738 loss -0.6738 (14.741 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2600 lr 4.967e-04 [train_loss] tar_ll 0.5740 loss -0.5740 (15.097 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2800 lr 4.961e-04 [train_loss] tar_ll 0.7799 loss -0.7799 (14.986 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3000 lr 4.956e-04 [train_loss] tar_ll 0.9187 loss -0.9187 (14.967 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3200 lr 4.950e-04 [train_loss] tar_ll 0.8336 loss -0.8336 (14.817 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3400 lr 4.943e-04 [train_loss] tar_ll 0.8965 loss -0.8965 (15.226 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3600 lr 4.936e-04 [train_loss] tar_ll 0.8605 loss -0.8605 (15.190 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3800 lr 4.929e-04 [train_loss] tar_ll 0.8495 loss -0.8495 (14.999 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4000 lr 4.921e-04 [train_loss] tar_ll 0.7287 loss -0.7287 (14.684 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4200 lr 4.913e-04 [train_loss] tar_ll 1.0016 loss -1.0016 (14.847 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4400 lr 4.905e-04 [train_loss] tar_ll 1.0774 loss -1.0774 (14.597 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4600 lr 4.896e-04 [train_loss] tar_ll 1.0923 loss -1.0923 (14.654 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4800 lr 4.887e-04 [train_loss] tar_ll 1.0513 loss -1.0513 (14.485 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5000 lr 4.878e-04 [train_loss] tar_ll 1.0678 loss -1.0678 (14.130 secs)\n",
      "100%|##########| 3000/3000 [01:09<00:00, 43.25it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.5455 loss -0.5455 (69.371 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 5200 lr 4.868e-04 [train_loss] tar_ll 0.9164 loss -0.9164 (15.132 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5400 lr 4.857e-04 [train_loss] tar_ll 0.9120 loss -0.9120 (14.636 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5600 lr 4.847e-04 [train_loss] tar_ll 0.9731 loss -0.9731 (14.939 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5800 lr 4.836e-04 [train_loss] tar_ll 1.1472 loss -1.1472 (15.156 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6000 lr 4.824e-04 [train_loss] tar_ll 1.0421 loss -1.0421 (14.904 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6200 lr 4.813e-04 [train_loss] tar_ll 1.0925 loss -1.0925 (15.077 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6400 lr 4.801e-04 [train_loss] tar_ll 1.0975 loss -1.0975 (14.788 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6600 lr 4.788e-04 [train_loss] tar_ll 1.1696 loss -1.1696 (14.855 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6800 lr 4.775e-04 [train_loss] tar_ll 1.2070 loss -1.2070 (14.205 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7000 lr 4.762e-04 [train_loss] tar_ll 1.2529 loss -1.2529 (14.620 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7200 lr 4.749e-04 [train_loss] tar_ll 1.1528 loss -1.1528 (14.667 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7400 lr 4.735e-04 [train_loss] tar_ll 1.2036 loss -1.2036 (14.802 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7600 lr 4.720e-04 [train_loss] tar_ll 1.2165 loss -1.2165 (14.892 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7800 lr 4.706e-04 [train_loss] tar_ll 1.1833 loss -1.1833 (14.794 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8000 lr 4.691e-04 [train_loss] tar_ll 1.1919 loss -1.1919 (14.462 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8200 lr 4.675e-04 [train_loss] tar_ll 1.2157 loss -1.2157 (15.272 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8400 lr 4.660e-04 [train_loss] tar_ll 1.2834 loss -1.2834 (15.126 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8600 lr 4.644e-04 [train_loss] tar_ll 1.3168 loss -1.3168 (14.895 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8800 lr 4.627e-04 [train_loss] tar_ll 1.2939 loss -1.2939 (14.954 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9000 lr 4.611e-04 [train_loss] tar_ll 1.2152 loss -1.2152 (15.113 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9200 lr 4.594e-04 [train_loss] tar_ll 1.3270 loss -1.3270 (14.873 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9400 lr 4.576e-04 [train_loss] tar_ll 1.2505 loss -1.2505 (14.692 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9600 lr 4.559e-04 [train_loss] tar_ll 1.3614 loss -1.3614 (14.749 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9800 lr 4.541e-04 [train_loss] tar_ll 1.2855 loss -1.2855 (14.713 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10000 lr 4.523e-04 [train_loss] tar_ll 1.3963 loss -1.3963 (14.633 secs)\n",
      "100%|##########| 3000/3000 [01:08<00:00, 44.03it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.8887 loss -0.8887 (68.138 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 10200 lr 4.504e-04 [train_loss] tar_ll 1.2522 loss -1.2522 (15.161 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10400 lr 4.485e-04 [train_loss] tar_ll 1.2671 loss -1.2671 (14.809 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10600 lr 4.466e-04 [train_loss] tar_ll 1.4014 loss -1.4014 (14.849 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10800 lr 4.446e-04 [train_loss] tar_ll 1.3750 loss -1.3750 (14.946 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11000 lr 4.426e-04 [train_loss] tar_ll 1.3190 loss -1.3190 (15.091 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11200 lr 4.406e-04 [train_loss] tar_ll 1.3392 loss -1.3392 (15.381 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11400 lr 4.386e-04 [train_loss] tar_ll 1.3217 loss -1.3217 (15.053 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11600 lr 4.365e-04 [train_loss] tar_ll 1.3362 loss -1.3362 (15.125 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11800 lr 4.344e-04 [train_loss] tar_ll 1.3069 loss -1.3069 (14.670 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12000 lr 4.322e-04 [train_loss] tar_ll 1.2915 loss -1.2915 (14.591 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12200 lr 4.301e-04 [train_loss] tar_ll 1.3309 loss -1.3309 (14.777 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12400 lr 4.279e-04 [train_loss] tar_ll 1.3668 loss -1.3668 (14.580 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12600 lr 4.257e-04 [train_loss] tar_ll 1.5050 loss -1.5050 (14.710 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12800 lr 4.234e-04 [train_loss] tar_ll 1.3582 loss -1.3582 (14.660 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13000 lr 4.211e-04 [train_loss] tar_ll 1.3224 loss -1.3224 (14.347 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13200 lr 4.188e-04 [train_loss] tar_ll 1.3337 loss -1.3337 (14.575 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13400 lr 4.165e-04 [train_loss] tar_ll 1.3491 loss -1.3491 (14.732 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13600 lr 4.141e-04 [train_loss] tar_ll 1.3541 loss -1.3541 (14.682 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13800 lr 4.118e-04 [train_loss] tar_ll 1.4405 loss -1.4405 (14.940 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14000 lr 4.094e-04 [train_loss] tar_ll 1.4124 loss -1.4124 (15.163 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14200 lr 4.069e-04 [train_loss] tar_ll 1.4450 loss -1.4450 (15.138 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14400 lr 4.045e-04 [train_loss] tar_ll 1.3535 loss -1.3535 (14.842 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14600 lr 4.020e-04 [train_loss] tar_ll 1.4325 loss -1.4325 (14.806 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14800 lr 3.995e-04 [train_loss] tar_ll 1.4203 loss -1.4203 (15.144 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15000 lr 3.969e-04 [train_loss] tar_ll 1.4359 loss -1.4359 (14.833 secs)\n",
      "100%|##########| 3000/3000 [01:08<00:00, 44.01it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.9486 loss -0.9486 (68.173 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 15200 lr 3.944e-04 [train_loss] tar_ll 1.4406 loss -1.4406 (14.973 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15400 lr 3.918e-04 [train_loss] tar_ll 1.4566 loss -1.4566 (14.834 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15600 lr 3.892e-04 [train_loss] tar_ll 1.4748 loss -1.4748 (14.929 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15800 lr 3.866e-04 [train_loss] tar_ll 1.4895 loss -1.4895 (15.002 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16000 lr 3.840e-04 [train_loss] tar_ll 1.4572 loss -1.4572 (14.967 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16200 lr 3.813e-04 [train_loss] tar_ll 1.4758 loss -1.4758 (15.359 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16400 lr 3.786e-04 [train_loss] tar_ll 1.4235 loss -1.4235 (15.047 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16600 lr 3.759e-04 [train_loss] tar_ll 1.5473 loss -1.5473 (14.948 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16800 lr 3.732e-04 [train_loss] tar_ll 1.4890 loss -1.4890 (14.840 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17000 lr 3.704e-04 [train_loss] tar_ll 1.5685 loss -1.5685 (14.934 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17200 lr 3.677e-04 [train_loss] tar_ll 1.5049 loss -1.5049 (14.669 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17400 lr 3.649e-04 [train_loss] tar_ll 1.5108 loss -1.5108 (14.651 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17600 lr 3.621e-04 [train_loss] tar_ll 1.5192 loss -1.5192 (14.590 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17800 lr 3.593e-04 [train_loss] tar_ll 1.5416 loss -1.5416 (14.917 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18000 lr 3.564e-04 [train_loss] tar_ll 1.5422 loss -1.5422 (14.814 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18200 lr 3.536e-04 [train_loss] tar_ll 1.5127 loss -1.5127 (14.720 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18400 lr 3.507e-04 [train_loss] tar_ll 1.4887 loss -1.4887 (14.950 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18600 lr 3.478e-04 [train_loss] tar_ll 1.4818 loss -1.4818 (14.256 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18800 lr 3.449e-04 [train_loss] tar_ll 1.5313 loss -1.5313 (14.833 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19000 lr 3.420e-04 [train_loss] tar_ll 1.5044 loss -1.5044 (14.891 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19200 lr 3.391e-04 [train_loss] tar_ll 1.3034 loss -1.3034 (14.903 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19400 lr 3.362e-04 [train_loss] tar_ll 1.4483 loss -1.4483 (15.087 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19600 lr 3.332e-04 [train_loss] tar_ll 1.5323 loss -1.5323 (14.950 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19800 lr 3.302e-04 [train_loss] tar_ll 1.5877 loss -1.5877 (14.983 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20000 lr 3.273e-04 [train_loss] tar_ll 1.5824 loss -1.5824 (15.009 secs)\n",
      "100%|##########| 3000/3000 [01:07<00:00, 44.24it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.9827 loss -0.9827 (67.817 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 20200 lr 3.243e-04 [train_loss] tar_ll 1.6255 loss -1.6255 (14.480 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20400 lr 3.213e-04 [train_loss] tar_ll 1.5784 loss -1.5784 (14.796 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20600 lr 3.182e-04 [train_loss] tar_ll 1.5128 loss -1.5128 (14.741 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20800 lr 3.152e-04 [train_loss] tar_ll 1.5758 loss -1.5758 (14.893 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21000 lr 3.122e-04 [train_loss] tar_ll 1.5615 loss -1.5615 (14.738 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21200 lr 3.091e-04 [train_loss] tar_ll 1.5289 loss -1.5289 (15.090 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21400 lr 3.061e-04 [train_loss] tar_ll 1.5123 loss -1.5123 (15.308 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21600 lr 3.030e-04 [train_loss] tar_ll 1.5140 loss -1.5140 (15.660 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21800 lr 2.999e-04 [train_loss] tar_ll 1.6185 loss -1.6185 (15.555 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22000 lr 2.968e-04 [train_loss] tar_ll 1.6122 loss -1.6122 (15.201 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22200 lr 2.938e-04 [train_loss] tar_ll 1.5829 loss -1.5829 (15.247 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22400 lr 2.907e-04 [train_loss] tar_ll 1.6007 loss -1.6007 (15.028 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22600 lr 2.876e-04 [train_loss] tar_ll 1.6199 loss -1.6199 (15.150 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22800 lr 2.844e-04 [train_loss] tar_ll 1.6840 loss -1.6840 (15.226 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23000 lr 2.813e-04 [train_loss] tar_ll 1.6131 loss -1.6131 (14.932 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23200 lr 2.782e-04 [train_loss] tar_ll 1.5810 loss -1.5810 (15.036 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23400 lr 2.751e-04 [train_loss] tar_ll 1.6527 loss -1.6527 (15.122 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23600 lr 2.720e-04 [train_loss] tar_ll 1.6036 loss -1.6036 (14.961 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23800 lr 2.688e-04 [train_loss] tar_ll 1.5997 loss -1.5997 (14.760 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24000 lr 2.657e-04 [train_loss] tar_ll 1.6210 loss -1.6210 (14.897 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24200 lr 2.626e-04 [train_loss] tar_ll 1.6820 loss -1.6820 (14.856 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24400 lr 2.594e-04 [train_loss] tar_ll 1.6871 loss -1.6871 (15.476 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24600 lr 2.563e-04 [train_loss] tar_ll 1.5630 loss -1.5630 (15.382 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24800 lr 2.531e-04 [train_loss] tar_ll 1.6608 loss -1.6608 (15.542 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25000 lr 2.500e-04 [train_loss] tar_ll 1.5769 loss -1.5769 (15.528 secs)\n",
      "100%|##########| 3000/3000 [01:10<00:00, 42.72it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.0735 loss -1.0735 (70.228 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 25200 lr 2.469e-04 [train_loss] tar_ll 1.6352 loss -1.6352 (15.233 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25400 lr 2.437e-04 [train_loss] tar_ll 1.6796 loss -1.6796 (14.900 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25600 lr 2.406e-04 [train_loss] tar_ll 1.6042 loss -1.6042 (14.760 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25800 lr 2.374e-04 [train_loss] tar_ll 1.6955 loss -1.6955 (14.648 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26000 lr 2.343e-04 [train_loss] tar_ll 1.6730 loss -1.6730 (14.384 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26200 lr 2.312e-04 [train_loss] tar_ll 1.7031 loss -1.7031 (14.550 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26400 lr 2.280e-04 [train_loss] tar_ll 1.6620 loss -1.6620 (14.778 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26600 lr 2.249e-04 [train_loss] tar_ll 1.7643 loss -1.7643 (15.217 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26800 lr 2.218e-04 [train_loss] tar_ll 1.6705 loss -1.6705 (15.478 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27000 lr 2.187e-04 [train_loss] tar_ll 1.6385 loss -1.6385 (15.282 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27200 lr 2.156e-04 [train_loss] tar_ll 1.6080 loss -1.6080 (15.304 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27400 lr 2.124e-04 [train_loss] tar_ll 1.6995 loss -1.6995 (15.188 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27600 lr 2.093e-04 [train_loss] tar_ll 1.7267 loss -1.7267 (15.509 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27800 lr 2.062e-04 [train_loss] tar_ll 1.6754 loss -1.6754 (15.758 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28000 lr 2.032e-04 [train_loss] tar_ll 1.7302 loss -1.7302 (15.434 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28200 lr 2.001e-04 [train_loss] tar_ll 1.6872 loss -1.6872 (14.610 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28400 lr 1.970e-04 [train_loss] tar_ll 1.7473 loss -1.7473 (15.146 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28600 lr 1.939e-04 [train_loss] tar_ll 1.6723 loss -1.6723 (14.143 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28800 lr 1.909e-04 [train_loss] tar_ll 1.6435 loss -1.6435 (14.644 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29000 lr 1.878e-04 [train_loss] tar_ll 1.6583 loss -1.6583 (13.213 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29200 lr 1.848e-04 [train_loss] tar_ll 1.7455 loss -1.7455 (15.250 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29400 lr 1.818e-04 [train_loss] tar_ll 1.7642 loss -1.7642 (14.585 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29600 lr 1.787e-04 [train_loss] tar_ll 1.6721 loss -1.6721 (14.855 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29800 lr 1.757e-04 [train_loss] tar_ll 1.7553 loss -1.7553 (15.928 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30000 lr 1.727e-04 [train_loss] tar_ll 1.7112 loss -1.7112 (15.493 secs)\n",
      "100%|##########| 3000/3000 [01:12<00:00, 41.40it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.1140 loss -1.1140 (72.473 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 30200 lr 1.698e-04 [train_loss] tar_ll 1.7404 loss -1.7404 (15.759 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30400 lr 1.668e-04 [train_loss] tar_ll 1.7865 loss -1.7865 (15.642 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30600 lr 1.638e-04 [train_loss] tar_ll 1.7855 loss -1.7855 (15.070 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30800 lr 1.609e-04 [train_loss] tar_ll 1.8264 loss -1.8264 (15.032 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31000 lr 1.580e-04 [train_loss] tar_ll 1.7677 loss -1.7677 (14.953 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31200 lr 1.551e-04 [train_loss] tar_ll 1.7041 loss -1.7041 (15.882 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31400 lr 1.522e-04 [train_loss] tar_ll 1.7116 loss -1.7116 (15.254 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31600 lr 1.493e-04 [train_loss] tar_ll 1.7572 loss -1.7572 (15.650 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31800 lr 1.464e-04 [train_loss] tar_ll 1.7851 loss -1.7851 (15.508 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32000 lr 1.436e-04 [train_loss] tar_ll 1.7743 loss -1.7743 (15.533 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32200 lr 1.407e-04 [train_loss] tar_ll 1.7359 loss -1.7359 (15.620 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32400 lr 1.379e-04 [train_loss] tar_ll 1.7338 loss -1.7338 (15.595 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32600 lr 1.351e-04 [train_loss] tar_ll 1.7546 loss -1.7546 (15.753 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32800 lr 1.323e-04 [train_loss] tar_ll 1.7546 loss -1.7546 (15.580 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33000 lr 1.296e-04 [train_loss] tar_ll 1.8228 loss -1.8228 (15.527 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33200 lr 1.268e-04 [train_loss] tar_ll 1.8027 loss -1.8027 (15.430 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33400 lr 1.241e-04 [train_loss] tar_ll 1.8495 loss -1.8495 (15.275 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33600 lr 1.214e-04 [train_loss] tar_ll 1.8364 loss -1.8364 (15.082 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33800 lr 1.187e-04 [train_loss] tar_ll 1.7870 loss -1.7870 (15.473 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34000 lr 1.160e-04 [train_loss] tar_ll 1.7687 loss -1.7687 (15.081 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34200 lr 1.134e-04 [train_loss] tar_ll 1.8467 loss -1.8467 (15.878 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34400 lr 1.108e-04 [train_loss] tar_ll 1.8153 loss -1.8153 (15.092 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34600 lr 1.082e-04 [train_loss] tar_ll 1.7391 loss -1.7391 (15.221 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34800 lr 1.056e-04 [train_loss] tar_ll 1.7589 loss -1.7589 (14.923 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35000 lr 1.031e-04 [train_loss] tar_ll 1.8019 loss -1.8019 (15.184 secs)\n",
      "100%|##########| 3000/3000 [01:12<00:00, 41.42it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.1959 loss -1.1959 (72.426 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 35200 lr 1.005e-04 [train_loss] tar_ll 1.7616 loss -1.7616 (15.733 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35400 lr 9.802e-05 [train_loss] tar_ll 1.8454 loss -1.8454 (15.424 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35600 lr 9.554e-05 [train_loss] tar_ll 1.7897 loss -1.7897 (15.420 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35800 lr 9.308e-05 [train_loss] tar_ll 1.7731 loss -1.7731 (15.454 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36000 lr 9.064e-05 [train_loss] tar_ll 1.8285 loss -1.8285 (15.170 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36200 lr 8.824e-05 [train_loss] tar_ll 1.8165 loss -1.8165 (15.401 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36400 lr 8.585e-05 [train_loss] tar_ll 1.7845 loss -1.7845 (15.124 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36600 lr 8.350e-05 [train_loss] tar_ll 1.8346 loss -1.8346 (15.045 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36800 lr 8.117e-05 [train_loss] tar_ll 1.8031 loss -1.8031 (14.848 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37000 lr 7.886e-05 [train_loss] tar_ll 1.8236 loss -1.8236 (14.899 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37200 lr 7.659e-05 [train_loss] tar_ll 1.8808 loss -1.8808 (15.325 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37400 lr 7.434e-05 [train_loss] tar_ll 1.7567 loss -1.7567 (15.703 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37600 lr 7.212e-05 [train_loss] tar_ll 1.8396 loss -1.8396 (15.493 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37800 lr 6.992e-05 [train_loss] tar_ll 1.8818 loss -1.8818 (15.757 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38000 lr 6.776e-05 [train_loss] tar_ll 1.7437 loss -1.7437 (15.672 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38200 lr 6.562e-05 [train_loss] tar_ll 1.7934 loss -1.7934 (15.578 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38400 lr 6.351e-05 [train_loss] tar_ll 1.8880 loss -1.8880 (15.749 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38600 lr 6.144e-05 [train_loss] tar_ll 1.8511 loss -1.8511 (15.684 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38800 lr 5.939e-05 [train_loss] tar_ll 1.8553 loss -1.8553 (15.183 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39000 lr 5.737e-05 [train_loss] tar_ll 1.8770 loss -1.8770 (15.188 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39200 lr 5.538e-05 [train_loss] tar_ll 1.8597 loss -1.8597 (15.773 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39400 lr 5.343e-05 [train_loss] tar_ll 1.8569 loss -1.8569 (15.309 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39600 lr 5.150e-05 [train_loss] tar_ll 1.8194 loss -1.8194 (15.179 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39800 lr 4.961e-05 [train_loss] tar_ll 1.9333 loss -1.9333 (15.001 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40000 lr 4.775e-05 [train_loss] tar_ll 1.8679 loss -1.8679 (14.804 secs)\n",
      "100%|##########| 3000/3000 [01:11<00:00, 42.01it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.2395 loss -1.2395 (71.408 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 40200 lr 4.592e-05 [train_loss] tar_ll 1.8643 loss -1.8643 (15.872 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40400 lr 4.412e-05 [train_loss] tar_ll 1.8201 loss -1.8201 (15.344 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40600 lr 4.235e-05 [train_loss] tar_ll 1.8650 loss -1.8650 (15.764 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40800 lr 4.062e-05 [train_loss] tar_ll 1.8867 loss -1.8867 (14.945 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41000 lr 3.892e-05 [train_loss] tar_ll 1.8664 loss -1.8664 (15.333 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41200 lr 3.725e-05 [train_loss] tar_ll 1.7982 loss -1.7982 (15.109 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41400 lr 3.562e-05 [train_loss] tar_ll 1.9637 loss -1.9637 (15.765 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41600 lr 3.402e-05 [train_loss] tar_ll 1.8322 loss -1.8322 (14.893 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41800 lr 3.245e-05 [train_loss] tar_ll 1.8513 loss -1.8513 (15.260 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42000 lr 3.092e-05 [train_loss] tar_ll 1.8557 loss -1.8557 (14.937 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42200 lr 2.943e-05 [train_loss] tar_ll 1.8670 loss -1.8670 (15.486 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42400 lr 2.797e-05 [train_loss] tar_ll 1.8903 loss -1.8903 (15.219 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42600 lr 2.654e-05 [train_loss] tar_ll 1.8438 loss -1.8438 (15.647 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42800 lr 2.515e-05 [train_loss] tar_ll 1.8733 loss -1.8733 (15.473 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43000 lr 2.379e-05 [train_loss] tar_ll 1.8558 loss -1.8558 (16.043 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43200 lr 2.247e-05 [train_loss] tar_ll 1.8777 loss -1.8777 (15.475 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43400 lr 2.119e-05 [train_loss] tar_ll 1.8822 loss -1.8822 (15.610 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43600 lr 1.994e-05 [train_loss] tar_ll 1.8950 loss -1.8950 (15.522 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43800 lr 1.873e-05 [train_loss] tar_ll 1.8869 loss -1.8869 (15.605 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44000 lr 1.756e-05 [train_loss] tar_ll 1.8682 loss -1.8682 (15.534 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44200 lr 1.642e-05 [train_loss] tar_ll 1.8969 loss -1.8969 (15.141 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44400 lr 1.532e-05 [train_loss] tar_ll 1.9792 loss -1.9792 (15.312 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44600 lr 1.425e-05 [train_loss] tar_ll 1.8939 loss -1.8939 (15.271 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44800 lr 1.323e-05 [train_loss] tar_ll 1.9135 loss -1.9135 (15.549 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45000 lr 1.224e-05 [train_loss] tar_ll 1.8767 loss -1.8767 (15.233 secs)\n",
      "100%|##########| 3000/3000 [01:10<00:00, 42.85it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.2760 loss -1.2760 (70.011 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 45200 lr 1.128e-05 [train_loss] tar_ll 1.8964 loss -1.8964 (15.705 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45400 lr 1.037e-05 [train_loss] tar_ll 1.9308 loss -1.9308 (15.195 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45600 lr 9.493e-06 [train_loss] tar_ll 1.9076 loss -1.9076 (15.892 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45800 lr 8.655e-06 [train_loss] tar_ll 1.9122 loss -1.9122 (15.536 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46000 lr 7.854e-06 [train_loss] tar_ll 1.8450 loss -1.8450 (15.723 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46200 lr 7.092e-06 [train_loss] tar_ll 1.9215 loss -1.9215 (15.358 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46400 lr 6.368e-06 [train_loss] tar_ll 1.9671 loss -1.9671 (15.581 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46600 lr 5.683e-06 [train_loss] tar_ll 1.9256 loss -1.9256 (15.294 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46800 lr 5.036e-06 [train_loss] tar_ll 1.8936 loss -1.8936 (15.464 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47000 lr 4.428e-06 [train_loss] tar_ll 1.8811 loss -1.8811 (14.602 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47200 lr 3.859e-06 [train_loss] tar_ll 1.9274 loss -1.9274 (14.555 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47400 lr 3.329e-06 [train_loss] tar_ll 1.8693 loss -1.8693 (14.141 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47600 lr 2.837e-06 [train_loss] tar_ll 1.9242 loss -1.9242 (14.664 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47800 lr 2.385e-06 [train_loss] tar_ll 1.9603 loss -1.9603 (14.944 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48000 lr 1.971e-06 [train_loss] tar_ll 1.8446 loss -1.8446 (14.650 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48200 lr 1.597e-06 [train_loss] tar_ll 1.9046 loss -1.9046 (15.113 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48400 lr 1.262e-06 [train_loss] tar_ll 1.9366 loss -1.9366 (14.724 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48600 lr 9.666e-07 [train_loss] tar_ll 1.8993 loss -1.8993 (14.592 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48800 lr 7.103e-07 [train_loss] tar_ll 1.8405 loss -1.8405 (14.446 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49000 lr 4.933e-07 [train_loss] tar_ll 1.9194 loss -1.9194 (14.673 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49200 lr 3.158e-07 [train_loss] tar_ll 1.9615 loss -1.9615 (15.073 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49400 lr 1.776e-07 [train_loss] tar_ll 1.9047 loss -1.9047 (14.576 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49600 lr 7.895e-08 [train_loss] tar_ll 1.9437 loss -1.9437 (14.269 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49800 lr 1.974e-08 [train_loss] tar_ll 1.8840 loss -1.8840 (13.923 secs)\n",
      "lbanp:lbanp-num_latents-8 step 50000 lr 0.000e+00 [train_loss] tar_ll 1.9092 loss -1.9092 (14.300 secs)\n",
      "100%|##########| 3000/3000 [01:06<00:00, 45.40it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.2820 loss -1.2820 (66.085 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [01:11<00:00, 42.13it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.2820 loss -1.2820 (71.211 secs)\n",
      "Experiment: lbanp-lbanp-num_latents-8\n",
      "Total number of parameters: 792514\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4547.074417114258 seconds\n",
      "Memory usage: 748.92578125 MB\n",
      "Memory usage: 4.4609375 MB\n",
      "Memory usage: 748.92578125 MB\n",
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "rbf-seed0.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8 step 200 lr 5.000e-04 [train_loss] tar_ll -0.6496 loss 0.6496 (15.756 secs)\n",
      "lbanp:lbanp-num_latents-8 step 400 lr 4.999e-04 [train_loss] tar_ll -0.5962 loss 0.5962 (14.930 secs)\n",
      "lbanp:lbanp-num_latents-8 step 600 lr 4.998e-04 [train_loss] tar_ll -0.5153 loss 0.5153 (15.248 secs)\n",
      "lbanp:lbanp-num_latents-8 step 800 lr 4.997e-04 [train_loss] tar_ll -0.3377 loss 0.3377 (15.282 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1000 lr 4.995e-04 [train_loss] tar_ll -0.0407 loss 0.0407 (15.400 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1200 lr 4.993e-04 [train_loss] tar_ll -0.0175 loss 0.0175 (15.090 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1400 lr 4.990e-04 [train_loss] tar_ll 0.2219 loss -0.2219 (15.446 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1600 lr 4.987e-04 [train_loss] tar_ll 0.3589 loss -0.3589 (15.061 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1800 lr 4.984e-04 [train_loss] tar_ll 0.6056 loss -0.6056 (14.958 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2000 lr 4.980e-04 [train_loss] tar_ll 0.5740 loss -0.5740 (14.721 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2200 lr 4.976e-04 [train_loss] tar_ll 0.6291 loss -0.6291 (15.464 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2400 lr 4.972e-04 [train_loss] tar_ll 0.6868 loss -0.6868 (15.329 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2600 lr 4.967e-04 [train_loss] tar_ll 0.7168 loss -0.7168 (15.522 secs)\n",
      "lbanp:lbanp-num_latents-8 step 2800 lr 4.961e-04 [train_loss] tar_ll 0.8426 loss -0.8426 (15.422 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3000 lr 4.956e-04 [train_loss] tar_ll 0.9418 loss -0.9418 (15.259 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3200 lr 4.950e-04 [train_loss] tar_ll 0.8924 loss -0.8924 (15.707 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3400 lr 4.943e-04 [train_loss] tar_ll 0.9411 loss -0.9411 (15.520 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3600 lr 4.936e-04 [train_loss] tar_ll 1.0000 loss -1.0000 (15.625 secs)\n",
      "lbanp:lbanp-num_latents-8 step 3800 lr 4.929e-04 [train_loss] tar_ll 0.9336 loss -0.9336 (15.339 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4000 lr 4.921e-04 [train_loss] tar_ll 0.9222 loss -0.9222 (15.767 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4200 lr 4.913e-04 [train_loss] tar_ll 1.0687 loss -1.0687 (14.766 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4400 lr 4.905e-04 [train_loss] tar_ll 1.0905 loss -1.0905 (15.065 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4600 lr 4.896e-04 [train_loss] tar_ll 1.1199 loss -1.1199 (15.069 secs)\n",
      "lbanp:lbanp-num_latents-8 step 4800 lr 4.887e-04 [train_loss] tar_ll 1.0757 loss -1.0757 (15.853 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5000 lr 4.878e-04 [train_loss] tar_ll 1.0916 loss -1.0916 (14.923 secs)\n",
      "100%|##########| 3000/3000 [01:09<00:00, 43.03it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.5859 loss -0.5859 (69.717 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 5200 lr 4.868e-04 [train_loss] tar_ll 1.0392 loss -1.0392 (14.652 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5400 lr 4.857e-04 [train_loss] tar_ll 1.1772 loss -1.1772 (14.650 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5600 lr 4.847e-04 [train_loss] tar_ll 1.1018 loss -1.1018 (14.595 secs)\n",
      "lbanp:lbanp-num_latents-8 step 5800 lr 4.836e-04 [train_loss] tar_ll 1.1885 loss -1.1885 (14.302 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6000 lr 4.824e-04 [train_loss] tar_ll 1.1779 loss -1.1779 (14.268 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6200 lr 4.813e-04 [train_loss] tar_ll 1.1558 loss -1.1558 (14.003 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6400 lr 4.801e-04 [train_loss] tar_ll 1.2325 loss -1.2325 (13.792 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6600 lr 4.788e-04 [train_loss] tar_ll 1.2416 loss -1.2416 (13.701 secs)\n",
      "lbanp:lbanp-num_latents-8 step 6800 lr 4.775e-04 [train_loss] tar_ll 1.2044 loss -1.2044 (13.712 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7000 lr 4.762e-04 [train_loss] tar_ll 1.2484 loss -1.2484 (13.667 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7200 lr 4.749e-04 [train_loss] tar_ll 1.1891 loss -1.1891 (13.876 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7400 lr 4.735e-04 [train_loss] tar_ll 1.2335 loss -1.2335 (13.787 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7600 lr 4.720e-04 [train_loss] tar_ll 0.9785 loss -0.9785 (13.642 secs)\n",
      "lbanp:lbanp-num_latents-8 step 7800 lr 4.706e-04 [train_loss] tar_ll 1.1128 loss -1.1128 (13.719 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8000 lr 4.691e-04 [train_loss] tar_ll 1.2656 loss -1.2656 (14.795 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8200 lr 4.675e-04 [train_loss] tar_ll 1.3077 loss -1.3077 (15.107 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8400 lr 4.660e-04 [train_loss] tar_ll 1.3933 loss -1.3933 (14.949 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8600 lr 4.644e-04 [train_loss] tar_ll 1.2398 loss -1.2398 (14.755 secs)\n",
      "lbanp:lbanp-num_latents-8 step 8800 lr 4.627e-04 [train_loss] tar_ll 1.2925 loss -1.2925 (14.545 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9000 lr 4.611e-04 [train_loss] tar_ll 1.3455 loss -1.3455 (15.295 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9200 lr 4.594e-04 [train_loss] tar_ll 1.1083 loss -1.1083 (15.946 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9400 lr 4.576e-04 [train_loss] tar_ll 1.2230 loss -1.2230 (15.256 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9600 lr 4.559e-04 [train_loss] tar_ll 1.2580 loss -1.2580 (15.046 secs)\n",
      "lbanp:lbanp-num_latents-8 step 9800 lr 4.541e-04 [train_loss] tar_ll 1.3270 loss -1.3270 (14.499 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10000 lr 4.523e-04 [train_loss] tar_ll 1.3235 loss -1.3235 (15.125 secs)\n",
      "100%|##########| 3000/3000 [01:11<00:00, 42.01it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.8041 loss -0.8041 (71.411 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 10200 lr 4.504e-04 [train_loss] tar_ll 1.3642 loss -1.3642 (15.525 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10400 lr 4.485e-04 [train_loss] tar_ll 1.2603 loss -1.2603 (15.117 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10600 lr 4.466e-04 [train_loss] tar_ll 1.3294 loss -1.3294 (15.307 secs)\n",
      "lbanp:lbanp-num_latents-8 step 10800 lr 4.446e-04 [train_loss] tar_ll 1.1818 loss -1.1818 (15.131 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11000 lr 4.426e-04 [train_loss] tar_ll 1.3427 loss -1.3427 (15.572 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11200 lr 4.406e-04 [train_loss] tar_ll 1.3550 loss -1.3550 (15.586 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11400 lr 4.386e-04 [train_loss] tar_ll 1.3344 loss -1.3344 (15.264 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11600 lr 4.365e-04 [train_loss] tar_ll 1.3906 loss -1.3906 (15.563 secs)\n",
      "lbanp:lbanp-num_latents-8 step 11800 lr 4.344e-04 [train_loss] tar_ll 1.3825 loss -1.3825 (14.835 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12000 lr 4.322e-04 [train_loss] tar_ll 1.3398 loss -1.3398 (15.090 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12200 lr 4.301e-04 [train_loss] tar_ll 1.4035 loss -1.4035 (15.353 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12400 lr 4.279e-04 [train_loss] tar_ll 1.4096 loss -1.4096 (14.923 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12600 lr 4.257e-04 [train_loss] tar_ll 1.4566 loss -1.4566 (14.935 secs)\n",
      "lbanp:lbanp-num_latents-8 step 12800 lr 4.234e-04 [train_loss] tar_ll 1.3711 loss -1.3711 (14.840 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13000 lr 4.211e-04 [train_loss] tar_ll 1.4450 loss -1.4450 (15.136 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13200 lr 4.188e-04 [train_loss] tar_ll 1.4651 loss -1.4651 (15.534 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13400 lr 4.165e-04 [train_loss] tar_ll 1.4332 loss -1.4332 (15.358 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13600 lr 4.141e-04 [train_loss] tar_ll 1.4830 loss -1.4830 (15.437 secs)\n",
      "lbanp:lbanp-num_latents-8 step 13800 lr 4.118e-04 [train_loss] tar_ll 1.4005 loss -1.4005 (15.073 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14000 lr 4.094e-04 [train_loss] tar_ll 1.4693 loss -1.4693 (15.441 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14200 lr 4.069e-04 [train_loss] tar_ll 1.5075 loss -1.5075 (15.292 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14400 lr 4.045e-04 [train_loss] tar_ll 1.4345 loss -1.4345 (15.309 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14600 lr 4.020e-04 [train_loss] tar_ll 1.3784 loss -1.3784 (15.118 secs)\n",
      "lbanp:lbanp-num_latents-8 step 14800 lr 3.995e-04 [train_loss] tar_ll 1.4756 loss -1.4756 (15.297 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15000 lr 3.969e-04 [train_loss] tar_ll 1.3859 loss -1.3859 (14.983 secs)\n",
      "100%|##########| 3000/3000 [01:11<00:00, 42.20it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 0.9553 loss -0.9553 (71.095 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 15200 lr 3.944e-04 [train_loss] tar_ll 1.4635 loss -1.4635 (14.494 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15400 lr 3.918e-04 [train_loss] tar_ll 1.4723 loss -1.4723 (14.525 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15600 lr 3.892e-04 [train_loss] tar_ll 1.4633 loss -1.4633 (15.458 secs)\n",
      "lbanp:lbanp-num_latents-8 step 15800 lr 3.866e-04 [train_loss] tar_ll 1.4446 loss -1.4446 (15.270 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16000 lr 3.840e-04 [train_loss] tar_ll 1.3082 loss -1.3082 (15.569 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16200 lr 3.813e-04 [train_loss] tar_ll 1.4390 loss -1.4390 (15.525 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16400 lr 3.786e-04 [train_loss] tar_ll 1.5016 loss -1.5016 (15.528 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16600 lr 3.759e-04 [train_loss] tar_ll 1.5089 loss -1.5089 (15.485 secs)\n",
      "lbanp:lbanp-num_latents-8 step 16800 lr 3.732e-04 [train_loss] tar_ll 1.4636 loss -1.4636 (15.481 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17000 lr 3.704e-04 [train_loss] tar_ll 1.4665 loss -1.4665 (15.471 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17200 lr 3.677e-04 [train_loss] tar_ll 1.5199 loss -1.5199 (14.898 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17400 lr 3.649e-04 [train_loss] tar_ll 1.4925 loss -1.4925 (15.015 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17600 lr 3.621e-04 [train_loss] tar_ll 1.5449 loss -1.5449 (14.972 secs)\n",
      "lbanp:lbanp-num_latents-8 step 17800 lr 3.593e-04 [train_loss] tar_ll 1.5300 loss -1.5300 (15.158 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18000 lr 3.564e-04 [train_loss] tar_ll 1.5446 loss -1.5446 (15.255 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18200 lr 3.536e-04 [train_loss] tar_ll 1.5721 loss -1.5721 (15.218 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18400 lr 3.507e-04 [train_loss] tar_ll 1.5742 loss -1.5742 (15.372 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18600 lr 3.478e-04 [train_loss] tar_ll 1.5527 loss -1.5527 (15.655 secs)\n",
      "lbanp:lbanp-num_latents-8 step 18800 lr 3.449e-04 [train_loss] tar_ll 1.6059 loss -1.6059 (15.458 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19000 lr 3.420e-04 [train_loss] tar_ll 1.5358 loss -1.5358 (15.324 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19200 lr 3.391e-04 [train_loss] tar_ll 1.5047 loss -1.5047 (15.541 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19400 lr 3.362e-04 [train_loss] tar_ll 1.4918 loss -1.4918 (15.265 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19600 lr 3.332e-04 [train_loss] tar_ll 1.5136 loss -1.5136 (15.212 secs)\n",
      "lbanp:lbanp-num_latents-8 step 19800 lr 3.302e-04 [train_loss] tar_ll 1.5569 loss -1.5569 (15.372 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20000 lr 3.273e-04 [train_loss] tar_ll 1.5788 loss -1.5788 (15.112 secs)\n",
      "100%|##########| 3000/3000 [01:10<00:00, 42.33it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.0580 loss -1.0580 (70.871 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 20200 lr 3.243e-04 [train_loss] tar_ll 1.5970 loss -1.5970 (15.675 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20400 lr 3.213e-04 [train_loss] tar_ll 1.5213 loss -1.5213 (14.701 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20600 lr 3.182e-04 [train_loss] tar_ll 1.5702 loss -1.5702 (14.921 secs)\n",
      "lbanp:lbanp-num_latents-8 step 20800 lr 3.152e-04 [train_loss] tar_ll 1.5920 loss -1.5920 (14.884 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21000 lr 3.122e-04 [train_loss] tar_ll 1.5190 loss -1.5190 (14.605 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21200 lr 3.091e-04 [train_loss] tar_ll 1.5672 loss -1.5672 (15.417 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21400 lr 3.061e-04 [train_loss] tar_ll 1.5600 loss -1.5600 (15.253 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21600 lr 3.030e-04 [train_loss] tar_ll 1.6111 loss -1.6111 (15.256 secs)\n",
      "lbanp:lbanp-num_latents-8 step 21800 lr 2.999e-04 [train_loss] tar_ll 1.5572 loss -1.5572 (15.646 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22000 lr 2.968e-04 [train_loss] tar_ll 1.6212 loss -1.6212 (15.367 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22200 lr 2.938e-04 [train_loss] tar_ll 1.5878 loss -1.5878 (15.867 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22400 lr 2.907e-04 [train_loss] tar_ll 1.6177 loss -1.6177 (15.512 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22600 lr 2.876e-04 [train_loss] tar_ll 1.6091 loss -1.6091 (14.629 secs)\n",
      "lbanp:lbanp-num_latents-8 step 22800 lr 2.844e-04 [train_loss] tar_ll 1.6128 loss -1.6128 (14.297 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23000 lr 2.813e-04 [train_loss] tar_ll 1.6214 loss -1.6214 (13.595 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23200 lr 2.782e-04 [train_loss] tar_ll 1.6675 loss -1.6675 (13.792 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23400 lr 2.751e-04 [train_loss] tar_ll 1.5913 loss -1.5913 (13.694 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23600 lr 2.720e-04 [train_loss] tar_ll 1.4330 loss -1.4330 (13.942 secs)\n",
      "lbanp:lbanp-num_latents-8 step 23800 lr 2.688e-04 [train_loss] tar_ll 1.6577 loss -1.6577 (13.594 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24000 lr 2.657e-04 [train_loss] tar_ll 1.6340 loss -1.6340 (13.950 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24200 lr 2.626e-04 [train_loss] tar_ll 1.6420 loss -1.6420 (15.057 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24400 lr 2.594e-04 [train_loss] tar_ll 1.7006 loss -1.7006 (14.729 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24600 lr 2.563e-04 [train_loss] tar_ll 1.5598 loss -1.5598 (14.920 secs)\n",
      "lbanp:lbanp-num_latents-8 step 24800 lr 2.531e-04 [train_loss] tar_ll 1.6675 loss -1.6675 (14.942 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25000 lr 2.500e-04 [train_loss] tar_ll 1.5912 loss -1.5912 (14.349 secs)\n",
      "100%|##########| 3000/3000 [01:23<00:00, 35.96it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.0408 loss -1.0408 (83.421 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 25200 lr 2.469e-04 [train_loss] tar_ll 1.6751 loss -1.6751 (25.583 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25400 lr 2.437e-04 [train_loss] tar_ll 1.6023 loss -1.6023 (25.456 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25600 lr 2.406e-04 [train_loss] tar_ll 1.6595 loss -1.6595 (25.824 secs)\n",
      "lbanp:lbanp-num_latents-8 step 25800 lr 2.374e-04 [train_loss] tar_ll 1.7041 loss -1.7041 (26.330 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26000 lr 2.343e-04 [train_loss] tar_ll 1.6681 loss -1.6681 (25.498 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26200 lr 2.312e-04 [train_loss] tar_ll 1.6475 loss -1.6475 (26.463 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26400 lr 2.280e-04 [train_loss] tar_ll 1.6472 loss -1.6472 (26.934 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26600 lr 2.249e-04 [train_loss] tar_ll 1.6556 loss -1.6556 (26.685 secs)\n",
      "lbanp:lbanp-num_latents-8 step 26800 lr 2.218e-04 [train_loss] tar_ll 1.6283 loss -1.6283 (27.045 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27000 lr 2.187e-04 [train_loss] tar_ll 1.6973 loss -1.6973 (27.215 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27200 lr 2.156e-04 [train_loss] tar_ll 1.7267 loss -1.7267 (26.176 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27400 lr 2.124e-04 [train_loss] tar_ll 1.6644 loss -1.6644 (27.397 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27600 lr 2.093e-04 [train_loss] tar_ll 1.7552 loss -1.7552 (25.280 secs)\n",
      "lbanp:lbanp-num_latents-8 step 27800 lr 2.062e-04 [train_loss] tar_ll 1.6467 loss -1.6467 (25.927 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28000 lr 2.032e-04 [train_loss] tar_ll 1.7965 loss -1.7965 (24.563 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28200 lr 2.001e-04 [train_loss] tar_ll 1.7427 loss -1.7427 (25.990 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28400 lr 1.970e-04 [train_loss] tar_ll 1.7051 loss -1.7051 (26.442 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28600 lr 1.939e-04 [train_loss] tar_ll 1.6019 loss -1.6019 (25.843 secs)\n",
      "lbanp:lbanp-num_latents-8 step 28800 lr 1.909e-04 [train_loss] tar_ll 1.7473 loss -1.7473 (26.054 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29000 lr 1.878e-04 [train_loss] tar_ll 1.6643 loss -1.6643 (26.120 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29200 lr 1.848e-04 [train_loss] tar_ll 1.7334 loss -1.7334 (27.672 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29400 lr 1.818e-04 [train_loss] tar_ll 1.7388 loss -1.7388 (26.745 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29600 lr 1.787e-04 [train_loss] tar_ll 1.7427 loss -1.7427 (26.626 secs)\n",
      "lbanp:lbanp-num_latents-8 step 29800 lr 1.757e-04 [train_loss] tar_ll 1.7110 loss -1.7110 (27.498 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30000 lr 1.727e-04 [train_loss] tar_ll 1.6878 loss -1.6878 (27.103 secs)\n",
      "100%|##########| 3000/3000 [02:04<00:00, 24.01it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.1404 loss -1.1404 (124.943 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 30200 lr 1.698e-04 [train_loss] tar_ll 1.7495 loss -1.7495 (26.218 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30400 lr 1.668e-04 [train_loss] tar_ll 1.6849 loss -1.6849 (25.830 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30600 lr 1.638e-04 [train_loss] tar_ll 1.7113 loss -1.7113 (25.542 secs)\n",
      "lbanp:lbanp-num_latents-8 step 30800 lr 1.609e-04 [train_loss] tar_ll 1.6769 loss -1.6769 (26.208 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31000 lr 1.580e-04 [train_loss] tar_ll 1.6314 loss -1.6314 (27.257 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31200 lr 1.551e-04 [train_loss] tar_ll 1.8023 loss -1.8023 (26.693 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31400 lr 1.522e-04 [train_loss] tar_ll 1.7935 loss -1.7935 (27.275 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31600 lr 1.493e-04 [train_loss] tar_ll 1.7464 loss -1.7464 (26.909 secs)\n",
      "lbanp:lbanp-num_latents-8 step 31800 lr 1.464e-04 [train_loss] tar_ll 1.8246 loss -1.8246 (26.476 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32000 lr 1.436e-04 [train_loss] tar_ll 1.7782 loss -1.7782 (26.554 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32200 lr 1.407e-04 [train_loss] tar_ll 1.7903 loss -1.7903 (25.260 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32400 lr 1.379e-04 [train_loss] tar_ll 1.7431 loss -1.7431 (26.005 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32600 lr 1.351e-04 [train_loss] tar_ll 1.7433 loss -1.7433 (25.230 secs)\n",
      "lbanp:lbanp-num_latents-8 step 32800 lr 1.323e-04 [train_loss] tar_ll 1.7657 loss -1.7657 (27.272 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33000 lr 1.296e-04 [train_loss] tar_ll 1.7693 loss -1.7693 (25.605 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33200 lr 1.268e-04 [train_loss] tar_ll 1.7536 loss -1.7536 (25.910 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33400 lr 1.241e-04 [train_loss] tar_ll 1.8537 loss -1.8537 (26.518 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33600 lr 1.214e-04 [train_loss] tar_ll 1.7456 loss -1.7456 (26.755 secs)\n",
      "lbanp:lbanp-num_latents-8 step 33800 lr 1.187e-04 [train_loss] tar_ll 1.8430 loss -1.8430 (27.820 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34000 lr 1.160e-04 [train_loss] tar_ll 1.8000 loss -1.8000 (27.248 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34200 lr 1.134e-04 [train_loss] tar_ll 1.8105 loss -1.8105 (26.740 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34400 lr 1.108e-04 [train_loss] tar_ll 1.7405 loss -1.7405 (27.714 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34600 lr 1.082e-04 [train_loss] tar_ll 1.7459 loss -1.7459 (28.107 secs)\n",
      "lbanp:lbanp-num_latents-8 step 34800 lr 1.056e-04 [train_loss] tar_ll 1.7846 loss -1.7846 (25.215 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35000 lr 1.031e-04 [train_loss] tar_ll 1.8184 loss -1.8184 (26.643 secs)\n",
      "100%|##########| 3000/3000 [02:07<00:00, 23.51it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.2173 loss -1.2173 (127.594 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 35200 lr 1.005e-04 [train_loss] tar_ll 1.8211 loss -1.8211 (25.777 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35400 lr 9.802e-05 [train_loss] tar_ll 1.8812 loss -1.8812 (26.932 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35600 lr 9.554e-05 [train_loss] tar_ll 1.7894 loss -1.7894 (26.742 secs)\n",
      "lbanp:lbanp-num_latents-8 step 35800 lr 9.308e-05 [train_loss] tar_ll 1.7731 loss -1.7731 (27.377 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36000 lr 9.064e-05 [train_loss] tar_ll 1.7322 loss -1.7322 (26.767 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36200 lr 8.824e-05 [train_loss] tar_ll 1.8023 loss -1.8023 (26.954 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36400 lr 8.585e-05 [train_loss] tar_ll 1.8157 loss -1.8157 (26.996 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36600 lr 8.350e-05 [train_loss] tar_ll 1.7883 loss -1.7883 (25.116 secs)\n",
      "lbanp:lbanp-num_latents-8 step 36800 lr 8.117e-05 [train_loss] tar_ll 1.7778 loss -1.7778 (26.664 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37000 lr 7.886e-05 [train_loss] tar_ll 1.8582 loss -1.8582 (25.283 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37200 lr 7.659e-05 [train_loss] tar_ll 1.8403 loss -1.8403 (26.836 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37400 lr 7.434e-05 [train_loss] tar_ll 1.7947 loss -1.7947 (25.910 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37600 lr 7.212e-05 [train_loss] tar_ll 1.8766 loss -1.8766 (25.501 secs)\n",
      "lbanp:lbanp-num_latents-8 step 37800 lr 6.992e-05 [train_loss] tar_ll 1.8853 loss -1.8853 (25.095 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38000 lr 6.776e-05 [train_loss] tar_ll 1.8123 loss -1.8123 (26.204 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38200 lr 6.562e-05 [train_loss] tar_ll 1.8278 loss -1.8278 (26.730 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38400 lr 6.351e-05 [train_loss] tar_ll 1.8170 loss -1.8170 (26.387 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38600 lr 6.144e-05 [train_loss] tar_ll 1.8234 loss -1.8234 (27.295 secs)\n",
      "lbanp:lbanp-num_latents-8 step 38800 lr 5.939e-05 [train_loss] tar_ll 1.8458 loss -1.8458 (25.509 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39000 lr 5.737e-05 [train_loss] tar_ll 1.7912 loss -1.7912 (25.994 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39200 lr 5.538e-05 [train_loss] tar_ll 1.8127 loss -1.8127 (26.149 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39400 lr 5.343e-05 [train_loss] tar_ll 1.8311 loss -1.8311 (25.101 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39600 lr 5.150e-05 [train_loss] tar_ll 1.8702 loss -1.8702 (25.609 secs)\n",
      "lbanp:lbanp-num_latents-8 step 39800 lr 4.961e-05 [train_loss] tar_ll 1.8147 loss -1.8147 (25.494 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40000 lr 4.775e-05 [train_loss] tar_ll 1.8628 loss -1.8628 (26.301 secs)\n",
      "100%|##########| 3000/3000 [02:07<00:00, 23.57it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.2383 loss -1.2383 (127.302 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 40200 lr 4.592e-05 [train_loss] tar_ll 1.8329 loss -1.8329 (27.131 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40400 lr 4.412e-05 [train_loss] tar_ll 1.8480 loss -1.8480 (26.928 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40600 lr 4.235e-05 [train_loss] tar_ll 1.9244 loss -1.9244 (26.510 secs)\n",
      "lbanp:lbanp-num_latents-8 step 40800 lr 4.062e-05 [train_loss] tar_ll 1.8684 loss -1.8684 (27.840 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41000 lr 3.892e-05 [train_loss] tar_ll 1.8924 loss -1.8924 (26.302 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41200 lr 3.725e-05 [train_loss] tar_ll 1.8701 loss -1.8701 (25.934 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41400 lr 3.562e-05 [train_loss] tar_ll 1.8243 loss -1.8243 (25.850 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41600 lr 3.402e-05 [train_loss] tar_ll 1.8669 loss -1.8669 (25.638 secs)\n",
      "lbanp:lbanp-num_latents-8 step 41800 lr 3.245e-05 [train_loss] tar_ll 1.9347 loss -1.9347 (26.672 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42000 lr 3.092e-05 [train_loss] tar_ll 1.8880 loss -1.8880 (25.352 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42200 lr 2.943e-05 [train_loss] tar_ll 1.7876 loss -1.7876 (26.306 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42400 lr 2.797e-05 [train_loss] tar_ll 1.9010 loss -1.9010 (25.960 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42600 lr 2.654e-05 [train_loss] tar_ll 1.8797 loss -1.8797 (27.120 secs)\n",
      "lbanp:lbanp-num_latents-8 step 42800 lr 2.515e-05 [train_loss] tar_ll 1.8230 loss -1.8230 (27.140 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43000 lr 2.379e-05 [train_loss] tar_ll 1.8117 loss -1.8117 (26.855 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43200 lr 2.247e-05 [train_loss] tar_ll 1.9225 loss -1.9225 (26.565 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43400 lr 2.119e-05 [train_loss] tar_ll 1.9762 loss -1.9762 (23.830 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43600 lr 1.994e-05 [train_loss] tar_ll 1.8366 loss -1.8366 (24.570 secs)\n",
      "lbanp:lbanp-num_latents-8 step 43800 lr 1.873e-05 [train_loss] tar_ll 1.9090 loss -1.9090 (25.200 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44000 lr 1.756e-05 [train_loss] tar_ll 1.8222 loss -1.8222 (24.929 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44200 lr 1.642e-05 [train_loss] tar_ll 1.8974 loss -1.8974 (25.675 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44400 lr 1.532e-05 [train_loss] tar_ll 1.8614 loss -1.8614 (23.755 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44600 lr 1.425e-05 [train_loss] tar_ll 1.8794 loss -1.8794 (24.190 secs)\n",
      "lbanp:lbanp-num_latents-8 step 44800 lr 1.323e-05 [train_loss] tar_ll 1.9269 loss -1.9269 (25.120 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45000 lr 1.224e-05 [train_loss] tar_ll 1.8962 loss -1.8962 (24.195 secs)\n",
      "100%|##########| 3000/3000 [01:58<00:00, 25.27it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.2565 loss -1.2565 (118.709 secs)\n",
      "\n",
      "lbanp:lbanp-num_latents-8 step 45200 lr 1.128e-05 [train_loss] tar_ll 1.8989 loss -1.8989 (24.165 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45400 lr 1.037e-05 [train_loss] tar_ll 1.9078 loss -1.9078 (24.130 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45600 lr 9.493e-06 [train_loss] tar_ll 1.8428 loss -1.8428 (24.375 secs)\n",
      "lbanp:lbanp-num_latents-8 step 45800 lr 8.655e-06 [train_loss] tar_ll 1.8225 loss -1.8225 (26.910 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46000 lr 7.854e-06 [train_loss] tar_ll 1.8891 loss -1.8891 (25.440 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46200 lr 7.092e-06 [train_loss] tar_ll 1.9069 loss -1.9069 (27.940 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46400 lr 6.368e-06 [train_loss] tar_ll 1.8820 loss -1.8820 (26.170 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46600 lr 5.683e-06 [train_loss] tar_ll 1.9592 loss -1.9592 (27.000 secs)\n",
      "lbanp:lbanp-num_latents-8 step 46800 lr 5.036e-06 [train_loss] tar_ll 1.8656 loss -1.8656 (26.430 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47000 lr 4.428e-06 [train_loss] tar_ll 1.8771 loss -1.8771 (25.240 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47200 lr 3.859e-06 [train_loss] tar_ll 1.9022 loss -1.9022 (26.805 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47400 lr 3.329e-06 [train_loss] tar_ll 1.8808 loss -1.8808 (24.715 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47600 lr 2.837e-06 [train_loss] tar_ll 1.8499 loss -1.8499 (25.260 secs)\n",
      "lbanp:lbanp-num_latents-8 step 47800 lr 2.385e-06 [train_loss] tar_ll 1.8213 loss -1.8213 (24.560 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48000 lr 1.971e-06 [train_loss] tar_ll 1.9325 loss -1.9325 (23.820 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48200 lr 1.597e-06 [train_loss] tar_ll 1.8987 loss -1.8987 (24.420 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48400 lr 1.262e-06 [train_loss] tar_ll 1.9318 loss -1.9318 (24.304 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48600 lr 9.666e-07 [train_loss] tar_ll 1.8724 loss -1.8724 (25.210 secs)\n",
      "lbanp:lbanp-num_latents-8 step 48800 lr 7.103e-07 [train_loss] tar_ll 1.9239 loss -1.9239 (24.365 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49000 lr 4.933e-07 [train_loss] tar_ll 1.9764 loss -1.9764 (24.855 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49200 lr 3.158e-07 [train_loss] tar_ll 1.8529 loss -1.8529 (25.340 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49400 lr 1.776e-07 [train_loss] tar_ll 1.8430 loss -1.8430 (24.515 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49600 lr 7.895e-08 [train_loss] tar_ll 1.9357 loss -1.9357 (25.463 secs)\n",
      "lbanp:lbanp-num_latents-8 step 49800 lr 1.974e-08 [train_loss] tar_ll 1.9389 loss -1.9389 (25.165 secs)\n",
      "lbanp:lbanp-num_latents-8 step 50000 lr 0.000e+00 [train_loss] tar_ll 1.9354 loss -1.9354 (25.355 secs)\n",
      "100%|##########| 3000/3000 [01:57<00:00, 25.53it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.2664 loss -1.2664 (117.532 secs)\n",
      "\n",
      "100%|##########| 3000/3000 [01:57<00:00, 25.43it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll 1.2664 loss -1.2664 (117.970 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 6251.444495677948 seconds\n",
      "Memory usage: 752.765625 MB\n",
      "Memory usage: 3.83984375 MB\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "for l_vals in [8, 64, 128]:\n",
    "    \n",
    "    #start track execution time\n",
    "    start_time = time.time()\n",
    "    #starttrack memory\n",
    "    process1 = psutil.Process()\n",
    "    memory_usage1 = process1.memory_info().rss / 1024 / 1024  # in MB\n",
    "    print(\"Memory usage:\", memory_usage1, \"MB\")\n",
    "\n",
    "\n",
    "    #run model:\n",
    "    %run gp.py --mode train --expid lbanp-num_latents-8 --model lbanp --num_latents {l_vals} --max_num_points 150 --num_steps 50000\n",
    "\n",
    "    #track time\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")\n",
    "    #track memory\n",
    "    process2 = psutil.Process()\n",
    "    memory_usage2 = process2.memory_info().rss / 1024 / 1024  # in MB\n",
    "    print(\"Memory usage:\", memory_usage2, \"MB\")\n",
    "    print(\"Memory usage:\", memory_usage2-memory_usage1, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c377c4a-c6ef-4667-aeb0-6e6d57a39abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb910d45-1322-45dc-8bf3-7589fa0e509a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mod_track={\n",
    "    'l': [8, 64, 128],\n",
    "    'memory': [744.46484375,748.92578125, 752.765625],\n",
    "    'time':[4457.026439905167,4547.074417114258,6251.444495677948],\n",
    "    'tar_ll': [1.1506,1.2820,1.2664], \n",
    "    'loss': [-1.1506,-1.2820,-1.2664],\n",
    "    'parameters':[784834,788418,792514]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70e1d87f-1156-4d6c-b98a-129acf5c257e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latentes = [8,64,128]#, 'LBANP8']\n",
    "colors = ['blue', 'orange', 'green']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for model, color in zip(latentes, colors):\n",
    "    plt.plot(mod_track['l'], mod_track['tar_ll'], marker='o', color=color, label=model)\n",
    "\n",
    "plt.title('tar_ll vs. l')\n",
    "plt.xlabel('l')\n",
    "plt.ylabel('tar_ll')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ae0fd48-e30b-4eb6-a328-9f950334500e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latentes = [8,64,128]#, 'LBANP8']\n",
    "colors = ['blue', 'orange', 'green']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for model, color in zip(latentes, colors):\n",
    "    plt.plot(mod_track['l'], mod_track['memory'], marker='o', color=color, label=model)\n",
    "\n",
    "plt.title('memory vs. l')\n",
    "plt.xlabel('l')\n",
    "plt.ylabel('Memory (MB)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48018b7b-a79b-4976-9547-478d2eff16a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: lbanp-lbanp-num_latents-8\n",
      "Experiment: lbanp-lbanp-num_latents-8\n",
      "Total number of parameters: 784834\n",
      "\n",
      "Total number of parameters: 784834\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 711.80078125 MB\n",
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "rbf-seed0.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8 step 200 lr 4.523e-04 [train_loss] tar_ll -0.7144 loss 0.7144 (24.254 secs)\n",
      "lbanp:lbanp-num_latents-8 step 200 lr 4.523e-04 [train_loss] tar_ll -0.7144 loss 0.7144 (24.254 secs)\n",
      "lbanp:lbanp-num_latents-8 step 400 lr 3.273e-04 [train_loss] tar_ll -0.6606 loss 0.6606 (22.494 secs)\n",
      "lbanp:lbanp-num_latents-8 step 400 lr 3.273e-04 [train_loss] tar_ll -0.6606 loss 0.6606 (22.494 secs)\n",
      "lbanp:lbanp-num_latents-8 step 600 lr 1.727e-04 [train_loss] tar_ll -0.5683 loss 0.5683 (22.777 secs)\n",
      "lbanp:lbanp-num_latents-8 step 600 lr 1.727e-04 [train_loss] tar_ll -0.5683 loss 0.5683 (22.777 secs)\n",
      "lbanp:lbanp-num_latents-8 step 800 lr 4.775e-05 [train_loss] tar_ll -0.5182 loss 0.5182 (22.724 secs)\n",
      "lbanp:lbanp-num_latents-8 step 800 lr 4.775e-05 [train_loss] tar_ll -0.5182 loss 0.5182 (22.724 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1000 lr 0.000e+00 [train_loss] tar_ll -0.4858 loss 0.4858 (24.603 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1000 lr 0.000e+00 [train_loss] tar_ll -0.4858 loss 0.4858 (24.603 secs)\n",
      "100%|##########| 3000/3000 [01:57<00:00, 25.59it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll -0.4818 loss 0.4818 (117.270 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 237.14434790611267 seconds\n",
      "Memory usage: 744.52734375 MB\n",
      "Memory usage: 32.7265625 MB\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "for l_vals in [8]:\n",
    "    \n",
    "    #start track execution time\n",
    "    start_time = time.time()\n",
    "    #starttrack memory\n",
    "    process1 = psutil.Process()\n",
    "    memory_usage1 = process1.memory_info().rss / 1024 / 1024  # in MB\n",
    "    print(\"Memory usage:\", memory_usage1, \"MB\")\n",
    "\n",
    "\n",
    "    #run model:\n",
    "    %run gp.py --mode train --expid lbanp-num_latents-8 --model lbanp --num_latents {l_vals} --num_steps 1000\n",
    "\n",
    "    #track time\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")\n",
    "    #track memory\n",
    "    process2 = psutil.Process()\n",
    "    memory_usage2 = process2.memory_info().rss / 1024 / 1024  # in MB\n",
    "    print(\"Memory usage:\", memory_usage2, \"MB\")\n",
    "    print(\"Memory usage:\", memory_usage2-memory_usage1, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e27a6391-2664-4178-bc0e-afa1658e115c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 1008.52734375 MB\n",
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment: lbanp-lbanp-num_latents-8\n",
      "Total number of parameters: 784834\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/user/fz287/hpc-work/MLMI4/evalsets\\gp\n",
      "rbf-seed0.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbanp:lbanp-num_latents-8 step 200 lr 4.523e-04 [train_loss] tar_ll -0.7552 loss 0.7552 (23.039 secs)\n",
      "lbanp:lbanp-num_latents-8 step 400 lr 3.273e-04 [train_loss] tar_ll -0.6494 loss 0.6494 (21.714 secs)\n",
      "lbanp:lbanp-num_latents-8 step 600 lr 1.727e-04 [train_loss] tar_ll -0.5708 loss 0.5708 (21.830 secs)\n",
      "lbanp:lbanp-num_latents-8 step 800 lr 4.775e-05 [train_loss] tar_ll -0.5670 loss 0.5670 (22.549 secs)\n",
      "lbanp:lbanp-num_latents-8 step 1000 lr 0.000e+00 [train_loss] tar_ll -0.5620 loss 0.5620 (26.165 secs)\n",
      "100%|##########| 3000/3000 [01:57<00:00, 25.44it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll -0.5588 loss 0.5588 (117.912 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 236.12142848968506 seconds\n",
      "Memory usage: 815.8515625 MB\n",
      "Memory usage: -192.67578125 MB\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "for l_vals in [8]:\n",
    "    \n",
    "    #start track execution time\n",
    "    start_time = time.time()\n",
    "    #starttrack memory\n",
    "    process1 = psutil.Process()\n",
    "    memory_usage1 = process1.memory_info().rss / 1024 / 1024  # in MB\n",
    "    print(\"Memory usage:\", memory_usage1, \"MB\")\n",
    "\n",
    "\n",
    "    #run model:\n",
    "    %run gp.py --mode train --expid lbanp-num_latents-8 --model lbanp --num_latents {l_vals} --num_steps 1000\n",
    "\n",
    "    #track time\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")\n",
    "    #track memory\n",
    "    process2 = psutil.Process()\n",
    "    memory_usage2 = process2.memory_info().rss / 1024 / 1024  # in MB\n",
    "    print(\"Memory usage:\", memory_usage2, \"MB\")\n",
    "    print(\"Memory usage:\", memory_usage2-memory_usage1, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351329fb-c9e8-4494-9336-49b11c1d1e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3559bba-3be2-478a-b82c-edc4c5574556",
   "metadata": {},
   "source": [
    "# INFERENCE TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74f3c7cd-134a-488b-ba29-cb9f2fa117bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anp\n",
      "banp\n",
      "bnp\n",
      "canp\n",
      "cnp\n",
      "isanp\n",
      "lbanp\n",
      "np\n",
      "tnpa\n",
      "tnpd\n"
     ]
    }
   ],
   "source": [
    "  # Define the directory path\n",
    "import os\n",
    "directory = \"/rds/user/fz287/hpc-work/MLMI4/results/gp/\"\n",
    "# List files in the directory\n",
    "files = os.listdir(directory)\n",
    "# Print the list of files\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30604183-0292-4196-bad6-6b248def0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/rds/user/fz287/hpc-work/MLMI4/results/gp/\"\n",
    "# List files in the directory\n",
    "files = os.listdir(directory)\n",
    "# Print the list of files\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53ce3a6e-c1b5-41f5-b3d3-28640e27787b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def load_saved_model(model_name, lbanp_latents=None):\n",
    "  # Define the directory path\n",
    "    if lbanp_latents==None:\n",
    "        directory = \"/rds/user/fz287/hpc-work/MLMI4/results/gp/\"+model_name+\"/\"+model_name\n",
    "    else:\n",
    "        directory = \"/rds/user/fz287/hpc-work/MLMI4/results/gp/\"+model_name+\"/\"+model_name+\"-num_latents-\"+str(lbanp_latents)\n",
    "    \n",
    "    args={\n",
    "    'model':model_name,\n",
    "    # LBANP Arguments\n",
    "    'num_latents':lbanp_latents,\n",
    "    'num_latents_per_layer':8,\n",
    "    'd_model':64,\n",
    "    'emb_depth':4,\n",
    "    'dim_feedforward':128,\n",
    "    'nhead':4,\n",
    "    'dropout':0.0,\n",
    "    'num_layers':6,\n",
    "    # OOD settings\n",
    "    'eval_kernel':'rbf',\n",
    "    't_noise':None,\n",
    "    }\n",
    "    \n",
    "    model_cls = getattr(load_module(f'models/'+args['model']+'.py'), args['model'].upper())\n",
    "\n",
    "    with open(f'configs/gp/'+args['model']+'.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    for key, val in args.items(): # Override the default arguments\n",
    "        if key in config:\n",
    "            config[key] = val\n",
    "            print(f\"Overriding argument {key}: {config[key]}\")\n",
    "    \n",
    "    model = model_cls(**config)\n",
    "    model.cuda()\n",
    "    \n",
    "\n",
    "    # Define the path to the checkpoint file\n",
    "    checkpoint_path = directory+'/ckpt.tar'\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    # Access the model state from the loaded checkpoint\n",
    "    model_state = checkpoint['model']\n",
    "    # Assuming you have already created your model object, load the model state\n",
    "    model.load_state_dict(model_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd54c144-1d50-4119-9049-4f1c0a6d0aac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim_x': 1, 'dim_y': 1, 'dim_hid': 128, 'dim_lat': 128, 'enc_pre_depth': 4, 'enc_post_depth': 2, 'dec_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "modelito=load_saved_model(model_name='np', lbanp_latents=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69b1a0-484c-49d3-b265-fcb33ad4627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--num_ctx 10 --num_tar 10\n",
    "train_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3f48b7-e857-4fe2-a0d0-12897501e02d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "True\n",
      "generating evaluation sets...\n",
      "Generating Evaluation Sets with rbf kernel\n",
      "Generating Evaluation sets with context and target points:   None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 456.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [01:11<00:00, 41.74it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll -0.5588 loss 0.5588 (71.873 secs)\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll -0.5588 loss 0.5588 (71.873 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 71873.43978881836 miliseconds\n",
      "EVal has finished\n"
     ]
    }
   ],
   "source": [
    "%run gp.py --mode eval --expid lbanp-num_latents-8 --model lbanp --num_latents 8 --custom_eval True --num_ctx 10 --num_tar 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb52338-228e-40d8-b1aa-d4cffafc10ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46822188-1f20-42ff-847f-2623f4e3b867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f465078-149f-4b05-b66a-053989dba158",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "True\n",
      "generating evaluation sets...\n",
      "Generating Evaluation Sets with rbf kernel\n",
      "Generating Evaluation sets with context and target points:   10 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 3000/3000 [00:06<00:00, 487.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "100%|##########| 3000/3000 [00:17<00:00, 173.62it/s]\n",
      "tnpd:lbanp-num_latents-8 rbf tar_ll 0.6120 loss -0.6120 (17.296 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 17278.765625 miliseconds\n",
      "Execution time: 5.759588541666667 miliseconds per batch\n",
      "Initial Memory Usage: 21.24951171875 MB\n",
      "Final Memory Usage: 21.8818359375 MB\n",
      "Memory Usage Change: 0.63232421875 MB\n",
      "Memory Usage Change: 0.00021077473958333332 MB\n",
      "Eval has finished\n"
     ]
    }
   ],
   "source": [
    "%run gp_track.py --mode eval --expid lbanp-num_latents-8 --model tnpd --custom_eval True --num_tar 10 --num_ctx 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dcc1ea1-df9e-4d7b-94e1-e9e7c3ae437d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "eval() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\OneDrive - University of Cambridge\\Courses Material\\MLMI 4 Advanced Machine Learning\\Project\\code\\latent-bottlenecked-anp-main\\latent-bottlenecked-anp-main\\regression\\gp_track.py:383\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m line\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 383\u001b[0m     main()\n",
      "File \u001b[1;32m~\\OneDrive - University of Cambridge\\Courses Material\\MLMI 4 Advanced Machine Learning\\Project\\code\\latent-bottlenecked-anp-main\\latent-bottlenecked-anp-main\\regression\\gp_track.py:117\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28meval\u001b[39m(args, model)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprof\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28meval\u001b[39m(args, model, args\u001b[38;5;241m.\u001b[39mnum_ctx)\n",
      "\u001b[1;31mTypeError\u001b[0m: eval() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "%run gp_track.py --mode prof --expid lbanp-num_latents-8 --model tnpd --custom_eval True --num_tar 10 --num_ctx 10 --eval_num_batches 1 --eval_batch_size 10 --eval_num_samples 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10681fa5-e7d7-4a77-9a78-85a329feb793",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting memory_profiler\n",
      "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\jmlr9\\.conda\\envs\\deepl_torch\\lib\\site-packages (from memory_profiler) (5.9.0)\n",
      "Installing collected packages: memory_profiler\n",
      "Successfully installed memory_profiler-0.61.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ad2536-b79f-4102-9fb7-5b386654f9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "Starting evaluation with 100 context points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 71.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation with 100 context points has finished\n",
      "Filename: C:\\Users\\jmlr9\\OneDrive - University of Cambridge\\Courses Material\\MLMI 4 Advanced Machine Learning\\Project\\code\\latent-bottlenecked-anp-main\\latent-bottlenecked-anp-main\\regression\\gp_track.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   122    666.5 MiB    666.5 MiB           1   @profile\n",
      "   123                                         def profile_memory(args, model, num_ctx):\n",
      "   124    666.5 MiB      0.0 MiB           1       path, filename = get_eval_path(args)\n",
      "   125    666.5 MiB      0.0 MiB           1       if not osp.isfile(osp.join(path, filename)):\n",
      "   126                                                 print('generating evaluation sets...')\n",
      "   127                                                 gen_evalset(args)\n",
      "   128                                         \n",
      "   129    666.5 MiB      0.0 MiB           1       print(f\"Starting evaluation with {num_ctx} context points\")\n",
      "   130    666.5 MiB      0.0 MiB           1       eval_batches = torch.load(osp.join(path, filename))\n",
      "   131                                         \n",
      "   132    666.5 MiB      0.0 MiB           1       torch.manual_seed(args.eval_seed)\n",
      "   133    666.5 MiB      0.0 MiB           1       torch.cuda.manual_seed(args.eval_seed)\n",
      "   134                                         \n",
      "   135    666.5 MiB      0.0 MiB           1       ravg = RunningAverage()\n",
      "   136    666.5 MiB      0.0 MiB           1       model.eval()\n",
      "   137                                         \n",
      "   138    666.5 MiB      0.0 MiB           2       with torch.no_grad():\n",
      "   139    666.5 MiB      0.0 MiB           2           for batch in tqdm(eval_batches, ascii=True):\n",
      "   140    666.5 MiB      0.0 MiB           7               for key, val in batch.items():\n",
      "   141    666.5 MiB      0.0 MiB           6                   batch[key] = val.cuda()\n",
      "   142                                         \n",
      "   143    666.5 MiB      0.0 MiB           1               if args.model in [\"np\", \"anp\", \"bnp\", \"banp\"]:\n",
      "   144                                                         outs = model(batch, args.eval_num_samples)\n",
      "   145                                                     else:\n",
      "   146    666.5 MiB      0.0 MiB           1                   outs = model(batch)\n",
      "   147                                         \n",
      "   148    666.5 MiB      0.0 MiB           3               for key, val in outs.items():\n",
      "   149    666.5 MiB      0.0 MiB           2                   ravg.update(key, val)\n",
      "   150                                         \n",
      "   151    666.5 MiB      0.0 MiB           1       torch.manual_seed(time.time())\n",
      "   152    666.5 MiB      0.0 MiB           1       torch.cuda.manual_seed(time.time())\n",
      "   153                                         \n",
      "   154    666.5 MiB      0.0 MiB           1       print(\"Evaluation with\", num_ctx, \"context points has finished\")\n",
      "   155    666.5 MiB      0.0 MiB           1       line = f'{args.model}:{args.expid} {args.eval_kernel} '\n",
      "   156    666.5 MiB      0.0 MiB           1       if args.t_noise is not None:\n",
      "   157                                                 line += f'tn {args.t_noise} '\n",
      "   158    666.5 MiB      0.0 MiB           1       line += ravg.info()\n",
      "   159                                         \n",
      "   160    666.5 MiB      0.0 MiB           1       return line\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run gp_track.py --mode prof --expid lbanp-num_latents-8 --model tnpd --custom_eval True --num_tar 10 --num_ctx 100 --eval_num_batches 1 --eval_batch_size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5110bf7-41b0-44be-88fd-8277b5617233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 128\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "Starting evaluation with 100 context points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 26.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation with 100 context points has finished\n",
      "Filename: C:\\Users\\jmlr9\\OneDrive - University of Cambridge\\Courses Material\\MLMI 4 Advanced Machine Learning\\Project\\code\\latent-bottlenecked-anp-main\\latent-bottlenecked-anp-main\\regression\\gp_track.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   122    683.8 MiB    683.8 MiB           1   @profile\n",
      "   123                                         def profile_memory(args, model, num_ctx):\n",
      "   124    683.8 MiB      0.0 MiB           1       path, filename = get_eval_path(args)\n",
      "   125    683.8 MiB      0.0 MiB           1       if not osp.isfile(osp.join(path, filename)):\n",
      "   126                                                 print('generating evaluation sets...')\n",
      "   127                                                 gen_evalset(args)\n",
      "   128                                         \n",
      "   129    683.8 MiB      0.0 MiB           1       print(f\"Starting evaluation with {num_ctx} context points\")\n",
      "   130    683.8 MiB      0.0 MiB           1       eval_batches = torch.load(osp.join(path, filename))\n",
      "   131                                         \n",
      "   132    683.8 MiB      0.0 MiB           1       torch.manual_seed(args.eval_seed)\n",
      "   133    683.8 MiB      0.0 MiB           1       torch.cuda.manual_seed(args.eval_seed)\n",
      "   134                                         \n",
      "   135    683.8 MiB      0.0 MiB           1       ravg = RunningAverage()\n",
      "   136    683.8 MiB      0.0 MiB           1       model.eval()\n",
      "   137                                         \n",
      "   138    683.8 MiB      0.0 MiB           2       with torch.no_grad():\n",
      "   139    683.8 MiB      0.0 MiB           2           for batch in tqdm(eval_batches, ascii=True):\n",
      "   140    683.8 MiB      0.0 MiB           7               for key, val in batch.items():\n",
      "   141    683.8 MiB      0.0 MiB           6                   batch[key] = val.cuda()\n",
      "   142                                         \n",
      "   143    683.8 MiB      0.0 MiB           1               if args.model in [\"np\", \"anp\", \"bnp\", \"banp\"]:\n",
      "   144                                                         outs = model(batch, args.eval_num_samples)\n",
      "   145                                                     else:\n",
      "   146    683.8 MiB      0.0 MiB           1                   outs = model(batch)\n",
      "   147                                         \n",
      "   148    683.8 MiB      0.0 MiB           3               for key, val in outs.items():\n",
      "   149    683.8 MiB      0.0 MiB           2                   ravg.update(key, val)\n",
      "   150                                         \n",
      "   151    683.8 MiB      0.0 MiB           1       torch.manual_seed(time.time())\n",
      "   152    683.8 MiB      0.0 MiB           1       torch.cuda.manual_seed(time.time())\n",
      "   153                                         \n",
      "   154    683.8 MiB      0.0 MiB           1       print(\"Evaluation with\", num_ctx, \"context points has finished\")\n",
      "   155    683.8 MiB      0.0 MiB           1       line = f'{args.model}:{args.expid} {args.eval_kernel} '\n",
      "   156    683.8 MiB      0.0 MiB           1       if args.t_noise is not None:\n",
      "   157                                                 line += f'tn {args.t_noise} '\n",
      "   158    683.8 MiB      0.0 MiB           1       line += ravg.info()\n",
      "   159                                         \n",
      "   160    683.8 MiB      0.0 MiB           1       return line\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run gp_track.py --mode prof --expid lbanp-num_latents-128_matern --model lbanp --custom_eval True --num_tar 100 --num_ctx 100 --num_latents 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "770f9fec-59eb-4f41-bf1f-14f098cecba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "Starting evaluation with 10 context points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n",
      "100%|##########| 3000/3000 [00:27<00:00, 110.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation with 10 context points has finished\n",
      "Filename: C:\\Users\\jmlr9\\OneDrive - University of Cambridge\\Courses Material\\MLMI 4 Advanced Machine Learning\\Project\\code\\latent-bottlenecked-anp-main\\latent-bottlenecked-anp-main\\regression\\gp_track.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   122    763.2 MiB    763.2 MiB           1   @profile\n",
      "   123                                         def profile_memory(args, model, num_ctx):\n",
      "   124    763.2 MiB      0.0 MiB           1       path, filename = get_eval_path(args)\n",
      "   125    763.2 MiB      0.0 MiB           1       if not osp.isfile(osp.join(path, filename)):\n",
      "   126                                                 print('generating evaluation sets...')\n",
      "   127                                                 gen_evalset(args)\n",
      "   128                                         \n",
      "   129    763.2 MiB      0.0 MiB           1       print(f\"Starting evaluation with {num_ctx} context points\")\n",
      "   130    783.3 MiB     20.1 MiB           1       eval_batches = torch.load(osp.join(path, filename))\n",
      "   131                                         \n",
      "   132    783.3 MiB      0.0 MiB           1       torch.manual_seed(args.eval_seed)\n",
      "   133    783.3 MiB      0.0 MiB           1       torch.cuda.manual_seed(args.eval_seed)\n",
      "   134                                         \n",
      "   135    783.3 MiB      0.0 MiB           1       ravg = RunningAverage()\n",
      "   136    783.3 MiB      0.0 MiB           1       model.eval()\n",
      "   137                                         \n",
      "   138    783.8 MiB      0.0 MiB           2       with torch.no_grad():\n",
      "   139    783.8 MiB      0.0 MiB        3001           for batch in tqdm(eval_batches, ascii=True):\n",
      "   140    783.8 MiB      0.0 MiB       21000               for key, val in batch.items():\n",
      "   141    783.8 MiB      0.0 MiB       18000                   batch[key] = val.cuda()\n",
      "   142                                         \n",
      "   143    783.8 MiB      0.0 MiB        3000               if args.model in [\"np\", \"anp\", \"bnp\", \"banp\"]:\n",
      "   144                                                         outs = model(batch, args.eval_num_samples)\n",
      "   145                                                     else:\n",
      "   146    783.8 MiB      0.4 MiB        3000                   outs = model(batch)\n",
      "   147                                         \n",
      "   148    783.8 MiB      0.0 MiB        9000               for key, val in outs.items():\n",
      "   149    783.8 MiB      0.0 MiB        6000                   ravg.update(key, val)\n",
      "   150                                         \n",
      "   151    783.8 MiB      0.0 MiB           1       torch.manual_seed(time.time())\n",
      "   152    783.8 MiB      0.0 MiB           1       torch.cuda.manual_seed(time.time())\n",
      "   153                                         \n",
      "   154    783.8 MiB      0.0 MiB           1       print(\"Evaluation with\", num_ctx, \"context points has finished\")\n",
      "   155    783.8 MiB      0.0 MiB           1       line = f'{args.model}:{args.expid} {args.eval_kernel} '\n",
      "   156    783.8 MiB      0.0 MiB           1       if args.t_noise is not None:\n",
      "   157                                                 line += f'tn {args.t_noise} '\n",
      "   158    783.8 MiB      0.0 MiB           1       line += ravg.info()\n",
      "   159                                         \n",
      "   160    783.8 MiB      0.0 MiB           1       return line\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run gp_track.py --mode prof --expid lbanp-num_latents-8 --model tnpd --custom_eval True --num_tar 10 --num_ctx 10 --eval_num_batches 1 --eval_batch_size 10 --eval_num_samples 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e6bde-4bb3-4ea9-9a7a-d886b3f8cbac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "935b97b1-d4e5-498e-8893-9fa276555ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "True\n",
      "generating evaluation sets...\n",
      "Generating Evaluation Sets with rbf kernel\n",
      "Generating Evaluation sets with context and target points:   10 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 368.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 37.96it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll -0.4332 loss 0.4332 (0.031 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.21196800470352173 miliseconds\n",
      "Initial Memory Usage: 20.41259765625 MB\n",
      "Final Memory Usage: 20.56884765625 MB\n",
      "Memory Usage Change: 0.15625 MB\n",
      "Eval has finished\n"
     ]
    }
   ],
   "source": [
    "%run gp_track.py --mode eval --expid lbanp-num_latents-8 --model lbanp --custom_eval True --num_tar 10 --num_ctx 10 --eval_num_batches 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0d5d4e7-7eda-4c1e-b8d3-7f5e105c08ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 8\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "True\n",
      "generating evaluation sets...\n",
      "Generating Evaluation Sets with rbf kernel\n",
      "Generating Evaluation sets with context and target points:   10 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 500.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 13.89it/s]\n",
      "lbanp:lbanp-num_latents-8 rbf tar_ll -0.4332 loss 0.4332 (0.073 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used for inference: 0.00244140625 MB\n",
      "Eval has finished\n"
     ]
    }
   ],
   "source": [
    "%run gp_track.py --mode eval --expid lbanp-num_latents-8 --model lbanp --num_latents 8 --custom_eval True --num_tar 10 --num_ctx 10 --eval_num_batches 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad08bca-6f0b-48da-bc85-c86c70037d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 64\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "True\n",
      "generating evaluation sets...\n",
      "Generating Evaluation Sets with rbf kernel\n",
      "Generating Evaluation sets with context and target points:   10 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 334.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 14.04it/s]\n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 0.8357 loss -0.8357 (0.074 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used for inference: 0.00244140625 MB\n",
      "Eval has finished\n"
     ]
    }
   ],
   "source": [
    "%run gp_track.py --mode eval --expid lbanp-num_latents-64_rbf --model lbanp --num_latents 64 --custom_eval True --num_tar 10 --num_ctx 10 --eval_num_batches 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a92e6d4-9929-4b24-b4ea-816e1c3b4e25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument num_latents: 64\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "True\n",
      "generating evaluation sets...\n",
      "Generating Evaluation Sets with rbf kernel\n",
      "Generating Evaluation sets with context and target points:   1000 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 10.32it/s]\n",
      "lbanp:lbanp-num_latents-64_rbf rbf tar_ll 2.3701 loss -2.3701 (0.099 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used for inference: 0.00244140625 MB\n",
      "Memory used for inference: 0.0009765625 MB\n",
      "Eval has finished\n"
     ]
    }
   ],
   "source": [
    "%run gp_track.py --mode eval --expid lbanp-num_latents-64_rbf --model lbanp --num_latents 64 --custom_eval True --num_tar 10 --num_ctx 1000 --eval_num_batches 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa3f9c30-69a1-4af0-be61-00bee5080b32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding argument pretrain: False\n",
      "Overriding argument d_model: 64\n",
      "Overriding argument emb_depth: 4\n",
      "Overriding argument dim_feedforward: 128\n",
      "Overriding argument nhead: 4\n",
      "Overriding argument dropout: 0.0\n",
      "Overriding argument num_layers: 6\n",
      "True\n",
      "generating evaluation sets...\n",
      "Generating Evaluation Sets with rbf kernel\n",
      "Generating Evaluation sets with context and target points:   10 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:00<00:00, 523.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]C:\\Users\\jmlr9\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n",
      "100%|##########| 1/1 [00:00<00:00, 19.60it/s]\n",
      "tnpa:tnpa_matern rbf tar_ll 1.1956 loss -1.1956 (0.052 secs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used for inference: 0.0009765625 MB\n",
      "Eval has finished\n"
     ]
    }
   ],
   "source": [
    "%run gp_track.py --mode eval --expid tnpa_matern --model tnpa --custom_eval True --num_tar 10 --num_ctx 10 --eval_num_batches 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38740b24-6551-402a-99df-0e08ac876a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
